LIBRARY,FREQUENCY,description,keywords,embedding_vectors,labels_6,labels_7
torch,554,pytorch is a python package that provides two high level feature you can reuse your favorite python package such a numpy scipy and cython to extend pytorch when needed our trunk health continuous integration signal can be found at hud pytorch org at a granular level pytorch is a library that consists of the following component usually pytorch is used either a elaborating further if you use numpy then you have used tensor a k a ndarray pytorch provides tensor that can live either on the cpu or the gpu and accelerates the computation by a huge amount we provide a wide variety of tensor routine to accelerate and fit your scientific computation need such a slicing indexing mathematical operation linear algebra reduction and they are fast pytorch ha a unique way of building neural network using and replaying a tape recorder most framework such a tensorflow theano caffe and cntk have a static view of the world one ha to build a neural network and reuse the same structure again and again changing the way the network behaves mean that one ha to start from scratch with pytorch we use a technique called reverse mode auto differentiation which allows you to change the way your network behaves arbitrarily with zero lag or overhead our inspiration come from several research paper on this topic a well a current and past work such a torch autograd autograd chainer etc while this technique is not unique to pytorch it s one of the fastest implementation of it to date you get the best of speed and flexibility for your crazy research pytorch is not a python binding into a monolithic c framework it is built to be deeply integrated into python you can use it naturally like you would use numpy scipy scikit learn etc you can write your new neural network layer in python itself using your favorite library and use package such a cython and numba our goal is to not reinvent the wheel where appropriate pytorch is designed to be intuitive linear in thought and easy to use when you execute a line of code it get executed there isn t an asynchronous view of the world when you drop into a debugger or receive error message and stack trace understanding them is straightforward the stack trace point to exactly where your code wa defined we hope you never spend hour debugging your code because of bad stack trace or asynchronous and opaque execution engine pytorch ha minimal framework overhead we integrate acceleration library such a intel mkl and nvidia cudnn nccl to maximize speed at the core it cpu and gpu tensor and neural network backends are mature and have been tested for year hence pytorch is quite fast whether you run small or large neural network the memory usage in pytorch is extremely efficient compared to torch or some of the alternative we ve written custom memory allocator for the gpu to make sure that your deep learning model are maximally memory efficient this enables you to train bigger deep learning model than before writing new neural network module or interfacing with pytorch s tensor api wa designed to be straightforward and with minimal abstraction you can write new neural network layer in python using the torch api or your favorite numpy based library such a scipy if you want to write your layer in c c we provide a convenient extension api that is efficient and with minimal boilerplate no wrapper code need to be written you can see a tutorial here and an example here command to install binary via conda or pip wheel are on our website http pytorch org get started locally python wheel for nvidia s jetson nano jetson tx tx jetson xavier nx agx and jetson agx orin are provided here and the l t container is published herethey require jetpack and above and dusty nv and ptrblck are maintaining them if you are installing from source you will need we highly recommend installing an anaconda environment you will get a high quality blas library mkl and you get controlled dependency version regardless of your linux distro if you want to compile with cuda support install the following note that cuda is not supported on macos note you could refer to the cudnn support matrix for cudnn version with the various supported cuda cuda driver and nvidia hardwareif you want to disable cuda support export the environment variable use cuda other potentially useful environment variable may be found in setup py if you are building for nvidia s jetson platform jetson nano tx tx agx xavier instruction to install pytorch for jetson nano are available hereif you want to compile with rocm support installif you want to disable rocm support export the environment variable use rocm other potentially useful environment variable may be found in setup py commonon linuxon macoson windowson linuxif you re compiling for amd rocm then first run this command install pytorchnote that if you are using anaconda you may experience an error caused by the linker this is caused by ld from the conda environment shadowing the system ld you should use a newer version of python that fix this issue the recommended python version is and on macoson windowschoose correct visual studio version sometimes there are regression in new version of visual studio so it s best to use the same visual studio version a pytorch ci s pytorch ci us visual c buildtools which come with visual studio enterprise professional or community edition you can also install the build tool from http visualstudio microsoft com visual cpp build tool the build tool do not come with visual studio code by default if you want to build legacy python code please refer to building on legacy code and cudacpu only buildsin this mode pytorch computation will run on your cpu not your gpunote on openmp the desired openmp implementation is intel openmp iomp in order to link against iomp you ll need to manually download the library and set up the building environment by tweaking cmake include path and lib the instruction here is an example for setting up both mkl and intel openmp without these configuration for cmake microsoft visual c openmp runtime vcomp will be used cuda based buildin this mode pytorch computation will leverage your gpu via cuda for faster number crunchingnvtx is needed to build pytorch with cuda nvtx is a part of cuda distributive where it is called nsight compute to install it onto an already installed cuda run cuda installation once again and check the corresponding checkbox make sure that cuda with nsight compute is installed after visual studio currently v and ninja are supported a the generator of cmake if ninja exe is detected in path then ninja will be used a the default generator otherwise it will use v if ninja is selected a the generator the latest msvc will get selected a the underlying toolchain additional library such a magma onednn a k a mkldnn or dnnl and sccache are often needed please refer to the installation helper to install them you can refer to the build pytorch bat script for some other environment variable configurationsyou can adjust the configuration of cmake variable optionally without building first by doing the following for example adjusting the pre detected directory for cudnn or blas can be done with such a step on linuxon macosyou can also pull a pre built docker image from docker hub and run with docker v please note that pytorch us shared memory to share data between process so if torch multiprocessing is used e g for multithreaded data loader the default shared memory segment size that container run with is not enough and you should increase shared memory size either with ipc host or shm size command line option to nvidia docker run note must be built with a docker version the dockerfile is supplied to build image with cuda support and cudnn v you can pas python version x y make variable to specify which python version is to be used by miniconda or leave it unset to use the default to build documentation in various format you will need sphinx and the readthedocs theme you can then build the documentation by running make format from the doc folder run make to get a list of all available output format if you get a katex error run npm install katex if it persists try npm install g katexnote if you installed nodejs with a different package manager e g conda then npm will probably install a version of katex that is not compatible with your version of nodejs and doc build will fail a combination of version that is known to work is node and katex to install the latter with npm you can run npm install g katex installation instruction and binary for previous pytorch version may be found on our website three pointer to get you started pytorch ha a day release cycle major release please let u know if you encounter a bug by filing an issue we appreciate all contribution if you are planning to contribute back bug fix please do so without any further discussion if you plan to contribute new feature utility function or extension to the core please first open an issue and discus the feature with u sending a pr without discussion might end up resulting in a rejected pr because we might be taking the core in a different direction than you might be aware of to learn more about making a contribution to pytorch please see our contribution page pytorch is a community driven project with several skillful engineer and researcher contributing to it pytorch is currently maintained by adam paszke sam gross soumith chintala and gregory chanan with major contribution coming from hundred of talented individual in various form and mean a non exhaustive but growing list need to mention trevor killeen sasank chilamkurthy sergey zagoruyko adam lerer francisco massa alykhan tejani luca antiga alban desmaison andreas koepf james bradbury zeming lin yuandong tian guillaume lample marat dukhan natalia gimelshein christian sarofeen martin raison edward yang zachary devito note this project is unrelated to hughperkins pytorch with the same name hugh is a valuable contributor to the torch community and ha helped with many thing torch and pytorch pytorch ha a bsd style license a found in the license file,"[('fast pytorch', 0.6115), ('ndarray pytorch', 0.576), ('appropriate pytorch', 0.5603), ('pytorch pytorch', 0.5563), ('mode pytorch computation', 0.5392), ('pytorch', 0.5339), ('crazy research pytorch', 0.5215), ('website http pytorch org', 0.4889), ('pytorch ci', 0.4738), ('tensor routine', 0.4681)]","[-7.29857087e-02 -3.96736078e-02 -1.40135419e-02  6.06785808e-03
 -1.56469997e-02 -1.77541170e-02 -1.16662346e-02 -3.77931409e-02
 -1.14481121e-01 -8.01590756e-02 -6.17945343e-02  3.41051705e-02
 -1.03490055e-01  6.33046031e-02 -9.17955860e-03  5.17810136e-02
 -7.14730993e-02  1.51139349e-01 -2.87064631e-02 -9.45715234e-02
 -5.30234016e-02 -4.46500815e-02  2.78462116e-02 -2.88440920e-02
  5.10723405e-02  1.43312374e-02  2.98691876e-02 -7.59944245e-02
  4.64224927e-02  1.94248580e-03  3.65766836e-03  3.17926817e-02
  3.99272367e-02  9.61098541e-03 -1.27062753e-01 -5.69446571e-02
 -3.68926041e-02  1.69559997e-02  3.53444442e-02  2.28176601e-02
 -2.87601184e-02 -2.99849641e-02 -4.91616465e-02  3.62611711e-02
  3.26259844e-02 -3.59588861e-02  5.13402596e-02 -5.12940213e-02
  4.07471918e-02 -5.44703789e-02 -6.14201650e-02 -2.37258859e-02
 -8.02380666e-02  1.75722614e-02  1.59515273e-02 -2.18222439e-02
  4.12272066e-02 -2.04762053e-02  5.59580550e-02 -5.73970750e-02
  4.81739230e-02 -5.32186218e-02 -3.36121805e-02 -6.74877549e-03
  1.65572055e-02  6.10864647e-02  5.77854179e-02  3.05107776e-02
  1.34900376e-01 -1.02367267e-01 -2.95207202e-02 -6.68650912e-03
 -9.41387340e-02  9.54032838e-02 -1.82363838e-05 -2.42917258e-02
  6.26014322e-02  2.09875330e-02 -1.40876193e-02 -6.77105114e-02
 -4.13547084e-02  2.40572542e-03  1.83768217e-02 -1.23497043e-02
  4.17861901e-02  1.65626388e-02 -1.32852480e-01  1.22169197e-01
  3.93851381e-03 -2.66662184e-02  1.13011405e-01 -4.13744301e-02
 -1.16914250e-02  1.40163805e-02  1.93645265e-02  9.90207307e-03
  3.74726392e-02 -1.18536636e-01 -4.40054238e-02  2.71482505e-02
  2.57952474e-02 -2.75990982e-02 -4.99511100e-02  3.47845852e-02
 -3.43060382e-02  4.71526310e-02  5.77412583e-02  7.87170753e-02
 -2.18352955e-03  4.02690545e-02  1.41022413e-03  6.66203648e-02
 -1.55310947e-02 -2.85861529e-02  3.91437598e-02  1.37317451e-02
 -5.07484190e-02  1.02998957e-01  1.21128708e-01  7.49489218e-02
 -3.45814750e-02  2.26713270e-02  1.38619784e-02  3.90032530e-02
  2.69799586e-03  8.51932094e-02 -1.03460148e-01  1.03186782e-32
 -7.92942941e-03  4.17389646e-02  1.04494402e-02  1.18007865e-02
  2.92954408e-02 -4.52082232e-02  6.60361350e-02  7.55914114e-03
  7.97225609e-02 -4.01257463e-02  4.67506051e-03  9.21108108e-03
 -2.66244318e-02  8.12391862e-02 -2.94408090e-02 -8.85415450e-02
  2.13536620e-02  4.25678715e-02  4.78214175e-02  2.17024386e-02
  4.34316620e-02  4.24673595e-02 -1.70090180e-02  8.30357522e-02
 -2.75800638e-02  2.27999240e-02 -1.54927606e-02 -1.76105257e-02
 -6.78006932e-03  2.32784133e-02 -5.27871624e-02 -2.53498703e-02
 -3.67122330e-02 -2.34031230e-02  8.15094169e-03 -4.54402044e-02
 -2.88008023e-02 -4.18167189e-02 -1.13613391e-02 -8.86490494e-02
 -1.18187137e-01  3.04021835e-02 -4.13251147e-02 -2.42549814e-02
 -3.92358787e-02 -1.28140394e-02 -4.22046669e-02  1.17736347e-01
  2.73114964e-02 -8.23649615e-02 -9.27770212e-02  4.40302342e-02
 -1.26627646e-02  4.57841642e-02  4.41758670e-02  6.76803757e-03
  2.27459837e-02  2.85721254e-02  4.59694006e-02  6.17521815e-02
  1.44732883e-02  5.83013818e-02  1.90321300e-02 -8.01953021e-03
 -8.29392001e-02 -2.22597495e-02 -1.18108526e-01  1.40677169e-02
 -5.44000901e-02  6.23590983e-02 -3.03116646e-02  1.26767829e-01
 -3.23234312e-03 -5.69579788e-02  2.96662841e-03  1.01865446e-02
  3.25206146e-02 -6.06774054e-02 -3.40179689e-02  1.73332915e-02
 -6.31504953e-02  5.15599698e-02  2.47611608e-02 -3.11117973e-02
 -7.33809546e-02  2.64828955e-03 -2.73483507e-02 -2.74998900e-02
  2.63496824e-02 -2.55205464e-02 -9.00907442e-02  2.87304260e-02
  2.63640378e-02  1.60763767e-02  1.04752779e-02 -8.01429053e-33
  4.49139625e-02  8.76682904e-03 -2.65674870e-02  6.70342371e-02
  4.73037958e-02 -1.85475394e-03 -5.70428893e-02 -9.15295482e-02
  4.52537276e-02  2.01462377e-02 -6.02674624e-03  1.83963254e-02
 -3.65613997e-02 -4.34686467e-02  5.48503809e-02  9.37185599e-04
 -1.45083377e-02 -1.92120508e-03  8.86778161e-03  5.04263900e-02
 -8.25100169e-02  7.15463310e-02 -7.95006678e-02 -9.37071815e-03
 -2.45209001e-02 -3.52744572e-02 -6.65468499e-02 -2.17148568e-02
  2.26124073e-03  7.23377161e-05 -9.64600127e-03  1.11429961e-02
  4.98199984e-02  2.85587143e-02 -7.60055380e-03  5.42572774e-02
  4.16402370e-02 -1.45780202e-03 -2.37587802e-02  3.52568030e-02
  1.25933439e-01  7.89386407e-02 -6.16918970e-03  6.52845576e-03
 -2.24156957e-03  2.99810320e-02 -1.41752675e-01  4.95703071e-02
 -3.64976376e-02  2.32344288e-02  1.35187823e-02 -1.15572978e-02
 -3.63494456e-02 -3.01254112e-02  2.09119245e-02  1.76334213e-02
  4.99623902e-02 -2.65177358e-02  1.09279901e-02  3.76909599e-02
 -1.12323910e-01 -8.25335607e-02 -1.15817646e-02  4.02723849e-02
 -2.51628812e-02 -5.65517284e-02 -5.33579551e-02  9.01088566e-02
  1.99420601e-02 -3.87281962e-02  1.97998565e-02  4.51964512e-02
  7.37989843e-02  1.17483720e-01 -6.45076036e-02  4.57005296e-03
  3.68650332e-02  3.53593677e-02 -4.43854295e-02  3.07589993e-02
  9.60422456e-02 -9.54045542e-03 -2.15064604e-02  4.51169275e-02
 -2.64707245e-02  1.31367087e-01  5.51444404e-02 -3.35739367e-03
  4.38596345e-02 -6.96633384e-02 -2.50366945e-02  1.31190708e-02
 -1.20196799e-02  1.08693071e-01  4.11633626e-02 -3.83787828e-08
 -3.17497551e-02 -2.43273517e-03  3.72642279e-02  5.71140030e-04
  5.40527962e-02 -1.27406353e-02 -1.94476675e-02  8.38060305e-02
 -4.76254076e-02  9.76257678e-03 -4.29157875e-02 -7.68928751e-02
 -4.22883332e-02  1.63744297e-02  6.21977113e-02  2.97513176e-02
  3.52450833e-02 -5.84716722e-02 -6.28872542e-04 -4.67810072e-02
  1.54344570e-02  2.60683578e-02 -1.41007621e-02  5.16812364e-03
 -3.28690000e-02 -3.04100793e-02  1.21650426e-02  7.01604597e-03
  5.03378138e-02 -9.36543848e-03 -4.15640548e-02 -4.04129811e-02
  4.66406308e-02 -6.08659536e-02  1.29936993e-01  1.07675917e-01
  6.18576109e-02 -5.18203303e-02 -3.24271768e-02  5.52896038e-02
 -5.13332263e-02  9.66223422e-03  6.44079410e-04 -2.42266823e-02
  6.02640957e-02 -3.16561311e-02 -2.28348970e-02 -2.96373107e-02
 -1.01170251e-02 -6.71573030e-03  2.91468464e-02  2.50064973e-02
 -2.02483553e-02 -7.93648046e-03  4.14905213e-02  1.11913413e-01
 -8.87783468e-02 -3.48567069e-02  1.31422821e-02  3.41242715e-03
  6.95370436e-02  5.48077859e-02 -5.48367798e-02 -3.94798964e-02]",2,2
tensorflow,213,tensorflow is an open source software library for high performance numerical computation it flexible architecture allows easy deployment of computation across a variety of platform cpu gpus tpus and from desktop to cluster of server to mobile and edge device originally developed by researcher and engineer from the google brain team within google s ai organization it come with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domain tensorflow is licensed under apache,"[('tensorflow', 0.7401), ('many other scientific domain tensorflow', 0.6584), ('flexible numerical computation core', 0.4332), ('deep learning', 0.3829), ('google s ai organization', 0.3821), ('flexible architecture', 0.3764), ('google brain team', 0.3684), ('platform cpu gpus tpus', 0.3613), ('open source software library', 0.3378), ('edge device', 0.2987)]","[-4.41984236e-02 -1.09338418e-01  3.75523120e-02 -6.00057282e-02
  8.24890584e-02 -4.75711152e-02 -5.30565754e-02  2.51197536e-02
 -1.34402225e-02 -1.77784972e-02 -8.60479921e-02 -3.01086437e-02
 -6.02132641e-02  1.79548992e-03  1.93502568e-03 -4.11127694e-02
 -2.45012846e-02  3.20869386e-02 -2.12851036e-02 -1.30356878e-01
 -4.25707698e-02  3.98718528e-02 -1.30772097e-02  1.58899475e-03
 -9.78470664e-04  7.42263645e-02 -1.08599998e-02 -1.42022565e-01
  3.27437744e-02 -6.55014766e-03 -1.02071287e-02  9.64850187e-03
  4.71507711e-03  3.81811298e-02 -6.36580959e-02  3.31682228e-02
 -4.90938015e-02 -4.53656018e-02 -1.57287996e-02 -4.45531756e-02
 -3.31613086e-02 -6.49364218e-02  1.22887539e-02 -4.47792821e-02
  8.86408389e-02  1.80261284e-02  2.25173086e-02 -1.15306959e-01
 -2.31295135e-02  1.99361108e-02 -3.49466614e-02 -8.51285681e-02
 -5.48832156e-02  8.48228261e-02 -7.17121512e-02 -6.73733884e-03
  4.10806164e-02 -6.88320026e-03  1.62451004e-03 -8.22282210e-03
 -1.00759100e-02 -8.78835991e-02  4.05119406e-03  1.54486764e-02
 -3.63357663e-02  5.28295226e-02 -9.17309057e-03  1.02966493e-02
  6.14090227e-02 -1.00714304e-01  8.13344568e-02  3.17448117e-02
 -3.28305252e-02 -7.23914290e-03 -3.69609729e-03 -1.43599079e-03
  5.77681474e-02 -2.73799971e-02  8.20818990e-02 -6.62635267e-02
  2.97083259e-02 -1.71580762e-02 -3.10782306e-02  1.28447125e-02
  3.34661715e-02 -2.07291394e-02 -7.15484247e-02  8.80392119e-02
 -1.64926033e-02 -1.48727791e-03  4.25984226e-02 -3.23301293e-02
  2.54538245e-02 -3.26760523e-02  4.63619828e-02  8.25122185e-03
 -7.75806680e-02 -8.91252458e-02 -4.99539562e-02  3.41689922e-02
 -1.18737161e-01  1.49925379e-03  4.47053388e-02  4.81082126e-02
 -2.87359152e-02  8.46514031e-02  4.03921865e-02 -3.37678823e-03
  9.50822160e-02 -2.90156733e-02 -2.99208332e-02  6.71075732e-02
  4.06970084e-02 -7.54432976e-02  1.83600057e-02 -7.02635124e-02
 -3.98105867e-02  7.65025169e-02  3.50047797e-02  1.09249830e-01
 -1.12119086e-01  3.65649313e-02 -3.26654613e-02 -8.72930512e-03
 -2.04302394e-03  1.50718652e-02 -8.76195654e-02  4.60425342e-33
  1.02570942e-02  4.28824779e-03  2.68881787e-02 -6.87931105e-02
  6.11587837e-02 -6.28679469e-02  2.82790624e-02  1.04069605e-03
  1.19376676e-02 -2.09535728e-03 -6.10620193e-02  1.45414501e-01
 -2.77088694e-02  1.31631240e-01  5.33098839e-02 -4.76143509e-02
  1.41801238e-02  5.01978882e-02  7.20799938e-02 -1.55935355e-03
  2.45469920e-02  6.20573387e-02  4.07773815e-02  9.34710652e-02
  2.34671924e-02  6.27924548e-03 -4.57534008e-02 -4.82183322e-02
  6.20497251e-03  1.55118303e-02 -5.82717657e-02  2.86109298e-02
 -4.72726822e-02  8.19104631e-03  1.48596782e-02  1.40242409e-02
 -7.13818287e-03 -6.28968030e-02  7.68921385e-03  1.67898014e-02
 -2.04171371e-02  4.88140881e-02  2.21376605e-02 -4.13218699e-02
 -1.19940946e-02 -2.14574859e-02  2.20675208e-02  2.55821627e-02
  4.94300574e-02 -5.72271608e-02 -2.83469800e-02  2.06819158e-02
 -7.33109657e-03 -7.42880777e-02  4.45740707e-02 -1.86625645e-02
  1.66003243e-03  5.78348897e-02  4.94083539e-02  8.17290470e-02
 -1.52327828e-02  2.14482732e-02 -2.74007898e-02 -1.56885870e-02
  3.61333638e-02 -1.15680881e-02  2.34961193e-02  4.28035334e-02
  9.24402382e-03  2.92042568e-02 -8.23360160e-02  7.03407750e-02
  3.02753523e-02  7.97685049e-03 -4.55205850e-02 -1.75612886e-02
  3.93302739e-02 -1.34543687e-01 -4.39116172e-02 -5.35312574e-03
 -1.35161489e-01  1.69179011e-02  7.37877490e-05  1.96509007e-02
  2.84204120e-03  1.24324122e-02 -2.80277040e-02  1.61576960e-02
 -5.60108647e-02 -5.83440997e-02 -9.79904234e-02 -1.62681602e-02
  8.40131938e-02  6.26077726e-02 -9.34203938e-02 -4.11873715e-33
 -1.21871002e-01 -2.00116094e-02 -6.95795864e-02  9.35161188e-02
  1.06673934e-01  1.64793674e-02  5.29420041e-02 -4.99502346e-02
  7.84921634e-04  5.40840551e-02  3.24851312e-02 -1.49185723e-02
  7.49930143e-02 -3.41328681e-02  1.59471463e-02 -6.72609732e-02
 -9.23045427e-02 -7.28558823e-02 -1.41289271e-02 -1.31858299e-02
 -3.60742360e-02  5.88296913e-02 -1.11268647e-02 -1.11294286e-02
  4.44210209e-02  2.42709126e-02 -9.14493203e-02 -6.56116903e-02
  6.20851517e-02  7.87141398e-02  8.85193329e-03 -2.49410868e-02
  1.07943751e-02  5.60709536e-02  1.08070284e-01  2.54249573e-02
  4.70024757e-02 -3.27337794e-02  4.17715870e-02 -3.21090035e-02
  6.68326393e-02  2.09123977e-02  8.31178799e-02  5.75441495e-03
 -1.57177057e-02  5.13668582e-02 -1.00466989e-01  5.68194799e-02
 -7.79418200e-02 -7.36578256e-02  2.29164143e-03  1.09611116e-02
 -3.92328426e-02 -1.02612153e-01  2.16487814e-02  2.21487638e-02
  4.73303422e-02 -1.24629773e-02  1.68757718e-02 -1.58568248e-02
 -1.49172219e-03 -5.22242896e-02  2.37852894e-02  2.95598339e-02
 -4.57500108e-02  7.54220188e-02 -8.37374944e-03  2.92717386e-02
 -7.08715320e-02 -5.05744107e-02  3.37662622e-02  9.25058499e-03
  6.04491401e-03  5.77810109e-02 -5.38401343e-02  1.88186746e-02
  2.49538794e-02 -2.30869707e-02  4.80298363e-02 -1.32949762e-02
  6.71137720e-02  1.58789922e-02  3.83781157e-02  5.01072630e-02
  5.06595746e-02  7.17647970e-02  4.81168069e-02 -3.42952199e-02
  2.15225033e-02 -2.26974841e-02 -4.45528142e-02  1.36768846e-02
  3.95529121e-02  9.66346115e-02 -7.90077373e-02 -2.76889054e-08
  4.37600647e-05  1.28331250e-02  8.19380730e-02 -1.11790355e-02
  5.15342504e-02 -8.69946275e-03  3.87721956e-02  4.68982942e-02
 -2.15304736e-02  1.09144384e-02 -1.34722795e-02 -5.39821126e-02
 -5.36090359e-02  1.06498577e-04  9.06490088e-02  5.04027084e-02
 -4.07694206e-02 -2.80015147e-03  3.82598266e-02 -8.40364844e-02
  3.06539945e-02  7.10854530e-02 -5.35856653e-03 -4.30149212e-02
 -3.41456267e-03 -7.31946230e-02  4.09532413e-02 -5.65154618e-03
  9.22913849e-03  2.75571328e-02 -5.08137271e-02 -5.10621220e-02
  5.90167232e-02 -3.64067666e-02  9.62798372e-02  4.31413203e-03
  5.34966327e-02 -2.88944710e-02  2.93812994e-03  6.43181428e-02
 -2.65936963e-02  8.72200802e-02  4.80443090e-02 -4.97647300e-02
  7.63691915e-03 -4.89581898e-02  1.61224399e-02 -4.40009758e-02
 -1.88040975e-02  6.26380444e-02 -7.08308667e-02  7.13098049e-02
 -4.59356047e-02  8.05362687e-02  4.94703427e-02  2.27457359e-02
 -2.76032165e-02 -1.40729666e-01 -2.81152502e-02  2.60950923e-02
 -1.65610705e-02  1.23978546e-02  5.14509305e-02  7.77910324e-03]",2,2
torchvision,142,the torchvision package consists of popular datasets model architecture and common image transformation for computer vision we recommend anaconda a python package management system please refer to pytorch org for the detail of pytorch torch installation the following is the corresponding torchvision version and supported python version torchtorchvisionpythonmain nightlymain nightly anaconda pip from source we don t officially support building from source using pip but if you do you ll need to use the no build isolation flag in case building torchvision from source fails install the nightly version of pytorch following the linked guide on the contributing page and retry the install by default gpu support is built if cuda is found and torch cuda is available is true it s possible to force building gpu support by setting force cuda environment variable which is useful when building a docker image torchvision currently support the following image backends pillow default pillow simd a much faster drop in replacement for pillow with simd if installed will be used a the default accimage if installed can be activated by calling torchvision set image backend accimage libpng can be installed via conda conda install libpng or any of the package manager for debian based and rhel based linux distribution libjpeg can be installed via conda conda install jpeg or any of the package manager for debian based and rhel based linux distribution libjpeg turbo can be used a well note libpng and libjpeg must be available at compilation time in order to be available make sure that it is available on the standard library location otherwise add the include and library path in the environment variable torchvision include and torchvision library respectively torchvision currently support the following video backends pyav default pythonic binding for ffmpeg library video reader this need ffmpeg to be installed and torchvision to be built from source there shouldn t be any conflicting version of ffmpeg installed currently this is only supported on linux torchvision provides an example project for how to use the model on c using jit script installation from source once installed the library can be accessed in cmake after properly configuring cmake prefix path via the torchvision torchvision target the torchvision package will also automatically look for the torch package and add it a a dependency to my target so make sure that it is also available to cmake via the cmake prefix path for an example setup take a look at example cpp hello world python linking is disabled by default when compiling torchvision with cmake this allows you to run model without any python dependency in some special case where torchvision s operator are used from python code you may need to link to python this can be done by passing duse python on to cmake in order to get the torchvision operator registered with torch eg for the jit all you need to do is to ensure that you include torchvision vision h in your project you can find the api documentation on the pytorch website http pytorch org vision stable index htmlsee the contributing file for how to help out this is a utility library that downloads and prepares public datasets we do not host or distribute these datasets vouch for their quality or fairness or claim that you have license to use the dataset it is your responsibility to determine whether you have permission to use the dataset under the dataset s license if you re a dataset owner and wish to update any part of it description citation etc or do not want your dataset to be included in this library please get in touch through a github issue thanks for your contribution to the ml community the pre trained model provided in this library may have their own license or term and condition derived from the dataset used for training it is your responsibility to determine whether you have permission to use the model for your use case more specifically swag model are released under the cc by nc license see swag license for additional detail,"[('torchvision package', 0.6482), ('python version torchtorchvisionpythonmain nightlymain', 0.6477), ('linux torchvision', 0.6405), ('torchvision library', 0.6145), ('torchvision torchvision', 0.5805), ('pytorch torch installation', 0.5681), ('docker image torchvision', 0.5679), ('torchvision version', 0.549), ('torch package', 0.5414), ('variable torchvision', 0.5393)]","[-3.74287814e-02 -7.13040605e-02  1.45112630e-02  9.76643059e-03
  5.57090379e-02 -1.77778564e-02 -3.95654002e-03  4.36067171e-02
 -5.31566814e-02 -6.06918298e-02  1.92789026e-02  4.63227294e-02
 -1.08713865e-01  4.41380963e-02  3.04978509e-02  2.61985324e-02
 -6.19601496e-02  2.10469253e-02 -1.93985477e-02 -7.55647942e-02
 -2.56689191e-02 -5.32285310e-02  8.74296352e-02  1.86805595e-02
  2.67316797e-03  2.31303517e-02 -2.46083662e-02 -1.14111304e-02
  5.16615547e-02 -2.39445157e-02 -6.56607375e-02  5.00491448e-02
 -2.15629358e-02 -2.25052442e-02  8.45798999e-02  5.73727675e-03
 -2.86104809e-02 -2.57125758e-02 -4.18235920e-02  4.70615812e-02
  6.42684929e-04 -2.02271268e-02 -3.73427495e-02  8.71260557e-03
 -8.19066614e-02  1.36138164e-02  8.33021402e-02 -2.15116292e-02
 -5.22012599e-02 -4.11359109e-02 -2.19671964e-03 -5.23637533e-02
 -3.05112805e-02  3.72262485e-02  5.24259210e-02 -7.96590224e-02
 -1.92341097e-02 -5.12075471e-03  2.04930194e-02 -6.14203885e-02
 -3.80779733e-03  4.23222929e-02 -5.05816229e-02 -2.21407991e-02
 -8.12673047e-02  4.14307415e-02  1.25147728e-02 -2.30717286e-02
  1.68097481e-01 -1.25671297e-01 -1.26931697e-01 -3.06126103e-02
 -6.00262685e-03  9.21759456e-02 -7.69095048e-02 -4.55561355e-02
  1.61531746e-01 -1.39426608e-02  2.33166330e-02 -1.14565872e-01
  4.68281060e-02 -1.90809034e-02  5.93933202e-02  2.26778574e-02
  3.62972096e-02 -5.78760309e-03  3.08797957e-04  6.86942488e-02
  4.48986553e-02 -5.54753430e-02 -5.73820397e-02 -8.81707519e-02
  3.91153172e-02 -4.31640167e-03 -1.58001203e-02 -6.95276866e-03
  3.35245579e-02 -3.64121236e-02 -6.41710013e-02  3.51179913e-02
 -2.34264378e-02 -1.02511697e-01  5.84865548e-02  4.71794121e-02
 -2.17872672e-02  2.85038594e-02 -1.50612919e-02  2.15539355e-02
 -1.58779602e-02  1.86120160e-02  3.26351486e-02 -1.59966461e-02
 -1.98610425e-02 -1.32052675e-01  4.35090736e-02 -9.63903293e-02
 -1.10744305e-01  8.69227871e-02  1.46934101e-02  1.06843673e-01
  1.08688092e-02  3.84706222e-02  4.76996824e-02  3.40447808e-03
 -6.58712536e-02  6.01261389e-03 -2.23425962e-02  5.86573114e-33
 -5.63406385e-03  2.68885288e-02 -7.51242861e-02 -2.90680081e-02
  3.86589542e-02 -3.12332436e-02  1.10772841e-01  1.48159349e-02
 -6.09978549e-02 -5.94850890e-02  2.51028351e-02  1.12274289e-03
 -3.21755223e-02  1.00868113e-01 -3.82408686e-02 -1.02478508e-02
  3.43730524e-02  8.44340771e-02  1.51720149e-02  1.81004405e-02
  6.58361381e-03  6.75177053e-02  2.40421668e-02  5.01717702e-02
 -3.20831873e-02 -5.63381016e-02  5.53722912e-03 -3.09076533e-02
  2.59670392e-02 -2.92296931e-02  8.75635259e-03  1.86748169e-02
  9.84444991e-02  2.21992489e-02  1.56724937e-02  6.60754042e-03
 -3.32061537e-02 -7.32317641e-02 -5.27704693e-03 -4.70638797e-02
 -2.44460534e-03  1.15840249e-01 -1.28058242e-02 -2.34811027e-02
  5.71615696e-02  2.20245551e-02 -3.92075069e-02  9.93625522e-02
  5.09876292e-03 -1.62898526e-02 -6.01511784e-02 -9.35047772e-03
 -2.06270348e-02  6.57414049e-02  1.39264613e-02 -3.38072143e-02
  7.40726071e-04 -1.21895336e-02  1.04111068e-01 -9.51664597e-02
  4.99161985e-03  6.94030896e-02  1.88206017e-04 -9.44938976e-03
  1.73558593e-02 -3.24745364e-02 -1.44790076e-02  4.29074876e-02
 -6.60901219e-02  4.64609005e-02 -1.46347508e-01  3.58230136e-02
 -3.57948523e-03  2.00389177e-02  1.15902703e-02 -1.75947584e-02
 -3.10115591e-02 -3.21047790e-02 -3.31408493e-02  4.18749303e-02
 -1.59541026e-01  3.08310036e-02  1.14825442e-02 -1.79358330e-02
 -2.87100710e-02 -1.23955542e-02  1.28650563e-02  3.29140350e-02
 -4.87644039e-03 -1.20859342e-02 -6.64398074e-02 -4.63017933e-02
  4.80570756e-02  2.52609327e-02 -3.51065211e-02 -5.14736523e-33
  1.53579824e-02  6.75113872e-02 -6.82047606e-02  3.73673439e-02
 -3.82042751e-02  3.30742896e-02 -6.88745594e-03 -1.15686297e-01
  2.41771434e-03 -6.92605376e-02  2.96587683e-02 -8.10808968e-03
 -2.91090254e-02 -4.26800400e-02  3.07243541e-02 -3.24044228e-02
 -3.85422446e-03 -3.78984138e-02  4.17340659e-02  2.52328273e-02
 -3.57065424e-02  7.64548555e-02 -9.09482464e-02 -5.24718035e-03
 -9.94955003e-02 -1.96482055e-02  1.03941202e-01 -1.74496528e-02
 -4.08144714e-03  1.46237004e-03  8.36357102e-02 -9.75790434e-03
 -1.95619911e-02  1.35850320e-02 -3.50601226e-02  2.47427281e-02
  5.34135625e-02 -7.58375078e-02 -4.09339704e-02 -1.42093934e-02
  1.34973422e-01  3.08109224e-02  9.48463939e-03  1.06880099e-01
 -8.89642388e-02 -3.04879900e-02 -4.22825813e-02  4.87140287e-03
  6.97224261e-03 -1.70699190e-02 -6.15945086e-02 -8.21774751e-02
 -7.79277459e-02 -4.02762592e-02 -3.03742825e-03 -2.76504830e-02
  2.28123888e-02  1.16223022e-01  3.80238023e-04  3.24857198e-02
 -2.20560506e-02 -6.04835413e-02  3.76160666e-02  5.50476089e-03
 -2.29612030e-02  4.11852337e-02 -8.51987526e-02  2.91238967e-02
  2.73382347e-02  1.24044064e-03  1.92465968e-02  7.46269003e-02
  8.00824240e-02  6.45932043e-03  2.16564946e-02  5.98848499e-02
  5.37987165e-02  8.29529464e-02  4.74620946e-02 -8.48721433e-03
 -1.04813948e-02  4.14045574e-03 -2.26941914e-03  4.66779396e-02
  9.55449417e-03  3.07255983e-02  4.91361469e-02  2.99813822e-02
  3.48550379e-02 -4.60448079e-02  5.54525070e-02 -1.99365169e-02
  7.60413632e-02  3.02110892e-02  9.68917459e-02 -2.90902999e-08
  2.52869204e-02  7.64528140e-02 -4.77052890e-02  7.58106681e-03
  1.25865471e-02  2.55098790e-02  8.34062546e-02  9.42025483e-02
 -4.52245660e-02  2.57113371e-02  8.29383805e-02 -2.53334399e-02
 -1.51067292e-02 -2.40158923e-02  4.63536419e-02  8.45907256e-02
 -3.87474895e-02  7.09654912e-02 -5.83264697e-03 -2.17518155e-02
 -3.56953442e-02 -2.00349502e-02  4.96538058e-02 -2.86714993e-02
 -3.30491886e-02  1.95440967e-02  5.22672944e-02  5.29848831e-03
 -2.68005468e-02 -1.55871529e-02  8.88544098e-02 -3.39053869e-02
  6.33236319e-02 -4.32379693e-02  1.55091971e-01 -1.00433053e-02
 -7.55074695e-02 -8.29855073e-03  3.65492590e-02  1.77779924e-02
 -3.48386988e-02 -5.76711223e-02 -2.74891220e-03  5.78332040e-03
 -5.86114638e-03  6.22056937e-03 -2.21163705e-02 -4.64417227e-02
 -9.31036472e-02 -6.83425041e-03 -3.69128305e-04  3.08531914e-02
 -1.02139395e-02  3.99680734e-02  5.11561669e-02  7.95844644e-02
  2.07858682e-02 -3.55232656e-02 -1.98075287e-02 -2.65588034e-02
 -1.44171854e-03  1.37093030e-02 -2.01096572e-02 -1.43389488e-02]",2,2
einops,125,http user image githubusercontent com f eb d e d c ae ead b mp flexible and powerful tensor operation for readable and reliable code support numpy pytorch tensorflow jax and others in case you need convincing argument for setting aside time to learn about einsum and einops tim rockt schel fairwriting better code with pytorch and einops andrej karpathy ai at teslaslowly but surely einops is seeping in to every nook and cranny of my code if you find yourself shuffling around bazillion dimensional tensor this might change your life nasim rahaman mila montreal more testimonialswatch a minute talk that focus on main problem of standard tensor manipulation method and how einops improves this process plain and simple tutorial are the most convenient way to see einops in actioneinops ha a minimalistic yet powerful api three operation provided einops tutorial show those cover stacking reshape transposition squeeze unsqueeze repeat tile concatenate view and numerous reduction and two corresponding layer einops keep a separate version for each framework with the same api layer behave similarly to operation and have the same parameter with the exception of the first argument which is passed during call example of using layer within a model einops stand for einstein inspired notation for operation though einstein operation is more attractive and easier to remember notation wa loosely inspired by einstein summation in particular by numpy einsum operation while these two line are doing the same job in some context the second one provides information about the input and output in other word einops focus on interface what is the input and output not how the output is computed the next operation look similar but it give the reader a hint this is not an independent batch of image we are processing but rather a sequence video semantic information make the code easier to read and maintain reconsider the same example the second line check that the input ha four dimension but you can also specify particular dimension that s opposed to just writing comment about shape since comment don t work and don t prevent mistake a we knowbelow we have at least two way to define the depth to space operationthere are at least four more way to do it which one is used by the framework these detail are ignored since usually it make no difference but it can make a big difference e g if you use grouped convolution in the next stage and you d like to specify this in your code these example demonstrated that we don t use separate operation for d d d pooling those are all defined in a uniform way space to depth and depth to space are defined in many framework but how about width to height here you go even simple function are defined differently by different frameworkssuppose x s shape wa then y ha shape einops work the same way in all framework example tile v repeat cause lot of confusion to copy image along width with einops you don t need to decipher which axis wa repeated testimonial provide user s perspective on the same question einops work with please use the following bibtex recordlink to paper at openreview http openreview net pdf id oapksvm bcj einops work with python or later,"[('reliable code support numpy pytorch tensorflow jax', 0.5966), ('standard tensor manipulation method', 0.5847), ('powerful tensor operation', 0.5288), ('einops tutorial show', 0.5014), ('model einops', 0.4997), ('layer einops', 0.4645), ('dimensional tensor', 0.415), ('einstein operation', 0.4072), ('einops', 0.3903), ('shape einops', 0.3891)]","[-1.50093198e-01 -5.50677814e-02 -1.07718445e-02  1.87159260e-03
  3.17550562e-02 -5.62972389e-02 -9.58988741e-02  2.68268343e-02
 -1.39200926e-01 -5.80835603e-02  1.22741926e-02 -2.06163917e-02
 -6.00039661e-02  3.93191027e-03 -1.73739940e-02  2.39208229e-02
 -2.40175035e-02  7.58406594e-02 -2.72492561e-02 -1.33953780e-01
 -7.07728118e-02  1.55318296e-03  2.05797236e-02 -3.64262722e-02
  4.03513312e-02 -3.71909216e-02  4.97765131e-02 -1.18434262e-02
 -9.83243110e-04 -1.52966781e-02 -3.84115726e-02  3.11037730e-02
  6.05115071e-02 -7.62934983e-03  1.49966739e-02  4.60079983e-02
  7.99606834e-03 -7.30965883e-02 -8.16333443e-02  1.42957540e-02
 -3.12867910e-02 -1.44211890e-03 -2.08835825e-02 -2.73088063e-03
  1.55903166e-02 -3.51060159e-03  3.44723016e-02 -8.37823749e-02
  9.46056098e-04 -2.26181597e-02 -5.90257309e-02 -5.99192008e-02
 -1.31476903e-02  2.86270622e-02 -5.17821219e-03 -7.26194680e-02
  4.06698249e-02 -1.45912785e-02 -5.68339005e-02 -9.69393998e-02
  5.06471135e-02 -1.61426831e-02 -5.89769036e-02  1.51349315e-02
 -1.82964634e-02  6.86241537e-02 -2.91846786e-02  4.86059114e-03
  6.64380193e-02 -8.39445740e-02 -8.40455294e-02 -4.12763506e-02
 -8.81190524e-02  1.23801388e-01  2.90955175e-02  8.34319089e-03
  1.28105894e-01 -3.66763212e-02  2.80327126e-02 -4.15232666e-02
 -7.61869252e-02  5.59162954e-03  1.94483008e-02 -3.51242013e-02
  2.11748201e-02  1.18005285e-02 -1.41165629e-02  1.01778634e-01
  4.65455614e-02  2.49672253e-02  9.40569192e-02 -5.30451797e-02
 -1.56236179e-02 -3.74575052e-03  7.58069679e-02  3.11926939e-02
  1.68963838e-02 -1.29150841e-02 -7.40064234e-02  9.91741661e-03
  2.59224679e-02 -2.92911399e-02  1.16293533e-02 -9.13226511e-03
  6.16438426e-02  8.35912079e-02  8.80976468e-02  3.53164338e-02
  2.36774646e-02 -2.31544022e-03  9.09061916e-03 -2.37777513e-02
 -6.52665794e-02 -6.78656101e-02  8.44117776e-02 -2.96810293e-03
  1.95031520e-02  8.47308040e-02  5.19670993e-02  7.19257295e-02
  2.86421329e-02  5.40097319e-02  1.40759274e-02  5.37824109e-02
 -1.07587473e-02  3.41787674e-02 -5.76423518e-02  5.62918935e-33
 -5.86895347e-02  2.96388865e-02 -8.30443483e-03 -3.68059799e-03
  2.47226749e-02 -9.25158709e-02  5.39848357e-02 -6.19996525e-03
 -4.14687693e-02  5.09263482e-03 -5.43454289e-02  8.26215521e-02
  1.92783251e-02  2.86688767e-02 -2.99062822e-02 -7.29036629e-02
  1.67719033e-02  5.98046370e-02  3.78665365e-02  8.46395046e-02
  5.11472113e-02 -1.46924444e-02 -2.20525432e-02 -7.81224156e-03
 -4.32690717e-02  7.67888725e-02  1.32083846e-02  6.65841112e-03
  9.96965822e-03  1.89823397e-02 -1.09778814e-01 -7.56576359e-02
 -4.38719206e-02 -1.19942408e-02 -3.55636403e-02  1.79197136e-02
 -4.34768610e-02 -5.76864369e-02 -2.50896346e-02 -1.57893211e-01
 -4.43570428e-02  3.14358994e-02 -2.79636774e-02 -8.44908180e-04
 -1.15297744e-02 -4.14060894e-03 -1.94012020e-02  5.61031774e-02
  5.88587411e-02 -3.56904529e-02 -6.56294227e-02 -1.85671020e-02
 -1.99761242e-02  4.06485908e-02  2.93561909e-02 -1.57994628e-02
  7.85958990e-02  3.38140875e-02  2.80496255e-02  4.30078320e-02
  1.90171204e-03  2.22608522e-02 -1.48597290e-03 -1.48655726e-02
 -3.38178151e-03 -1.95165593e-02 -8.37803259e-02 -5.18760690e-03
 -7.18594119e-02  7.81268552e-02 -9.78717953e-02  9.57583934e-02
 -3.73073301e-04 -8.76707863e-03 -4.90325987e-02 -7.16781393e-02
  8.31057355e-02 -1.05692685e-01  5.52354800e-03 -9.07469448e-03
 -5.83455414e-02  5.18573858e-02  4.73321751e-02 -6.52006492e-02
 -4.86002602e-02  3.26485448e-02 -8.97387508e-03  4.72377315e-02
  1.66701563e-02 -4.07268927e-02 -1.61007512e-02 -1.59850493e-02
  5.90179041e-02 -1.53293796e-02 -4.21236195e-02 -3.94441354e-33
 -4.32415344e-02  3.01763341e-02 -8.07003751e-02  6.76994249e-02
 -6.14417205e-03 -3.43088917e-02 -5.10459095e-02  1.73948500e-02
  3.09737958e-02 -2.45439615e-02  7.24800723e-03 -8.21664855e-02
  1.01562981e-02 -4.07734625e-02  8.84799361e-02 -2.08722055e-02
 -5.35272881e-02 -5.51070198e-02  6.60236403e-02 -5.73035236e-03
 -7.07213134e-02  7.29819685e-02 -3.76833417e-02 -2.89756320e-02
 -9.60670114e-02  4.13279253e-04 -5.59835322e-02 -5.14014298e-03
 -8.48396402e-03 -3.25860316e-03  3.04314587e-02 -2.52853371e-02
 -1.71758104e-02  6.63635358e-02  1.95517205e-02  1.21656684e-02
  5.31464927e-02  4.03025039e-02  7.74751743e-03  1.08166346e-02
  8.76621827e-02  5.58460243e-02 -1.22284079e-02  3.99591178e-02
 -2.94997171e-02  4.89436984e-02 -6.07112460e-02  5.46522848e-02
 -1.89325102e-02 -2.19693314e-02 -2.93566510e-02 -2.55987681e-02
 -1.07672224e-02 -7.03272875e-03 -5.52332550e-02  9.57614332e-02
  5.37074544e-02  3.08874976e-02  2.04561260e-02  3.19910720e-02
 -7.20511302e-02 -6.66954592e-02  6.34079948e-02  2.88455300e-02
  2.65541933e-02 -1.81441978e-02 -1.13884754e-01  9.35201198e-02
 -9.84566286e-02  6.60023466e-03  1.41430488e-02  4.91749868e-02
 -3.96049721e-03  1.28146317e-02  1.56018222e-02 -1.50406389e-02
  6.76396713e-02  3.66539992e-02 -5.92605490e-03  1.36167398e-02
  4.72274050e-02  3.12966853e-02  6.17714301e-02  1.29591897e-01
  4.92347367e-02  7.40937367e-02  9.63580981e-02  2.39601247e-02
  2.05547139e-02 -4.06779945e-02 -2.56160693e-03  2.85506174e-02
  3.85148861e-02  1.10580929e-01  6.83485763e-03 -2.92727709e-08
 -4.74779233e-02  4.67585623e-02  4.52219620e-02 -1.08573996e-02
 -3.04732807e-02  5.85838668e-02  4.47841547e-02  5.45653477e-02
 -2.27504559e-02  3.55753042e-02 -3.73137780e-02 -4.76985089e-02
 -8.94583315e-02  1.11234477e-02  2.78008208e-02  7.88753182e-02
 -3.07574868e-02  9.56708491e-02  4.36398759e-03 -3.57342400e-02
  5.08367680e-02  5.62214889e-02  5.77922873e-02  4.68983594e-03
 -6.18971512e-02  4.86424938e-02  3.64557765e-02  8.31608996e-02
  4.07944135e-02 -2.83531398e-02 -4.02864031e-02 -1.67498924e-02
  9.17339772e-02 -3.16006020e-02  4.91459854e-02  2.20028255e-02
  2.09800787e-02  1.41919020e-03 -4.36389223e-02  8.47279876e-02
 -8.10111985e-02  4.88738641e-02 -2.88849156e-02 -1.00237839e-02
  9.55319107e-02  5.91577813e-02 -5.21553494e-03 -1.14719108e-01
 -1.51202371e-02  4.57397252e-02 -7.10691418e-03  9.00707394e-03
 -4.02049012e-02  5.29294834e-02  6.24563061e-02  6.56207502e-02
 -8.72449726e-02 -7.22110271e-02  4.58284840e-02  1.32966340e-02
 -1.81131959e-02  9.79045779e-02  1.91370305e-02  2.86726467e-02]",2,2
transformers,113,english espa ol state of the art machine learning for jax pytorch and tensorflow transformer provides thousand of pretrained model to perform task on different modality such a text vision and audio these model can be applied on text for task like text classification information extraction question answering summarization translation text generation in over language image for task like image classification object detection and segmentation audio for task like speech recognition and audio classification transformer model can also perform task on several modality combined such a table question answering optical character recognition information extraction from scanned document video classification and visual question answering transformer provides apis to quickly download and use those pretrained model on a given text fine tune them on your own datasets and then share them with the community on our model hub at the same time each python module defining an architecture is fully standalone and can be modified to enable quick research experiment transformer is backed by the three most popular deep learning library jax pytorch and tensorflow with a seamless integration between them it s straightforward to train your model with one before loading them for inference with the other online demo you can test most of our model directly on their page from the model hub we also offer private model hosting versioning an inference api for public and private model here are a few example in natural language processing masked word completion with bert name entity recognition with electra text generation with gpt natural language inference with roberta summarization with bart question answering with distilbert translation with t in computer vision image classification with vit object detection with detr semantic segmentation with segformer panoptic segmentation with detr in audio automatic speech recognition with wav vec keyword spotting with wav vec in multimodal task visual question answering with vilt write with transformer built by the hugging face team is the official demo of this repo s text generation capability if you are looking for custom support from the hugging face team quick tour to immediately use a model on a given input text image audio we provide the pipeline api pipeline group together a pretrained model with the preprocessing that wa used during that model s training here is how to quickly use a pipeline to classify positive versus negative text from transformer import pipeline allocate a pipeline for sentiment analysis classifier pipeline sentiment analysis classifier we are very happy to introduce pipeline to the transformer repository label positive score the second line of code downloads and cache the pretrained model used by the pipeline while the third evaluates it on the given text here the answer is positive with a confidence of many task have a pre trained pipeline ready to go in nlp but also in computer vision and speech for example we can easily extract detected object in an image import request from pil import image from transformer import pipeline download an image with cute cat url http huggingface co datasets huggingface documentation image resolve main coco sample png image data request get url stream true raw image image open image data allocate a pipeline for object detection object detector pipeline object detection object detector image score label remote box xmin ymin xmax ymax score label remote box xmin ymin xmax ymax score label couch box xmin ymin xmax ymax score label cat box xmin ymin xmax ymax score label cat box xmin ymin xmax ymax here we get a list of object detected in the image with a box surrounding the object and a confidence score here is the original image on the left with the prediction displayed on the right you can learn more about the task supported by the pipeline api in this tutorial in addition to pipeline to download and use any of the pretrained model on your given task all it take is three line of code here is the pytorch version from transformer import autotokenizer automodel tokenizer autotokenizer from pretrained bert base uncased model automodel from pretrained bert base uncased input tokenizer hello world return tensor pt output model input and here is the equivalent code for tensorflow from transformer import autotokenizer tfautomodel tokenizer autotokenizer from pretrained bert base uncased model tfautomodel from pretrained bert base uncased input tokenizer hello world return tensor tf output model input the tokenizer is responsible for all the preprocessing the pretrained model expects and can be called directly on a single string a in the above example or a list it will output a dictionary that you can use in downstream code or simply directly pas to your model using the argument unpacking operator the model itself is a regular pytorch nn module or a tensorflow tf kera model depending on your backend which you can use a usual this tutorial explains how to integrate such a model into a classic pytorch or tensorflow training loop or how to use our trainer api to quickly fine tune on a new dataset why should i use transformer easy to use state of the art model high performance on natural language understanding generation computer vision and audio task low barrier to entry for educator and practitioner few user facing abstraction with just three class to learn a unified api for using all our pretrained model lower compute cost smaller carbon footprint researcher can share trained model instead of always retraining practitioner can reduce compute time and production cost dozen of architecture with over pretrained model across all modality choose the right framework for every part of a model s lifetime train state of the art model in line of code move a single model between tf pytorch jax framework at will seamlessly pick the right framework for training evaluation and production easily customize a model or an example to your need we provide example for each architecture to reproduce the result published by it original author model internals are exposed a consistently a possible model file can be used independently of the library for quick experiment why shouldn t i use transformer this library is not a modular toolbox of building block for neural net the code in the model file is not refactored with additional abstraction on purpose so that researcher can quickly iterate on each of the model without diving into additional abstraction file the training api is not intended to work on any model but is optimized to work with the model provided by the library for generic machine learning loop you should use another library possibly accelerate while we strive to present a many use case a possible the script in our example folder are just that example it is expected that they won t work out of the box on your specific problem and that you will be required to change a few line of code to adapt them to your need installation with pip this repository is tested on python flax pytorch and tensorflow you should install transformer in a virtual environment if you re unfamiliar with python virtual environment check out the user guide first create a virtual environment with the version of python you re going to use and activate it then you will need to install at least one of flax pytorch or tensorflow please refer to tensorflow installation page pytorch installation page and or flax and jax installation page regarding the specific installation command for your platform when one of those backends ha been installed transformer can be installed using pip a follows pip install transformer if you d like to play with the example or need the bleeding edge of the code and can t wait for a new release you must install the library from source with conda since transformer version v we now have a conda channel huggingface transformer can be installed using conda a follows conda install c huggingface transformer follow the installation page of flax pytorch or tensorflow to see how to install them with conda note on window you may be prompted to activate developer mode in order to benefit from caching if this is not an option for you please let u know in this issue model architecture all the model checkpoint provided by transformer are seamlessly integrated from the huggingface co model hub where they are uploaded directly by user and organization current number of checkpoint transformer currently provides the following architecture see here for a high level summary of each them albert from google research and the toyota technological institute at chicago released with the paper albert a lite bert for self supervised learning of language representation by zhenzhong lan mingda chen sebastian goodman kevin gimpel piyush sharma radu soricut bart from facebook released with the paper bart denoising sequence to sequence pre training for natural language generation translation and comprehension by mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy f stoyanov and luke zettlemoyer barthez from cole polytechnique released with the paper barthez a skilled pretrained french sequence to sequence model by moussa kamal eddine antoine j p tixier michalis vazirgiannis bartpho from vinai research released with the paper bartpho pre trained sequence to sequence model for vietnamese by nguyen luong tran duong minh le and dat quoc nguyen beit from microsoft released with the paper beit bert pre training of image transformer by hangbo bao li dong furu wei bert from google released with the paper bert pre training of deep bidirectional transformer for language understanding by jacob devlin ming wei chang kenton lee and kristina toutanova bert for sequence generation from google released with the paper leveraging pre trained checkpoint for sequence generation task by sascha rothe shashi narayan aliaksei severyn bertweet from vinai research released with the paper bertweet a pre trained language model for english tweet by dat quoc nguyen thanh vu and anh tuan nguyen bigbird pegasus from google research released with the paper big bird transformer for longer sequence by manzil zaheer guru guruganesh avinava dubey joshua ainslie chris alberti santiago ontanon philip pham anirudh ravula qifan wang li yang amr ahmed bigbird roberta from google research released with the paper big bird transformer for longer sequence by manzil zaheer guru guruganesh avinava dubey joshua ainslie chris alberti santiago ontanon philip pham anirudh ravula qifan wang li yang amr ahmed blenderbot from facebook released with the paper recipe for building an open domain chatbot by stephen roller emily dinan naman goyal da ju mary williamson yinhan liu jing xu myle ott kurt shuster eric m smith y lan boureau jason weston blenderbotsmall from facebook released with the paper recipe for building an open domain chatbot by stephen roller emily dinan naman goyal da ju mary williamson yinhan liu jing xu myle ott kurt shuster eric m smith y lan boureau jason weston bloom from bigscience workshop released by the bigsicence workshop bort from alexa released with the paper optimal subarchitecture extraction for bert by adrian de wynter and daniel j perry byt from google research released with the paper byt towards a token free future with pre trained byte to byte model by linting xue aditya barua noah constant ramus al rfou sharan narang mihir kale adam robert colin raffel camembert from inria facebook sorbonne released with the paper camembert a tasty french language model by louis martin benjamin muller pedro javier ortiz su rez yoann dupont laurent romary ric villemonte de la clergerie djam seddah and beno t sagot canine from google research released with the paper canine pre training an efficient tokenization free encoder for language representation by jonathan h clark dan garrette iulia turc john wieting clip from openai released with the paper learning transferable visual model from natural language supervision by alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sastry amanda askell pamela mishkin jack clark gretchen krueger ilya sutskever codegen from salesforce released with the paper a conversational paradigm for program synthesis by erik nijkamp bo pang hiroaki hayashi lifu tu huan wang yingbo zhou silvio savarese caiming xiong conditional detr from microsoft research asia released with the paper conditional detr for fast training convergence by depu meng xiaokang chen zejia fan gang zeng houqiang li yuhui yuan lei sun jingdong wang convbert from yitutech released with the paper convbert improving bert with span based dynamic convolution by zihang jiang weihao yu daquan zhou yunpeng chen jiashi feng shuicheng yan convnext from facebook ai released with the paper a convnet for the s by zhuang liu hanzi mao chao yuan wu christoph feichtenhofer trevor darrell saining xie cpm from tsinghua university released with the paper cpm a large scale generative chinese pre trained language model by zhengyan zhang xu han hao zhou pei ke yuxian gu deming ye yujia qin yusheng su haozhe ji jian guan fanchao qi xiaozhi wang yanan zheng guoyang zeng huanqi cao shengqi chen daixuan li zhenbo sun zhiyuan liu minlie huang wentao han jie tang juanzi li xiaoyan zhu maosong sun ctrl from salesforce released with the paper ctrl a conditional transformer language model for controllable generation by nitish shirish keskar bryan mccann lav r varshney caiming xiong and richard socher cvt from microsoft released with the paper cvt introducing convolution to vision transformer by haiping wu bin xiao noel codella mengchen liu xiyang dai lu yuan lei zhang data vec from facebook released with the paper data vec a general framework for self supervised learning in speech vision and language by alexei baevski wei ning hsu qiantong xu arun babu jiatao gu michael auli deberta from microsoft released with the paper deberta decoding enhanced bert with disentangled attention by pengcheng he xiaodong liu jianfeng gao weizhu chen deberta v from microsoft released with the paper deberta decoding enhanced bert with disentangled attention by pengcheng he xiaodong liu jianfeng gao weizhu chen decision transformer from berkeley facebook google released with the paper decision transformer reinforcement learning via sequence modeling by lili chen kevin lu aravind rajeswaran kimin lee aditya grover michael laskin pieter abbeel aravind srinivas igor mordatch deformable detr from sensetime research released with the paper deformable detr deformable transformer for end to end object detection by xizhou zhu weijie su lewei lu bin li xiaogang wang jifeng dai deit from facebook released with the paper training data efficient image transformer distillation through attention by hugo touvron matthieu cord matthijs douze francisco massa alexandre sablayrolles herv j gou detr from facebook released with the paper end to end object detection with transformer by nicolas carion francisco massa gabriel synnaeve nicolas usunier alexander kirillov sergey zagoruyko dialogpt from microsoft research released with the paper dialogpt large scale generative pre training for conversational response generation by yizhe zhang siqi sun michel galley yen chun chen chris brockett xiang gao jianfeng gao jingjing liu bill dolan distilbert from huggingface released together with the paper distilbert a distilled version of bert smaller faster cheaper and lighter by victor sanh lysandre debut and thomas wolf the same method ha been applied to compress gpt into distilgpt roberta into distilroberta multilingual bert into distilmbert and a german version of distilbert dit from microsoft research released with the paper dit self supervised pre training for document image transformer by junlong li yiheng xu tengchao lv lei cui cha zhang furu wei donut from naver released together with the paper ocr free document understanding transformer by geewook kim teakgyu hong moonbin yim jeongyeon nam jinyoung park jinyeong yim wonseok hwang sangdoo yun dongyoon han seunghyun park dpr from facebook released with the paper dense passage retrieval for open domain question answering by vladimir karpukhin barlas o uz sewon min patrick lewis ledell wu sergey edunov danqi chen and wen tau yih dpt from intel lab released with the paper vision transformer for dense prediction by ren ranftl alexey bochkovskiy vladlen koltun electra from google research stanford university released with the paper electra pre training text encoders a discriminator rather than generator by kevin clark minh thang luong quoc v le christopher d manning encoderdecoder from google research released with the paper leveraging pre trained checkpoint for sequence generation task by sascha rothe shashi narayan aliaksei severyn ernie from baidu released with the paper ernie enhanced representation through knowledge integration by yu sun shuohuan wang yukun li shikun feng xuyi chen han zhang xin tian danxiang zhu hao tian hua wu esm from meta ai are transformer protein language model esm b wa released with the paper biological structure and function emerge from scaling unsupervised learning to million protein sequence by alexander rives joshua meier tom sercu siddharth goyal zeming lin jason liu demi guo myle ott c lawrence zitnick jerry ma and rob fergus esm v wa released with the paper language model enable zero shot prediction of the effect of mutation on protein function by joshua meier roshan rao robert verkuil jason liu tom sercu and alexander rives esm wa released with the paper language model of protein sequence at the scale of evolution enable accurate structure prediction by zeming lin halil akin roshan rao brian hie zhongkai zhu wenting lu allan do santos costa maryam fazel zarandi tom sercu sal candido alexander rives flan t from google ai released in the repository google research t x by hyung won chung le hou shayne longpre barret zoph yi tay william fedus eric li xuezhi wang mostafa dehghani siddhartha brahma albert webson shixiang shane gu zhuyun dai mirac suzgun xinyun chen aakanksha chowdhery sharan narang gaurav mishra adam yu vincent zhao yanping huang andrew dai hongkun yu slav petrov ed h chi jeff dean jacob devlin adam robert denny zhou quoc v le and jason wei flaubert from cnrs released with the paper flaubert unsupervised language model pre training for french by hang le lo c vial jibril frej vincent segonne maximin coavoux benjamin lecouteux alexandre allauzen beno t crabb laurent besacier didier schwab flava from facebook ai released with the paper flava a foundational language and vision alignment model by amanpreet singh ronghang hu vedanuj goswami guillaume couairon wojciech galuba marcus rohrbach and douwe kiela fnet from google research released with the paper fnet mixing token with fourier transforms by james lee thorp joshua ainslie ilya eckstein santiago ontanon funnel transformer from cmu google brain released with the paper funnel transformer filtering out sequential redundancy for efficient language processing by zihang dai guokun lai yiming yang quoc v le glpn from kaist released with the paper global local path network for monocular depth estimation with vertical cutdepth by doyeon kim woonghyun ga pyungwhan ahn donggyu joo sehwan chun junmo kim gpt from openai released with the paper improving language understanding by generative pre training by alec radford karthik narasimhan tim salimans and ilya sutskever gpt neo from eleutherai released in the repository eleutherai gpt neo by sid black stella biderman leo gao phil wang and connor leahy gpt neox from eleutherai released with the paper gpt neox b an open source autoregressive language model by sid black stella biderman eric hallahan quentin anthony leo gao laurence golding horace he connor leahy kyle mcdonell jason phang michael pieler usvsn sai prashanth shivanshu purohit laria reynolds jonathan tow ben wang samuel weinbach gpt neox japanese from abeja released by shinya otani takayoshi makabe anuj arora and kyo hattori gpt from openai released with the paper language model are unsupervised multitask learner by alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever gpt j from eleutherai released in the repository kingoflolz mesh transformer jax by ben wang and aran komatsuzaki groupvit from ucsd nvidia released with the paper groupvit semantic segmentation emerges from text supervision by jiarui xu shalini de mello sifei liu wonmin byeon thomas breuel jan kautz xiaolong wang hubert from facebook released with the paper hubert self supervised speech representation learning by masked prediction of hidden unit by wei ning hsu benjamin bolte yao hung hubert tsai kushal lakhotia ruslan salakhutdinov abdelrahman mohamed i bert from berkeley released with the paper i bert integer only bert quantization by sehoon kim amir gholami zhewei yao michael w mahoney kurt keutzer imagegpt from openai released with the paper generative pretraining from pixel by mark chen alec radford rewon child jeffrey wu heewoo jun david luan ilya sutskever layoutlm from microsoft research asia released with the paper layoutlm pre training of text and layout for document image understanding by yiheng xu minghao li lei cui shaohan huang furu wei ming zhou layoutlmv from microsoft research asia released with the paper layoutlmv multi modal pre training for visually rich document understanding by yang xu yiheng xu tengchao lv lei cui furu wei guoxin wang yijuan lu dinei florencio cha zhang wanxiang che min zhang lidong zhou layoutlmv from microsoft research asia released with the paper layoutlmv pre training for document ai with unified text and image masking by yupan huang tengchao lv lei cui yutong lu furu wei layoutxlm from microsoft research asia released with the paper layoutxlm multimodal pre training for multilingual visually rich document understanding by yiheng xu tengchao lv lei cui guoxin wang yijuan lu dinei florencio cha zhang furu wei led from allenai released with the paper longformer the long document transformer by iz beltagy matthew e peter arman cohan levit from meta ai released with the paper levit a vision transformer in convnet s clothing for faster inference by ben graham alaaeldin el nouby hugo touvron pierre stock armand joulin herv j gou matthijs douze lilt from south china university of technology released with the paper lilt a simple yet effective language independent layout transformer for structured document understanding by jiapeng wang lianwen jin kai ding longformer from allenai released with the paper longformer the long document transformer by iz beltagy matthew e peter arman cohan longt from google ai released with the paper longt efficient text to text transformer for long sequence by mandy guo joshua ainslie david uthus santiago ontanon jianmo ni yun hsuan sung yinfei yang luke from studio ousia released with the paper luke deep contextualized entity representation with entity aware self attention by ikuya yamada akari asai hiroyuki shindo hideaki takeda yuji matsumoto lxmert from unc chapel hill released with the paper lxmert learning cross modality encoder representation from transformer for open domain question answering by hao tan and mohit bansal m ctc t from facebook released with the paper pseudo labeling for massively multilingual speech recognition by loren lugosch tatiana likhomanenko gabriel synnaeve and ronan collobert m m from facebook released with the paper beyond english centric multilingual machine translation by angela fan shruti bhosale holger schwenk zhiyi ma ahmed el kishky siddharth goyal mandeep baines onur celebi guillaume wenzek vishrav chaudhary naman goyal tom birch vitaliy liptchinsky sergey edunov edouard grave michael auli armand joulin marianmt machine translation model trained using opus data by j rg tiedemann the marian framework is being developed by the microsoft translator team markuplm from microsoft research asia released with the paper markuplm pre training of text and markup language for visually rich document understanding by junlong li yiheng xu lei cui furu wei maskformer from meta and uiuc released with the paper per pixel classification is not all you need for semantic segmentation by bowen cheng alexander g schwing alexander kirillov mbart from facebook released with the paper multilingual denoising pre training for neural machine translation by yinhan liu jiatao gu naman goyal xian li sergey edunov marjan ghazvininejad mike lewis luke zettlemoyer mbart from facebook released with the paper multilingual translation with extensible multilingual pretraining and finetuning by yuqing tang chau tran xian li peng jen chen naman goyal vishrav chaudhary jiatao gu angela fan megatron bert from nvidia released with the paper megatron lm training multi billion parameter language model using model parallelism by mohammad shoeybi mostofa patwary raul puri patrick legresley jared casper and bryan catanzaro megatron gpt from nvidia released with the paper megatron lm training multi billion parameter language model using model parallelism by mohammad shoeybi mostofa patwary raul puri patrick legresley jared casper and bryan catanzaro mluke from studio ousia released with the paper mluke the power of entity representation in multilingual pretrained language model by ryokan ri ikuya yamada and yoshimasa tsuruoka mobilebert from cmu google brain released with the paper mobilebert a compact task agnostic bert for resource limited device by zhiqing sun hongkun yu xiaodan song renjie liu yiming yang and denny zhou mobilevit from apple released with the paper mobilevit light weight general purpose and mobile friendly vision transformer by sachin mehta and mohammad rastegari mpnet from microsoft research released with the paper mpnet masked and permuted pre training for language understanding by kaitao song xu tan tao qin jianfeng lu tie yan liu mt from google ai released with the paper mt a massively multilingual pre trained text to text transformer by linting xue noah constant adam robert mihir kale ramus al rfou aditya siddhant aditya barua colin raffel mvp from ruc ai box released with the paper mvp multi task supervised pre training for natural language generation by tianyi tang junyi li wayne xin zhao and ji rong wen nezha from huawei noah s ark lab released with the paper nezha neural contextualized representation for chinese language understanding by junqiu wei xiaozhe ren xiaoguang li wenyong huang yi liao yasheng wang jiashu lin xin jiang xiao chen and qun liu nllb from meta released with the paper no language left behind scaling human centered machine translation by the nllb team nystr mformer from the university of wisconsin madison released with the paper nystr mformer a nystr m based algorithm for approximating self attention by yunyang xiong zhanpeng zeng rudrasis chakraborty mingxing tan glenn fung yin li vikas singh opt from meta ai released with the paper opt open pre trained transformer language model by susan zhang stephen roller naman goyal mikel artetxe moya chen shuohui chen et al owl vit from google ai released with the paper simple open vocabulary object detection with vision transformer by matthias minderer alexey gritsenko austin stone maxim neumann dirk weissenborn alexey dosovitskiy aravindh mahendran anurag arnab mostafa dehghani zhuoran shen xiao wang xiaohua zhai thomas kipf and neil houlsby pegasus from google released with the paper pegasus pre training with extracted gap sentence for abstractive summarization by jingqing zhang yao zhao mohammad saleh and peter j liu pegasus x from google released with the paper investigating efficiently extending transformer for long input summarization by jason phang yao zhao and peter j liu perceiver io from deepmind released with the paper perceiver io a general architecture for structured input output by andrew jaegle sebastian borgeaud jean baptiste alayrac carl doersch catalin ionescu david ding skanda koppula daniel zoran andrew brock evan shelhamer olivier h naff matthew m botvinick andrew zisserman oriol vinyals jo o carreira phobert from vinai research released with the paper phobert pre trained language model for vietnamese by dat quoc nguyen and anh tuan nguyen plbart from ucla nlp released with the paper unified pre training for program understanding and generation by wasi uddin ahmad saikat chakraborty baishakhi ray kai wei chang poolformer from sea ai lab released with the paper metaformer is actually what you need for vision by yu weihao and luo mi and zhou pan and si chenyang and zhou yichen and wang xinchao and feng jiashi and yan shuicheng prophetnet from microsoft research released with the paper prophetnet predicting future n gram for sequence to sequence pre training by yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming zhou qdqbert from nvidia released with the paper integer quantization for deep learning inference principle and empirical evaluation by hao wu patrick judd xiaojie zhang mikhail isaev and paulius micikevicius rag from facebook released with the paper retrieval augmented generation for knowledge intensive nlp task by patrick lewis ethan perez aleksandara piktus fabio petroni vladimir karpukhin naman goyal heinrich k ttler mike lewis wen tau yih tim rockt schel sebastian riedel douwe kiela realm from google research released with the paper realm retrieval augmented language model pre training by kelvin guu kenton lee zora tung panupong pasupat and ming wei chang reformer from google research released with the paper reformer the efficient transformer by nikita kitaev ukasz kaiser anselm levskaya regnet from meta platform released with the paper designing network design space by ilija radosavovic raj prateek kosaraju ross girshick kaiming he piotr doll r rembert from google research released with the paper rethinking embedding coupling in pre trained language model by hyung won chung thibault f vry henry tsai m johnson sebastian ruder resnet from microsoft research released with the paper deep residual learning for image recognition by kaiming he xiangyu zhang shaoqing ren jian sun roberta from facebook released together with the paper roberta a robustly optimized bert pretraining approach by yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roformer from zhuiyitechnology released together with the paper roformer enhanced transformer with rotary position embedding by jianlin su and yu lu and shengfeng pan and bo wen and yunfeng liu segformer from nvidia released with the paper segformer simple and efficient design for semantic segmentation with transformer by enze xie wenhai wang zhiding yu anima anandkumar jose m alvarez ping luo sew from asapp released with the paper performance efficiency trade offs in unsupervised pre training for speech recognition by felix wu kwangyoun kim jing pan kyu han kilian q weinberger yoav artzi sew d from asapp released with the paper performance efficiency trade offs in unsupervised pre training for speech recognition by felix wu kwangyoun kim jing pan kyu han kilian q weinberger yoav artzi speechtotexttransformer from facebook released together with the paper fairseq s t fast speech to text modeling with fairseq by changhan wang yun tang xutai ma anne wu dmytro okhonko juan pino speechtotexttransformer from facebook released together with the paper large scale self and semi supervised learning for speech translation by changhan wang anne wu juan pino alexei baevski michael auli alexis conneau splinter from tel aviv university released together with the paper few shot question answering by pretraining span selection by ori ram yuval kirstain jonathan berant amir globerson omer levy squeezebert from berkeley released with the paper squeezebert what can computer vision teach nlp about efficient neural network by forrest n iandola albert e shaw ravi krishna and kurt w keutzer swin transformer from microsoft released with the paper swin transformer hierarchical vision transformer using shifted window by ze liu yutong lin yue cao han hu yixuan wei zheng zhang stephen lin baining guo swin transformer v from microsoft released with the paper swin transformer v scaling up capacity and resolution by ze liu han hu yutong lin zhuliang yao zhenda xie yixuan wei jia ning yue cao zheng zhang li dong furu wei baining guo t from google ai released with the paper exploring the limit of transfer learning with a unified text to text transformer by colin raffel and noam shazeer and adam robert and katherine lee and sharan narang and michael matena and yanqi zhou and wei li and peter j liu t v from google ai released in the repository google research text to text transfer transformer by colin raffel and noam shazeer and adam robert and katherine lee and sharan narang and michael matena and yanqi zhou and wei li and peter j liu table transformer from microsoft research released with the paper pubtables m towards comprehensive table extraction from unstructured document by brandon smock rohith pesala robin abraham tapa from google ai released with the paper tapa weakly supervised table parsing via pre training by jonathan herzig pawe krzysztof nowak thomas m ller francesco piccinno and julian martin eisenschlos tapex from microsoft research released with the paper tapex table pre training via learning a neural sql executor by qian liu bei chen jiaqi guo morteza ziyadi zeqi lin weizhu chen jian guang lou time series transformer from huggingface trajectory transformer from the university of california at berkeley released with the paper offline reinforcement learning a one big sequence modeling problem by michael janner qiyang li sergey levine transformer xl from google cmu released with the paper transformer xl attentive language model beyond a fixed length context by zihang dai zhilin yang yiming yang jaime carbonell quoc v le ruslan salakhutdinov trocr from microsoft released together with the paper trocr transformer based optical character recognition with pre trained model by minghao li tengchao lv lei cui yijuan lu dinei florencio cha zhang zhoujun li furu wei ul from google research released with the paper unifying language learning paradigm by yi tay mostafa dehghani vinh q tran xavier garcia dara bahri tal schuster huaixiu steven zheng neil houlsby donald metzler unispeech from microsoft research released with the paper unispeech unified speech representation learning with labeled and unlabeled data by chengyi wang yu wu yao qian kenichi kumatani shujie liu furu wei michael zeng xuedong huang unispeechsat from microsoft research released with the paper unispeech sat universal speech representation learning with speaker aware pre training by sanyuan chen yu wu chengyi wang zhengyang chen zhuo chen shujie liu jian wu yao qian furu wei jinyu li xiangzhan yu van from tsinghua university and nankai university released with the paper visual attention network by meng hao guo cheng ze lu zheng ning liu ming ming cheng shi min hu videomae from multimedia computing group nanjing university released with the paper videomae masked autoencoders are data efficient learner for self supervised video pre training by zhan tong yibing song jue wang limin wang vilt from naver ai lab kakao enterprise kakao brain released with the paper vilt vision and language transformer without convolution or region supervision by wonjae kim bokyung son ildoo kim vision transformer vit from google ai released with the paper an image is worth x word transformer for image recognition at scale by alexey dosovitskiy lucas beyer alexander kolesnikov dirk weissenborn xiaohua zhai thomas unterthiner mostafa dehghani matthias minderer georg heigold sylvain gelly jakob uszkoreit neil houlsby visualbert from ucla nlp released with the paper visualbert a simple and performant baseline for vision and language by liunian harold li mark yatskar da yin cho jui hsieh kai wei chang vitmae from meta ai released with the paper masked autoencoders are scalable vision learner by kaiming he xinlei chen saining xie yanghao li piotr doll r ross girshick vitmsn from meta ai released with the paper masked siamese network for label efficient learning by mahmoud assran mathilde caron ishan misra piotr bojanowski florian bordes pascal vincent armand joulin michael rabbat nicolas ballas wav vec from facebook ai released with the paper wav vec a framework for self supervised learning of speech representation by alexei baevski henry zhou abdelrahman mohamed michael auli wav vec conformer from facebook ai released with the paper fairseq s t fast speech to text modeling with fairseq by changhan wang yun tang xutai ma anne wu sravya popuri dmytro okhonko juan pino wav vec phoneme from facebook ai released with the paper simple and effective zero shot cross lingual phoneme recognition by qiantong xu alexei baevski michael auli wavlm from microsoft research released with the paper wavlm large scale self supervised pre training for full stack speech processing by sanyuan chen chengyi wang zhengyang chen yu wu shujie liu zhuo chen jinyu li naoyuki kanda takuya yoshioka xiong xiao jian wu long zhou shuo ren yanmin qian yao qian jian wu michael zeng furu wei whisper from openai released with the paper robust speech recognition via large scale weak supervision by alec radford jong wook kim tao xu greg brockman christine mcleavey ilya sutskever x clip from microsoft research released with the paper expanding language image pretrained model for general video recognition by bolin ni houwen peng minghao chen songyang zhang gaofeng meng jianlong fu shiming xiang haibin ling xglm from facebook ai released with the paper few shot learning with multilingual language model by xi victoria lin todor mihaylov mikel artetxe tianlu wang shuohui chen daniel simig myle ott naman goyal shruti bhosale jingfei du ramakanth pasunuru sam shleifer punit singh koura vishrav chaudhary brian o horo jeff wang luke zettlemoyer zornitsa kozareva mona diab veselin stoyanov xian li xlm from facebook released together with the paper cross lingual language model pretraining by guillaume lample and alexis conneau xlm prophetnet from microsoft research released with the paper prophetnet predicting future n gram for sequence to sequence pre training by yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming zhou xlm roberta from facebook ai released together with the paper unsupervised cross lingual representation learning at scale by alexis conneau kartikay khandelwal naman goyal vishrav chaudhary guillaume wenzek francisco guzm n edouard grave myle ott luke zettlemoyer and veselin stoyanov xlm roberta xl from facebook ai released together with the paper larger scale transformer for multilingual masked language modeling by naman goyal jingfei du myle ott giri anantharaman alexis conneau xlnet from google cmu released with the paper xlnet generalized autoregressive pretraining for language understanding by zhilin yang zihang dai yiming yang jaime carbonell ruslan salakhutdinov quoc v le xl r from facebook ai released with the paper xl r self supervised cross lingual speech representation learning at scale by arun babu changhan wang andros tjandra kushal lakhotia qiantong xu naman goyal kritika singh patrick von platen yatharth saraf juan pino alexei baevski alexis conneau michael auli xlsr wav vec from facebook ai released with the paper unsupervised cross lingual representation learning for speech recognition by alexis conneau alexei baevski ronan collobert abdelrahman mohamed michael auli yolos from huazhong university of science technology released with the paper you only look at one sequence rethinking transformer in vision through object detection by yuxin fang bencheng liao xinggang wang jiemin fang jiyang qi rui wu jianwei niu wenyu liu yoso from the university of wisconsin madison released with the paper you only sample almost once linear cost self attention via bernoulli sampling by zhanpeng zeng yunyang xiong sathya n ravi shailesh acharya glenn fung vikas singh want to contribute a new model we have added a detailed guide and template to guide you in the process of adding a new model you can find them in the template folder of the repository be sure to check the contributing guideline and contact the maintainer or open an issue to collect feedback before starting your pr to check if each model ha an implementation in flax pytorch or tensorflow or ha an associated tokenizer backed by the tokenizers library refer to this table these implementation have been tested on several datasets see the example script and should match the performance of the original implementation you can find more detail on performance in the example section of the documentation learn more section description documentation full api documentation and tutorial task summary task supported by transformer preprocessing tutorial using the tokenizer class to prepare data for the model training and fine tuning using the model provided by transformer in a pytorch tensorflow training loop and the trainer api quick tour fine tuning usage script example script for fine tuning model on a wide range of task model sharing and uploading upload and share your fine tuned model with the community migration migrate to transformer from pytorch transformer or pytorch pretrained bert citation we now have a paper you can cite for the transformer library inproceedings wolf etal transformer title transformer state of the art natural language processing author thomas wolf and lysandre debut and victor sanh and julien chaumond and clement delangue and anthony moi and pierric cistac and tim rault and r mi louf and morgan funtowicz and joe davison and sam shleifer and patrick von platen and clara ma and yacine jernite and julien plu and canwen xu and teven le scao and sylvain gugger and mariama drame and quentin lhoest and alexander m rush booktitle proceeding of the conference on empirical method in natural language processing system demonstration month oct year address online publisher association for computational linguistics url http www aclweb org anthology emnlp demo page english espa ol state of the art machine learning for jax pytorch and tensorflow transformer provides thousand of pretrained model to perform task on different modality such a text vision and audio these model can be applied on text for task like text classification information extraction question answering summarization translation text generation in over language image for task like image classification object detection and segmentation audio for task like speech recognition and audio classification transformer model can also perform task on several modality combined such a table question answering optical character recognition information extraction from scanned document video classification and visual question answering transformer provides apis to quickly download and use those pretrained model on a given text fine tune them on your own datasets and then share them with the community on our model hub at the same time each python module defining an architecture is fully standalone and can be modified to enable quick research experiment transformer is backed by the three most popular deep learning library jax pytorch and tensorflow with a seamless integration between them it s straightforward to train your model with one before loading them for inference with the other online demo you can test most of our model directly on their page from the model hub we also offer private model hosting versioning an inference api for public and private model here are a few example in natural language processing masked word completion with bert name entity recognition with electra text generation with gpt natural language inference with roberta summarization with bart question answering with distilbert translation with t in computer vision image classification with vit object detection with detr semantic segmentation with segformer panoptic segmentation with detr in audio automatic speech recognition with wav vec keyword spotting with wav vec in multimodal task visual question answering with vilt write with transformer built by the hugging face team is the official demo of this repo s text generation capability if you are looking for custom support from the hugging face team quick tour to immediately use a model on a given input text image audio we provide the pipeline api pipeline group together a pretrained model with the preprocessing that wa used during that model s training here is how to quickly use a pipeline to classify positive versus negative text from transformer import pipeline allocate a pipeline for sentiment analysis classifier pipeline sentiment analysis classifier we are very happy to introduce pipeline to the transformer repository label positive score the second line of code downloads and cache the pretrained model used by the pipeline while the third evaluates it on the given text here the answer is positive with a confidence of many task have a pre trained pipeline ready to go in nlp but also in computer vision and speech for example we can easily extract detected object in an image import request from pil import image from transformer import pipeline download an image with cute cat url http huggingface co datasets huggingface documentation image resolve main coco sample png image data request get url stream true raw image image open image data allocate a pipeline for object detection object detector pipeline object detection object detector image score label remote box xmin ymin xmax ymax score label remote box xmin ymin xmax ymax score label couch box xmin ymin xmax ymax score label cat box xmin ymin xmax ymax score label cat box xmin ymin xmax ymax here we get a list of object detected in the image with a box surrounding the object and a confidence score here is the original image on the left with the prediction displayed on the right you can learn more about the task supported by the pipeline api in this tutorial in addition to pipeline to download and use any of the pretrained model on your given task all it take is three line of code here is the pytorch version from transformer import autotokenizer automodel tokenizer autotokenizer from pretrained bert base uncased model automodel from pretrained bert base uncased input tokenizer hello world return tensor pt output model input and here is the equivalent code for tensorflow from transformer import autotokenizer tfautomodel tokenizer autotokenizer from pretrained bert base uncased model tfautomodel from pretrained bert base uncased input tokenizer hello world return tensor tf output model input the tokenizer is responsible for all the preprocessing the pretrained model expects and can be called directly on a single string a in the above example or a list it will output a dictionary that you can use in downstream code or simply directly pas to your model using the argument unpacking operator the model itself is a regular pytorch nn module or a tensorflow tf kera model depending on your backend which you can use a usual this tutorial explains how to integrate such a model into a classic pytorch or tensorflow training loop or how to use our trainer api to quickly fine tune on a new dataset why should i use transformer easy to use state of the art model high performance on natural language understanding generation computer vision and audio task low barrier to entry for educator and practitioner few user facing abstraction with just three class to learn a unified api for using all our pretrained model lower compute cost smaller carbon footprint researcher can share trained model instead of always retraining practitioner can reduce compute time and production cost dozen of architecture with over pretrained model across all modality choose the right framework for every part of a model s lifetime train state of the art model in line of code move a single model between tf pytorch jax framework at will seamlessly pick the right framework for training evaluation and production easily customize a model or an example to your need we provide example for each architecture to reproduce the result published by it original author model internals are exposed a consistently a possible model file can be used independently of the library for quick experiment why shouldn t i use transformer this library is not a modular toolbox of building block for neural net the code in the model file is not refactored with additional abstraction on purpose so that researcher can quickly iterate on each of the model without diving into additional abstraction file the training api is not intended to work on any model but is optimized to work with the model provided by the library for generic machine learning loop you should use another library possibly accelerate while we strive to present a many use case a possible the script in our example folder are just that example it is expected that they won t work out of the box on your specific problem and that you will be required to change a few line of code to adapt them to your need installation with pip this repository is tested on python flax pytorch and tensorflow you should install transformer in a virtual environment if you re unfamiliar with python virtual environment check out the user guide first create a virtual environment with the version of python you re going to use and activate it then you will need to install at least one of flax pytorch or tensorflow please refer to tensorflow installation page pytorch installation page and or flax and jax installation page regarding the specific installation command for your platform when one of those backends ha been installed transformer can be installed using pip a follows pip install transformer if you d like to play with the example or need the bleeding edge of the code and can t wait for a new release you must install the library from source with conda since transformer version v we now have a conda channel huggingface transformer can be installed using conda a follows conda install c huggingface transformer follow the installation page of flax pytorch or tensorflow to see how to install them with conda note on window you may be prompted to activate developer mode in order to benefit from caching if this is not an option for you please let u know in this issue model architecture all the model checkpoint provided by transformer are seamlessly integrated from the huggingface co model hub where they are uploaded directly by user and organization current number of checkpoint transformer currently provides the following architecture see here for a high level summary of each them albert from google research and the toyota technological institute at chicago released with the paper albert a lite bert for self supervised learning of language representation by zhenzhong lan mingda chen sebastian goodman kevin gimpel piyush sharma radu soricut bart from facebook released with the paper bart denoising sequence to sequence pre training for natural language generation translation and comprehension by mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy f stoyanov and luke zettlemoyer barthez from cole polytechnique released with the paper barthez a skilled pretrained french sequence to sequence model by moussa kamal eddine antoine j p tixier michalis vazirgiannis bartpho from vinai research released with the paper bartpho pre trained sequence to sequence model for vietnamese by nguyen luong tran duong minh le and dat quoc nguyen beit from microsoft released with the paper beit bert pre training of image transformer by hangbo bao li dong furu wei bert from google released with the paper bert pre training of deep bidirectional transformer for language understanding by jacob devlin ming wei chang kenton lee and kristina toutanova bert for sequence generation from google released with the paper leveraging pre trained checkpoint for sequence generation task by sascha rothe shashi narayan aliaksei severyn bertweet from vinai research released with the paper bertweet a pre trained language model for english tweet by dat quoc nguyen thanh vu and anh tuan nguyen bigbird pegasus from google research released with the paper big bird transformer for longer sequence by manzil zaheer guru guruganesh avinava dubey joshua ainslie chris alberti santiago ontanon philip pham anirudh ravula qifan wang li yang amr ahmed bigbird roberta from google research released with the paper big bird transformer for longer sequence by manzil zaheer guru guruganesh avinava dubey joshua ainslie chris alberti santiago ontanon philip pham anirudh ravula qifan wang li yang amr ahmed blenderbot from facebook released with the paper recipe for building an open domain chatbot by stephen roller emily dinan naman goyal da ju mary williamson yinhan liu jing xu myle ott kurt shuster eric m smith y lan boureau jason weston blenderbotsmall from facebook released with the paper recipe for building an open domain chatbot by stephen roller emily dinan naman goyal da ju mary williamson yinhan liu jing xu myle ott kurt shuster eric m smith y lan boureau jason weston bloom from bigscience workshop released by the bigsicence workshop bort from alexa released with the paper optimal subarchitecture extraction for bert by adrian de wynter and daniel j perry byt from google research released with the paper byt towards a token free future with pre trained byte to byte model by linting xue aditya barua noah constant ramus al rfou sharan narang mihir kale adam robert colin raffel camembert from inria facebook sorbonne released with the paper camembert a tasty french language model by louis martin benjamin muller pedro javier ortiz su rez yoann dupont laurent romary ric villemonte de la clergerie djam seddah and beno t sagot canine from google research released with the paper canine pre training an efficient tokenization free encoder for language representation by jonathan h clark dan garrette iulia turc john wieting clip from openai released with the paper learning transferable visual model from natural language supervision by alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sastry amanda askell pamela mishkin jack clark gretchen krueger ilya sutskever codegen from salesforce released with the paper a conversational paradigm for program synthesis by erik nijkamp bo pang hiroaki hayashi lifu tu huan wang yingbo zhou silvio savarese caiming xiong conditional detr from microsoft research asia released with the paper conditional detr for fast training convergence by depu meng xiaokang chen zejia fan gang zeng houqiang li yuhui yuan lei sun jingdong wang convbert from yitutech released with the paper convbert improving bert with span based dynamic convolution by zihang jiang weihao yu daquan zhou yunpeng chen jiashi feng shuicheng yan convnext from facebook ai released with the paper a convnet for the s by zhuang liu hanzi mao chao yuan wu christoph feichtenhofer trevor darrell saining xie cpm from tsinghua university released with the paper cpm a large scale generative chinese pre trained language model by zhengyan zhang xu han hao zhou pei ke yuxian gu deming ye yujia qin yusheng su haozhe ji jian guan fanchao qi xiaozhi wang yanan zheng guoyang zeng huanqi cao shengqi chen daixuan li zhenbo sun zhiyuan liu minlie huang wentao han jie tang juanzi li xiaoyan zhu maosong sun ctrl from salesforce released with the paper ctrl a conditional transformer language model for controllable generation by nitish shirish keskar bryan mccann lav r varshney caiming xiong and richard socher cvt from microsoft released with the paper cvt introducing convolution to vision transformer by haiping wu bin xiao noel codella mengchen liu xiyang dai lu yuan lei zhang data vec from facebook released with the paper data vec a general framework for self supervised learning in speech vision and language by alexei baevski wei ning hsu qiantong xu arun babu jiatao gu michael auli deberta from microsoft released with the paper deberta decoding enhanced bert with disentangled attention by pengcheng he xiaodong liu jianfeng gao weizhu chen deberta v from microsoft released with the paper deberta decoding enhanced bert with disentangled attention by pengcheng he xiaodong liu jianfeng gao weizhu chen decision transformer from berkeley facebook google released with the paper decision transformer reinforcement learning via sequence modeling by lili chen kevin lu aravind rajeswaran kimin lee aditya grover michael laskin pieter abbeel aravind srinivas igor mordatch deformable detr from sensetime research released with the paper deformable detr deformable transformer for end to end object detection by xizhou zhu weijie su lewei lu bin li xiaogang wang jifeng dai deit from facebook released with the paper training data efficient image transformer distillation through attention by hugo touvron matthieu cord matthijs douze francisco massa alexandre sablayrolles herv j gou detr from facebook released with the paper end to end object detection with transformer by nicolas carion francisco massa gabriel synnaeve nicolas usunier alexander kirillov sergey zagoruyko dialogpt from microsoft research released with the paper dialogpt large scale generative pre training for conversational response generation by yizhe zhang siqi sun michel galley yen chun chen chris brockett xiang gao jianfeng gao jingjing liu bill dolan distilbert from huggingface released together with the paper distilbert a distilled version of bert smaller faster cheaper and lighter by victor sanh lysandre debut and thomas wolf the same method ha been applied to compress gpt into distilgpt roberta into distilroberta multilingual bert into distilmbert and a german version of distilbert dit from microsoft research released with the paper dit self supervised pre training for document image transformer by junlong li yiheng xu tengchao lv lei cui cha zhang furu wei donut from naver released together with the paper ocr free document understanding transformer by geewook kim teakgyu hong moonbin yim jeongyeon nam jinyoung park jinyeong yim wonseok hwang sangdoo yun dongyoon han seunghyun park dpr from facebook released with the paper dense passage retrieval for open domain question answering by vladimir karpukhin barlas o uz sewon min patrick lewis ledell wu sergey edunov danqi chen and wen tau yih dpt from intel lab released with the paper vision transformer for dense prediction by ren ranftl alexey bochkovskiy vladlen koltun electra from google research stanford university released with the paper electra pre training text encoders a discriminator rather than generator by kevin clark minh thang luong quoc v le christopher d manning encoderdecoder from google research released with the paper leveraging pre trained checkpoint for sequence generation task by sascha rothe shashi narayan aliaksei severyn ernie from baidu released with the paper ernie enhanced representation through knowledge integration by yu sun shuohuan wang yukun li shikun feng xuyi chen han zhang xin tian danxiang zhu hao tian hua wu esm from meta ai are transformer protein language model esm b wa released with the paper biological structure and function emerge from scaling unsupervised learning to million protein sequence by alexander rives joshua meier tom sercu siddharth goyal zeming lin jason liu demi guo myle ott c lawrence zitnick jerry ma and rob fergus esm v wa released with the paper language model enable zero shot prediction of the effect of mutation on protein function by joshua meier roshan rao robert verkuil jason liu tom sercu and alexander rives esm wa released with the paper language model of protein sequence at the scale of evolution enable accurate structure prediction by zeming lin halil akin roshan rao brian hie zhongkai zhu wenting lu allan do santos costa maryam fazel zarandi tom sercu sal candido alexander rives flan t from google ai released in the repository google research t x by hyung won chung le hou shayne longpre barret zoph yi tay william fedus eric li xuezhi wang mostafa dehghani siddhartha brahma albert webson shixiang shane gu zhuyun dai mirac suzgun xinyun chen aakanksha chowdhery sharan narang gaurav mishra adam yu vincent zhao yanping huang andrew dai hongkun yu slav petrov ed h chi jeff dean jacob devlin adam robert denny zhou quoc v le and jason wei flaubert from cnrs released with the paper flaubert unsupervised language model pre training for french by hang le lo c vial jibril frej vincent segonne maximin coavoux benjamin lecouteux alexandre allauzen beno t crabb laurent besacier didier schwab flava from facebook ai released with the paper flava a foundational language and vision alignment model by amanpreet singh ronghang hu vedanuj goswami guillaume couairon wojciech galuba marcus rohrbach and douwe kiela fnet from google research released with the paper fnet mixing token with fourier transforms by james lee thorp joshua ainslie ilya eckstein santiago ontanon funnel transformer from cmu google brain released with the paper funnel transformer filtering out sequential redundancy for efficient language processing by zihang dai guokun lai yiming yang quoc v le glpn from kaist released with the paper global local path network for monocular depth estimation with vertical cutdepth by doyeon kim woonghyun ga pyungwhan ahn donggyu joo sehwan chun junmo kim gpt from openai released with the paper improving language understanding by generative pre training by alec radford karthik narasimhan tim salimans and ilya sutskever gpt neo from eleutherai released in the repository eleutherai gpt neo by sid black stella biderman leo gao phil wang and connor leahy gpt neox from eleutherai released with the paper gpt neox b an open source autoregressive language model by sid black stella biderman eric hallahan quentin anthony leo gao laurence golding horace he connor leahy kyle mcdonell jason phang michael pieler usvsn sai prashanth shivanshu purohit laria reynolds jonathan tow ben wang samuel weinbach gpt neox japanese from abeja released by shinya otani takayoshi makabe anuj arora and kyo hattori gpt from openai released with the paper language model are unsupervised multitask learner by alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever gpt j from eleutherai released in the repository kingoflolz mesh transformer jax by ben wang and aran komatsuzaki groupvit from ucsd nvidia released with the paper groupvit semantic segmentation emerges from text supervision by jiarui xu shalini de mello sifei liu wonmin byeon thomas breuel jan kautz xiaolong wang hubert from facebook released with the paper hubert self supervised speech representation learning by masked prediction of hidden unit by wei ning hsu benjamin bolte yao hung hubert tsai kushal lakhotia ruslan salakhutdinov abdelrahman mohamed i bert from berkeley released with the paper i bert integer only bert quantization by sehoon kim amir gholami zhewei yao michael w mahoney kurt keutzer imagegpt from openai released with the paper generative pretraining from pixel by mark chen alec radford rewon child jeffrey wu heewoo jun david luan ilya sutskever layoutlm from microsoft research asia released with the paper layoutlm pre training of text and layout for document image understanding by yiheng xu minghao li lei cui shaohan huang furu wei ming zhou layoutlmv from microsoft research asia released with the paper layoutlmv multi modal pre training for visually rich document understanding by yang xu yiheng xu tengchao lv lei cui furu wei guoxin wang yijuan lu dinei florencio cha zhang wanxiang che min zhang lidong zhou layoutlmv from microsoft research asia released with the paper layoutlmv pre training for document ai with unified text and image masking by yupan huang tengchao lv lei cui yutong lu furu wei layoutxlm from microsoft research asia released with the paper layoutxlm multimodal pre training for multilingual visually rich document understanding by yiheng xu tengchao lv lei cui guoxin wang yijuan lu dinei florencio cha zhang furu wei led from allenai released with the paper longformer the long document transformer by iz beltagy matthew e peter arman cohan levit from meta ai released with the paper levit a vision transformer in convnet s clothing for faster inference by ben graham alaaeldin el nouby hugo touvron pierre stock armand joulin herv j gou matthijs douze lilt from south china university of technology released with the paper lilt a simple yet effective language independent layout transformer for structured document understanding by jiapeng wang lianwen jin kai ding longformer from allenai released with the paper longformer the long document transformer by iz beltagy matthew e peter arman cohan longt from google ai released with the paper longt efficient text to text transformer for long sequence by mandy guo joshua ainslie david uthus santiago ontanon jianmo ni yun hsuan sung yinfei yang luke from studio ousia released with the paper luke deep contextualized entity representation with entity aware self attention by ikuya yamada akari asai hiroyuki shindo hideaki takeda yuji matsumoto lxmert from unc chapel hill released with the paper lxmert learning cross modality encoder representation from transformer for open domain question answering by hao tan and mohit bansal m ctc t from facebook released with the paper pseudo labeling for massively multilingual speech recognition by loren lugosch tatiana likhomanenko gabriel synnaeve and ronan collobert m m from facebook released with the paper beyond english centric multilingual machine translation by angela fan shruti bhosale holger schwenk zhiyi ma ahmed el kishky siddharth goyal mandeep baines onur celebi guillaume wenzek vishrav chaudhary naman goyal tom birch vitaliy liptchinsky sergey edunov edouard grave michael auli armand joulin marianmt machine translation model trained using opus data by j rg tiedemann the marian framework is being developed by the microsoft translator team markuplm from microsoft research asia released with the paper markuplm pre training of text and markup language for visually rich document understanding by junlong li yiheng xu lei cui furu wei maskformer from meta and uiuc released with the paper per pixel classification is not all you need for semantic segmentation by bowen cheng alexander g schwing alexander kirillov mbart from facebook released with the paper multilingual denoising pre training for neural machine translation by yinhan liu jiatao gu naman goyal xian li sergey edunov marjan ghazvininejad mike lewis luke zettlemoyer mbart from facebook released with the paper multilingual translation with extensible multilingual pretraining and finetuning by yuqing tang chau tran xian li peng jen chen naman goyal vishrav chaudhary jiatao gu angela fan megatron bert from nvidia released with the paper megatron lm training multi billion parameter language model using model parallelism by mohammad shoeybi mostofa patwary raul puri patrick legresley jared casper and bryan catanzaro megatron gpt from nvidia released with the paper megatron lm training multi billion parameter language model using model parallelism by mohammad shoeybi mostofa patwary raul puri patrick legresley jared casper and bryan catanzaro mluke from studio ousia released with the paper mluke the power of entity representation in multilingual pretrained language model by ryokan ri ikuya yamada and yoshimasa tsuruoka mobilebert from cmu google brain released with the paper mobilebert a compact task agnostic bert for resource limited device by zhiqing sun hongkun yu xiaodan song renjie liu yiming yang and denny zhou mobilevit from apple released with the paper mobilevit light weight general purpose and mobile friendly vision transformer by sachin mehta and mohammad rastegari mpnet from microsoft research released with the paper mpnet masked and permuted pre training for language understanding by kaitao song xu tan tao qin jianfeng lu tie yan liu mt from google ai released with the paper mt a massively multilingual pre trained text to text transformer by linting xue noah constant adam robert mihir kale ramus al rfou aditya siddhant aditya barua colin raffel mvp from ruc ai box released with the paper mvp multi task supervised pre training for natural language generation by tianyi tang junyi li wayne xin zhao and ji rong wen nezha from huawei noah s ark lab released with the paper nezha neural contextualized representation for chinese language understanding by junqiu wei xiaozhe ren xiaoguang li wenyong huang yi liao yasheng wang jiashu lin xin jiang xiao chen and qun liu nllb from meta released with the paper no language left behind scaling human centered machine translation by the nllb team nystr mformer from the university of wisconsin madison released with the paper nystr mformer a nystr m based algorithm for approximating self attention by yunyang xiong zhanpeng zeng rudrasis chakraborty mingxing tan glenn fung yin li vikas singh opt from meta ai released with the paper opt open pre trained transformer language model by susan zhang stephen roller naman goyal mikel artetxe moya chen shuohui chen et al owl vit from google ai released with the paper simple open vocabulary object detection with vision transformer by matthias minderer alexey gritsenko austin stone maxim neumann dirk weissenborn alexey dosovitskiy aravindh mahendran anurag arnab mostafa dehghani zhuoran shen xiao wang xiaohua zhai thomas kipf and neil houlsby pegasus from google released with the paper pegasus pre training with extracted gap sentence for abstractive summarization by jingqing zhang yao zhao mohammad saleh and peter j liu pegasus x from google released with the paper investigating efficiently extending transformer for long input summarization by jason phang yao zhao and peter j liu perceiver io from deepmind released with the paper perceiver io a general architecture for structured input output by andrew jaegle sebastian borgeaud jean baptiste alayrac carl doersch catalin ionescu david ding skanda koppula daniel zoran andrew brock evan shelhamer olivier h naff matthew m botvinick andrew zisserman oriol vinyals jo o carreira phobert from vinai research released with the paper phobert pre trained language model for vietnamese by dat quoc nguyen and anh tuan nguyen plbart from ucla nlp released with the paper unified pre training for program understanding and generation by wasi uddin ahmad saikat chakraborty baishakhi ray kai wei chang poolformer from sea ai lab released with the paper metaformer is actually what you need for vision by yu weihao and luo mi and zhou pan and si chenyang and zhou yichen and wang xinchao and feng jiashi and yan shuicheng prophetnet from microsoft research released with the paper prophetnet predicting future n gram for sequence to sequence pre training by yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming zhou qdqbert from nvidia released with the paper integer quantization for deep learning inference principle and empirical evaluation by hao wu patrick judd xiaojie zhang mikhail isaev and paulius micikevicius rag from facebook released with the paper retrieval augmented generation for knowledge intensive nlp task by patrick lewis ethan perez aleksandara piktus fabio petroni vladimir karpukhin naman goyal heinrich k ttler mike lewis wen tau yih tim rockt schel sebastian riedel douwe kiela realm from google research released with the paper realm retrieval augmented language model pre training by kelvin guu kenton lee zora tung panupong pasupat and ming wei chang reformer from google research released with the paper reformer the efficient transformer by nikita kitaev ukasz kaiser anselm levskaya regnet from meta platform released with the paper designing network design space by ilija radosavovic raj prateek kosaraju ross girshick kaiming he piotr doll r rembert from google research released with the paper rethinking embedding coupling in pre trained language model by hyung won chung thibault f vry henry tsai m johnson sebastian ruder resnet from microsoft research released with the paper deep residual learning for image recognition by kaiming he xiangyu zhang shaoqing ren jian sun roberta from facebook released together with the paper roberta a robustly optimized bert pretraining approach by yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roformer from zhuiyitechnology released together with the paper roformer enhanced transformer with rotary position embedding by jianlin su and yu lu and shengfeng pan and bo wen and yunfeng liu segformer from nvidia released with the paper segformer simple and efficient design for semantic segmentation with transformer by enze xie wenhai wang zhiding yu anima anandkumar jose m alvarez ping luo sew from asapp released with the paper performance efficiency trade offs in unsupervised pre training for speech recognition by felix wu kwangyoun kim jing pan kyu han kilian q weinberger yoav artzi sew d from asapp released with the paper performance efficiency trade offs in unsupervised pre training for speech recognition by felix wu kwangyoun kim jing pan kyu han kilian q weinberger yoav artzi speechtotexttransformer from facebook released together with the paper fairseq s t fast speech to text modeling with fairseq by changhan wang yun tang xutai ma anne wu dmytro okhonko juan pino speechtotexttransformer from facebook released together with the paper large scale self and semi supervised learning for speech translation by changhan wang anne wu juan pino alexei baevski michael auli alexis conneau splinter from tel aviv university released together with the paper few shot question answering by pretraining span selection by ori ram yuval kirstain jonathan berant amir globerson omer levy squeezebert from berkeley released with the paper squeezebert what can computer vision teach nlp about efficient neural network by forrest n iandola albert e shaw ravi krishna and kurt w keutzer swin transformer from microsoft released with the paper swin transformer hierarchical vision transformer using shifted window by ze liu yutong lin yue cao han hu yixuan wei zheng zhang stephen lin baining guo swin transformer v from microsoft released with the paper swin transformer v scaling up capacity and resolution by ze liu han hu yutong lin zhuliang yao zhenda xie yixuan wei jia ning yue cao zheng zhang li dong furu wei baining guo t from google ai released with the paper exploring the limit of transfer learning with a unified text to text transformer by colin raffel and noam shazeer and adam robert and katherine lee and sharan narang and michael matena and yanqi zhou and wei li and peter j liu t v from google ai released in the repository google research text to text transfer transformer by colin raffel and noam shazeer and adam robert and katherine lee and sharan narang and michael matena and yanqi zhou and wei li and peter j liu table transformer from microsoft research released with the paper pubtables m towards comprehensive table extraction from unstructured document by brandon smock rohith pesala robin abraham tapa from google ai released with the paper tapa weakly supervised table parsing via pre training by jonathan herzig pawe krzysztof nowak thomas m ller francesco piccinno and julian martin eisenschlos tapex from microsoft research released with the paper tapex table pre training via learning a neural sql executor by qian liu bei chen jiaqi guo morteza ziyadi zeqi lin weizhu chen jian guang lou time series transformer from huggingface trajectory transformer from the university of california at berkeley released with the paper offline reinforcement learning a one big sequence modeling problem by michael janner qiyang li sergey levine transformer xl from google cmu released with the paper transformer xl attentive language model beyond a fixed length context by zihang dai zhilin yang yiming yang jaime carbonell quoc v le ruslan salakhutdinov trocr from microsoft released together with the paper trocr transformer based optical character recognition with pre trained model by minghao li tengchao lv lei cui yijuan lu dinei florencio cha zhang zhoujun li furu wei ul from google research released with the paper unifying language learning paradigm by yi tay mostafa dehghani vinh q tran xavier garcia dara bahri tal schuster huaixiu steven zheng neil houlsby donald metzler unispeech from microsoft research released with the paper unispeech unified speech representation learning with labeled and unlabeled data by chengyi wang yu wu yao qian kenichi kumatani shujie liu furu wei michael zeng xuedong huang unispeechsat from microsoft research released with the paper unispeech sat universal speech representation learning with speaker aware pre training by sanyuan chen yu wu chengyi wang zhengyang chen zhuo chen shujie liu jian wu yao qian furu wei jinyu li xiangzhan yu van from tsinghua university and nankai university released with the paper visual attention network by meng hao guo cheng ze lu zheng ning liu ming ming cheng shi min hu videomae from multimedia computing group nanjing university released with the paper videomae masked autoencoders are data efficient learner for self supervised video pre training by zhan tong yibing song jue wang limin wang vilt from naver ai lab kakao enterprise kakao brain released with the paper vilt vision and language transformer without convolution or region supervision by wonjae kim bokyung son ildoo kim vision transformer vit from google ai released with the paper an image is worth x word transformer for image recognition at scale by alexey dosovitskiy lucas beyer alexander kolesnikov dirk weissenborn xiaohua zhai thomas unterthiner mostafa dehghani matthias minderer georg heigold sylvain gelly jakob uszkoreit neil houlsby visualbert from ucla nlp released with the paper visualbert a simple and performant baseline for vision and language by liunian harold li mark yatskar da yin cho jui hsieh kai wei chang vitmae from meta ai released with the paper masked autoencoders are scalable vision learner by kaiming he xinlei chen saining xie yanghao li piotr doll r ross girshick vitmsn from meta ai released with the paper masked siamese network for label efficient learning by mahmoud assran mathilde caron ishan misra piotr bojanowski florian bordes pascal vincent armand joulin michael rabbat nicolas ballas wav vec from facebook ai released with the paper wav vec a framework for self supervised learning of speech representation by alexei baevski henry zhou abdelrahman mohamed michael auli wav vec conformer from facebook ai released with the paper fairseq s t fast speech to text modeling with fairseq by changhan wang yun tang xutai ma anne wu sravya popuri dmytro okhonko juan pino wav vec phoneme from facebook ai released with the paper simple and effective zero shot cross lingual phoneme recognition by qiantong xu alexei baevski michael auli wavlm from microsoft research released with the paper wavlm large scale self supervised pre training for full stack speech processing by sanyuan chen chengyi wang zhengyang chen yu wu shujie liu zhuo chen jinyu li naoyuki kanda takuya yoshioka xiong xiao jian wu long zhou shuo ren yanmin qian yao qian jian wu michael zeng furu wei whisper from openai released with the paper robust speech recognition via large scale weak supervision by alec radford jong wook kim tao xu greg brockman christine mcleavey ilya sutskever x clip from microsoft research released with the paper expanding language image pretrained model for general video recognition by bolin ni houwen peng minghao chen songyang zhang gaofeng meng jianlong fu shiming xiang haibin ling xglm from facebook ai released with the paper few shot learning with multilingual language model by xi victoria lin todor mihaylov mikel artetxe tianlu wang shuohui chen daniel simig myle ott naman goyal shruti bhosale jingfei du ramakanth pasunuru sam shleifer punit singh koura vishrav chaudhary brian o horo jeff wang luke zettlemoyer zornitsa kozareva mona diab veselin stoyanov xian li xlm from facebook released together with the paper cross lingual language model pretraining by guillaume lample and alexis conneau xlm prophetnet from microsoft research released with the paper prophetnet predicting future n gram for sequence to sequence pre training by yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming zhou xlm roberta from facebook ai released together with the paper unsupervised cross lingual representation learning at scale by alexis conneau kartikay khandelwal naman goyal vishrav chaudhary guillaume wenzek francisco guzm n edouard grave myle ott luke zettlemoyer and veselin stoyanov xlm roberta xl from facebook ai released together with the paper larger scale transformer for multilingual masked language modeling by naman goyal jingfei du myle ott giri anantharaman alexis conneau xlnet from google cmu released with the paper xlnet generalized autoregressive pretraining for language understanding by zhilin yang zihang dai yiming yang jaime carbonell ruslan salakhutdinov quoc v le xl r from facebook ai released with the paper xl r self supervised cross lingual speech representation learning at scale by arun babu changhan wang andros tjandra kushal lakhotia qiantong xu naman goyal kritika singh patrick von platen yatharth saraf juan pino alexei baevski alexis conneau michael auli xlsr wav vec from facebook ai released with the paper unsupervised cross lingual representation learning for speech recognition by alexis conneau alexei baevski ronan collobert abdelrahman mohamed michael auli yolos from huazhong university of science technology released with the paper you only look at one sequence rethinking transformer in vision through object detection by yuxin fang bencheng liao xinggang wang jiemin fang jiyang qi rui wu jianwei niu wenyu liu yoso from the university of wisconsin madison released with the paper you only sample almost once linear cost self attention via bernoulli sampling by zhanpeng zeng yunyang xiong sathya n ravi shailesh acharya glenn fung vikas singh want to contribute a new model we have added a detailed guide and template to guide you in the process of adding a new model you can find them in the template folder of the repository be sure to check the contributing guideline and contact the maintainer or open an issue to collect feedback before starting your pr to check if each model ha an implementation in flax pytorch or tensorflow or ha an associated tokenizer backed by the tokenizers library refer to this table these implementation have been tested on several datasets see the example script and should match the performance of the original implementation you can find more detail on performance in the example section of the documentation learn more section description documentation full api documentation and tutorial task summary task supported by transformer preprocessing tutorial using the tokenizer class to prepare data for the model training and fine tuning using the model provided by transformer in a pytorch tensorflow training loop and the trainer api quick tour fine tuning usage script example script for fine tuning model on a wide range of task model sharing and uploading upload and share your fine tuned model with the community migration migrate to transformer from pytorch transformer or pytorch pretrained bert citation we now have a paper you can cite for the transformer library inproceedings wolf etal transformer title transformer state of the art natural language processing author thomas wolf and lysandre debut and victor sanh and julien chaumond and clement delangue and anthony moi and pierric cistac and tim rault and r mi louf and morgan funtowicz and joe davison and sam shleifer and patrick von platen and clara ma and yacine jernite and julien plu and canwen xu and teven le scao and sylvain gugger and mariama drame and quentin lhoest and alexander m rush booktitle proceeding of the conference on empirical method in natural language processing system demonstration month oct year address online publisher association for computational linguistics url http www aclweb org anthology emnlp demo page english espa ol state of the art machine learning for jax pytorch and tensorflow transformer provides thousand of pretrained model to perform task on different modality such a text vision and audio these model can be applied on transformer model can also perform task on several modality combined such a table question answering optical character recognition information extraction from scanned document video classification and visual question answering transformer provides apis to quickly download and use those pretrained model on a given text fine tune them on your own datasets and then share them with the community on our model hub at the same time each python module defining an architecture is fully standalone and can be modified to enable quick research experiment transformer is backed by the three most popular deep learning library jax pytorch and tensorflow with a seamless integration between them it s straightforward to train your model with one before loading them for inference with the other you can test most of our model directly on their page from the model hub we also offer private model hosting versioning an inference api for public and private model here are a few example in natural language processing in computer vision in audio in multimodal task write with transformer built by the hugging face team is the official demo of this repo s text generation capability to immediately use a model on a given input text image audio we provide the pipeline api pipeline group together a pretrained model with the preprocessing that wa used during that model s training here is how to quickly use a pipeline to classify positive versus negative text the second line of code downloads and cache the pretrained model used by the pipeline while the third evaluates it on the given text here the answer is positive with a confidence of many task have a pre trained pipeline ready to go in nlp but also in computer vision and speech for example we can easily extract detected object in an image here we get a list of object detected in the image with a box surrounding the object and a confidence score here is the original image on the left with the prediction displayed on the right you can learn more about the task supported by the pipeline api in this tutorial in addition to pipeline to download and use any of the pretrained model on your given task all it take is three line of code here is the pytorch version and here is the equivalent code for tensorflow the tokenizer is responsible for all the preprocessing the pretrained model expects and can be called directly on a single string a in the above example or a list it will output a dictionary that you can use in downstream code or simply directly pas to your model using the argument unpacking operator the model itself is a regular pytorch nn module or a tensorflow tf kera model depending on your backend which you can use a usual this tutorial explains how to integrate such a model into a classic pytorch or tensorflow training loop or how to use our trainer api to quickly fine tune on a new dataset easy to use state of the art model lower compute cost smaller carbon footprint choose the right framework for every part of a model s lifetime easily customize a model or an example to your need this repository is tested on python flax pytorch and tensorflow you should install transformer in a virtual environment if you re unfamiliar with python virtual environment check out the user guide first create a virtual environment with the version of python you re going to use and activate it then you will need to install at least one of flax pytorch or tensorflow please refer to tensorflow installation page pytorch installation page and or flax and jax installation page regarding the specific installation command for your platform when one of those backends ha been installed transformer can be installed using pip a follows if you d like to play with the example or need the bleeding edge of the code and can t wait for a new release you must install the library from source since transformer version v we now have a conda channel huggingface transformer can be installed using conda a follows follow the installation page of flax pytorch or tensorflow to see how to install them with conda note on window you may be prompted to activate developer mode in order to benefit from caching if this is not an option for you please let u know in this issue all the model checkpoint provided by transformer are seamlessly integrated from the huggingface co model hub where they are uploaded directly by user and organization current number of checkpoint transformer currently provides the following architecture see here for a high level summary of each them to check if each model ha an implementation in flax pytorch or tensorflow or ha an associated tokenizer backed by the tokenizers library refer to this table these implementation have been tested on several datasets see the example script and should match the performance of the original implementation you can find more detail on performance in the example section of the documentation we now have a paper you can cite for the transformer library,"[('popular deep learning library jax pytorch', 0.5737), ('transformer language model', 0.4801), ('language transformer', 0.4753), ('training text', 0.4731), ('training api', 0.468), ('tensorflow', 0.4649), ('transformer protein language model esm b', 0.4539), ('extensible multilingual pretraining', 0.4495), ('model training', 0.4445), ('tensorflow transformer', 0.4439)]","[-1.33851737e-01 -1.52286977e-01  1.61171425e-02 -4.92900088e-02
  4.84740734e-02  2.26938557e-02 -8.39477330e-02  4.38576192e-02
 -3.39150019e-02 -1.21650718e-01 -1.93961372e-03 -1.10526718e-02
 -3.22300047e-02  7.00161001e-03  4.02434953e-02  8.84063542e-03
 -6.62627770e-03  1.30146742e-01 -7.52988085e-02 -1.39025375e-01
  2.89587621e-02  2.52524670e-02  2.65908502e-02 -4.06464469e-03
  2.40699220e-02 -3.82418022e-03  4.76387963e-02 -8.65088850e-02
  3.34743336e-02 -3.01123299e-02 -5.22396602e-02  2.29015332e-02
 -9.32567846e-03  5.61961420e-02 -6.95628822e-02  5.35250753e-02
 -6.19689077e-02 -1.10420294e-01  1.00766262e-02  2.36685108e-03
 -8.44183117e-02  5.44609223e-03  3.34967412e-02 -5.73785752e-02
  2.73720268e-02  2.61245761e-03  5.48956692e-02 -4.30468544e-02
 -2.27435250e-02  5.49468352e-03 -4.83559072e-02 -9.78561863e-02
 -1.07771745e-02  8.84145945e-02 -4.31760252e-02 -5.04299812e-02
  1.08096073e-03  2.78550405e-02 -1.79660190e-02 -4.23460864e-02
 -2.32532769e-02 -2.81155072e-02  9.99091193e-03  2.36966740e-02
 -9.18034986e-02  6.25292286e-02  2.68840436e-02  5.05820476e-02
  9.22451690e-02 -1.25955135e-01 -5.67755438e-02  1.82069349e-03
 -8.37000646e-03  8.10716003e-02 -2.48707691e-03 -1.63938105e-02
  9.69737172e-02 -3.82764591e-03  8.86000618e-02 -8.72121379e-02
  1.87264774e-02  1.95066165e-02  3.58036831e-02 -4.87434864e-03
  7.97712356e-02  1.04325684e-02 -4.14508320e-02  6.99703470e-02
  6.85983850e-03  1.12111578e-02 -5.78997331e-03 -1.01803944e-01
  2.10616216e-02  1.93872433e-02 -3.29635292e-02  7.18196929e-02
 -1.80125870e-02 -2.01702379e-02 -8.20450410e-02  3.95530201e-02
 -2.90276557e-02  9.14198905e-03  8.16133469e-02  4.09588926e-02
 -3.87274325e-02 -3.52456458e-02  1.78043055e-03  8.06412697e-02
  2.64865924e-02  2.53858380e-02  7.86477551e-02  1.08351260e-02
 -3.41596245e-03 -8.34899619e-02  8.69843811e-02  1.96407307e-02
  3.57850008e-02 -4.17130440e-02  6.91199116e-03  8.84945095e-02
 -9.88675132e-02  4.05861698e-02  4.36039455e-03  1.56255178e-02
 -2.74505671e-02  1.25848977e-02 -6.03954941e-02  6.72461013e-33
  4.22030129e-02  1.09836794e-02  5.44104911e-02  1.77750934e-03
  4.90627326e-02 -1.14283986e-01  8.56164843e-02  6.50193170e-03
 -4.31089550e-02 -1.00240186e-01 -5.04306471e-03  1.23333015e-01
 -3.03973686e-02  9.01184604e-02 -5.18133081e-02 -6.27504885e-02
 -1.15739356e-03  2.11220514e-02  5.11569045e-02  5.20475507e-02
  7.54209757e-02  3.07578780e-02  2.05584113e-02  4.53992151e-02
 -6.35980219e-02  2.99218819e-02  1.86212528e-02 -5.54194003e-02
 -3.05779167e-02 -3.73051502e-03 -1.10154584e-01 -7.76734203e-02
 -4.10370380e-02  8.03654827e-03  3.73760760e-02 -3.43559049e-02
 -4.02104445e-02 -3.61472964e-02  3.93699715e-03 -7.07829297e-02
 -1.68520566e-02  2.86330692e-02 -4.23710197e-02  5.19611500e-03
 -6.77798735e-03 -5.48730269e-02 -1.03993071e-02  9.38090496e-03
  1.11428760e-02 -1.03694782e-01  1.55389600e-03 -3.28008756e-02
 -6.56771064e-02  4.04749298e-03  7.20553473e-02  2.79596839e-02
  9.06905979e-02  6.97296038e-02  9.16875340e-03  5.02053462e-02
 -1.98962670e-02  1.10384049e-02  2.37996858e-02  6.95145130e-03
  5.78743555e-02 -4.59428132e-02 -7.85320252e-02 -3.43123004e-02
 -1.26124825e-02 -1.24003750e-03 -4.80344929e-02  1.10796494e-02
  3.98757569e-02 -8.04447511e-04 -1.66087970e-02 -1.16303349e-02
  4.67983931e-02 -7.24066794e-02 -2.32319639e-04  7.88859203e-02
 -5.49030602e-02  2.44873408e-02  5.71455583e-02  8.63822270e-03
 -4.25617434e-02  3.53007056e-02  2.82408521e-02 -4.66014929e-02
  9.01741236e-02  9.07949216e-05 -2.30990387e-02  7.92893488e-03
  2.90759150e-02  2.44970229e-02  3.16782971e-03 -4.54220752e-33
 -3.95325795e-02  3.92426886e-02 -6.66232407e-02  7.97485188e-02
  1.90977417e-02  6.59114402e-03  9.02477093e-03  4.58133295e-02
 -1.84578206e-02 -1.05236582e-02  5.44740073e-02 -4.89431322e-02
  7.06184581e-02  2.55188067e-02  4.66495492e-02 -4.39056642e-02
 -6.08452037e-03 -5.91153838e-02  1.46612925e-02  2.40760110e-02
 -4.62227538e-02  1.08179674e-01 -8.68332237e-02 -1.51408175e-02
 -4.56156768e-02 -2.49588527e-02 -1.18132874e-01  3.28537114e-02
 -4.09141183e-02  3.78305912e-02  2.16844361e-02 -6.00011386e-02
  4.64676879e-02  1.17813786e-02 -4.26345840e-02 -1.41758211e-02
  3.33414786e-03  2.56532785e-02  2.45433487e-02 -1.10042654e-02
  1.11499287e-01 -3.24449986e-02  2.17826018e-05  1.80575978e-02
 -5.94493188e-02 -2.44638859e-03 -1.22959770e-01  7.43280677e-03
 -2.01587975e-02 -2.40647793e-02 -1.16097024e-02 -4.53558192e-03
 -1.79982614e-02 -4.58520874e-02 -2.40847245e-02 -3.34391221e-02
  5.79972491e-02 -1.64021198e-02  5.03752120e-02  2.40415875e-02
 -8.31857100e-02 -1.38623975e-02  4.10669371e-02  1.26469173e-02
  1.22518605e-02 -3.32828835e-02 -5.76810762e-02  4.04309779e-02
 -1.02975024e-02 -3.62812728e-02  6.06884286e-02  6.24198504e-02
  3.65119502e-02  9.90270674e-02  4.22116853e-02 -2.31039003e-02
 -4.71162330e-03  3.90655473e-02 -1.23020425e-03 -4.65040579e-02
  9.53652058e-03  2.53587477e-02 -2.72359066e-02  8.02960321e-02
  1.06008105e-01  9.40758139e-02  7.58772418e-02  2.51182467e-02
  1.63180251e-02 -4.17598821e-02 -6.98141614e-03  5.22020273e-02
  3.48690082e-03  6.27139211e-02 -4.63927499e-05 -2.91505025e-08
 -9.92516428e-02  4.60637137e-02  2.01915647e-03 -1.95989348e-02
  3.16596637e-03 -5.60247758e-03  1.94907039e-02  1.15648374e-01
  4.30438947e-03  6.66796118e-02 -1.91435087e-02 -1.42673049e-02
 -7.29501918e-02 -1.32482443e-02  2.25291564e-03  9.86367613e-02
  4.09535579e-02  5.79507574e-02  3.26832682e-02 -4.52145971e-02
  7.63815343e-02  6.62783682e-02  2.57196836e-02 -1.04433307e-02
  1.63011532e-02 -1.06148869e-02  1.25100268e-02  3.26416641e-02
  5.04300697e-03 -6.41856492e-02 -1.03502549e-01  4.91198851e-03
  3.60128507e-02 -7.35118017e-02  6.16058446e-02  1.13801226e-01
  2.44332124e-02 -4.34949957e-02 -5.52985780e-02  5.34794740e-02
  2.78836638e-02  6.93619624e-02 -5.08897267e-02 -6.92301104e-03
  2.39892304e-02  1.71729736e-02 -4.47518453e-02 -9.35851857e-02
  7.93496729e-04  1.75858624e-02 -1.09328106e-02  1.06340246e-02
  3.12897228e-02  4.08907756e-02 -1.29666703e-03  7.38568157e-02
 -5.67710027e-02 -1.05454825e-01  3.63622140e-03  3.94729227e-02
 -2.02813763e-02  5.42075112e-02 -3.20970500e-03  5.24261966e-02]",2,2
tensorboardx,31,enforce protobuf s version upper boundfix deprecation warningscomet integration improvement fix a comet plugin bug if writer is reused remove a dependency issue support logging to comet ml simutaneously support for type hint dropped python support bug fix see the commit log in github global summarywriter that mimic python s default logger class concurrent write is supported x speed up for add audio please install the soundfile package for this feature support jax tensor the add graph function is delegated to the one in torch utils tensorboard bug fix see the commit log in github now you can tag hparams trial with custom name instead of the default epoch timefixed a bug that add hparams are rendered incorrectly with non string valuessupports logging to amazon s or google cloud storagebug fix and error message for add embedding functiondraw openvino format with add openvino graphuse new jit backend for pytorch this work better with pytorch and support hparams pluginadd embedding now support numpy array inputdraw label text on image with bounding box provided crc c speed up optional by installing crc c manually rewrite add graph onnx backend is replaced by jit to support more advanced structure now you can add mesh to visualize colorful point cloud or mesh able to write to s fixed raw histogram issue that nothing is shown in tensorboardusers can use various image video dimension permutation by passing dataformats parameter you can bybass the writer by passing write to disk true to summarywritermany graph related bug is fixed in this version new function add image this function accepts d iamge tensor see documentation make add image with box usable api change add video now accepts bxtxcxhxw instead of bxcxtxhxw tensor add api for custom scalaradd support for logging directly to s add support for caffe graphpytorch jit graph support alpha release made add text compatible with tensorboard fix the issue of strange histogram if default binning method is usedsupports passing matplotlib figure to add image resolve namespace confliction with tf tensorboardadd image box functionsupports custom timestamp for eventsupports tensorshape information in graph visualization drop support for add add video functionsupports pytorch hacky support graph the pretty one support markdown for add text functionit s ready to log precision recall curve need tensorboard add context manager for the summarywriter classpackage name renamed to tensorboardx to fix namespace confliction with tensorflow s tensorboardsupports multi scalar and json exportmultiple embeddings in one experimentsupports chainer and mxnetremove tensorflow dependency for embedding functionfixed incorrect image label pairing in embedding function unifies api call and add docstring documentation is available at http tensorboard pytorch readthedocs io add travis test py py add support for python in pypi support embeddingsupports graph summaryfixed np histogram issuesupports text summarysupports audio summarysimplifies add image apispeed up add histogram api by xfirst commit reference http github com teamhg memex tensorboard logger http github com dmlc tensorboard,"[('video functionsupports pytorch hacky support', 0.4243), ('protobuf', 0.4148), ('http tensorboard pytorch readthedocs io', 0.4057), ('python support bug fix', 0.376), ('pypi support embeddingsupports graph', 0.364), ('custom scalaradd support', 0.3636), ('tensorflow dependency', 0.357), ('openvino graphuse', 0.3312), ('eventsupports tensorshape information', 0.3275), ('usable api change', 0.3259)]","[-9.15079713e-02 -9.21121463e-02 -6.26801467e-03 -5.31560071e-02
  6.47803321e-02  2.30566859e-02 -7.35025257e-02  7.61790574e-03
 -9.76306051e-02 -3.34094055e-02 -3.79473716e-02 -6.16165856e-03
 -1.43966749e-01  6.22765049e-02  6.19387664e-02  3.33516076e-02
  4.90456857e-02  7.38794357e-02 -5.44374660e-02 -6.48719072e-02
 -6.65670112e-02 -8.49763751e-02  1.49093159e-02  4.16556926e-04
  1.25475433e-02 -4.83731478e-02 -9.98938642e-03 -1.72200166e-02
  4.52778302e-02 -3.11707016e-02  2.13378649e-02  4.11961265e-02
  6.09929767e-03 -1.36785312e-02 -2.80339969e-04  1.25628896e-03
 -3.28679532e-02 -9.07492861e-02 -4.26363051e-02 -1.25028472e-02
  2.25739628e-02 -2.67459592e-03 -3.18508781e-02 -1.43272579e-02
  2.84254388e-03  4.93485713e-04  6.52786866e-02 -6.31257817e-02
  3.99358459e-02 -3.24299447e-02 -7.50936940e-02 -4.10075374e-02
 -5.15537895e-02 -1.74527466e-02 -5.91137446e-03 -2.99945530e-02
  3.98393795e-02 -2.68449355e-02  6.57804683e-02 -2.15554424e-02
  4.27553877e-02 -9.26566310e-03  5.19163674e-03  6.52245060e-02
 -1.21918367e-02  5.01485802e-02  5.66384383e-03 -6.94656372e-03
  9.26754326e-02 -6.50752783e-02 -5.78520633e-02  1.32657224e-02
 -1.10311732e-01  8.55330676e-02  1.75514538e-02  6.56527514e-03
  5.62664941e-02 -7.53094675e-03  3.77373062e-02 -9.20417830e-02
  2.46197842e-02 -2.46107653e-02 -9.37683415e-03  2.25162748e-02
  8.57296810e-02  1.60403512e-02 -7.33276680e-02  6.35670647e-02
  1.88705139e-02 -4.30453680e-02  2.52453946e-02  1.59037169e-02
  3.06461994e-02  2.97645982e-02  8.64916854e-03  2.39030495e-02
  3.32035087e-02 -5.47947958e-02 -6.02609478e-02  4.77172621e-02
 -6.53197393e-02 -6.30853325e-02  4.70013134e-02  5.23585677e-02
 -1.87606048e-02  5.69614954e-02  1.76609457e-02  6.99328110e-02
  1.20483777e-02  2.56691687e-02 -5.67544252e-03  4.00230959e-02
 -1.47873368e-02 -8.86984468e-02  4.55035456e-02  7.54024833e-02
 -5.11747897e-02  4.24596779e-02  5.63873984e-02  4.91879880e-02
 -2.44622789e-02  6.46636263e-02  1.71597197e-03  4.55572717e-02
 -3.98897268e-02  1.83368847e-02 -4.88144048e-02  1.03564204e-32
  2.13470124e-02 -2.79516913e-02 -1.29542854e-02 -1.70916729e-02
  9.67461839e-02 -5.32383583e-02  2.23596729e-02 -1.83149222e-02
 -7.95497466e-03 -9.92855355e-02 -1.56805124e-02  1.77175198e-02
 -1.05625559e-02  7.23456964e-02  1.73705840e-03 -8.24226588e-02
 -3.51521708e-02  3.60832475e-02  5.71159311e-02  7.72661790e-02
  4.59544100e-02 -3.10220979e-02 -2.44432944e-03  8.28178301e-02
 -2.59647854e-02  7.23043606e-02 -1.46006458e-02  9.99220461e-03
  3.24916504e-02  1.79015268e-02 -6.08506054e-02 -3.25360298e-02
 -2.78671030e-02 -2.18149908e-02 -3.19378860e-02 -5.80297336e-02
 -9.23190042e-02 -8.64985660e-02 -9.65199545e-02 -7.68332407e-02
 -7.64303654e-02  5.83914295e-02 -1.03206135e-01 -7.29237124e-02
 -1.24263968e-02 -2.36653220e-02  3.68352747e-03  6.95739686e-02
  1.21165626e-02 -5.37877455e-02 -6.62879571e-02  1.16888620e-02
 -1.16299409e-02  3.70360352e-03  2.31943671e-02 -3.50798629e-02
  8.01318288e-02  3.14672254e-02  6.23802990e-02  3.63168796e-03
  3.20644975e-02  4.70438376e-02  2.86243111e-02  1.14633795e-02
 -4.81231213e-02  1.24836545e-02 -2.10982021e-02 -1.05344160e-02
 -6.69938372e-03  8.21596161e-02 -9.21345726e-02  1.37187451e-01
 -4.92826551e-02 -1.53506044e-02 -1.69900514e-03 -2.57898234e-02
 -3.27751264e-02 -1.10596314e-01  6.08807337e-03  2.35042162e-02
 -7.18367398e-02 -3.65596525e-02  1.09868072e-01  9.44331009e-03
 -5.75362109e-02 -5.30819222e-02  9.71514825e-03  2.32160147e-02
  2.81725843e-02 -2.73418222e-02 -3.11373565e-02 -3.81512754e-02
  6.11993447e-02  2.07915176e-02  1.38681140e-02 -8.19697998e-33
 -2.71977447e-02  4.74514365e-02 -2.89874664e-03  9.70815048e-02
  3.32450098e-03  2.28957534e-02  4.44734097e-02 -1.27685824e-02
  7.91540444e-02 -2.38446868e-03 -4.91738180e-03 -7.54005313e-02
 -1.50702400e-02 -6.11808300e-02  1.78797208e-02  1.01266860e-03
 -8.38704780e-02 -3.31521705e-02 -8.35675746e-03 -2.26337966e-02
 -7.07728192e-02  5.65589368e-02 -4.43735719e-02 -5.03773056e-02
 -5.33847255e-04  1.08293863e-02 -2.62707565e-02 -4.69246022e-02
  5.93624916e-03 -3.61700803e-02  4.61888164e-02  3.61386104e-03
 -5.54875210e-02  3.19435745e-02  4.12988849e-02  4.74354438e-02
  8.80974084e-02  1.82132050e-02 -4.36381996e-02  2.88623143e-02
  1.56070605e-01  9.10994783e-02 -4.59156148e-02  3.49715315e-02
  1.75685494e-03  4.44194004e-02 -1.20840207e-01  4.63349000e-02
 -4.98078689e-02 -5.04841581e-02 -4.03276756e-02 -6.24787342e-03
  2.57204268e-02 -4.61190157e-02 -1.07155368e-02  1.49492407e-02
  1.43789411e-01  8.33000094e-02  1.10191125e-02  1.06517235e-02
 -4.24810052e-02 -5.27704209e-02 -3.51062641e-02 -2.72509679e-02
  1.14602465e-02  3.08698695e-02 -8.53398442e-02  6.64733723e-02
 -4.62948121e-02  9.74315684e-03  7.92349726e-02 -2.17504557e-02
  7.89060257e-03  3.81181799e-02  1.67633351e-02  9.21149850e-02
  2.97764093e-02  5.04994169e-02  5.65745011e-02  4.55659628e-02
 -3.23705841e-03 -2.64852587e-02  6.26101419e-02  7.30718002e-02
  6.22816384e-02  8.46053138e-02  6.50509521e-02  5.96649982e-02
  6.12429604e-02 -3.96180749e-02 -2.60369089e-02  2.59353742e-02
 -2.14485582e-02  1.24669053e-01  9.57772136e-02 -3.94107005e-08
 -6.38523027e-02  5.40012941e-02 -4.86593582e-02  2.31034905e-02
 -1.32127823e-02  1.52503997e-02  7.63486996e-02  5.81561215e-02
  3.13937217e-02  2.16867477e-02 -6.24210015e-02 -4.39770743e-02
 -3.50343660e-02  3.15717258e-03  4.32563946e-02  4.91098203e-02
 -4.29443382e-02  7.43571892e-02 -6.77225832e-03 -1.31350467e-02
  6.75013568e-03  1.95043329e-02  4.04577097e-03 -4.39419821e-02
 -1.17614977e-01 -6.96815252e-02  3.76955830e-02  7.62915090e-02
 -7.23838457e-04 -3.44713107e-02 -5.02265319e-02 -1.88910924e-02
  6.29436225e-02 -7.57893324e-02  7.85890445e-02  7.05210194e-02
 -3.82893570e-02 -4.97013070e-02 -5.83253932e-05  1.18823439e-01
 -9.50171351e-02  9.25182656e-04  3.78613477e-03 -5.92299663e-02
  4.80231345e-02  5.34953885e-02 -2.73157153e-02 -6.22582622e-02
 -4.21001017e-02  5.04773781e-02 -1.47096766e-02  1.31089687e-02
 -5.12254164e-02  2.10148711e-02  3.11325490e-02  9.23152193e-02
 -5.40818013e-02 -8.28680769e-02 -2.51297001e-02 -5.36346510e-02
  2.74788458e-02 -3.87108279e-03  2.54636258e-02 -2.43445113e-02]",2,2
pytorch-lightning,29,the lightweight pytorch wrapper for high performance ai research scale your model not the boilerplate website key feature how to use doc example community lightning ai license lightning disentangles pytorch code to decouple the science from the engineering lightning structure pytorch code with these principle lightning force the following structure to your code which make it reusable and shareable once you do this you can train on multiple gpus tpus cpu ipus hpus and even in bit precision without changing your code get started in just minuteslightning is rigorously tested across multiple cpu gpus tpus ipus and hpus and against major python and pytorch version simple installation from pypia lightningmodule defines a full system ie a gan autoencoder bert or a simple image classifier note training step defines the training loop forward defines how the lightningmodule behaves during inference prediction lightning ha over advanced feature designed for professional ai research at scale here are some example for complex professional level work you have optional full control of the training loop and optimizers in the pytorch lightning release lightninglite now enables you to leverage all the capability of pytorch lightning accelerator without any refactoring to your training loop check out the blogpost and doc for more info the pytorch lightning community is maintained bywant to help u build lightning and reduce boilerplate for thousand of researcher learn how to make your first contribution herepytorch lightning is also part of the pytorch ecosystem which requires project to have solid testing documentation and support if you have any question please,"[('pytorch lightning accelerator', 0.6975), ('engineering lightning structure pytorch code', 0.6372), ('pytorch lightning community', 0.6332), ('pytorch version', 0.5853), ('lightweight pytorch wrapper', 0.5834), ('pytorch lightning release lightninglite', 0.583), ('pytorch code', 0.5418), ('pytorch ecosystem', 0.4342), ('doc example community lightning ai license lightning', 0.4227), ('pypia lightningmodule', 0.4066)]","[-1.36237606e-01 -5.96479662e-02 -7.20883254e-03  3.50001454e-02
  4.90150452e-02 -1.83729604e-02  1.19804107e-02  2.88201254e-02
 -5.70988581e-02 -3.35030444e-02 -7.35382130e-03 -1.37939183e-02
 -1.30524755e-01  1.11762695e-02  5.24368547e-02  5.84295020e-02
 -5.21169007e-02  8.45203400e-02 -2.08749566e-02 -7.46618062e-02
  8.09209701e-03 -2.69158222e-02 -3.72771127e-03  5.70764095e-02
  1.51644740e-02  5.35012856e-02 -1.68512613e-02 -4.83596921e-02
  6.21895790e-02 -4.46417443e-02 -3.64176445e-02 -2.06544288e-02
  7.47447610e-02  5.42918332e-02 -7.43492460e-03 -7.80318864e-04
 -5.74604422e-02  1.56902075e-02  1.84966046e-02 -1.60653703e-02
  2.40191445e-02 -3.04892678e-02 -8.06323066e-02  4.08330932e-02
  2.65110168e-03  1.48268128e-02  2.43648104e-02 -7.46045336e-02
 -4.80353050e-02 -6.45070076e-02 -6.01348607e-03 -5.54704443e-02
 -5.74549213e-02  2.77069770e-02 -1.16339680e-02 -4.80477959e-02
  1.06623638e-02 -4.15469194e-03  7.95856863e-02 -4.07444593e-03
  6.00590277e-03  2.16756258e-02 -3.77891436e-02  5.01318835e-02
 -2.29626019e-02  4.69745435e-02 -2.80872881e-02  8.59201998e-02
  7.09878728e-02 -1.20082147e-01  2.19892934e-02 -4.07039896e-02
 -2.65871529e-02  6.86424747e-02 -2.12806221e-02 -2.64600385e-02
  4.28347327e-02  3.93951964e-03 -4.52127382e-02 -1.02991782e-01
 -2.20070872e-03 -3.34611372e-03 -2.47130319e-02  8.84105638e-02
 -1.10860355e-03  7.07159638e-02 -2.83970740e-02  5.03011160e-02
  6.25392469e-03 -2.80242451e-02  8.97157267e-02 -2.36628801e-02
  7.14477152e-02  7.52186924e-02 -3.58107733e-03  6.20960779e-02
  3.83671634e-02 -1.42711863e-01 -9.09866169e-02  3.37330140e-02
 -5.04218154e-02 -1.64453518e-02 -1.27480784e-03 -2.51190849e-02
 -3.84275950e-02  3.45852524e-02 -1.76211670e-02 -3.22454842e-03
  7.52247423e-02  8.11644867e-02  5.05156294e-02 -3.16320993e-02
 -6.63348511e-02 -8.30279961e-02  7.15770423e-02 -7.68088084e-03
 -6.77732378e-02  1.17080152e-01  1.12845443e-01  4.12396118e-02
 -4.55224067e-02  2.32071113e-02  1.27387419e-02  2.06225701e-02
 -3.27252485e-02 -6.77668815e-03 -7.52597675e-02  1.00482646e-32
 -4.31964286e-02 -2.02982016e-02 -8.87762234e-02  8.19381792e-04
  6.44388795e-02 -1.16331249e-01  9.45436731e-02  6.42874977e-03
 -2.96088904e-02 -3.57274227e-02  2.58450788e-02  1.76995154e-02
 -4.09142710e-02  7.63696954e-02  1.01408269e-02 -5.44025339e-02
 -7.35658184e-02  3.83034833e-02  3.94063368e-02  7.09756240e-02
  1.33495312e-02 -3.08978930e-02 -4.12506424e-03  1.50342301e-01
 -3.29949036e-02 -4.24547074e-03  4.76021692e-03 -3.65091711e-02
  1.90256741e-02  2.21941434e-02 -1.64906047e-02  4.37791348e-02
  5.30950800e-02 -1.99077204e-02 -3.97718064e-02  3.16444486e-02
 -5.73376194e-02 -6.35346845e-02 -2.40267944e-02 -6.08270168e-02
 -1.20348804e-01  1.14664175e-02 -8.57789889e-02 -2.37296466e-02
  9.66817513e-02 -5.85488752e-02 -3.36763263e-02  4.71708253e-02
  4.74816263e-02 -5.36243096e-02 -6.06220439e-02  5.10852337e-02
 -1.95372105e-02  5.88133037e-02  4.41198908e-02 -2.04113405e-02
  8.14023521e-03  6.14563785e-02  1.04261331e-01  2.39743888e-02
 -3.35845212e-03  6.40519187e-02  2.86765378e-02 -5.58129400e-02
  1.49700716e-02  3.67168114e-02 -3.99536565e-02 -4.16657003e-03
 -7.55711421e-02  7.79602155e-02 -5.38894795e-02  2.34873779e-02
 -5.76727986e-02  1.20379496e-02  4.96079139e-02 -2.81703677e-02
 -3.22818682e-02  1.28080035e-02  2.32095923e-02 -2.97858734e-02
 -1.08650230e-01  2.64925733e-02  5.09442128e-02  3.10222544e-02
 -3.74707989e-02 -2.40919236e-02  6.84788032e-03  4.55439389e-02
 -5.27180322e-02 -3.20627762e-04  2.27978695e-02 -3.08187306e-02
  2.88009979e-02  1.51299434e-02 -1.67519581e-02 -8.68462500e-33
  2.36276798e-02  3.04504838e-02 -9.03431550e-02  7.49783069e-02
  2.25285813e-02 -2.13283184e-03 -1.01600818e-01 -7.48478994e-02
  6.67087454e-03  7.40650669e-02  1.81673970e-02  5.22141755e-02
 -8.84093624e-03 -3.08151590e-03  6.31374419e-02 -1.74370576e-02
 -1.17375627e-02 -1.20520107e-02 -1.03636310e-02  3.63702103e-02
 -2.63226405e-03  7.08243102e-02 -8.04393291e-02 -1.76619471e-03
 -4.24772361e-03 -2.92762704e-02 -2.40471903e-02  1.03277005e-02
  6.18859529e-02 -4.96882349e-02  3.07367109e-02  6.69293553e-02
 -2.77469773e-02 -2.78370678e-02 -5.03349267e-02 -1.30351996e-02
  3.74260359e-02 -4.12375107e-02 -4.10418399e-02 -6.01431020e-02
  1.50460854e-01  1.75472163e-02  5.72013110e-02 -3.28772068e-02
 -2.65277270e-02  1.33537678e-02 -6.86468929e-02  6.69349125e-03
  2.64357738e-02  7.58446380e-03  1.47142680e-02  2.42451970e-02
 -2.92198062e-02 -2.80172992e-02 -2.71556657e-02 -3.25700231e-02
  7.47949705e-02  5.23114502e-02 -2.87967501e-03  1.09459059e-02
 -8.60072374e-02 -7.02532753e-02  3.69527116e-02  2.84026340e-02
 -3.27093080e-02 -5.45119010e-02  3.88463098e-03  4.29839641e-02
 -5.38807102e-02 -3.42779383e-02  6.33307397e-02  2.12188680e-02
  2.30394173e-02  3.12188752e-02 -7.97749236e-02  3.83007303e-02
  9.23484117e-02  2.66403910e-02  1.25269433e-02 -8.40329900e-02
  1.37769744e-01  5.80294542e-02  2.05680430e-02  2.72972067e-03
  1.24167940e-02  2.86585465e-02  3.59637924e-02  2.32644882e-02
  3.45856212e-02  2.21941974e-02 -3.80101008e-03 -4.10598842e-03
  4.78001162e-02  5.63548580e-02  6.10228553e-02 -3.98613018e-08
 -1.63460672e-02  7.37487823e-02 -1.79118086e-02 -6.31063199e-03
  6.07689358e-02  3.20465639e-02 -3.23464745e-03  1.19301245e-01
 -4.40964177e-02 -2.07624082e-02  1.99133325e-02 -5.75341880e-02
 -4.16320749e-02  4.45426069e-02  5.24837784e-02  8.13962892e-02
 -2.72047948e-02 -9.52450791e-04  9.53070167e-03 -1.78113598e-02
 -3.89667414e-02  6.99409097e-03 -3.52316946e-02 -1.88385341e-02
 -7.46680200e-02  4.33663512e-03  4.30380814e-02 -2.10654624e-02
  1.30471941e-02 -4.18116637e-02 -1.70064755e-02  1.20183378e-02
  9.31732431e-02 -6.41005337e-02  1.27941221e-01  5.53220399e-02
 -5.26235029e-02 -5.72030097e-02 -9.96446796e-03  8.39177892e-02
 -6.25635087e-02 -8.88343975e-02  6.36397535e-03 -4.15910222e-02
 -2.71570720e-02  2.42304355e-02 -6.52566999e-02 -1.10950775e-01
 -4.10919972e-02  5.41251861e-02  2.32003964e-02 -1.61266793e-02
 -9.10549890e-03 -6.19133040e-02 -2.68770214e-02  1.48988679e-01
 -5.41118383e-02 -3.59486639e-02 -1.61606614e-02 -8.95180926e-02
  3.61197479e-02  1.43720675e-03  2.88909953e-02  3.07584852e-02]",2,2
wandb,23,use w b to build better model faster track and visualize all the piece of your machine learning pipeline from datasets to production model sign up for a free account documentation in your training script if you re already using tensorboard or tensorboardx you can integrate with one line run wandb login from your terminal to signup or authenticate your machine we store your api key in netrc you can also set the wandb api key environment variable with a key from your setting run your script with python my script py and all metadata will be synced to the cloud you will see a url in your terminal log when your script start and finish data is staged locally in a directory named wandb relative to your script if you want to test your script without syncing to the cloud you can set the environment variable wandb mode dryrun if you are using docker to run your code we provide a wrapper command wandb docker that mount your current directory set environment variable and ensures the wandb library is installed training your model in docker give you the ability to restore the exact code and environment with the wandb restore command sign up for a free account introduction video framework specific and detailed usage can be found in our documentation to run basic test use make test more detailed information can be found at contributing md we use circleci for ci if you d like a free academic account for your research group reach out to u we make it easy to cite w b in your published paper learn more got question feedback or want to join a community of ml engineer working on exciting project join our slack community follow u on twitter,"[('training script', 0.4365), ('wrapper command wandb docker', 0.3732), ('variable wandb mode dryrun', 0.3381), ('wandb library', 0.3374), ('wandb', 0.3285), ('pipeline', 0.3166), ('script py', 0.2973), ('cloud', 0.294), ('tensorboard', 0.2609), ('docker', 0.2591)]","[-6.74198717e-02 -6.45002425e-02  8.15972779e-03  8.84030480e-04
 -4.22392366e-03  3.33266109e-02 -1.51745304e-02  1.94820948e-02
 -1.14305124e-01 -4.78179455e-02 -1.67457294e-02  1.16936688e-04
 -3.58113348e-02  3.79949762e-03 -6.09824620e-03 -2.03849785e-02
 -1.64657757e-02  2.18825415e-02  5.62156551e-03 -5.59662692e-02
 -4.47821729e-02  6.03074692e-02  3.38820554e-02  8.86327773e-03
 -6.30438477e-02 -6.67146361e-03 -1.00745158e-02 -7.13178292e-02
  1.23900836e-02 -9.32607129e-02  5.54690361e-02  1.28281703e-02
 -1.51148054e-03 -1.84385702e-02  3.20212953e-02  1.11619987e-01
 -5.89745911e-03 -9.81957316e-02 -4.13172804e-02 -5.27011454e-02
  6.74862936e-02 -3.24639566e-02 -5.64902872e-02  7.00487057e-04
  1.54069252e-02  1.97962169e-02 -3.48385214e-03 -1.31993040e-01
  3.36407460e-02  3.32288370e-02 -3.98131795e-02 -1.51664421e-01
 -2.03404520e-02  4.96738032e-02 -1.47932973e-02 -1.04949055e-02
  4.46208455e-02  5.54873496e-02  7.30841467e-03 -1.52797380e-03
 -8.20545256e-02  5.82012795e-02 -4.49944399e-02  3.31126750e-02
  7.31115341e-02  1.30426195e-02 -3.53930555e-02  7.99051151e-02
  1.23221867e-01 -6.60269633e-02 -2.09487043e-02 -5.65614663e-02
 -8.39486718e-02  2.42095944e-02 -4.88552190e-02 -4.10675853e-02
  2.89388094e-02  3.51517349e-02  1.44336512e-02 -5.82123660e-02
 -1.22547612e-01 -5.49634621e-02  2.95025874e-02  4.43348289e-02
 -1.78751873e-03  6.07705601e-02  2.80596148e-02  7.16527775e-02
  8.03602263e-02 -3.35953087e-02  1.05144888e-01 -2.07119565e-02
  3.42174470e-02  5.35914823e-02  1.84398796e-02  4.72699665e-02
 -5.06880619e-02 -3.07822898e-02 -3.90937626e-02  6.14559650e-02
 -6.11445121e-02 -2.68851928e-02  5.22133373e-02 -2.87662148e-02
  1.17037427e-02  2.90142512e-03 -2.00236086e-02 -8.91845152e-02
 -3.13348249e-02 -2.50134505e-02 -3.05189956e-02 -1.88255236e-02
 -4.41592596e-02 -7.25034550e-02  7.87026659e-02 -2.66731773e-02
 -1.55317830e-02 -1.98954493e-02 -1.96666177e-02  3.14382687e-02
  7.86728598e-03 -2.75203958e-03  8.70840922e-02  2.38811262e-02
 -3.94390337e-02 -1.73718762e-02 -1.04352295e-01  2.56347786e-33
  2.04361901e-02 -1.59923270e-01  5.28352242e-03  1.82741880e-02
  9.27575901e-02  3.67601751e-04  1.40723720e-01  2.37608105e-02
 -3.81525010e-02  4.03789617e-02 -7.38230944e-02  8.07906240e-02
 -4.64818105e-02  1.06577978e-01  2.99727377e-02 -1.14105530e-01
 -3.19662467e-02  5.23527451e-02 -6.87027536e-03  2.25717928e-02
  2.51638927e-02  3.02239880e-02 -3.06892172e-02  4.33275402e-02
  6.36998937e-02 -4.33107052e-04  2.63081659e-02 -1.99072529e-02
 -1.98802687e-02  3.37014124e-02 -4.43006903e-02 -7.08756074e-02
 -1.52477091e-02 -1.20768724e-02 -5.23413382e-02 -2.79260371e-02
 -5.86915910e-02 -1.94507986e-02 -1.55254165e-02  4.91461791e-02
 -4.52643260e-02 -5.06963320e-02 -5.89658506e-04 -9.72321664e-04
  1.64160114e-02 -6.50293604e-02  1.31799188e-02  1.83787663e-02
 -3.55472951e-03  3.60899568e-02 -1.40026622e-02 -2.81937700e-02
  2.67648418e-02 -5.03320284e-02  1.39496084e-02 -1.59403519e-03
 -2.10468769e-02  5.77007011e-02  5.64818308e-02 -8.40944424e-03
 -4.54408452e-02  5.82326092e-02  4.47557606e-02  6.71328530e-02
  3.35767269e-02 -2.11826898e-02 -3.47990021e-02  5.86558320e-02
  5.41153103e-02  6.92267641e-02 -7.12538138e-02  5.39404973e-02
 -2.44999323e-02 -3.58269699e-02  7.60717094e-02 -7.47816935e-02
  6.59997016e-03 -2.47729048e-02 -3.95785868e-02 -9.12614341e-04
 -5.04463539e-03  5.69216572e-02  1.32425257e-03  4.56581041e-02
  1.12554338e-02 -4.48156111e-02  3.70833278e-02  4.40572165e-02
 -3.69834192e-02  3.81697156e-02 -1.43224625e-02  1.92541331e-02
  3.32518928e-02 -4.47168089e-02 -4.66236770e-02 -3.12872258e-33
  7.03314766e-02  2.95789149e-02 -8.93966556e-02  1.10219508e-01
  1.10486513e-02  4.59061936e-02  6.38276115e-02 -4.54045925e-03
 -2.28467304e-02  2.05075229e-03 -4.44593877e-02  3.00248042e-02
  2.71850452e-02 -4.14775498e-03  3.73445600e-02  6.89086467e-02
 -1.31399378e-01 -1.84615701e-02  7.44796768e-02 -2.08119005e-02
 -1.92315727e-02  7.91954920e-02  2.71725897e-02  1.24929333e-02
 -5.63878082e-02 -6.80912361e-02 -5.68441339e-02  2.99396366e-02
  1.81908421e-02  2.28629820e-02 -3.93420272e-03  4.93259802e-02
 -1.71853546e-02  5.42532541e-02 -6.67604133e-02  2.27221064e-02
  1.07736595e-01  6.59840181e-02 -1.69916991e-02  1.61206871e-02
  6.58596978e-02 -4.09437492e-02 -1.58134010e-02  2.34168395e-03
 -8.96954462e-02 -1.82371903e-02 -8.44886452e-02  3.28089260e-02
 -7.71911517e-02 -1.42465159e-02 -5.31708002e-02 -3.91515791e-02
  1.62829191e-03 -4.72403094e-02 -4.79169972e-02 -6.05951101e-02
  3.77658568e-02 -5.55073842e-02 -7.36222193e-02  8.15876201e-02
  2.60107722e-02 -7.10615069e-02  6.93797469e-02 -3.09095308e-02
 -5.35674319e-02 -4.23987433e-02 -1.05345175e-01  3.10872775e-02
 -9.82685834e-02 -1.39736291e-02  2.16420535e-02 -1.87240541e-02
  8.06659013e-02  3.81872803e-02 -4.87092584e-02  4.50249501e-02
 -4.10851464e-03 -2.80690119e-02  3.74336056e-02  4.28231359e-02
  3.28834876e-02  5.52050807e-02  2.02936847e-02 -1.97343118e-02
  1.64975841e-02  6.13776557e-02  4.07786816e-02  3.63578834e-02
  7.26409405e-02  7.01186829e-04 -1.34793939e-02  1.47585049e-02
  3.89384031e-02  1.11472674e-01  3.77237722e-02 -2.66119056e-08
 -2.07941551e-02  7.98094049e-02  6.63290471e-02  5.88452630e-02
 -1.25657450e-02  3.02919056e-02  1.50530245e-02  1.27456486e-01
 -3.36262956e-02  7.14702578e-03  5.03351763e-02 -4.25465517e-02
 -5.81686869e-02  8.94919187e-02  4.64996547e-02  1.19358212e-01
 -7.22103578e-04  6.33186549e-02 -2.67930049e-02 -4.99092042e-02
  5.53984381e-02  5.17855138e-02  5.61606027e-02 -5.33127785e-03
 -3.60927954e-02 -1.38074653e-02  6.70497492e-03  6.25572428e-02
 -5.81203848e-02 -1.48668727e-02  4.57718670e-02 -2.60597132e-02
 -5.13311569e-03 -1.39931748e-02  2.74704732e-02  5.18462434e-02
 -3.63289118e-02 -1.08267732e-01  7.87369674e-04  5.81401959e-02
 -5.17474003e-02 -5.66702075e-02 -3.11685968e-02 -4.76532616e-02
  1.09712221e-02  6.45529770e-04  2.59202789e-03 -6.76724985e-02
 -8.82984549e-02  3.71701606e-02  4.88422997e-02  5.51355742e-02
 -1.90811418e-02  4.08864170e-02  8.14025700e-02  1.21329039e-01
 -3.28672044e-02 -5.09464368e-02 -1.59816649e-02  8.74570571e-03
 -9.02071595e-02  3.08425762e-02  3.20674665e-02 -3.74811981e-03]",2,2
jax,19,quickstart transformation install guide neural net library change log reference docsjax is autograd and xla brought together for high performance machine learning research with it updated version of autograd jax can automatically differentiate native python and numpy function it can differentiate through loop branch recursion and closure and it can take derivative of derivative of derivative it support reverse mode differentiation a k a backpropagation via grad a well a forward mode differentiation and the two can be composed arbitrarily to any order what s new is that jax us xla to compile and run your numpy program on gpus and tpus compilation happens under the hood by default with library call getting just in time compiled and executed but jax also let you just in time compile your own python function into xla optimized kernel using a one function api jit compilation and automatic differentiation can be composed arbitrarily so you can express sophisticated algorithm and get maximal performance without leaving python you can even program multiple gpus or tpu core at once using pmap and differentiate through the whole thing dig a little deeper and you ll see that jax is really an extensible system for composable function transformation both grad and jit are instance of such transformation others are vmap for automatic vectorization and pmap for single program multiple data spmd parallel programming of multiple accelerator with more to come this is a research project not an official google product expect bug and sharp edge please help by trying it out reporting bug and letting u know what you think jump right in using a notebook in your browser connected to a google cloud gpu here are some starter notebook jax now run on cloud tpus to try out the preview see the cloud tpu colabs for a deeper dive into jax you can also take a look at the mini library in jax example library like stax for building neural network and optimizers for first order stochastic optimization or the example at it core jax is an extensible system for transforming numerical function here are four transformation of primary interest grad jit vmap and pmap jax ha roughly the same api a autograd the most popular function is grad for reverse mode gradient you can differentiate to any order with grad for more advanced autodiff you can use jax vjp for reverse mode vector jacobian product and jax jvp for forward mode jacobian vector product the two can be composed arbitrarily with one another and with other jax transformation here s one way to compose those to make a function that efficiently computes full hessian matrix a with autograd you re free to use differentiation with python control structure see the reference doc on automatic differentiation and the jax autodiff cookbook for more you can use xla to compile your function end to end with jit used either a an jit decorator or a a higher order function you can mix jit and grad and any other jax transformation however you like using jit put constraint on the kind of python control flow the function can use see the gotchas notebook for more vmap is the vectorizing map it ha the familiar semantics of mapping a function along array ax but instead of keeping the loop on the outside it push the loop down into a function s primitive operation for better performance using vmap can save you from having to carry around batch dimension in your code for example consider this simple unbatched neural network prediction function we often instead write jnp dot activation w to allow for a batch dimension on the left side of activation but we ve written this particular prediction function to apply only to single input vector if we wanted to apply this function to a batch of input at once semantically we could just writebut pushing one example through the network at a time would be slow it s better to vectorize the computation so that at every layer we re doing matrix matrix multiplication rather than matrix vector multiplication the vmap function doe that transformation for u that is if we writethen the vmap function will push the outer loop inside the function and our machine will end up executing matrix matrix multiplication exactly a if we d done the batching by hand it s easy enough to manually batch a simple neural network without vmap but in other case manual vectorization can be impractical or impossible take the problem of efficiently computing per example gradient that is for a fixed set of parameter we want to compute the gradient of our loss function evaluated separately at each example in a batch with vmap it s easy of course vmap can be arbitrarily composed with jit grad and any other jax transformation we use vmap with both forward and reverse mode automatic differentiation for fast jacobian and hessian matrix calculation in jax jacfwd jax jacrev and jax hessian for parallel programming of multiple accelerator like multiple gpus use pmap with pmap you write single program multiple data spmd program including fast parallel collective communication operation applying pmap will mean that the function you write is compiled by xla similarly to jit then replicated and executed in parallel across device here s an example on an gpu machine in addition to expressing pure map you can use fast collective communication operation between device you can even nest pmap function for more sophisticated communication pattern it all composes so you re free to differentiate through parallel computation when reverse mode differentiating a pmap function e g with grad the backward pas of the computation is parallelized just like the forward pas see the spmd cookbook and the spmd mnist classifier from scratch example for more for a more thorough survey of current gotchas with example and explanation we highly recommend reading the gotchas notebook some standouts jax is written in pure python but it depends on xla which need to be installed a the jaxlib package use the following instruction to install a binary package with pip or conda or to build jax from source we support installing or building jaxlib on linux ubuntu or later and macos or later platform window user can use jax on cpu and gpu via the window subsystem for linux in addition there is some initial community driven native window support but since it is still somewhat immature there are no official binary release and it must be built from source for window for an unofficial discussion of native window build see also the issue thread to install a cpu only version of jax which might be useful for doing local development on a laptop you can runon linux it is often necessary to first update pip to a version that support manylinux wheel these pip installation do not work with window and may fail silently see above if you want to install jax with both cpu and nvidia gpu support you must first install cuda and cudnn if they have not already been installed unlike some other popular deep learning system jax doe not bundle cuda or cudnn a part of the pip package jax provides pre built cuda compatible wheel for linux only with cuda or newer and cudnn or newer other combination of operating system cuda and cudnn are possible but require building from source next runthese pip installation do not work with window and may fail silently see above the jaxlib version must correspond to the version of the existing cuda installation you want to use you can specify a particular cuda and cudnn version for jaxlib explicitly you can find your cuda version with the command some gpu functionality expects the cuda installation to be at usr local cuda x x where x x should be replaced with the cuda version number e g cuda if cuda is installed elsewhere on your system you can either create a symlink please let u know on the issue tracker if you run into any error or problem with the prebuilt wheel jax also provides pre built wheel for google cloud tpu to install jax along with appropriate version of jaxlib and libtpu you can run the following in your cloud tpu vm colab tpu runtimes come with jax pre installed but before importing jax you must run the following code to initialize the tpu colab tpu runtimes use an older tpu architecture than cloud tpu vms so installing jax tpu should be avoided on colab if for any reason you would like to update the jax jaxlib library on a colab tpu runtime follow the cpu instruction above i e install jax cpu there is a community supported conda build of jax to install using conda simply runto install on a machine with an nvidia gpu runnote the cudatoolkit distributed by conda forge is missing ptxas which jax requires you must therefore either install the cuda nvcc package from the nvidia channel or install cuda on your machine separately so that ptxas is in your path the channel order above is important conda forge before nvidia we are working on simplifying this if you would like to override which release of cuda is used by jax or to install the cuda build on a machine without gpus follow the instruction in the tip trick section of the conda forge website see the conda forge jaxlib and jax repository for more detail see building jax from source multiple google research group develop and share library for training neural network in jax if you want a fully featured library for neural network training with example and how to guide try flax in addition deepmind ha open sourced an ecosystem of library around jax including haiku for neural network module optax for gradient processing and optimization rlax for rl algorithm and chex for reliable code and testing watch the neurips jax ecosystem at deepmind talk here to cite this repository in the above bibtex entry name are in alphabetical order the version number is intended to be that from jax version py and the year corresponds to the project s open source release a nascent version of jax supporting only automatic differentiation and compilation to xla wa described in a paper that appeared at sysml we re currently working on covering jax s idea and capability in a more comprehensive and up to date paper for detail about the jax api see the reference documentation for getting started a a jax developer see the developer documentation,"[('other popular deep learning system jax doe', 0.6017), ('autograd jax', 0.5613), ('jax version py', 0.538), ('jax cpu', 0.5162), ('other jax transformation', 0.5117), ('jax developer', 0.507), ('jaxlib', 0.4936), ('core jax', 0.4936), ('jaxlib version', 0.471), ('jax api', 0.4643)]","[-9.44390371e-02 -5.77334911e-02 -1.04184654e-02 -6.62607253e-02
  2.04505981e-03 -5.21802269e-02 -9.85826179e-02  1.26911839e-02
 -7.48922676e-02 -9.05530900e-02  2.67854650e-02 -5.79575635e-03
 -5.72001701e-03 -9.68921930e-03  1.17087578e-02  8.83574225e-03
  4.68797199e-02  2.87242532e-02 -2.12135874e-02 -6.42650723e-02
 -9.80234332e-03  2.55120173e-02 -2.07358841e-02 -3.16016674e-02
  2.41140984e-02  4.75770347e-02  3.45617905e-02 -4.45261933e-02
  1.49606848e-02 -3.58487591e-02 -9.15110707e-02  3.99178565e-02
 -9.35495831e-03 -4.66770940e-02 -5.05802408e-02  6.48134127e-02
 -2.92395093e-02 -2.05054600e-02 -2.09740195e-02 -3.02656870e-02
 -6.89760521e-02  2.10708436e-02  7.86739141e-02 -2.12640334e-02
 -6.21423125e-02 -4.94893230e-02  8.82790536e-02 -9.75153968e-02
 -1.46713946e-02 -2.43593641e-02  3.66028259e-03 -6.06827885e-02
  4.50262567e-04  9.00096595e-02 -3.48887816e-02 -2.43337937e-02
 -4.52533141e-02  4.88815196e-02 -9.61005166e-02 -3.35886851e-02
 -1.47036705e-02  1.76543761e-02 -4.27897349e-02  2.44726986e-02
 -5.07951304e-02  1.27226859e-02  2.08253432e-02  8.37554783e-03
  7.75380507e-02 -1.11801863e-01 -6.77473322e-02  2.73828337e-04
  1.23565714e-03  6.15684427e-02  6.39974512e-03  3.39609459e-02
  9.77828950e-02 -4.33845595e-02  6.82794228e-02 -1.19517863e-01
  1.37802949e-02  4.99841869e-02 -8.93481541e-03  5.22498898e-02
  4.86901850e-02  4.17228937e-02 -2.27171443e-02  6.14175797e-02
 -2.55429503e-02 -4.36497713e-03  4.35995236e-02 -7.73050934e-02
  2.18306128e-02 -7.41384039e-03 -3.02208401e-02  5.46703041e-02
 -1.99598148e-02 -4.76001389e-02 -6.04898185e-02  6.61429763e-02
  2.35693958e-02  3.60536538e-02  6.69053793e-02 -3.99813354e-02
  2.93061696e-02  3.20563465e-02  5.96469780e-03  8.75385255e-02
 -1.10593969e-02 -1.10255263e-03  4.19785036e-03  7.12522911e-03
 -6.89654797e-02 -1.07971720e-01  4.18148674e-02  3.52801383e-03
  3.84505354e-02 -3.62538360e-02 -5.48297092e-02  8.92161056e-02
 -1.24373613e-02 -2.37339605e-02 -4.47850265e-02  3.89308855e-02
  2.24314909e-02  5.57414293e-02 -7.43758753e-02  3.81653885e-33
  1.73681299e-03  3.42581496e-02 -1.88109502e-02 -7.26716639e-03
  2.87335534e-02 -8.88468698e-02  6.58904612e-02 -4.38943394e-02
 -6.31307364e-02 -6.69635534e-02  8.48803110e-03  1.13315724e-01
  1.11605404e-02  5.15599400e-02  8.04044306e-02 -8.01974162e-02
 -3.09384819e-02 -7.63943652e-03  8.56293291e-02  1.93474740e-02
  6.53048083e-02  3.44505310e-02 -1.90740898e-02 -1.53510375e-02
 -4.67832759e-02  4.02965769e-02  6.28385767e-02  2.81685181e-02
 -7.80172944e-02  2.18954980e-02 -3.10512558e-02 -1.02438584e-01
 -4.14001457e-02  4.13867384e-02  5.57215996e-02  6.63748756e-02
 -1.30169420e-02 -6.33883551e-02 -2.96876905e-03 -1.22397572e-01
 -3.62119600e-02  5.35578877e-02 -5.53455278e-02  1.69659108e-02
  4.97070216e-02 -8.30028430e-02 -6.73655188e-03 -7.07870722e-03
  7.14212249e-04 -7.88573995e-02  2.44361162e-02 -4.56126481e-02
  8.35694280e-03 -5.88452863e-03  5.43214083e-02 -3.28962319e-03
  1.21733598e-01  8.13311413e-02 -8.77873600e-03  7.08329976e-02
 -4.13273871e-02 -5.44195697e-02 -2.97887288e-02  1.09642921e-02
  2.85461824e-02  1.58374961e-02  7.73753691e-03 -7.21784085e-02
 -4.36869226e-02 -2.28454303e-02 -2.04252619e-02 -3.79766570e-03
  5.79682626e-02  3.72166857e-02  6.60943659e-03 -1.79386213e-02
  5.35218231e-02 -8.39643106e-02  1.03820143e-02  5.20553850e-02
 -3.30872312e-02  5.62794693e-02  9.82074905e-03  6.44754693e-02
 -8.34576786e-03  4.85911481e-02  1.37230912e-02  9.01053753e-03
  4.08936925e-02  2.38113068e-02  5.13692424e-02  8.99757259e-03
  8.75015855e-02  2.77360845e-02 -3.47930081e-02 -2.59918772e-33
 -1.31527826e-01  3.28848548e-02 -7.55863786e-02  4.73786630e-02
  3.44672166e-02  7.56762829e-03 -3.91956940e-02  1.34535916e-02
 -8.49485695e-02 -5.92651293e-02  1.68631412e-02 -3.50484438e-02
  6.17052382e-03  9.42599308e-03 -1.76568087e-02 -2.37244796e-02
  5.53555740e-03 -6.86168596e-02  4.93339114e-02 -9.95508768e-03
 -1.95092373e-02  1.33168787e-01 -2.72399131e-02 -9.59958229e-03
 -2.62861475e-02 -3.55249643e-02 -8.85159075e-02  7.40404874e-02
 -5.09079732e-02  1.32397665e-02  4.14962545e-02 -4.05379049e-02
  1.97709017e-02 -3.48257832e-02 -2.36076228e-02 -3.29802334e-02
  2.72740386e-02  3.03570312e-02 -1.62466783e-02 -3.61123942e-02
  6.34664968e-02 -1.13891222e-01  4.13806327e-02  1.07436460e-02
 -6.91758171e-02  1.04542775e-02 -8.57969522e-02  3.29713919e-03
  1.26938857e-02  1.86921488e-02 -2.09983010e-02  2.67887837e-03
 -8.62864405e-03 -5.81001397e-03 -2.75885928e-02  7.04335943e-02
  2.43365988e-02  5.95265403e-02  5.91430403e-02  8.02625343e-02
 -3.03085335e-02 -4.75632223e-05  2.70685498e-02  1.52000815e-01
  3.89939547e-02  7.29246857e-03 -7.95811266e-02  8.17458481e-02
 -1.88976765e-01  1.04909595e-02  1.03356764e-02  3.25468145e-02
 -1.96523257e-02  9.11675673e-03  3.42114829e-02 -6.54090643e-02
  4.18476909e-02  2.89891753e-02 -6.37126639e-02  1.13530522e-02
  2.93558072e-02  1.83241237e-02 -5.86261116e-02  9.03333277e-02
  4.53173136e-03  7.91038107e-03  4.30434048e-02  1.28351767e-02
 -2.52656303e-02 -5.19469716e-02 -2.83240573e-03  5.22785336e-02
 -5.69144413e-02  3.99571937e-03  4.80376231e-03 -2.57370321e-08
 -3.49439420e-02  2.79820897e-02  1.99592374e-02  6.92415750e-03
  3.57049517e-02  8.56835619e-02 -4.23862040e-02  1.43420383e-01
  7.02456906e-02  7.08666667e-02  3.39615084e-02  3.84781417e-03
 -7.39817023e-02 -4.00027037e-02  3.05363555e-02  8.40090960e-02
  2.53174994e-02  4.15520854e-02 -1.19625023e-02 -5.60166016e-02
  3.89115252e-02  3.93621065e-02  3.96356322e-02 -3.16478834e-02
 -2.79389005e-02 -1.22614726e-02  6.74523190e-02  1.41064869e-02
  3.33689749e-02  1.66386887e-02 -7.35452101e-02  3.00875250e-02
  2.49826685e-02 -4.25920598e-02  1.91892888e-02  1.23204537e-01
 -8.63276888e-04  1.11139175e-02 -4.80170920e-02  9.43947583e-02
 -2.86285821e-02  3.95593457e-02 -3.66870984e-02  2.11004447e-02
  5.82688786e-02  2.88945567e-02 -5.96521683e-02 -4.14932929e-02
  8.60177353e-02  4.13991790e-03 -1.91498194e-02  4.77487082e-03
  4.25783880e-02 -5.93634807e-02  6.22191876e-02  9.40292254e-02
  1.84492636e-02 -1.13241464e-01  3.78617272e-02  8.83068293e-02
 -6.70780102e-03  1.13204680e-02  4.89586927e-02  8.79333243e-02]",2,0
tensorflow-gpu,18,tensorflow is an open source software library for high performance numerical computation it flexible architecture allows easy deployment of computation across a variety of platform cpu gpus tpus and from desktop to cluster of server to mobile and edge device originally developed by researcher and engineer from the google brain team within google s ai organization it come with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domain tensorflow is licensed under apache,"[('tensorflow', 0.7401), ('many other scientific domain tensorflow', 0.6584), ('flexible numerical computation core', 0.4332), ('deep learning', 0.3829), ('google s ai organization', 0.3821), ('flexible architecture', 0.3764), ('google brain team', 0.3684), ('platform cpu gpus tpus', 0.3613), ('open source software library', 0.3378), ('edge device', 0.2987)]","[-4.41984236e-02 -1.09338418e-01  3.75523120e-02 -6.00057282e-02
  8.24890584e-02 -4.75711152e-02 -5.30565754e-02  2.51197536e-02
 -1.34402225e-02 -1.77784972e-02 -8.60479921e-02 -3.01086437e-02
 -6.02132641e-02  1.79548992e-03  1.93502568e-03 -4.11127694e-02
 -2.45012846e-02  3.20869386e-02 -2.12851036e-02 -1.30356878e-01
 -4.25707698e-02  3.98718528e-02 -1.30772097e-02  1.58899475e-03
 -9.78470664e-04  7.42263645e-02 -1.08599998e-02 -1.42022565e-01
  3.27437744e-02 -6.55014766e-03 -1.02071287e-02  9.64850187e-03
  4.71507711e-03  3.81811298e-02 -6.36580959e-02  3.31682228e-02
 -4.90938015e-02 -4.53656018e-02 -1.57287996e-02 -4.45531756e-02
 -3.31613086e-02 -6.49364218e-02  1.22887539e-02 -4.47792821e-02
  8.86408389e-02  1.80261284e-02  2.25173086e-02 -1.15306959e-01
 -2.31295135e-02  1.99361108e-02 -3.49466614e-02 -8.51285681e-02
 -5.48832156e-02  8.48228261e-02 -7.17121512e-02 -6.73733884e-03
  4.10806164e-02 -6.88320026e-03  1.62451004e-03 -8.22282210e-03
 -1.00759100e-02 -8.78835991e-02  4.05119406e-03  1.54486764e-02
 -3.63357663e-02  5.28295226e-02 -9.17309057e-03  1.02966493e-02
  6.14090227e-02 -1.00714304e-01  8.13344568e-02  3.17448117e-02
 -3.28305252e-02 -7.23914290e-03 -3.69609729e-03 -1.43599079e-03
  5.77681474e-02 -2.73799971e-02  8.20818990e-02 -6.62635267e-02
  2.97083259e-02 -1.71580762e-02 -3.10782306e-02  1.28447125e-02
  3.34661715e-02 -2.07291394e-02 -7.15484247e-02  8.80392119e-02
 -1.64926033e-02 -1.48727791e-03  4.25984226e-02 -3.23301293e-02
  2.54538245e-02 -3.26760523e-02  4.63619828e-02  8.25122185e-03
 -7.75806680e-02 -8.91252458e-02 -4.99539562e-02  3.41689922e-02
 -1.18737161e-01  1.49925379e-03  4.47053388e-02  4.81082126e-02
 -2.87359152e-02  8.46514031e-02  4.03921865e-02 -3.37678823e-03
  9.50822160e-02 -2.90156733e-02 -2.99208332e-02  6.71075732e-02
  4.06970084e-02 -7.54432976e-02  1.83600057e-02 -7.02635124e-02
 -3.98105867e-02  7.65025169e-02  3.50047797e-02  1.09249830e-01
 -1.12119086e-01  3.65649313e-02 -3.26654613e-02 -8.72930512e-03
 -2.04302394e-03  1.50718652e-02 -8.76195654e-02  4.60425342e-33
  1.02570942e-02  4.28824779e-03  2.68881787e-02 -6.87931105e-02
  6.11587837e-02 -6.28679469e-02  2.82790624e-02  1.04069605e-03
  1.19376676e-02 -2.09535728e-03 -6.10620193e-02  1.45414501e-01
 -2.77088694e-02  1.31631240e-01  5.33098839e-02 -4.76143509e-02
  1.41801238e-02  5.01978882e-02  7.20799938e-02 -1.55935355e-03
  2.45469920e-02  6.20573387e-02  4.07773815e-02  9.34710652e-02
  2.34671924e-02  6.27924548e-03 -4.57534008e-02 -4.82183322e-02
  6.20497251e-03  1.55118303e-02 -5.82717657e-02  2.86109298e-02
 -4.72726822e-02  8.19104631e-03  1.48596782e-02  1.40242409e-02
 -7.13818287e-03 -6.28968030e-02  7.68921385e-03  1.67898014e-02
 -2.04171371e-02  4.88140881e-02  2.21376605e-02 -4.13218699e-02
 -1.19940946e-02 -2.14574859e-02  2.20675208e-02  2.55821627e-02
  4.94300574e-02 -5.72271608e-02 -2.83469800e-02  2.06819158e-02
 -7.33109657e-03 -7.42880777e-02  4.45740707e-02 -1.86625645e-02
  1.66003243e-03  5.78348897e-02  4.94083539e-02  8.17290470e-02
 -1.52327828e-02  2.14482732e-02 -2.74007898e-02 -1.56885870e-02
  3.61333638e-02 -1.15680881e-02  2.34961193e-02  4.28035334e-02
  9.24402382e-03  2.92042568e-02 -8.23360160e-02  7.03407750e-02
  3.02753523e-02  7.97685049e-03 -4.55205850e-02 -1.75612886e-02
  3.93302739e-02 -1.34543687e-01 -4.39116172e-02 -5.35312574e-03
 -1.35161489e-01  1.69179011e-02  7.37877490e-05  1.96509007e-02
  2.84204120e-03  1.24324122e-02 -2.80277040e-02  1.61576960e-02
 -5.60108647e-02 -5.83440997e-02 -9.79904234e-02 -1.62681602e-02
  8.40131938e-02  6.26077726e-02 -9.34203938e-02 -4.11873715e-33
 -1.21871002e-01 -2.00116094e-02 -6.95795864e-02  9.35161188e-02
  1.06673934e-01  1.64793674e-02  5.29420041e-02 -4.99502346e-02
  7.84921634e-04  5.40840551e-02  3.24851312e-02 -1.49185723e-02
  7.49930143e-02 -3.41328681e-02  1.59471463e-02 -6.72609732e-02
 -9.23045427e-02 -7.28558823e-02 -1.41289271e-02 -1.31858299e-02
 -3.60742360e-02  5.88296913e-02 -1.11268647e-02 -1.11294286e-02
  4.44210209e-02  2.42709126e-02 -9.14493203e-02 -6.56116903e-02
  6.20851517e-02  7.87141398e-02  8.85193329e-03 -2.49410868e-02
  1.07943751e-02  5.60709536e-02  1.08070284e-01  2.54249573e-02
  4.70024757e-02 -3.27337794e-02  4.17715870e-02 -3.21090035e-02
  6.68326393e-02  2.09123977e-02  8.31178799e-02  5.75441495e-03
 -1.57177057e-02  5.13668582e-02 -1.00466989e-01  5.68194799e-02
 -7.79418200e-02 -7.36578256e-02  2.29164143e-03  1.09611116e-02
 -3.92328426e-02 -1.02612153e-01  2.16487814e-02  2.21487638e-02
  4.73303422e-02 -1.24629773e-02  1.68757718e-02 -1.58568248e-02
 -1.49172219e-03 -5.22242896e-02  2.37852894e-02  2.95598339e-02
 -4.57500108e-02  7.54220188e-02 -8.37374944e-03  2.92717386e-02
 -7.08715320e-02 -5.05744107e-02  3.37662622e-02  9.25058499e-03
  6.04491401e-03  5.77810109e-02 -5.38401343e-02  1.88186746e-02
  2.49538794e-02 -2.30869707e-02  4.80298363e-02 -1.32949762e-02
  6.71137720e-02  1.58789922e-02  3.83781157e-02  5.01072630e-02
  5.06595746e-02  7.17647970e-02  4.81168069e-02 -3.42952199e-02
  2.15225033e-02 -2.26974841e-02 -4.45528142e-02  1.36768846e-02
  3.95529121e-02  9.66346115e-02 -7.90077373e-02 -2.76889054e-08
  4.37600647e-05  1.28331250e-02  8.19380730e-02 -1.11790355e-02
  5.15342504e-02 -8.69946275e-03  3.87721956e-02  4.68982942e-02
 -2.15304736e-02  1.09144384e-02 -1.34722795e-02 -5.39821126e-02
 -5.36090359e-02  1.06498577e-04  9.06490088e-02  5.04027084e-02
 -4.07694206e-02 -2.80015147e-03  3.82598266e-02 -8.40364844e-02
  3.06539945e-02  7.10854530e-02 -5.35856653e-03 -4.30149212e-02
 -3.41456267e-03 -7.31946230e-02  4.09532413e-02 -5.65154618e-03
  9.22913849e-03  2.75571328e-02 -5.08137271e-02 -5.10621220e-02
  5.90167232e-02 -3.64067666e-02  9.62798372e-02  4.31413203e-03
  5.34966327e-02 -2.88944710e-02  2.93812994e-03  6.43181428e-02
 -2.65936963e-02  8.72200802e-02  4.80443090e-02 -4.97647300e-02
  7.63691915e-03 -4.89581898e-02  1.61224399e-02 -4.40009758e-02
 -1.88040975e-02  6.26380444e-02 -7.08308667e-02  7.13098049e-02
 -4.59356047e-02  8.05362687e-02  4.94703427e-02  2.27457359e-02
 -2.76032165e-02 -1.40729666e-01 -2.81152502e-02  2.60950923e-02
 -1.65610705e-02  1.23978546e-02  5.14509305e-02  7.77910324e-03]",2,2
tensorflow-probability,13,tensorflow probability is a library for probabilistic reasoning and statistical analysis in tensorflow a part of the tensorflow ecosystem tensorflow probability provides integration of probabilistic method with deep network gradient based inference via automatic differentiation and scalability to large datasets and model via hardware acceleration e g gpus and distributed computation our probabilistic machine learning tool are structured a follows layer tensorflow numerical operation in particular the linearoperator class enables matrix free implementation that can exploit special structure diagonal low rank etc for efficient computation it is built and maintained by the tensorflow probability team and is now part of tf linalg in core tf layer statistical building blockslayer model buildinglayer probabilistic inferencetensorflow probability is under active development interface may change at any time see tensorflow probability example for end to end example it includes tutorial notebook such a it also includes example script such a representation learning with a latent code and variational inference for additional detail on installing tensorflow guidance installing prerequisite and optionally setting up virtual environment see the tensorflow installation guide to install the latest stable version run the following for cpu only usage and a smaller install install with tensorflow cpu to use a pre version of tensorflow run note since tensorflow is not included a a dependency of the tensorflow probability package in setup py you must explicitly install the tensorflow package tensorflow or tensorflow cpu this allows u to maintain one package instead of separate package for cpu and gpu enabled tensorflow see the tfp release note for more detail about dependency between tensorflow and tensorflow probability there are also nightly build of tensorflow probability under the pip package tfp nightly which depends on one of tf nightly or tf nightly cpu nightly build include newer feature but may be le stable than the versioned release both stable and nightly doc are available here you can also install from source this requires the bazel build system it is highly recommended that you install the nightly build of tensorflow tf nightly before trying to build tensorflow probability from source a part of tensorflow we re committed to fostering an open and welcoming environment see the tensorflow community page for more detail check out our latest publicity here we re eager to collaborate with you see contributing md for a guide on how to contribute this project adheres to tensorflow s code of conduct by participating you are expected to uphold this code if you use tensorflow probability in a paper please cite we re aware there s a lot more to tensorflow probability than distribution but the distribution paper lay out our vision and is a fine thing to cite for now,"[('tensorflow probability package', 0.738), ('tensorflow probability team', 0.7113), ('tensorflow probability', 0.6695), ('tensorflow ecosystem tensorflow probability', 0.6658), ('tensorflow', 0.6618), ('tensorflow probability example', 0.6436), ('tensorflow package tensorflow', 0.6174), ('tensorflow cpu', 0.5865), ('tensorflow installation guide', 0.5349), ('tensorflow guidance', 0.5328)]","[ 9.79672838e-03 -7.66210780e-02  5.59374131e-02 -1.44364983e-02
  4.11125161e-02 -7.91378040e-03  3.97070125e-02  5.40270424e-03
 -3.76961157e-02 -3.21845375e-02 -2.32717171e-02 -4.01458405e-02
 -2.90208384e-02  2.36592093e-03  1.32573759e-02  2.39661924e-04
 -4.63506468e-02  5.60573749e-02 -3.82299572e-02 -6.25100955e-02
 -7.22466558e-02 -4.14525010e-02  4.67450358e-02 -5.11685275e-02
 -2.78163329e-02 -6.70635253e-02  1.56614110e-02 -2.01312429e-03
 -2.64377147e-02 -2.11430211e-02 -3.81344967e-02  9.78407171e-03
  1.23873122e-01 -5.09057306e-02  4.61958069e-03 -5.35770580e-02
 -7.92234465e-02 -4.30544242e-02 -1.62295569e-02  5.18310741e-02
 -2.35614590e-02  2.25517750e-02 -6.40649721e-02 -9.96949431e-03
  2.01567411e-02  1.23280426e-03  5.31343259e-02 -1.09158397e-01
  1.81321389e-04 -1.01864827e-03 -6.43929690e-02 -7.83040002e-02
 -5.75212389e-02  1.54180359e-02  1.63700953e-02 -3.95728499e-02
  7.98176825e-02 -5.05827880e-03 -2.40439754e-02 -7.83586726e-02
 -3.78692220e-03 -5.14354669e-02 -9.21745673e-02  3.70807424e-02
  5.19806752e-03  2.94553414e-02 -1.29144993e-02  1.00687779e-01
  1.50479034e-01 -1.05385497e-01 -1.40000358e-01  1.87609084e-02
 -4.52482589e-02  5.05827889e-02 -1.22636175e-02 -1.63865671e-03
  3.11506316e-02 -2.29313001e-02 -1.77768636e-02 -3.94548774e-02
 -2.89022662e-02 -5.36640026e-02  1.00851506e-01 -6.52830303e-02
  3.48979123e-02  3.09668984e-02 -2.22628135e-02  4.48866002e-02
 -1.33337239e-02 -3.90255339e-02  1.59834623e-02 -3.67321596e-02
 -1.52517846e-02  2.72694249e-02  3.15965563e-02  1.00503951e-01
 -1.20686278e-01 -9.96984020e-02 -1.96400546e-02 -3.21010202e-02
 -3.94203365e-02 -8.35065320e-02  4.68204282e-02  3.05894427e-02
 -5.11386357e-02  5.51136434e-02 -3.02332062e-02  4.00197394e-02
  4.64906543e-02  5.80401309e-02  4.52186260e-03  2.77584791e-02
  7.13449121e-02 -5.98328896e-02 -4.11422807e-04  2.92246193e-02
 -9.91292521e-02  6.16961308e-02 -1.51460581e-02  9.17256251e-02
 -1.15396433e-01  5.99631816e-02  4.65785787e-02 -8.41575488e-02
 -2.02776138e-02  2.57174056e-02 -7.32153095e-03  4.26799297e-33
  8.81964341e-03 -2.46802215e-02 -5.40086888e-02 -5.82715916e-03
  7.74532035e-02 -5.17136343e-02  3.81127037e-02  1.17382826e-02
  2.74002980e-02 -3.70271988e-02 -4.61875321e-03  8.48762840e-02
 -2.30359938e-02  3.75448912e-02 -7.54540190e-02 -5.40743880e-02
  2.21247580e-02  3.78101505e-02 -1.40992925e-02  5.26937917e-02
  2.99383624e-04  3.30810472e-02  1.21924076e-02  8.35283250e-02
  7.59862214e-02  6.55165985e-02  5.05593047e-02  4.50558960e-02
  3.13759185e-02  8.28519557e-03 -6.47735596e-02 -2.49410439e-02
 -8.56664404e-02 -7.50719979e-02  3.13174278e-02  3.49857993e-02
 -2.59110797e-02 -1.92849431e-02 -7.50729516e-02 -7.81845972e-02
 -1.98411699e-02  4.91238199e-03 -5.74655607e-02  1.24259200e-02
 -3.21340896e-02 -4.81382422e-02 -4.51507699e-03  7.84414355e-03
  1.42062172e-01 -9.70333442e-02 -2.12436598e-02 -9.06432793e-02
  5.21400236e-02  1.93797406e-02 -8.57864134e-03  1.90901998e-02
  1.75323095e-02  4.30935249e-02  2.45299432e-02  6.57220930e-02
 -6.11255504e-02  1.61297992e-02  8.32022205e-02 -5.73056713e-02
  4.39760042e-03 -1.45760849e-02 -1.41101167e-01  3.52858417e-02
  3.04570701e-02  3.17545049e-02 -4.54675555e-02  7.83667564e-02
  4.44152877e-02  6.45332830e-03 -1.58720613e-02  3.52202877e-02
 -4.46165614e-02 -3.75898108e-02  2.33126171e-02  2.97722965e-02
 -1.02090128e-01  2.47534774e-02 -8.31990037e-03  3.04864328e-02
 -2.34158877e-02  5.53222373e-02  3.46466675e-02  3.15422714e-02
 -1.72815230e-02  4.08925116e-02 -1.51241412e-02 -2.01248843e-02
  5.56829907e-02  6.88262582e-02 -3.92196216e-02 -4.35710279e-33
 -6.26766384e-02  3.16091813e-02 -4.42827195e-02  6.30143434e-02
  3.63348089e-02  5.51117361e-02  8.58191960e-03 -4.79645059e-02
  1.54175133e-01  1.96403880e-02 -3.62742543e-02  4.71096188e-02
  6.00817911e-02 -5.36977239e-02 -4.61125467e-03 -4.21084277e-02
 -7.03449324e-02  4.60884115e-03  2.02712547e-02 -9.35053686e-04
 -3.49059887e-02  2.17702258e-02 -5.01822643e-02 -7.34557491e-03
  2.67534163e-02  1.31499507e-02  5.73853552e-02 -7.53113925e-02
  8.96190107e-02 -4.51435000e-02 -1.16513697e-02 -2.98552751e-03
 -2.72314977e-02  8.51464421e-02 -1.82543043e-02 -4.33553569e-02
  7.08925202e-02  4.33637248e-03 -1.26940226e-02  2.53086463e-02
  2.08049659e-02  7.24695995e-02  1.63950976e-02 -4.92653660e-02
 -4.06515896e-02  2.52452381e-02 -1.15843946e-02  1.55352126e-03
 -1.77543552e-03 -4.07275483e-02 -3.65112200e-02  2.09164899e-02
 -4.78843376e-02 -1.08476393e-02 -6.80373982e-02  4.09754887e-02
  4.25681956e-02  4.88106012e-02  1.39550403e-01  3.69922705e-02
 -3.05626020e-02  1.24401855e-03  2.15246100e-02 -5.75244240e-03
 -1.92173272e-02  5.09075867e-03 -9.84760523e-02 -9.96015515e-05
 -2.25854176e-03  1.50978556e-02 -9.44237560e-02  7.13035977e-03
 -3.76118557e-03  4.93061617e-02 -1.00929486e-02  2.96065845e-02
  5.22994883e-02  4.55125347e-02  4.34301794e-02 -3.20811830e-02
  5.18394262e-02 -2.60738637e-02 -5.62190898e-02  1.10210560e-01
  9.59593356e-02  1.84527989e-02  3.59861143e-02 -9.20021385e-02
  5.06870672e-02 -7.38095166e-03 -1.00216186e-02 -8.22551362e-03
  4.18205261e-02  7.47425556e-02 -3.72342952e-02 -2.44472567e-08
  1.80648062e-02  5.83028533e-02  1.04483493e-01  1.94734186e-02
  8.96431878e-02  5.95194399e-02  2.58982889e-02  8.27322453e-02
 -1.73212364e-02  1.09900720e-03 -4.06035781e-03  6.41013682e-02
 -5.70257679e-02 -4.59415056e-02  1.50579605e-02  6.32105097e-02
 -4.68424521e-02  6.42288253e-02  2.29841135e-02 -6.86318949e-02
  5.45531362e-02 -5.53905964e-03 -3.20510417e-02  1.30765969e-02
  4.12552338e-03 -3.45529243e-02  1.51286777e-02  2.31627934e-02
 -4.27494273e-02 -4.19598259e-02 -6.48573935e-02 -4.35772426e-02
  6.37281686e-02 -7.64805377e-02  9.64181796e-02  6.99362457e-02
 -9.96940304e-03 -8.31136927e-02  9.50930547e-03  6.42674565e-02
 -1.90949515e-02 -2.96128970e-02  5.37340008e-02  6.07494323e-04
  3.57843079e-02 -4.11328953e-03 -3.08738835e-02 -2.62078829e-02
 -6.58444781e-03  1.46274641e-02 -2.13329010e-02  5.39793335e-02
 -6.31865263e-02  7.31907710e-02 -3.91540825e-02  1.13054663e-01
  5.13191074e-02 -7.14649931e-02 -1.19699938e-02 -4.62325588e-02
 -3.08358837e-02  7.90426508e-02  2.70450264e-02 -8.75769332e-02]",2,2
onnx,13,open neural network exchange onnx is an open ecosystem that empowers ai developer to choose the right tool a their project evolves onnx provides an open source format for ai model both deep learning and traditional ml it defines an extensible computation graph model a well a definition of built in operator and standard data type currently we focus on the capability needed for inferencing scoring onnx is widely supported and can be found in many framework tool and hardware enabling interoperability between different framework and streamlining the path from research to production help increase the speed of innovation in the ai community we invite the community to join u and further evolve onnx onnx is a community project we encourage you to join the effort and contribute feedback idea and code you can participate in the special interest group and working group to shape the future of onnx check out our contribution guide to get started if you think some operator should be added to onnx specification please read this document we encourage you to open issue or use slack if you have not joined yet please use this link to join the group for more real time discussion stay up to date with the latest onnx news facebook twitter onnx released package are published in pypi weekly package are published in test pypi to enable experimentation and early testing a binary build of onnx is available from conda in conda forge you can also use the onnx dev docker image for a linux based installation without having to worry about dependency versioning before building from source uninstall any existing version of onnx pip uninstall onnx generally speaking you need to install protobuf c c library and tool before proceeding forward then depending on how you installed protobuf you need to set environment variable cmake args to donnx use protobuf shared libs on or donnx use protobuf shared libs off for example you may need to run the following command linux window the on off depends on what kind of protobuf library you have shared library are file ending with dll so dylib static library are file ending with a lib this option depends on how you get your protobuf library and how it wa built and it is default off you don t need to run the command above if you d prefer to use a static protobuf library if you are building onnx from source it is recommended that you also build protobuf locally a a static library the version distributed with conda forge is a dll but onnx expects it to be a static library building protobuf locally also let you control the version of protobuf the tested and recommended version is the instruction in this readme assume you are using visual studio it is recommended that you run all the command from a shell started from x native tool command prompt for v and keep the build system generator for cmake e g cmake g visual studio consistent while building protobuf a well a onnx you can get protobuf by running the following command then it will be built a a static library and installed to protobuf install dir please add the bin directory which contains protoc exe to your path please note if your protobuf install dir contains space do not add quotation mark around it alternative if you don t want to change your path you can set onnx protoc executable instead then you can build onnx a first you need to install protobuf ubuntu user the quickest way to install protobuf is to runthen you can build onnx a otherwise you may need to install it from source you can use the following command to do it debian ubuntu centos rhel fedora here dcmake position independent code on is crucial by default static library are built without fpic flag they are not position independent code but shared library must be position independent code python c c extension like onnx are shared library so if a static library wa not built with fpic it can t be linked to such a shared library once build is successful update path to include protobuf path then you can build onnx a once build is successful update path to include protobuf path then you can build onnx a after installation runto verify it work for full list refer to cmakelists txt environment variablesuse msvc static runtime should be or not on or off when set to onnx link statically to runtime library default use msvc static runtime debug should be or when set to onnx is built in debug mode or debug version of the dependency you need to open the cmakelists file and append a letter d at the end of the package name line for example name protobuf lite would become name protobuf lited default debug cmake variablesonnx use protobuf shared libs should be on or off default onnx use protobuf shared libs off use msvc static runtime onnx use protobuf shared libs determines how onnx link to protobuf library onnx use lite proto should be on or off when set to on onnx us lite protobuf instead of full protobuf default onnx use lite proto offonnx werror should be on or off when set to on warning are treated a error default onnx werror off in local build on in ci and release pipeline note the import onnx command doe not work from the source checkout directory in this case you ll see modulenotfounderror no module named onnx onnx cpp py export change into another directory to fix this error building onnx on ubuntu work well but on centos rhel and other manylinux system you might need to open the cmakelists file cmakelists and replace all instance of lib with lib onnx us pytest a test driver in order to run test you will first need to install pytest after installing pytest use the following command to run test check out the contributor guide for instruction apache license v onnx open source code of conduct,"[('open neural network exchange onnx', 0.6678), ('onnx specification', 0.6041), ('onnx', 0.5669), ('onnx link', 0.5607), ('onnx onnx', 0.5565), ('onnx protoc', 0.5324), ('onnx check', 0.5268), ('latest onnx news facebook twitter onnx', 0.5227), ('project evolves onnx', 0.5187), ('import onnx command doe', 0.4847)]","[-1.11287370e-01 -1.18903302e-01  1.28925499e-02 -1.26771312e-02
  5.38990088e-02 -6.07772032e-04  8.78821407e-03  3.65561526e-03
 -5.94434850e-02 -3.97282168e-02 -3.06008868e-02  1.52545488e-02
 -4.82619740e-02  1.76604353e-02 -8.64744186e-03  3.78376581e-02
  3.68514992e-02 -7.13322163e-02 -1.00198644e-03  5.80230057e-02
 -1.25404904e-02 -5.14078513e-02  9.71866027e-02 -2.91984882e-02
 -3.12981009e-02 -6.31027296e-02  2.97181378e-03 -3.95016558e-02
  1.25020435e-02 -2.81005558e-02 -8.44369084e-03  5.05334623e-02
 -1.37231394e-03  1.40705230e-02 -8.44995007e-02  3.14868800e-02
 -1.58099439e-02 -4.07826575e-03 -3.05741671e-02 -3.08800768e-02
  5.62257841e-02 -5.57345264e-02  2.84950119e-02  2.80624814e-02
  4.67548445e-02 -1.63545031e-02 -3.50043327e-02 -8.61659646e-02
 -5.50187156e-02  2.13810615e-02 -1.31397769e-02 -1.03994109e-01
  3.04662343e-02  5.67750745e-02  2.30591428e-02  7.93497488e-02
 -8.09020549e-02  1.45465015e-02 -4.83766058e-03  4.83021624e-02
 -4.56936546e-02  2.39800625e-02 -6.00861832e-02  4.29625325e-02
  5.17331883e-02  8.55527595e-02 -2.24605054e-02  4.77360450e-02
 -2.73340605e-02 -8.36446434e-02 -4.49827425e-02  7.61142047e-03
  6.56940509e-03  2.99156513e-02 -5.22619635e-02 -7.80064519e-03
  1.12845138e-01  6.03207015e-03  1.26922810e-02 -1.30808249e-01
 -1.02127464e-02  3.52529846e-02 -7.97611941e-03 -2.63404176e-02
  1.78478546e-02  4.11242843e-02 -5.73395118e-02  4.89918403e-02
  2.87173465e-02  6.55671880e-02 -5.25539890e-02  5.43139409e-04
  5.19363694e-02  2.43272223e-02  7.66015574e-02  2.37418860e-02
  5.81104448e-03  4.63831238e-02  1.53790647e-02 -8.20462901e-06
 -9.89798922e-03  5.12574473e-03 -2.46529579e-02 -4.98929396e-02
 -3.19965966e-02  2.46926770e-02  7.38838986e-02 -1.45240799e-02
  3.46166082e-02 -1.18638633e-03 -1.11637199e-02 -4.33767997e-02
  3.87134776e-02 -1.10526502e-01 -3.48968022e-02 -4.00225545e-05
 -5.83496802e-02 -5.40265366e-02  7.78535232e-02  2.95943413e-02
 -6.28889129e-02 -4.64349687e-02 -7.47413114e-02  3.95335723e-03
 -4.04338501e-02  3.73055600e-02 -8.55343267e-02  4.48730201e-33
 -3.50924879e-02 -1.56446174e-02 -7.22830929e-03  2.49861516e-02
  3.57564203e-02  6.21017860e-03  1.51919380e-01 -7.46367499e-03
 -9.71325934e-02 -5.66449203e-02 -9.35367048e-02  1.13246858e-01
 -4.83479127e-02  6.24268614e-02 -2.23895069e-02 -7.29119033e-02
  6.48590848e-02  4.06348445e-02  6.36322657e-03 -2.87854783e-02
  6.24035336e-02 -1.07394280e-02 -5.29029919e-03  7.14192167e-03
  3.13132778e-02 -2.74293348e-02 -3.70561145e-03  7.11173052e-03
  5.97937033e-02 -1.19363433e-02 -1.04613649e-03 -1.74201969e-02
 -1.02822231e-02  2.23700386e-02  1.56631414e-02  2.15066820e-02
 -5.29124402e-02 -1.04479762e-02 -1.94508750e-02 -1.14976512e-02
  3.02083557e-03 -1.12390658e-02 -6.91999197e-02 -6.53920397e-02
 -9.55132954e-03 -2.45471522e-02  9.44314245e-03 -1.46679273e-02
  1.48020312e-01 -9.23268870e-02  1.29062710e-02 -8.46711639e-03
 -5.53757548e-02 -4.37644683e-02  6.23599514e-02 -1.89321376e-02
 -1.59546100e-02  3.40365656e-02  2.19665351e-03  6.45069256e-02
  1.06121115e-02 -9.53414384e-03  3.78961861e-02 -2.00729575e-02
  3.63340899e-02  8.40390995e-02 -5.70330359e-02 -5.21647185e-02
  5.93085773e-03 -2.96423994e-02 -7.33475834e-02 -7.12565891e-03
 -1.90381426e-02  1.48556316e-02  7.58868158e-02  5.18714041e-02
 -6.36125281e-02 -4.90228944e-02  7.33331591e-02  5.20917773e-02
 -1.39190136e-02  9.17936582e-03 -9.36400518e-03  9.13082883e-02
  3.12657543e-02 -2.69108801e-03 -1.71005670e-02  6.38334081e-03
 -2.67530009e-02  1.04626022e-01  1.07093053e-02 -1.16407953e-01
 -5.08369021e-02  1.90012474e-02  2.48082578e-02 -4.73635290e-33
  2.01126025e-03  9.45315659e-02 -6.02742769e-02  4.74375673e-02
 -8.24405849e-02  4.40653004e-02  2.24190820e-02  2.70473957e-02
 -3.37186009e-02  4.97643203e-02  9.44557339e-02 -2.15480700e-02
  8.14988241e-02 -3.46381962e-02  5.81768900e-03 -7.73716122e-02
 -7.15903491e-02 -1.42901158e-02  5.68126477e-02  5.56125417e-02
  1.11158408e-01  2.08319351e-02 -7.64855817e-02  5.28735258e-02
 -9.24778450e-03  1.65466703e-02  7.39115477e-02  7.02641234e-02
 -2.80393604e-02  1.98304206e-02 -7.34996945e-02 -2.49537397e-02
 -5.38874343e-02  5.58266789e-02  5.64438626e-02  6.10975884e-02
  7.26606995e-02 -3.97346206e-02 -8.23897310e-03 -5.05296253e-02
  8.96708593e-02 -5.71151227e-02 -5.03095277e-02  5.89848571e-02
  5.73783647e-03 -2.88626626e-02 -1.92604303e-01  5.32580875e-02
 -6.52213395e-02 -8.06954503e-03  4.36144993e-02  1.39683262e-02
 -1.53937098e-02 -1.13801606e-01 -4.69124019e-02  1.01449853e-03
  1.60340350e-02  7.25113461e-03  6.36380985e-02  6.04140293e-03
  5.62608391e-02 -8.99454206e-02  9.33947321e-03  1.19842701e-02
 -1.82437971e-02  4.83175144e-02 -2.73684096e-02  1.10265724e-01
  3.18500996e-02 -1.27055673e-02  2.31075939e-02 -4.30759462e-03
  1.42790908e-02 -9.43518803e-03 -1.36186630e-02 -5.10810241e-02
  4.30396833e-02 -4.92045097e-02  2.41190512e-02  4.11138646e-02
  4.90058912e-03  9.23611689e-03  6.43780604e-02  5.64334393e-02
  5.21977916e-02  1.38381854e-01  3.33749466e-02  1.22179419e-01
  3.66357937e-02  6.78416044e-02 -7.37354085e-02 -3.88376415e-02
 -1.13976533e-02  4.29366082e-02 -4.55427095e-02 -3.44440281e-08
 -6.52878210e-02  1.20188175e-02  1.22070044e-01  5.69572002e-02
  4.23296392e-02  2.97695938e-02  7.10308552e-02 -9.63275060e-02
 -7.99491443e-03  4.73489575e-02 -3.81042808e-02  6.11718372e-02
 -1.17632365e-02 -5.35496660e-02  3.48988399e-02  3.47197279e-02
 -7.51976818e-02  5.26228128e-03  8.26423243e-03 -1.53087089e-02
  5.94870793e-03  1.85880121e-02  5.74503615e-02  1.14515074e-01
  3.27589065e-02 -6.55324981e-02 -6.71041980e-02  8.54292437e-02
  1.59347374e-02 -4.39508334e-02 -5.13353851e-03 -2.87439786e-02
  7.85687640e-02 -5.92796393e-02  2.66437158e-02  9.03317407e-02
  5.79667790e-03 -1.64703745e-02 -3.41792055e-03 -1.65103097e-02
 -6.05849065e-02 -5.83085343e-02 -7.23865209e-03 -6.07456043e-02
 -4.74499948e-02  4.22054157e-02 -7.03032613e-02 -9.40806568e-02
 -3.22465450e-02 -3.32548991e-02 -1.27354562e-02 -3.13648731e-02
 -5.85877635e-02 -5.73984301e-03 -3.19891796e-02  9.67901386e-03
 -4.59123440e-02 -6.03840947e-02 -3.90917528e-03  4.12734747e-02
 -6.85639912e-03  2.05475688e-02 -2.89010326e-03  1.07710599e-03]",2,2
albumentations,13,albumentations is a python library for image augmentation image augmentation is used in deep learning and computer vision task to increase the quality of trained model the purpose of image augmentation is to create new training sample from the existing data here is an example of how you can apply some pixel level augmentation from albumentations to create new image from the original one alexander buslaev computer vision engineer at mapbox kaggle masteralex parinov kaggle mastervladimir i iglovikov staff engineer at lyft level kaggle grandmasterevegene khvedchenya computer vision research engineer at pi ata farm kaggle grandmastermikhail druzhinin kaggle expertalbumentations requires python or higher to install the latest version from pypi other installation option are described in the documentation the full documentation is available at http albumentations ai doc please start with the introduction article about why image augmentation is important and how it help to build better model if you want to use albumentations for a specific task such a classification segmentation or object detection refer to the set of article that ha an in depth description of this task we also have a list of example on applying albumentations for different use case we have example of using albumentations along with pytorch and tensorflow check the online demo of the library with it you can apply augmentation to different image and see the result also we have a list of all available augmentation and their target pixel level transforms will change just an input image and will leave any additional target such a mask bounding box and keypoints unchanged the list of pixel level transforms spatial level transforms will simultaneously change both an input image a well a additional target such a mask bounding box and keypoints the following table show which additional target are supported by each transform to run the benchmark yourself follow the instruction in benchmark readme mdresults for running the benchmark on the first image from the imagenet validation set using an intel r xeon r gold cpu all output are converted to a contiguous numpy array with the np uint data type the table show how many image per second can be processed on a single core higher is better python and library version python default jun gcc numpy pillow simd post opencv python scikit image scipy to create a pull request to the repository follow the documentation at http albumentations ai doc contributing in some system in the multiple gpu regime pytorch may deadlock the dataloader if opencv wa compiled with opencl optimization adding the following two line before the library import may help for more detail http github com pytorch pytorch issue if you find this library useful for your research please consider citing albumentations fast and flexible image augmentation,"[('image augmentation', 0.555), ('image augmentation image augmentation', 0.5389), ('albumentations', 0.5317), ('pixel level augmentation', 0.5242), ('flexible image augmentation', 0.498), ('available augmentation', 0.4914), ('augmentation', 0.469), ('tensorflow', 0.3971), ('http albumentations', 0.389), ('imagenet validation', 0.3827)]","[-2.81295255e-02 -8.53292048e-02  7.00275749e-02 -1.93128884e-02
  5.70881516e-02  5.72730415e-03 -4.72776964e-02 -1.48157673e-02
 -1.06520370e-01 -6.55992776e-02  2.44300696e-03 -7.31538143e-03
 -6.78364048e-03 -1.77494111e-03 -6.50937157e-03 -3.02888304e-02
 -8.47901404e-03  1.34081200e-01 -9.56156477e-02 -6.18876405e-02
  1.56544205e-02 -6.30377606e-02  4.31553423e-02 -6.45581484e-02
  3.14580649e-02  1.48723172e-02 -2.61264089e-02 -6.53247908e-02
  3.97317074e-02 -8.51890072e-02  1.33262994e-02  2.34048758e-02
  8.90254602e-03  1.48331700e-02 -3.61167416e-02  2.75830664e-02
  3.02659087e-02 -3.51367518e-02 -4.17942218e-02  2.14702357e-02
 -2.36101225e-02 -7.98860490e-02 -3.27663645e-02 -2.85173822e-02
  4.42870110e-02  3.94198578e-03  2.54785232e-02 -9.87870544e-02
 -3.83420363e-02  2.33298000e-02  1.21685565e-02 -7.98137486e-02
 -1.07299656e-01  1.05722100e-01 -3.93410139e-02 -4.39824909e-02
  1.22418441e-03 -3.38599980e-02  3.08238268e-02  2.22260784e-02
  5.84439002e-02 -7.31861442e-02 -5.35253286e-02  1.04693204e-01
 -3.17770392e-02  1.58462506e-02  1.49908820e-02 -4.83940840e-02
  7.04191551e-02 -1.19690694e-01 -2.11045407e-02 -8.24041956e-04
 -5.95337711e-02  2.96727289e-02 -3.10562365e-03 -1.57056618e-02
  1.16463915e-01 -1.80852488e-02  6.28304258e-02 -1.01524405e-01
  2.80264784e-02  1.96857881e-02  3.64587381e-02 -5.99302500e-02
  2.92648841e-02 -1.08161699e-02 -1.07211255e-01  5.57565503e-02
 -2.64700912e-02 -1.27351163e-02  5.84723614e-02 -3.54486778e-02
 -6.40201569e-02 -2.04815343e-02  4.61329594e-02 -6.13034423e-03
 -2.38377042e-02 -1.15886629e-01 -3.47709768e-02  2.74024513e-02
 -5.85095324e-02 -8.23628977e-02 -1.30967759e-02  4.86266008e-03
  2.81863324e-02 -2.04153955e-02  4.13683280e-02 -7.68651674e-03
  4.63572182e-02  6.41253442e-02 -8.91531759e-04  4.17393595e-02
  1.91720836e-02 -1.15472227e-01  4.88918610e-02  3.82914767e-02
 -9.26519707e-02  1.20604727e-02  5.67339584e-02 -2.47548595e-02
 -7.66940042e-02  1.61145031e-02  3.33594419e-02 -2.38042586e-02
 -1.67913642e-03 -2.44940408e-02 -6.09708130e-02  5.51662070e-33
  2.59309001e-02  2.18895022e-02  5.40788583e-02 -1.41225383e-02
  7.16226026e-02 -4.12256196e-02 -3.03522311e-02 -4.55250684e-03
 -2.77493726e-02 -7.34012127e-02 -1.12858281e-04  8.35429728e-02
 -2.16928162e-02  1.53155819e-01  5.34526333e-02  3.71220708e-02
 -1.10874521e-02  9.00892615e-02  8.73827338e-02  7.99000636e-02
 -4.25779149e-02 -7.53330588e-02  3.58877555e-02  1.32211819e-01
  1.86294597e-02 -4.19061594e-02  5.31386072e-03  8.77376366e-03
  9.31435730e-03  3.04158451e-03 -9.24996287e-02  1.17810741e-02
  3.02200802e-02 -2.78299823e-02  1.94482431e-02  4.27884003e-03
 -6.30791066e-03 -3.62406597e-02 -2.59751510e-02 -5.38514107e-02
 -2.17997823e-02  6.38305619e-02 -4.53584827e-02 -2.37623509e-02
 -2.09556445e-02  5.95154474e-04 -4.88980561e-02  6.35395721e-02
 -1.01492798e-03  3.61837149e-02  1.73670258e-02  1.52290426e-02
  6.36153482e-03 -7.12724254e-02 -1.09025324e-02  5.79905659e-02
  7.56915146e-03  3.89344469e-02  2.30911989e-02  1.31628131e-02
  1.35080917e-02 -2.79008858e-02  3.70052755e-02 -2.19615679e-02
 -4.07954492e-02 -3.21116671e-02  2.76296283e-03 -1.86344553e-02
 -1.25308782e-02  1.22738285e-02 -1.33499920e-01  3.07059642e-02
 -2.35140076e-04  8.31432734e-03  1.93045922e-02 -5.87351918e-02
 -1.69540774e-02 -9.24281552e-02  4.52153459e-02  1.08103082e-01
 -1.22459173e-01  3.75882648e-02 -9.22580599e-04  3.22745703e-02
 -6.02735952e-02 -1.52708804e-02 -1.13009745e-02  1.60435215e-03
  3.05675939e-02 -1.07619143e-03 -3.29801477e-02  5.00289015e-02
  2.48627085e-02  6.70344308e-02 -1.64084993e-02 -4.51320623e-33
  2.11872789e-03  1.03969336e-01 -7.70093948e-02  8.19032788e-02
  1.39661152e-02  5.73958876e-03  5.89208901e-02  6.40740171e-02
  5.45826852e-02 -3.85814644e-02  4.01291363e-02  4.57935445e-02
 -2.56368406e-02 -8.46890733e-02 -1.00453988e-01 -1.22518040e-01
 -8.79921839e-02 -4.86658923e-02  2.21347218e-05  3.09398919e-02
  3.87134850e-02  3.45942304e-02  3.92838046e-02  2.04676366e-03
 -6.32326538e-03  2.84170881e-02 -3.95319648e-02  1.03376741e-02
  8.65649804e-02  1.20969936e-02  5.67512177e-02 -3.99748534e-02
  5.53672984e-02  4.34672572e-02  3.96724790e-02  4.11811396e-02
  1.32215589e-01 -3.11855003e-02 -5.97992213e-03  5.00015821e-03
  6.09757788e-02  5.52838892e-02  3.69941145e-02  7.08853826e-02
 -2.03314517e-02 -3.39275599e-02 -1.22144334e-02  3.67965139e-02
 -5.28176799e-02 -4.37350608e-02 -4.33124118e-02 -2.82342918e-02
 -8.65714028e-02  3.93841080e-02  2.14618128e-02 -7.73544330e-03
  2.02491805e-02 -2.08758377e-02  5.63303903e-02  2.41899863e-02
 -3.63797694e-02 -5.74304909e-02 -4.28939424e-03 -4.64934111e-02
 -3.68223824e-02  3.76384184e-02 -1.01984568e-01 -1.88651904e-02
 -4.33844589e-02  4.45137843e-02  3.68730463e-02 -1.25481989e-02
  2.31025461e-02  9.55731794e-02  2.46688593e-02 -2.46252976e-02
  3.44413072e-02 -1.07241469e-02 -7.98315555e-03 -9.44157615e-02
 -2.74649449e-02 -1.49503592e-02 -4.10411283e-02  1.22730829e-01
  6.03966527e-02  8.61747786e-02  5.29834777e-02 -6.57970011e-02
  4.57696654e-02 -3.17352638e-02 -6.89304769e-02  1.40027646e-02
  3.99273355e-03  9.43841189e-02 -4.52272333e-02 -2.81972721e-08
 -6.63617253e-02 -2.50713788e-02  1.07874036e-01 -5.85077405e-02
  4.03651446e-02 -1.13345245e-02  5.47670610e-02  7.40119591e-02
  3.34102251e-02 -5.12033887e-02  2.58836038e-02  3.07626314e-02
 -6.88937530e-02 -2.40982622e-02  5.33092245e-02  5.02525903e-02
  3.58315161e-03  1.02963924e-01  1.20510841e-02 -4.60954756e-03
 -6.26133233e-02  1.43387020e-02  4.76884358e-02 -6.45707175e-02
  6.33695349e-02 -1.50363380e-02  1.14786429e-02  8.82963687e-02
 -4.56586033e-02 -2.42828131e-02  2.11226381e-02 -1.75449997e-02
  1.00737482e-01 -2.58219130e-02  1.25252828e-02  9.52809602e-02
 -9.99840163e-03 -3.67737338e-02 -5.93320467e-02 -1.20841675e-02
 -2.40615066e-02  5.65319844e-02  3.34647484e-02 -5.14261685e-02
  6.58750087e-02 -4.40156236e-02  4.12286445e-02 -1.86694972e-02
 -3.95768136e-02 -2.87181158e-02 -1.13460962e-02 -5.13509894e-03
 -3.97718437e-02  3.12790908e-02 -9.06433165e-03  2.88049001e-02
  2.42047515e-02 -2.01904271e-02  8.33332464e-02  9.79887098e-02
  8.26145988e-03  1.38795115e-02  2.69860160e-02 -4.68351208e-02]",2,2
timm,13,thanks to the following for hardware support and a big thanks to all github sponsor who helped with some of my cost before i joined hugging face more model more fixespytorch image model timm is a collection of image model layer utility optimizers scheduler data loader augmentation and reference training validation script that aim to pull together a wide variety of sota model with ability to reproduce imagenet training result the work of many others is present here i ve tried to make sure all source material is acknowledged via link to github arxiv paper etc in the readme documentation and code docstrings please let me know if i missed anything all model architecture family include variant with pretrained weight there are specific model variant without any weight it is not a bug help training new or better weight is always appreciated here are some example training hparams to get you started a full version of the list below with source link can be found in the documentation several le common feature that i often utilize in my project are included many of their addition are the reason why i maintain my own set of model instead of using others via pip model validation result can be found in the documentation and in the result tablesmy current documentation for timm cover the basic hugging face timm doc will be the documentation focus going forward and will eventually replace the github io doc above getting started with pytorch image model timm a practitioner s guide by chris hughes is an extensive blog post covering many aspect of timm in detail timmdocs is quickly becoming a much more comprehensive set of documentation for timm a big thanks to aman arora for his effort creating timmdocs paperswithcode is a good resource for browsing the model within timm the root folder of the repository contains reference train validation and inference script that work with the included model and other feature of this repository they are adaptable for other datasets and use case with a little hacking see documentation for some basic and training hparams for some train example that produce sota imagenet result one of the greatest asset of pytorch is the community and their contribution a few of my favourite resource that pair well with the model and component here are listed below the code here is licensed apache i ve taken care to make sure any third party code included or adapted ha compatible permissive license such a mit bsd etc i ve made an effort to avoid any gpl lgpl conflict that said it is your responsibility to ensure you comply with license here and condition of any dependent license where applicable i ve linked the source reference for various component in docstrings if you think i ve missed anything please create an issue so far all of the pretrained weight available here are pretrained on imagenet with a select few that have some additional pretraining see extra note below imagenet wa released for non commercial research purpose only http image net org download it s not clear what the implication of that are for the use of pretrained weight from that dataset any model i have trained with imagenet are done for research purpose and one should assume that the original dataset license applies to the weight it s best to seek legal advice if you intend to use the pretrained weight in a commercial product several weight included or reference here were pretrained with proprietary datasets that i do not have access to these include the facebook wsl ssl swsl resne xt and the google noisy student efficientnet model the facebook model have an explicit non commercial license cc by nc http github com facebookresearch semi supervised imagenet k model http github com facebookresearch wsl image the google model do not appear to have any restriction beyond the apache license and imagenet concern in either case you should contact facebook or google with any question,"[('more fixespytorch image model timm', 0.561), ('image model layer utility optimizers', 0.4561), ('pytorch image model timm', 0.4487), ('model architecture family', 0.443), ('imagenet training result', 0.4254), ('sota imagenet', 0.4156), ('more model', 0.4066), ('sota model', 0.4038), ('additional pretraining', 0.3966), ('model', 0.3957)]","[-4.42800336e-02 -6.99424446e-02  3.29316482e-02 -9.90444887e-03
  5.39960787e-02 -3.53736308e-04 -1.25537273e-02  5.43826185e-02
 -4.49792631e-02 -4.99478495e-03  3.84693928e-02 -8.52119830e-03
  7.10653793e-03  6.24919347e-02  1.78102646e-02 -2.37673949e-02
  1.12040509e-02  7.28530586e-02 -5.43957874e-02 -8.14642981e-02
 -2.72311121e-02 -4.09063064e-02  3.09739020e-02 -3.06982622e-02
 -1.92646962e-02 -2.69629564e-02 -4.25889306e-02 -3.66089307e-02
  2.99788509e-02 -6.16733171e-02 -4.51089330e-02  6.02635555e-03
  4.83547151e-02  3.55905667e-02 -2.78259981e-02  4.46777008e-02
  5.54625364e-03 -3.87524767e-03 -1.91815756e-02 -1.23771140e-03
 -1.77296996e-02 -1.88215375e-02 -1.97440088e-02 -5.04184924e-02
  8.49199742e-02 -2.32831687e-02  3.64311896e-02 -9.47185308e-02
  3.69680976e-03 -3.42507251e-02 -8.71502683e-02 -4.09802943e-02
 -1.05144478e-01  2.04195362e-02  3.34317125e-02  8.00056368e-05
  9.31919087e-03  4.15361300e-03 -1.20487865e-02 -1.28961205e-02
 -5.33648347e-03 -1.14415335e-02 -1.39641151e-01  4.69777212e-02
  5.22623993e-02  3.58552560e-02 -9.96208563e-03 -1.43494774e-02
  1.38312757e-01 -1.25274643e-01 -2.64795441e-02 -2.13512778e-02
  5.66706731e-05  1.80639476e-02 -9.39879846e-03 -3.28545719e-02
  9.39979106e-02  8.43036696e-02  5.93265332e-02 -1.39613450e-01
  5.68805449e-03 -4.23277132e-02  4.90221046e-02 -5.71872480e-02
  6.21207282e-02  3.62845883e-02 -7.76536390e-02  7.11302832e-02
  2.94698477e-02 -6.74490258e-02  2.24584825e-02 -4.84644296e-03
 -3.26431245e-02  6.48286799e-03  5.15234172e-02  1.40585927e-02
  1.11339148e-02 -9.00146142e-02 -3.67934071e-02  5.62152788e-02
 -2.41241953e-03 -8.87003019e-02  4.10809815e-02  3.35563570e-02
  7.93254897e-02 -4.91773803e-03  5.15863597e-02  4.77807671e-02
  6.12529032e-02 -2.01608595e-02  2.13118102e-02  9.99436807e-03
 -6.25669286e-02 -5.86857609e-02  1.05949946e-01 -3.75868157e-02
 -7.30211362e-02 -1.56552959e-02  8.57576132e-02  4.39990945e-02
 -9.87524316e-02  6.58659264e-05 -1.88866481e-02  9.28841252e-03
  5.05000632e-03 -2.25215126e-02 -1.33005410e-01  7.35904206e-33
 -1.40356664e-02  4.67145629e-02  7.54077882e-02 -4.60110977e-02
  8.06484967e-02 -5.97542524e-02  7.05593005e-02  1.17777120e-02
  2.59940587e-02 -9.34252143e-02 -9.26482528e-02 -2.33323537e-02
 -9.62349027e-02  1.36863813e-01  3.10013518e-02 -9.44865569e-02
 -9.45147499e-02  4.93218563e-02  8.60026702e-02  4.56283316e-02
  2.17344798e-02 -6.11609267e-03  1.07180160e-02  5.52069582e-02
  2.84407903e-02  1.53319966e-02  2.82992292e-02 -9.92826000e-03
 -2.67429613e-02  1.23109086e-03 -2.91140433e-02  5.98026104e-02
  3.11766919e-02  2.48612855e-02 -2.99044847e-02 -8.13800376e-03
 -4.73589785e-02 -3.51081043e-02 -3.57014826e-03 -7.21360296e-02
 -7.00399429e-02  4.94961143e-02 -5.49185723e-02 -5.25258109e-03
 -4.01625149e-02 -1.48004796e-02 -1.09516522e-02  7.56293833e-02
 -3.26645598e-02  3.37451883e-02  3.99777181e-02  1.46833342e-02
 -5.36234975e-02  8.25707242e-03 -7.65542090e-02  2.02391837e-02
  3.06678489e-02  1.42566599e-02  3.56071331e-02  4.73151729e-02
  4.16834392e-02 -3.70202325e-02  7.62834176e-02  1.24504848e-03
  6.86034858e-02 -3.80190276e-02  2.40987241e-02 -4.61871549e-02
 -2.85049547e-02  7.13049844e-02 -1.01974085e-01  2.89980620e-02
 -7.71905528e-03 -5.02890982e-02  8.00454989e-02  2.45451403e-04
 -3.02021764e-02 -2.71121543e-02 -9.15871486e-02  4.99660037e-02
 -6.49011433e-02  5.01085334e-02 -1.97867267e-02 -6.43116683e-02
 -3.27333547e-02  2.63791848e-02  7.64876232e-02 -2.40556952e-02
  5.35223633e-02  3.51359658e-02  2.46300288e-02  3.23264301e-02
 -9.36065614e-03  5.35547398e-02 -3.75789851e-02 -4.24600241e-33
  3.73027734e-02  4.10471112e-02 -5.95827103e-02  9.10391286e-02
 -1.11957593e-03 -5.23421057e-02  6.76663890e-02  3.10513880e-02
  4.96137841e-03 -3.90208103e-02  1.02191254e-01 -2.29461603e-02
  4.77683032e-03 -7.44923651e-02 -6.68280348e-02 -3.99866849e-02
  8.64132028e-03 -8.19575563e-02  6.63031340e-02  1.82000920e-02
  2.38596592e-02  9.02117565e-02 -4.99160700e-02  2.63271574e-02
 -7.94583634e-02  2.99359132e-02 -5.36746494e-02  3.22013758e-02
  6.00502715e-02  7.49426708e-03  6.38613617e-03 -5.28944023e-02
  6.47735372e-02 -1.49634238e-02  3.59503105e-02  7.61656240e-02
  7.90218115e-02 -6.05987618e-04 -3.92320286e-03  8.18603411e-02
  1.21113069e-01  4.67205932e-03  4.68185591e-03  7.16322437e-02
 -1.55900163e-03 -4.64070998e-02 -6.33681491e-02 -3.47985066e-02
 -4.45264280e-02  2.03262120e-02 -2.38730889e-02 -7.40381703e-02
 -2.61892136e-02 -2.77127493e-02 -1.96877997e-02  9.11053363e-03
  5.61570451e-02 -4.18119058e-02 -1.34336073e-02  1.39029103e-03
 -2.45569330e-02 -1.01910926e-01 -4.17293608e-02 -7.34178424e-02
 -1.48794951e-03  2.72702724e-02 -9.22753215e-02 -3.08800079e-02
 -2.75018755e-02  6.89097121e-02 -3.62413190e-03 -2.81861257e-02
  2.72887312e-02  9.19684619e-02 -7.99004547e-03 -4.24368270e-02
  1.60081163e-02  5.73719181e-02  3.68485413e-02 -6.53510541e-02
 -3.01833283e-02  1.89271085e-02  4.53883456e-03  1.00919195e-01
  3.21724154e-02  1.30957603e-01  6.26368672e-02 -7.87554402e-03
  7.61506259e-02 -4.77626771e-02 -1.56338848e-02  3.18778493e-02
  9.88295674e-02  7.27247968e-02  2.31448398e-03 -3.03297334e-08
 -4.38210107e-02 -6.99751126e-03  6.01083972e-02 -1.15549508e-02
  3.39425169e-02 -2.62232386e-02  3.03658494e-03  1.18924066e-01
  2.36839466e-02 -3.98604646e-02  1.70428976e-02 -3.15778493e-03
 -5.57354242e-02 -3.13074216e-02  6.05464466e-02  1.70905907e-02
  3.44753228e-02  6.52542412e-02  4.27832976e-02 -1.04030393e-01
 -2.75271423e-02 -1.38622208e-03 -1.81017518e-02 -2.81224456e-02
  4.30191606e-02 -3.38968675e-04 -5.09092584e-02  8.29631761e-02
  9.46690049e-03 -1.69838183e-02  2.04879865e-02 -3.91700938e-02
  7.51710609e-02 -4.68347520e-02  9.19993669e-02  6.64315820e-02
  6.60684798e-03 -2.56067440e-02 -1.49195138e-02  1.22669507e-02
 -3.86632569e-02 -1.42912166e-02  3.22961668e-03 -1.87953864e-03
  8.79284516e-02 -5.49684698e-03  2.69521102e-02 -1.27683625e-01
 -2.86817309e-02 -1.70168411e-02  1.37153566e-02  4.83728759e-02
 -4.01007570e-02  4.73897196e-02 -9.29324422e-03 -1.02526313e-02
  7.21907541e-02 -6.23910800e-02  5.77558279e-02  8.06850865e-02
 -9.12182871e-03  1.47772757e-02  8.25936999e-03 -2.77042133e-03]",2,2
torchmetrics,12,machine learning metric for distributed scalable pytorch application what is torchmetrics implementing a metric built in metric doc community license simple installation from pypiinstall using condapip from sourcepip from archiveextra dependency for specialized metric install latest developer versiontorchmetrics is a collection of pytorch metric implementation and an easy to use api to create custom metric it offer you can use torchmetrics with any pytorch model or with pytorch lightning to enjoy additional feature such a the module based metric contain internal metric state similar to the parameter of the pytorch module that automate accumulation and synchronization across device this can be run on cpu single gpu or multi gpus for the single gpu cpu case module metric usage remains the same when using multiple gpus or multiple node implementing your own metric is a easy a subclassing an torch nn module simply subclass torchmetrics metric and implement the following method similar to torch nn most metric have both a module based and a functional version the functional version are simple python function that a input take torch tensor and return the corresponding metric a a torch tensor we currently have implemented metric within the following domain in total torchmetrics contains metric the lightning torchmetrics team is hard at work adding even more metric but we re looking for incredible contributor like you to submit new metric and improve existing one join our slack to get help become a contributor for help or question join our huge community on slack we re excited to continue the strong legacy of open source software and have been inspired over the year by caffe theano kera pytorch torchbearer ignite sklearn and fast ai if you want to cite this framework feel free to use github s built in citation option to generate a bibtex or apa style citation based on this file but only if you loved it please observe the apache license that is listed in this repository in addition the lightning framework is patent pending,"[('scalable pytorch application', 0.6201), ('total torchmetrics', 0.548), ('torchmetrics', 0.5134), ('torch tensor', 0.4923), ('multi gpus', 0.4905), ('metric implementation', 0.4829), ('machine learning metric', 0.4735), ('multiple gpus', 0.4658), ('specialized metric install', 0.4583), ('pytorch module', 0.4543)]","[-8.44253898e-02 -1.03152528e-01 -2.97903921e-02 -1.73360705e-02
 -6.13802765e-03 -5.17949052e-02 -2.80110817e-02  7.02329502e-02
 -9.03299153e-02 -6.61812201e-02 -5.33902459e-02 -8.65702331e-03
 -1.16533093e-01  5.88946193e-02 -1.42753031e-02  1.25982873e-02
 -3.33977910e-03  9.09351930e-02 -8.54193419e-02 -1.38905987e-01
  2.75641121e-03 -7.25566819e-02  6.65644482e-02  1.88365281e-02
 -4.42720056e-02 -5.45439683e-03  2.68458240e-02 -5.45765907e-02
  3.34364250e-02  2.20693853e-02 -2.70378329e-02  7.20264250e-03
 -2.50920653e-02  3.61966691e-03 -1.78357568e-02  1.74994103e-03
 -3.78918182e-03 -3.48007604e-02 -1.69216376e-02 -2.18686163e-02
 -1.93021297e-02 -5.25422059e-02 -2.13181805e-02  6.99433871e-03
  1.13640986e-02 -5.85158467e-02  2.47983504e-02 -3.84037308e-02
 -3.03100664e-02 -9.76522043e-02 -3.06376070e-02 -8.38170201e-02
 -6.50259554e-02  3.20192613e-02  1.47393933e-02 -1.45649929e-02
  2.42136121e-02 -3.13601755e-02  5.95614389e-02 -9.65377986e-02
  3.81269455e-02 -1.81690268e-02 -5.45243770e-02  2.16664802e-02
 -5.54852299e-02  4.74095605e-02  3.50624998e-03 -2.37952322e-02
  1.33456051e-01 -8.25988352e-02 -7.77525036e-03 -2.74622981e-02
 -1.20000891e-01  1.04602963e-01 -4.52580452e-02 -2.20855996e-02
  8.79338756e-02  4.61679436e-02  4.10534069e-02 -9.10205022e-02
  2.65843775e-02 -1.48968231e-02  3.39986496e-02  5.72262332e-02
  5.60213253e-02 -1.17018435e-03 -6.30556941e-02  6.24480732e-02
  5.20689180e-03 -4.08689007e-02  5.58221564e-02 -2.20865682e-02
 -3.60198431e-02 -6.20365748e-03 -1.69468801e-02 -3.59515995e-02
  3.20043117e-02 -7.94805661e-02 -5.34001924e-02  5.47869131e-02
 -5.56654623e-03 -5.63074499e-02 -6.27538422e-03  6.74060658e-02
 -2.67749578e-02  8.18689466e-02 -3.44739761e-03  6.95278421e-02
  3.67689915e-02  3.76659888e-03  1.12685952e-02  2.45977137e-02
 -3.66442315e-02 -6.85885549e-02  4.93452400e-02 -9.43203515e-04
 -4.99141067e-02  5.68673760e-02  3.47741283e-02  1.28481328e-01
 -9.22079906e-02  3.29845995e-02 -3.56752612e-02  2.77748369e-02
 -2.30177771e-02  3.07564437e-02 -1.12996742e-01  4.63970008e-33
 -4.57827114e-02  4.57469076e-02 -3.97793241e-02 -2.85091083e-02
 -7.78729841e-03 -5.31614944e-02  4.49593179e-02 -3.86157585e-03
  7.02157477e-03 -7.49503896e-02  2.43168604e-03  1.09910712e-01
 -3.54643688e-02  7.48443827e-02  4.07279506e-02 -3.04873250e-02
  1.95326414e-02  8.14196020e-02  5.35016768e-02  1.31457031e-03
  4.62160744e-02 -8.31063278e-03  3.35221589e-02  1.01182476e-01
 -4.17010561e-02  7.87211582e-02 -2.40834001e-02  9.60710086e-03
 -4.95516695e-03  2.90106703e-02 -1.89778395e-02  9.93944053e-03
 -1.31280033e-03 -9.07965563e-03 -9.62720439e-03 -2.04459783e-02
 -2.05360334e-02 -7.13491291e-02  3.25426571e-02 -3.77523750e-02
 -5.85004613e-02  7.42346197e-02 -1.91307347e-02 -8.80263522e-02
 -3.95373143e-02  7.25237429e-02  4.77556745e-03  6.86605573e-02
  3.03306449e-02 -7.09170029e-02 -6.74999058e-02  4.87416424e-02
 -7.15218997e-03  8.43706429e-02  2.93192156e-02 -2.06962582e-02
  4.70712408e-02 -7.09049962e-03  1.11321062e-01  3.85482349e-02
 -8.88695940e-03  3.55188474e-02 -2.26005679e-03 -8.13673995e-03
 -4.98075783e-02 -2.96547767e-02 -4.77357768e-02  4.13731784e-02
 -4.16541882e-02  7.89068714e-02 -1.10568047e-01  4.83578146e-02
  2.10471768e-02  7.18099251e-03  4.28092014e-03 -1.46434708e-02
  7.42674917e-02 -6.91063926e-02 -4.02464941e-02  8.07285085e-02
 -1.81593925e-01  6.37197271e-02  5.34714982e-02 -5.43671921e-02
 -5.56738675e-02 -1.39055159e-02 -1.47515237e-02 -1.42490640e-02
 -7.36008361e-02  3.61418389e-02 -1.04401223e-01  8.31999443e-03
  6.58906624e-02  5.76506853e-02 -4.80173752e-02 -3.40399948e-33
 -3.40615101e-02  4.70415615e-02 -3.71018276e-02  1.22930847e-01
  7.50226602e-02  1.96648892e-02 -3.25392149e-02 -6.96403682e-02
  2.33508348e-02 -2.06178091e-02 -5.35302912e-04 -4.51046368e-03
  4.85070087e-02 -1.06430324e-02  2.79790368e-02  3.36073004e-02
 -2.27014571e-02 -3.60553339e-02  6.98966469e-05  7.11597409e-03
 -4.52407636e-02  9.66511220e-02 -9.54527855e-02 -2.83137728e-02
 -6.41790479e-02 -7.33136432e-03 -4.36199158e-02 -9.79167372e-02
 -1.01169283e-02  1.82837658e-02  7.97227845e-02 -6.13963092e-03
 -9.91060305e-03  3.36316302e-02 -3.63403149e-02 -8.64798389e-03
  1.28273189e-01 -8.32444895e-03  4.81857965e-03  5.40192574e-02
  1.41696855e-01  1.03376834e-02 -2.53050737e-02  1.20625738e-02
 -3.67157385e-02  4.15643677e-02 -8.06831717e-02  8.17623734e-03
 -4.08374816e-02 -2.65658256e-02  1.73032116e-02 -2.11668536e-02
  8.19147006e-03 -8.14073719e-03 -7.33357389e-04 -1.21080661e-02
  8.91614705e-02  9.00906026e-02  2.65080910e-02  3.64810117e-02
 -3.33067402e-02 -8.53775665e-02  4.12706025e-02  2.84132035e-03
 -2.16984283e-02  2.73003447e-04  1.79520082e-02  4.30885516e-02
 -2.30355877e-02 -1.68799199e-02  3.16865630e-02  7.24555627e-02
  5.27042337e-02  5.74359186e-02 -7.04904944e-02  6.68647662e-02
  2.76246611e-02  9.08445753e-03  3.05079892e-02 -2.50616092e-02
  3.30415703e-02  4.07695808e-02  4.32883017e-02  2.53591910e-02
  4.44262587e-02  4.87133339e-02  6.99062794e-02  2.49687321e-02
  4.82780784e-02 -5.22436611e-02 -2.59607919e-02  1.27637750e-02
  6.35339916e-02  5.11243530e-02  2.26744991e-02 -2.78034165e-08
  2.24389578e-03  2.58178189e-02 -7.04366667e-03  1.37411114e-02
 -2.69027203e-02  1.42552769e-02  2.13730671e-02  1.31103590e-01
 -7.98774604e-03  3.31374630e-02 -1.24230897e-02 -6.97260126e-02
 -5.29524907e-02  4.02894914e-02  4.91556674e-02  7.34770745e-02
 -3.43930237e-02  3.85103337e-02  1.25947194e-02 -5.86012080e-02
  2.24347832e-03  6.78825378e-02  2.36970317e-02 -3.76172294e-03
 -1.73144210e-02 -6.26673177e-02  1.98770519e-02 -2.39814539e-02
 -1.59471743e-02  2.28690840e-02  5.36093663e-04 -2.54506320e-02
  1.32504463e-01 -9.51374508e-03  1.18446708e-01  5.08863702e-02
 -3.65156792e-02  4.10858542e-02 -2.31971405e-03  4.10559848e-02
 -6.12639934e-02  9.47376247e-04 -8.53900891e-03 -1.51245166e-02
  3.09895985e-02 -4.02362738e-03 -4.58729900e-02 -4.00832295e-02
 -4.89453897e-02  8.78810734e-02  2.19653808e-02  6.06476218e-02
 -2.34760176e-02 -1.00137312e-02  5.82076497e-02  7.41318762e-02
 -3.33195948e-03 -5.50830401e-02 -1.64517686e-02 -3.18863168e-02
  3.33640538e-02 -4.43804227e-02 -6.58187345e-02 -1.61755402e-02]",2,2
mlflow,11,mlflow is a platform to streamline machine learning development including tracking experiment packaging code into reproducible run and sharing and deploying model mlflow offer a set of lightweight apis that can be used with any existing machine learning application or library tensorflow pytorch xgboost etc wherever you currently run ml code e g in notebook standalone application or the cloud mlflow s current component are mlflow tracking an api to log parameter code and result in machine learning experiment and compare them using an interactive ui mlflow project a code packaging format for reproducible run using conda and docker so you can share your ml code with others mlflow model a model packaging format and tool that let you easily deploy the same model from any ml library to batch and real time scoring on platform such a docker apache spark azure ml and aws sagemaker mlflow model registry a centralized model store set of apis and ui to collaboratively manage the full lifecycle of mlflow model nightly job status install mlflow from pypi via pip install mlflowmlflow requires conda to be on the path for the project feature nightly snapshot of mlflow master are also available here install a lower dependency subset of mlflow from pypi via pip install mlflow skinny extra dependency can be added per desired scenario for example pip install mlflow skinny panda numpy allows for mlflow pyfunc log model support official documentation for mlflow can be found at http mlflow org doc latest index html the current mlflow roadmap is available at http github com mlflow mlflow milestone we are seeking contribution to all of our roadmap item with the help wanted label please see the contributing section for more information for help or question about mlflow usage e g how do i do x see the doc or stack overflow to report a bug file a documentation issue or submit a feature request please open a github issue for release announcement and other discussion please subscribe to our mailing list mlflow user googlegroups com or join u on slack the program in example use the mlflow tracking api for instance run this program will use mlflow tracking api which log tracking data in mlruns this can then be viewed with the tracking ui the mlflow tracking ui will show run logged in mlruns at http localhost start it with note running mlflow ui from within a clone of mlflow is not recommended doing so will run the dev ui from source we recommend running the ui from a different working directory specifying a backend store via the backend store uri option alternatively see instruction for running the dev ui in the contributor guide the mlflow run command let you run a project packaged with a mlproject file from a local path or a git uri see example sklearn elasticnet wine for a sample project with an mlproject file to illustrate managing model the mlflow sklearn package can log scikit learn model a mlflow artifact and then load them again for serving there is an example training application in example sklearn logistic regression train py that you can run a follows note if using mlflow skinny pip install mlflow skinny for model serving additional required dependency namely flask will need to be installed for the mlflow server to function we happily welcome contribution to mlflow we are also seeking contribution to item on the mlflow roadmap please see our contribution guide to learn more about contributing to mlflow,"[('cloud mlflow', 0.7113), ('mlflow server', 0.693), ('mlflow roadmap', 0.6869), ('deploying model mlflow', 0.6811), ('mlflow tracking api', 0.6781), ('current mlflow roadmap', 0.6773), ('mlflow ui', 0.6716), ('http github com mlflow mlflow milestone', 0.6634), ('mlflow tracking ui', 0.657), ('mlflow model', 0.6466)]","[-3.53442021e-02 -1.30376413e-01  1.25466883e-02 -5.81093207e-02
  6.72158748e-02  1.08279241e-02 -3.10276486e-02 -2.03332342e-02
 -4.84729372e-02 -2.02723667e-02 -9.97228269e-03 -5.50644696e-02
 -1.44047709e-02 -1.04943536e-01 -1.21957306e-02 -2.20093112e-02
 -7.21447077e-03  6.18342720e-02 -6.89948574e-02 -1.09123617e-01
 -5.52344806e-02 -3.26085277e-02 -7.86417201e-02  7.80940950e-02
 -5.59373684e-02  1.23649225e-01  2.23466009e-02 -5.05758412e-02
  2.30081007e-02 -4.38874364e-02 -4.53118607e-02  9.62670613e-03
  1.83670577e-02  5.01020700e-02 -8.10656250e-02 -1.94247961e-02
  5.23927696e-02 -3.48026045e-02  4.33955565e-02 -5.80414869e-02
 -3.43865082e-02 -9.67256203e-02  2.49908376e-03 -7.77400807e-02
  7.38409758e-02 -3.32668871e-02  1.35357967e-02 -1.43093273e-01
 -8.74198303e-02  2.60531995e-02 -1.46819884e-03 -1.38450608e-01
  9.36318561e-03  8.78931060e-02 -4.97533605e-02 -3.66973318e-02
 -3.71117797e-03  1.76284905e-03  2.76679508e-02  6.86057191e-03
 -8.62146467e-02 -2.32911464e-02 -5.64361550e-02  6.18253052e-02
 -3.30209509e-02 -4.15214291e-03 -3.54450904e-02  4.36724126e-02
  1.07335158e-01 -4.13578302e-02 -3.99747901e-02 -5.80436038e-03
 -5.41520417e-02 -2.24416070e-02 -1.89078487e-02 -1.01428507e-02
  7.79723525e-02 -9.84073058e-03  2.97317095e-02 -7.87670389e-02
  4.33585905e-02  6.97546154e-02  2.94226618e-03  5.11270836e-02
 -8.11176002e-02 -6.21334976e-03 -6.44094869e-02  5.13484627e-02
  1.81919727e-02  1.23440903e-02 -1.74155142e-02  1.25127332e-02
 -1.17708705e-02 -2.92492919e-02  5.15432172e-02  6.62965924e-02
 -7.01221898e-02 -6.35962710e-02  8.54496937e-03 -8.92206095e-03
 -6.57946542e-02  3.88721712e-02  1.29535915e-02  2.26339488e-03
 -1.66103872e-03  2.00398024e-02 -1.24529155e-03  7.75886402e-02
  9.29246917e-02 -8.82291608e-03  3.07419654e-02 -3.10767479e-02
 -9.22651310e-03 -8.22719708e-02  4.55206409e-02  2.68218350e-02
 -5.55961914e-02  8.08642618e-03 -6.66995645e-02  1.10042952e-01
 -1.10248886e-01  4.14691642e-02  8.88671540e-03 -5.28522059e-02
 -4.69540209e-02  8.90026800e-03 -7.94638023e-02  4.56552014e-33
  1.78831220e-02 -2.28605811e-02 -2.32002325e-02 -4.11257334e-03
  1.03262059e-01 -3.48838642e-02  7.35048531e-03 -4.97780629e-02
  3.48519944e-02 -6.11835383e-02  2.31074588e-03  1.26642644e-01
 -1.08535610e-01  4.88457009e-02  1.89596582e-02 -9.32141766e-02
  1.79833882e-02  9.42484811e-02 -1.00166528e-02  3.24438177e-02
  4.37798016e-02 -6.40077963e-02  1.10866772e-02  7.81680048e-02
  1.17572866e-01  4.47914973e-02  2.52355058e-02  2.80655678e-02
  3.65435518e-02  5.26759848e-02 -2.55238498e-04 -5.91001660e-03
  7.37993000e-03 -1.58005059e-02 -1.47855969e-03  1.31946299e-02
 -6.45742416e-02 -6.58743083e-02  3.97228338e-02 -3.53493281e-02
  3.91704089e-04  1.51958503e-02 -9.37350839e-02 -2.51654051e-02
 -2.76648030e-02 -9.32326168e-03 -8.85305405e-02 -1.43651385e-02
  1.55361801e-01 -6.97018504e-02  3.38817537e-02  1.71950962e-02
  4.01870999e-03 -2.53792889e-02  1.67898741e-02 -5.03682084e-02
 -2.52413936e-02 -1.14932880e-02  1.78826135e-02  1.09310582e-01
 -8.88087079e-02 -2.07069106e-02 -1.73439980e-02 -3.82403471e-03
  6.76273853e-02 -2.66613513e-02 -6.44799322e-03  4.66539972e-02
 -1.76655725e-02  1.89182721e-02 -1.05034061e-01  3.40936892e-02
  2.04099398e-02  3.83773036e-02  1.94069240e-02 -1.92023218e-02
 -1.21049350e-02 -1.90323442e-02 -5.36188446e-02  2.72759944e-02
 -1.29559770e-01  6.83082491e-02 -1.49525488e-02  6.19407743e-02
  3.66073549e-02 -4.37455066e-02 -7.59043777e-03  1.66763663e-02
 -1.55564193e-02 -2.31599957e-02 -6.38313964e-02 -2.00487841e-02
  6.08390430e-03  6.08944818e-02 -2.68530026e-02 -5.72289608e-33
 -3.05545684e-02  5.49432077e-02 -2.24807542e-02  8.09797794e-02
  1.78612508e-02  1.18972128e-03  3.49399261e-02  1.04967197e-02
  5.59755042e-02  7.98960850e-02 -1.61280930e-02  2.67458949e-02
 -7.79741481e-02 -5.45957386e-02  1.20562427e-02 -1.19620904e-01
  1.99409686e-02 -1.01315819e-01  9.57761239e-03 -9.22438316e-03
 -4.51326482e-02  3.12511809e-02 -7.48267621e-02 -4.01793048e-02
 -2.91598355e-03  3.93166170e-02 -8.68202001e-02  2.35260408e-02
  7.74808004e-02  2.95359059e-03  6.21369854e-02  3.17745209e-02
  3.09110880e-02 -2.35754028e-02  5.69201168e-03  5.48453862e-03
  6.66118413e-02  2.07072105e-02  5.89285828e-02 -4.99353297e-02
  8.79364386e-02 -3.22796665e-02  3.48767452e-02 -3.57965119e-02
 -3.15056331e-02  2.82569509e-02 -2.58572437e-02  5.26799895e-02
 -6.48046285e-03 -8.53423327e-02 -6.96042106e-02  1.82709040e-03
 -3.50440182e-02  3.33855189e-02  2.24764049e-02  7.08624050e-02
  2.80743036e-02  3.19282934e-02  3.82885262e-02 -9.37797036e-03
 -4.95155118e-02  7.86598492e-03  1.77944396e-02  4.08629104e-02
 -7.83608034e-02  5.04426584e-02  1.20454244e-02  2.49121891e-04
 -8.93909633e-02 -1.49759194e-02 -1.78331975e-02  1.86255260e-03
 -1.23751787e-02  6.02694936e-02 -3.47530507e-02 -3.65255736e-02
 -6.27174554e-03 -4.04917449e-02  3.57188168e-03  4.05051978e-04
  4.06447686e-02  1.69939883e-02 -1.60780549e-02  2.43481789e-02
  8.12889636e-02 -5.34698647e-03  3.57986018e-02 -6.45576492e-02
  2.83107087e-02  2.51664966e-02 -7.69131705e-02 -4.39089164e-02
  2.96244910e-03  5.27794287e-02  2.59026350e-03 -2.94431342e-08
 -8.65022764e-02  7.61569887e-02  4.56039459e-02  1.39621329e-02
  1.97997782e-02  1.13263302e-01  2.97838170e-02  1.52968377e-01
  5.65071627e-02  3.96102667e-02 -1.17613254e-02 -3.93272564e-02
 -6.89768791e-02 -1.21306600e-02  5.73947802e-02  3.52842398e-02
 -1.55755235e-02  3.84026282e-02  2.16356684e-02 -5.70241772e-02
 -2.20243633e-02  3.00966576e-02  3.24171851e-03 -4.04508337e-02
 -1.05396267e-02 -5.29853925e-02  5.03567532e-02  3.80655527e-02
 -4.54553030e-02 -1.77463423e-02 -3.50464066e-03  1.45346168e-02
  7.44904131e-02 -2.75600944e-02  6.69723749e-02 -5.40527888e-03
 -3.36160883e-02  1.01779122e-03  1.95007417e-02  4.58594561e-02
 -3.77492886e-03  2.69851051e-02  2.12428514e-02 -4.22221571e-02
  2.02294309e-02  8.47721007e-03 -3.30788009e-02 -2.90659703e-02
 -5.72899096e-02  5.45582958e-02 -3.67496572e-02  2.09870879e-02
  6.78831860e-02  1.29873037e-01  9.53558274e-03  7.75676146e-02
  9.66032688e-03 -8.88626203e-02  5.01251742e-02  1.68438777e-02
 -2.36736331e-02  6.76815212e-02  7.35779852e-02  4.05604839e-02]",2,2
tensorflow-datasets,10,tensorflow datasets is a library of public datasets ready to use with tensorflow each dataset definition contains the logic necessary to download and prepare the dataset a well a to read it into a model using the tf data dataset api usage outside of tensorflow is also supported see the readme on github for further documentation,"[('tensorflow datasets', 0.8029), ('tensorflow', 0.5789), ('dataset api usage', 0.5634), ('dataset definition', 0.4996), ('tf data', 0.4907), ('dataset', 0.4851), ('public datasets', 0.4248), ('library', 0.2544), ('github', 0.1881), ('further documentation', 0.179)]","[-2.57585291e-02 -1.09775059e-01 -1.38138421e-02 -2.63253711e-02
  6.69185817e-02 -7.23646767e-03 -9.36925411e-03  4.01127376e-02
 -3.23693939e-02 -5.40466141e-03 -3.29588167e-02 -2.04834179e-03
 -4.16953154e-02 -7.45069236e-02  1.27088521e-02 -1.01054143e-02
 -1.95648223e-02  1.53766219e-02 -2.41777897e-02 -9.38833356e-02
 -4.01839912e-02 -3.32710310e-03  1.40437076e-03  3.74991149e-02
 -2.36210413e-02  3.86409322e-03  1.11730425e-02 -7.99769834e-02
  9.24680009e-03  3.73583063e-02 -5.25508523e-02  7.38972202e-02
  4.46711592e-02  3.58044319e-02 -7.18896985e-02  2.49947160e-02
 -1.12932481e-04 -4.25403938e-02 -5.07230349e-02  8.04855488e-03
 -3.36065888e-02 -3.09227314e-02 -5.42359948e-02 -1.16353687e-02
  6.15623258e-02  2.21296046e-02  3.93649153e-02 -1.05506577e-01
 -2.26895157e-02  3.09444405e-02 -6.20048232e-02 -4.89187613e-02
 -2.63488200e-02  4.14119288e-02  4.49887430e-03 -8.11932236e-02
  4.08330187e-02  2.89959069e-02 -3.88741791e-02 -3.35767753e-02
  2.43700221e-02 -7.02209175e-02 -6.08573966e-02  6.07855394e-02
  2.09381934e-02  4.09810841e-02 -2.98584010e-02  2.76344735e-02
  1.32937446e-01 -1.30788505e-01 -8.75394195e-02  3.80604006e-02
 -8.97996500e-03  1.10369995e-02 -2.13845391e-02 -2.82814074e-02
  5.89917675e-02  5.72999520e-03  7.45761022e-02 -9.56717357e-02
  2.38200347e-03  1.93470065e-02  5.91910072e-02  4.87440899e-02
 -2.65139900e-02 -1.45469559e-03  1.57222543e-02  7.62333870e-02
  5.18611027e-03 -1.08592473e-02  4.81493399e-02 -1.46675827e-02
  4.01900969e-02  1.22404592e-02  3.90966684e-02  6.25906810e-02
 -6.40380010e-02 -2.22077742e-02  6.83298893e-03 -4.19113338e-02
 -1.31579950e-01  1.15656084e-03  6.43643215e-02  8.41379091e-02
 -4.98124883e-02  6.74363822e-02 -1.39306476e-02 -2.53305864e-02
  6.90235496e-02  4.10033874e-02  4.81905136e-03  5.19081801e-02
 -1.30069377e-02 -9.60276052e-02  8.09922814e-02  1.61018297e-02
 -1.03389442e-01  3.67930345e-02  1.14395507e-02  7.16027394e-02
 -9.84940752e-02  4.12285179e-02  4.25196290e-02 -4.62839492e-02
 -3.82740051e-02 -2.06143372e-02 -7.76117072e-02  1.44153159e-33
  8.41649696e-02  3.32635315e-03 -3.91098950e-03 -1.21974675e-02
  3.24090831e-02 -8.80408436e-02  5.23514152e-02 -2.05891915e-02
 -1.70092806e-02 -5.46746841e-03  5.56468405e-03  1.32942051e-01
 -5.75297885e-03  8.51110369e-02 -4.27161604e-02 -5.04501052e-02
  2.15055607e-02  3.73669975e-02  2.37095784e-02  5.81711642e-02
  7.29542896e-02  3.11544407e-02  4.46199253e-02  9.43663046e-02
  8.09911117e-02 -2.61659175e-03 -9.92360711e-03  8.80107749e-03
 -1.25214383e-02  2.46802345e-02 -5.46832457e-02 -2.11496521e-02
 -4.53833630e-03 -4.75836499e-03  3.98620032e-02 -2.99611837e-02
 -5.01463711e-02  2.00392362e-02  1.24548925e-02 -2.46795602e-02
  5.04111685e-02  3.08170319e-02 -3.09303477e-02 -1.64380409e-02
 -3.96880172e-02 -4.86893617e-02  3.36587504e-02  2.83532273e-02
  1.19358972e-01 -1.14113558e-02 -2.11365381e-03 -2.53046956e-02
 -1.01454342e-02 -9.06174164e-03  1.75102837e-02 -1.76323354e-02
 -4.19039652e-03 -1.89769659e-02  5.07073812e-02  5.26852943e-02
 -8.47446844e-02  2.36632898e-02  8.90363380e-02  3.08782905e-02
 -3.61458920e-02  4.80401190e-03 -1.08340405e-01  7.93375820e-03
  3.27283926e-02 -2.29691435e-02 -6.21913299e-02  1.00100346e-01
 -5.17846085e-02  2.57732272e-02  9.16519726e-04 -4.72028777e-02
  2.47026905e-02 -4.30829078e-02 -2.87929699e-02  3.56023200e-02
 -8.11708346e-02 -1.30728967e-02  2.19167825e-02  2.78442372e-02
 -4.01584357e-02  2.84125917e-02 -2.54850630e-02  4.58784960e-02
 -6.25022035e-03 -5.68408892e-02 -7.75684938e-02  3.61064859e-02
 -2.41690371e-02 -2.89063039e-03 -5.65228574e-02 -2.90888236e-33
 -6.46551652e-03  3.28824259e-02 -7.05239847e-02  6.59584627e-02
  1.22912582e-02  8.29985365e-02  5.56484237e-03  2.75231861e-02
  1.21149465e-01  5.31138554e-02  2.39296164e-02 -4.75823600e-03
  5.23699727e-03 -1.06966220e-01 -2.56294273e-02 -9.59853753e-02
 -4.09974493e-02 -4.99173254e-02 -4.74503487e-02 -2.84329783e-02
 -1.01559043e-01 -1.60753559e-02 -5.79098165e-02 -3.35319191e-02
  2.27584485e-02 -1.61916427e-02 -9.43669602e-02 -4.00184914e-02
  4.51464728e-02 -1.24496929e-02  9.53065790e-03 -6.29310235e-02
 -2.42387112e-02  1.52840922e-02 -3.35439555e-02 -9.66641819e-04
  8.46013874e-02  8.30260199e-03  2.19872296e-02  1.09172624e-03
  8.37815255e-02  7.79498816e-02  3.54981124e-02 -8.77230056e-03
 -8.05699080e-02 -1.68909028e-03 -1.08098842e-01  5.51756620e-02
 -5.78581803e-02 -8.80557075e-02  3.08640692e-02 -3.35670896e-02
  6.05848571e-03 -6.71360046e-02  7.44219357e-03 -6.58519391e-04
  1.26943260e-01  2.27344669e-02  3.39616351e-02  1.73895694e-02
  8.08952004e-03 -7.51579646e-03  2.95769866e-03  6.47498071e-02
 -6.94536269e-02 -1.66772548e-02 -1.19645774e-01 -5.87173691e-03
 -3.72312963e-02 -1.54166156e-02 -2.97450945e-02 -6.61823992e-03
  3.92486516e-04  3.79727595e-02 -4.34480980e-02  2.66617909e-02
 -8.31057318e-03  3.17913597e-03  9.16014463e-02  2.86880806e-02
  8.25742111e-02  2.48213112e-02  3.87278348e-02  5.19313291e-02
  1.04339793e-01  7.84464106e-02  4.42325994e-02 -7.28602409e-02
  2.51105875e-02 -4.37113736e-03 -6.92995787e-02 -6.06427640e-02
 -2.20650174e-02  1.46592915e-01  4.23372351e-02 -2.51372381e-08
 -6.37819469e-02  5.75681999e-02  7.77056292e-02  2.57495828e-02
 -1.46205397e-02  7.46237338e-02  5.23218624e-02  1.46060616e-01
  6.76792162e-03 -5.05213486e-03  6.56017102e-03  3.21969949e-02
 -8.30956921e-02 -8.47284030e-03  3.12416125e-02  2.62236893e-02
 -7.38104852e-03 -3.70380953e-02  1.54636437e-02 -1.82052478e-02
  4.32910360e-02 -1.25492951e-02 -1.06462715e-02 -5.63368984e-02
  6.60836920e-02 -7.89670460e-03  4.06374075e-02  5.56243360e-02
 -8.12641010e-02 -6.65308386e-02 -7.89148286e-02 -5.27188070e-02
  1.84181500e-02 -9.35884044e-02  7.06409737e-02  3.07741277e-02
 -3.00566573e-02 -7.62540847e-02 -1.48031153e-02  6.65944964e-02
 -5.17777167e-02  5.03445119e-02  3.07017379e-02 -2.45938394e-02
  1.55330440e-02  8.31775088e-03  2.83760251e-03  1.64345969e-02
 -4.75898162e-02 -6.75299764e-03 -3.78385894e-02  5.92520572e-02
 -5.63943461e-02  1.21640965e-01  1.95971895e-02  7.47335628e-02
 -4.27125357e-02 -6.41786829e-02  7.54637178e-03 -7.14783520e-02
 -5.90555593e-02  9.84166116e-02 -1.00760837e-03 -1.80026144e-02]",2,2
imgaug,10,a library for image augmentation in machine learning experiment particularly convolutional neural network support the augmentation of image keypoints landmark bounding box heatmaps and segmentation map in a variety of different way,"[('image augmentation', 0.6102), ('bounding box heatmaps', 0.5074), ('image keypoints', 0.5051), ('augmentation', 0.474), ('convolutional neural network', 0.4145), ('segmentation map', 0.4129), ('machine learning experiment', 0.2902), ('library', 0.1825), ('variety', 0.0705), ('different way', -0.0518)]","[-1.72076896e-02 -8.12318772e-02  8.60957503e-02 -4.16749949e-03
 -4.29655454e-04 -2.75426507e-02 -1.76854581e-02  2.21602526e-03
 -8.41058642e-02 -1.90772191e-02 -2.34646071e-02 -7.32302060e-03
 -3.19969538e-03 -5.08401729e-02 -5.56029892e-03 -5.78920282e-02
  8.48334376e-03  9.91577879e-02 -1.06307589e-01 -1.06207788e-01
  7.32842134e-03 -8.60985667e-02 -1.37697980e-02 -8.04795101e-02
  3.22124697e-02  1.83258802e-02  4.30952013e-02 -8.54679383e-03
 -1.92476008e-02 -4.89700325e-02 -2.15362441e-02 -2.10626540e-03
 -4.60718246e-03  4.92627993e-02 -5.50753959e-02  3.13820653e-02
  2.20220070e-02 -1.02797840e-02 -1.81252267e-02  2.13007559e-04
 -6.10324144e-02 -6.60529286e-02  3.99545841e-02 -7.03072827e-03
  1.40967831e-01  4.08582687e-02 -1.55771868e-02 -1.20823659e-01
 -1.17300963e-03 -1.63708348e-02 -1.85302179e-02 -5.24835438e-02
 -8.08436349e-02  7.23167136e-02 -3.97802107e-02 -2.26579625e-02
  1.86481699e-03 -2.39924211e-02  6.52143657e-02  9.32952482e-03
  7.33206198e-02 -2.26246137e-02 -6.89963102e-02  8.63535553e-02
  5.94108459e-03 -4.04184721e-02  1.74414180e-02 -1.80543046e-02
  7.26989731e-02 -1.38434559e-01  3.40933502e-02  5.01006767e-02
  2.03010142e-02 -3.70539129e-02  8.23392719e-03 -4.70355637e-02
  2.60850098e-02 -2.55063176e-02  7.90753886e-02 -8.56116563e-02
 -2.13327628e-04 -5.02041192e-04  2.15507429e-02  3.57460044e-02
  3.90296020e-02 -1.48647586e-02 -9.12984386e-02  1.98840853e-02
 -5.09656630e-02  5.83279282e-02  2.29130983e-02 -6.04597814e-02
 -9.23402384e-02 -4.16632695e-03 -2.46432219e-02 -4.17149328e-02
 -3.68284099e-02 -1.38428509e-01 -7.46500818e-03  3.24012153e-02
 -5.53717539e-02 -7.82514215e-02  1.07524246e-02  8.93895235e-03
  9.22237150e-03 -2.67438032e-02  2.47652624e-02 -2.73946095e-02
  6.17933311e-02  1.53385745e-02 -3.62971313e-02 -2.95620039e-02
 -2.08503567e-02 -1.02470130e-01  5.27620725e-02  6.83683250e-03
 -4.89463173e-02 -1.55571185e-03  5.09041324e-02  7.80314265e-04
 -6.80285990e-02  2.12967154e-02  3.09544662e-03  6.96890196e-03
 -6.68842569e-02 -2.29348745e-02 -9.08810571e-02  4.12741266e-33
 -4.95136157e-03 -1.42527279e-02  5.90515276e-03  1.29990336e-02
  1.04343385e-01 -4.93711978e-02  5.00198873e-03 -5.72271273e-02
 -1.58960149e-02 -4.56218198e-02 -3.33821401e-02  8.34319890e-02
 -6.14320487e-02  1.74158871e-01  8.34542736e-02  1.64490100e-02
  1.30065260e-02  5.76711744e-02 -2.99424529e-02  4.59679440e-02
 -5.51388599e-02 -4.02903892e-02  8.46839100e-02  1.07582159e-01
  2.04087794e-02  6.69515282e-02 -1.69665238e-03  1.90034900e-02
 -1.67966038e-02  2.10553110e-02  2.67858319e-02  4.05616798e-02
 -1.38225574e-02  6.99591450e-03 -1.16252238e-02 -2.94254348e-02
  6.40516030e-03 -4.11085188e-02 -1.53418933e-03  3.84785384e-02
  2.74482258e-02  2.60477196e-02  2.05413741e-03 -3.26239839e-02
  2.44610347e-02  2.88219918e-02  4.32718247e-02  1.02210857e-01
 -3.49060283e-03 -1.06720384e-02  2.44549178e-02  1.21116545e-02
 -3.80819887e-02 -8.71406868e-02  6.43139379e-03  6.91378489e-02
  5.35541726e-03  5.17640412e-02  5.03952093e-02  3.00113298e-02
 -3.38530168e-03  1.83269493e-02  3.46178114e-02 -7.02969637e-03
 -4.01741564e-02 -5.44323074e-03  2.74324585e-02 -6.17665835e-02
  5.82752982e-04  6.82726577e-02 -8.13997388e-02  2.37437785e-02
  4.21239325e-04 -2.51823338e-03  7.73343211e-03  2.45137159e-02
 -2.47311853e-02 -1.98303163e-03 -2.10873336e-02  9.60876495e-02
 -1.01077713e-01  1.39546460e-02  2.23331228e-02 -5.36803566e-02
 -5.81430532e-02 -6.83805346e-02  2.03695633e-02 -8.25248566e-03
  4.53888327e-02 -2.25049704e-02 -7.81209990e-02  2.66768355e-02
 -2.99886800e-02  9.20143202e-02 -4.17497382e-03 -3.85216331e-33
 -1.08864931e-02  6.58519939e-02 -7.59272128e-02  1.03537731e-01
  5.71742048e-03  8.78794938e-02 -1.59538332e-02 -5.95134646e-02
  7.65477493e-02 -1.05301710e-02 -1.04041994e-02  2.03446914e-02
  1.64444409e-02 -3.45292576e-02 -2.38758121e-02 -8.24959874e-02
 -5.90491444e-02 -3.44699211e-02 -5.80350757e-02  2.20736414e-02
 -4.77634110e-02  1.08710624e-01 -1.34741431e-02 -1.69941671e-02
 -3.71012874e-02  3.77801829e-03 -9.06441063e-02 -1.66028682e-02
  8.76377616e-03  2.37702951e-02  1.88914239e-02 -2.64272634e-02
  4.23327684e-02  1.97164575e-03 -7.23685371e-03  8.84337053e-02
  1.38779759e-01 -2.95015667e-02 -2.26538368e-02 -1.09362574e-02
  8.73303488e-02  1.51675064e-02  3.78611796e-02  9.53539833e-03
 -6.23725131e-02 -4.13242951e-02 -4.44993600e-02  3.32296155e-02
 -4.31675203e-02  2.82072406e-02  1.28859440e-02 -3.14178392e-02
 -7.02999309e-02 -2.24008709e-02  7.75311235e-03  6.94470713e-03
 -1.72462072e-02  2.06139195e-03  1.40639499e-01  1.07624065e-02
 -4.51476425e-02 -1.19189650e-01  4.61984202e-02 -1.34967500e-03
 -4.18595932e-02  6.68973243e-03 -6.47561699e-02 -7.42016500e-03
 -4.88513429e-03 -3.72682363e-02  1.42587228e-02  1.14278672e-02
  5.41473702e-02  5.34488000e-02 -2.38886885e-02 -4.55993563e-02
  1.36103481e-02  9.28853173e-03  1.20630050e-02 -5.53600416e-02
  1.48078445e-02 -5.55664785e-02 -2.11524963e-03  1.58864826e-01
  7.05312863e-02  1.15688138e-01  4.00249101e-02 -5.71790151e-02
  6.78867474e-02 -9.93865505e-02 -5.16466908e-02  4.11403067e-02
 -3.38440575e-02  8.11193734e-02 -2.19645798e-02 -2.74451217e-08
 -2.05448437e-02 -2.84734629e-02  9.41921398e-02  1.54264632e-03
  4.33096886e-02 -2.99914982e-02  5.63179795e-03  1.77272439e-01
 -1.13480026e-02 -1.18067684e-02  2.58148145e-02  2.61845738e-02
 -4.53938134e-02  7.47719256e-04  4.49444652e-02  2.48919800e-02
  4.09765244e-02  9.98038277e-02  2.70926598e-02  9.89085343e-03
 -2.42347717e-02  1.16594285e-02  7.58657902e-02 -7.29240058e-03
  2.21033785e-02 -1.60384197e-02 -4.04378586e-02  2.67201755e-02
  1.88712887e-02 -5.88288624e-03  1.13673834e-02 -2.14736201e-02
  1.11048482e-01 -3.78605127e-02  5.52640259e-02  8.58534873e-02
 -2.72361301e-02 -2.03324314e-02 -9.76463109e-02  3.99852246e-02
  1.90951712e-02  2.03393623e-02  6.77016452e-02 -2.03013662e-02
  6.44591451e-03  2.44214549e-04  3.84551808e-02 -3.17763649e-02
 -4.10876758e-02  2.85065640e-02  6.09655492e-03  3.41330608e-03
  1.88089479e-02  1.10585373e-02 -1.82453766e-02  2.31133886e-02
  4.69433367e-02 -8.77091959e-02  8.55277926e-02  1.43972158e-01
 -1.79894865e-02  3.31856944e-02 -6.53289780e-02 -2.23016571e-02]",2,2
kornia,10,english website doc try it now tutorial example blog community kornia is a differentiable computer vision library for pytorch it consists of a set of routine and differentiable module to solve generic computer vision problem at it core the package us pytorch a it main backend both for efficiency and to take advantage of the reverse mode auto differentiation to define and compute the gradient of complex function inspired by existing package this library is composed by a subset of package containing operator that can be inserted within neural network to train model to perform image transformation epipolar geometry depth estimation and low level image processing such a filtering and edge detection that operate directly on tensor at a granular level kornia is a library that consists of the following component run our jupyter notebook tutorial to learn to use the library triangular flag on post updatesif you are using kornia in your research related document it is recommended that you cite the paper see more in citation we appreciate all contribution if you are planning to contribute back bug fix please do so without any further discussion if you plan to contribute new feature utility function or extension please first open an issue and discus the feature with u please consider reading the contributing note the participation in this open source project is subject to code of conduct made with contrib rock,"[('differentiable computer vision library', 0.5992), ('pytorch', 0.4957), ('granular level kornia', 0.4523), ('jupyter notebook tutorial', 0.4411), ('tutorial example blog community kornia', 0.4407), ('library', 0.4066), ('kornia', 0.3992), ('tensor', 0.3881), ('edge detection', 0.3805), ('low level image processing', 0.3664)]","[-6.65648133e-02 -5.96898794e-02  5.40021341e-03 -5.86688817e-02
  8.69381204e-02 -7.77702406e-02 -6.72657602e-03  1.55391516e-02
 -6.30792677e-02 -7.62497308e-03 -2.41636317e-02 -1.04296010e-03
 -3.42814326e-02  3.68520431e-02  2.26332955e-02 -5.70098171e-03
 -3.33935134e-02  1.15454860e-01 -5.77904806e-02 -7.73999766e-02
 -4.39673811e-02 -5.22416122e-02  5.83379492e-02 -2.41938662e-02
  1.41202621e-02 -4.08279710e-03  1.70191899e-02 -4.22298312e-02
  1.63375176e-02 -3.98967601e-02 -7.23685920e-02  1.45206107e-02
  2.76871733e-02 -2.57660523e-02 -3.91777307e-02 -1.65456869e-02
  4.59498465e-02 -2.83877980e-02 -2.64554154e-02  2.71530095e-02
 -7.17993081e-02  3.57920527e-02 -3.25880498e-02 -3.51244584e-03
  6.44283965e-02 -2.03452185e-02 -1.01952273e-02 -9.25296023e-02
 -2.34605651e-03 -3.48400846e-02 -1.07952699e-01 -6.27772436e-02
 -6.71117753e-02 -9.34005231e-02 -6.07346967e-02  8.65909830e-03
  9.65414511e-04  2.72285119e-02  1.27473578e-01 -2.05576811e-02
  4.03022543e-02 -7.36489668e-02 -4.74306494e-02  6.77461997e-02
 -2.36324687e-02 -5.01312837e-02  2.82751583e-02  2.22046506e-02
  1.52077228e-01 -8.34823251e-02 -9.02618319e-02  8.45389534e-03
  2.40822174e-02  4.68814187e-02 -1.29220756e-02 -5.42338304e-02
  4.04387526e-02  3.46245477e-03  1.05479378e-02 -6.31839111e-02
 -4.10375781e-02  4.63026855e-03 -8.05923808e-03 -4.06110240e-03
  8.28816183e-03  1.56021323e-02 -8.35473016e-02  6.84836507e-02
  4.34544403e-03 -1.76937375e-02  1.48240685e-01 -5.04829213e-02
 -2.60532238e-02 -4.94624302e-03 -5.34795932e-02 -3.05324607e-02
  9.23805162e-02 -8.18765610e-02 -1.25035495e-02  6.39377236e-02
 -4.47351113e-02 -7.32196644e-02  7.93188959e-02  3.74646746e-02
  2.16884054e-02  9.19349119e-03  1.70954056e-02  8.74319114e-03
  4.76334244e-02  3.86172123e-02  1.49960360e-02 -4.06332612e-02
 -7.49990111e-03 -1.22973651e-01  6.99207261e-02 -1.64483450e-02
  6.85166344e-02  3.80149223e-02  7.25874156e-02 -4.48979298e-03
 -1.33692641e-02 -3.09922099e-02 -4.23663519e-02 -3.37864794e-02
  7.81859644e-03  1.01061389e-02 -7.35342130e-02  3.81883217e-33
  8.24403241e-02 -2.99582686e-02  4.64817807e-02 -2.87648048e-02
  4.34467532e-02 -9.69383270e-02  4.14543971e-02  2.81394459e-03
 -3.75988595e-02 -6.75811712e-03 -2.71463059e-02  5.70340715e-02
 -6.52575195e-02  8.36889595e-02  3.57501721e-03 -8.45342353e-02
 -6.55253977e-02 -4.58007753e-02  4.53053601e-02  9.73183736e-02
  3.93149108e-02  1.81765622e-03  3.34434174e-02  9.21097100e-02
  1.31267542e-03  3.23346928e-02 -2.13336112e-04 -3.55400406e-02
  4.59701307e-02  2.95604151e-02 -3.68186980e-02  1.80681124e-02
 -4.11966927e-02  1.87934358e-02 -4.78384010e-02  8.97415634e-03
 -5.96250631e-02 -5.76560609e-02  5.22911437e-02 -7.21694678e-02
 -5.95231391e-02  5.13276309e-02  1.62280705e-02 -3.26099247e-02
 -1.76610332e-02  6.88508078e-02  1.66349839e-02  1.39153212e-01
  1.87890027e-02 -1.52120441e-02 -1.08321235e-01 -3.23514231e-02
  6.93535199e-04  1.12814764e-02 -3.98418866e-02  5.59245422e-02
  5.45230284e-02  2.37185266e-02  2.22741440e-02 -1.05900522e-02
  1.48104522e-02  7.72203803e-02  3.26901339e-02  1.91404093e-02
 -5.31197414e-02 -4.62049507e-02 -7.81129301e-02 -7.98092864e-04
 -4.74995933e-02  2.90298071e-02 -5.81905544e-02  1.02431573e-01
  3.36406427e-03 -4.78582941e-02 -1.88515596e-02  1.07203200e-02
 -6.22503385e-02 -1.10085504e-02 -6.04334846e-02  2.18735132e-02
 -7.16847330e-02  1.30824381e-02  4.92239147e-02 -5.01308180e-02
 -4.34471034e-02  2.10723896e-02 -5.08020772e-03  4.00809720e-02
 -6.19673310e-03 -2.80178487e-02 -2.51548700e-02  1.30842067e-02
  3.51133049e-02  2.39282623e-02 -2.30742376e-02 -2.56334194e-33
  5.02341911e-02  6.33205026e-02 -3.82490866e-02  9.66713503e-02
  1.37877101e-02  9.15079340e-02  3.00205834e-02  9.37909354e-03
  5.38498014e-02 -1.86961964e-02  4.39914456e-03  2.33638436e-02
  3.36410217e-02  2.41711978e-02  8.56748782e-03 -1.63276605e-02
 -3.36420126e-02 -6.38068886e-03 -5.45415804e-02  6.77599746e-04
 -5.61324656e-02  1.17897332e-01 -1.05823375e-01 -5.95838614e-02
  2.20076852e-02 -3.10363788e-02 -7.33546913e-02 -3.10036112e-02
 -4.39187363e-02 -1.70171689e-02  2.48720013e-02  2.80104554e-03
  1.53644476e-03  2.04499494e-02  4.89209443e-02 -5.83503731e-02
  5.23292907e-02 -5.93467802e-03 -3.59977074e-02  3.15044001e-02
  1.37184411e-01  6.37285262e-02  5.81441857e-02  2.60757860e-02
  1.14269275e-02 -6.30821940e-03 -5.35502136e-02  4.86567318e-02
 -1.08061461e-02 -2.17693988e-02 -9.63654667e-02 -1.96448546e-02
  1.21422075e-02 -3.57383676e-02  2.12155282e-02  5.17152250e-02
  7.20929205e-02  5.33809736e-02  4.83146273e-02  5.83738461e-02
 -9.02298838e-02 -6.88208938e-02 -6.34205043e-02  1.83073189e-02
 -2.19298769e-02 -2.90782657e-02 -1.33326814e-01  4.79799993e-02
 -6.70417072e-03 -7.06635043e-02 -2.81934272e-02  1.61198508e-02
  8.37282911e-02 -1.73078962e-02 -1.67038199e-02 -1.75492354e-02
  5.91004966e-03  5.91220744e-02  1.62723027e-02  2.26360504e-02
  1.05447777e-01 -1.32452650e-02  2.25620605e-02  1.11094132e-01
  6.83272704e-02  1.42395183e-01  2.83493791e-02 -8.90516266e-02
  5.28946295e-02 -2.99148187e-02 -3.57307540e-03  3.98147758e-03
  3.22203338e-02  1.09108560e-01 -7.07600906e-04 -2.38463276e-08
 -7.75331445e-03 -9.97567736e-03  3.44879478e-02 -6.06129020e-02
  4.07225545e-03  1.77517310e-02  6.47513419e-02  1.00566909e-01
 -4.40845191e-02  5.05764782e-02  6.46594446e-03 -4.69089411e-02
 -3.31694521e-02  1.98617652e-02  5.13001755e-02  1.13171294e-01
  1.79393869e-02  6.93406984e-02  2.66772956e-02  3.54714617e-02
  3.38008702e-02 -4.82070707e-02 -1.88975241e-02  6.10659868e-02
 -3.13521065e-02 -1.59110371e-02 -2.93485094e-02  1.84538830e-02
  1.80050973e-02 -1.03502899e-01 -3.04377284e-02  2.60158964e-02
  7.71249682e-02  6.22418988e-03  9.50385556e-02  3.56183387e-02
 -4.45944779e-02 -2.49217451e-02 -2.50889938e-02  8.74805674e-02
 -9.08226147e-02 -2.69381385e-02 -1.03216162e-02 -3.95246334e-02
  3.75048704e-02  4.82843257e-02  1.76316705e-02 -8.40778276e-02
 -4.36526649e-02  2.67360080e-02 -2.39243694e-02  3.12001817e-02
  9.98122804e-03  8.98027048e-03  5.78602292e-02  4.96006086e-02
  3.45733277e-02 -2.49278098e-02 -1.27129882e-04  5.65859750e-02
  2.43634614e-03  9.58612934e-02 -3.02672782e-03  3.28196492e-03]",2,2
accelerate,9,run your raw pytorch training script on any kind of device easy to integrate accelerate wa created for pytorch user who like to write the training loop of pytorch model but are reluctant to write and maintain the boilerplate code needed to use multi gpus tpu fp accelerate abstract exactly and only the boilerplate code related to multi gpus tpu fp and leaf the rest of your code unchanged here is an example import torch import torch nn functional a f from datasets import load dataset from accelerate import accelerator accelerator accelerator device cpu device accelerator device model torch nn transformer to device optimizer torch optim adam model parameter dataset load dataset my dataset data torch utils data dataloader dataset shuffle true model optimizer data accelerator prepare model optimizer data model train for epoch in range for source target in data source source to device target target to device optimizer zero grad output model source loss f cross entropy output target loss backward accelerator backward loss optimizer step a you can see in this example by adding line to any standard pytorch training script you can now run on any kind of single or distributed node setting single cpu single gpu multi gpus and tpus a well a with or without mixed precision fp in particular the same code can then be run without modification on your local machine for debugging or your training environment accelerate even handle the device placement for you which requires a few more change to your code but is safer in general so you can even simplify your training loop further import torch import torch nn functional a f from datasets import load dataset from accelerate import accelerator device cpu accelerator accelerator model torch nn transformer to device model torch nn transformer optimizer torch optim adam model parameter dataset load dataset my dataset data torch utils data dataloader dataset shuffle true model optimizer data accelerator prepare model optimizer data model train for epoch in range for source target in data source source to device target target to device optimizer zero grad output model source loss f cross entropy output target loss backward accelerator backward loss optimizer step want to learn more check out the documentation or have look at our example launching script accelerate also provides an optional cli tool that allows you to quickly configure and test your training environment before launching the script no need to remember how to use torch distributed launch or to write a specific launcher for tpu training on your machine s just run accelerate config and answer the question asked this will generate a config file that will be used automatically to properly set the default option when doing accelerate launch my script py args to my script for instance here is how you would run the glue example on the mrpc task from the root of the repo accelerate launch example nlp example py this cli tool is optional and you can still use python my script py or python m torch distributed launch my script py at your convenance launching multi cpu run using mpi here is another way to launch multi cpu run using mpi you can learn how to install open mpi on this page you can use intel mpi or mvapich a well once you have mpi setup on your cluster just run mpirun np python example nlp example py launching training using deepspeed accelerate support training on single multiple gpus using deepspeed to use it you don t need to change anything in your training code you can set everything using just accelerate config however if you desire to tweak your deepspeed related args from your python script we provide you the deepspeedplugin from accelerator import accelerator deepspeedplugin deepspeed need to know your gradient accumulation step before hand so don t forget to pas it remember you still need to do gradient accumulation by yourself just like you would have done without deepspeed deepspeed plugin deepspeedplugin zero stage gradient accumulation step accelerator accelerator fp true deepspeed plugin deepspeed plugin how to save your transformer accelerator wait for everyone unwrapped model accelerator unwrap model model unwrapped model save pretrained save dir save function accelerator save state dict accelerator get state dict model note deepspeed support is experimental for now in case you get into some problem please open an issue launching your training from a notebook accelerate also provides a notebook launcher function you can use in a notebook to launch a distributed training this is especially useful for colab or kaggle notebook with a tpu backend just define your training loop in a training function then in your last cell add from accelerate import notebook launcher notebook launcher training function an example can be found in this notebook why should i use accelerate you should use accelerate when you want to easily run your training script in a distributed environment without having to renounce full control over your training loop this is not a high level framework above pytorch just a thin wrapper so you don t have to learn a new library in fact the whole api of accelerate is in one class the accelerator object why shouldn t i use accelerate you shouldn t use accelerate if you don t want to write a training loop yourself there are plenty of high level library above pytorch that will offer you that accelerate is not one of them framework using accelerate if you like the simplicity of accelerate but would prefer a higher level abstraction around your training loop some framework that are built on top of accelerate are listed below animus is a minimalistic framework to run machine learning experiment animus highlight common breakpoints in ml experiment and provides a unified interface for them within iexperiment catalyst is a pytorch framework for deep learning research and development it focus on reproducibility rapid experimentation and codebase reuse so you can create something new rather than write yet another train loop catalyst provides a runner to connect all part of the experiment hardware backend data transformation model train and inference logic fastai is a pytorch framework for deep learning that simplifies training fast and accurate neural net using modern best practice fastai provides a learner to handle the training fine tuning and inference of deep learning algorithm kornia is a differentiable library that allows classical computer vision to be integrated into deep learning model kornia provides a trainer with the specific purpose to train and fine tune the supported deep learning algorithm within the library pytorch accelerated is a lightweight training library with a streamlined feature set centred around a general purpose trainer that place a huge emphasis on simplicity and transparency enabling user to understand exactly what is going on under the hood but without having to write and maintain the boilerplate themselves installation this repository is tested on python and pytorch you should install accelerate in a virtual environment if you re unfamiliar with python virtual environment check out the user guide first create a virtual environment with the version of python you re going to use and activate it then you will need to install pytorch refer to the official installation page regarding the specific install command for your platform then accelerate can be installed using pip a follows pip install accelerate supported integration cpu only multi cpu on one node machine multi cpu on several node machine single gpu multi gpu on one node machine multi gpu on several node machine tpu fp with native amp apex on the roadmap deepspeed support experimental pytorch fully sharded data parallel fsdp support experimental citing accelerate if you use accelerate in your publication please cite it by using the following bibtex entry misc accelerate title accelerate training and inference at scale made simple efficient and adaptable author sylvain gugger lysandre debut thomas wolf philipp schmid zachary mueller sourab mangrulkar howpublished url http github com huggingface accelerate year run your raw pytorch training script on any kind of device easy to integrate accelerate wa created for pytorch user who like to write the training loop of pytorch model but are reluctant to write and maintain the boilerplate code needed to use multi gpus tpu fp accelerate abstract exactly and only the boilerplate code related to multi gpus tpu fp and leaf the rest of your code unchanged here is an example import torch import torch nn functional a f from datasets import load dataset from accelerate import accelerator accelerator accelerator device cpu device accelerator device model torch nn transformer to device optimizer torch optim adam model parameter dataset load dataset my dataset data torch utils data dataloader dataset shuffle true model optimizer data accelerator prepare model optimizer data model train for epoch in range for source target in data source source to device target target to device optimizer zero grad output model source loss f cross entropy output target loss backward accelerator backward loss optimizer step a you can see in this example by adding line to any standard pytorch training script you can now run on any kind of single or distributed node setting single cpu single gpu multi gpus and tpus a well a with or without mixed precision fp in particular the same code can then be run without modification on your local machine for debugging or your training environment accelerate even handle the device placement for you which requires a few more change to your code but is safer in general so you can even simplify your training loop further import torch import torch nn functional a f from datasets import load dataset from accelerate import accelerator device cpu accelerator accelerator model torch nn transformer to device model torch nn transformer optimizer torch optim adam model parameter dataset load dataset my dataset data torch utils data dataloader dataset shuffle true model optimizer data accelerator prepare model optimizer data model train for epoch in range for source target in data source source to device target target to device optimizer zero grad output model source loss f cross entropy output target loss backward accelerator backward loss optimizer step want to learn more check out the documentation or have look at our example launching script accelerate also provides an optional cli tool that allows you to quickly configure and test your training environment before launching the script no need to remember how to use torch distributed launch or to write a specific launcher for tpu training on your machine s just run accelerate config and answer the question asked this will generate a config file that will be used automatically to properly set the default option when doing accelerate launch my script py args to my script for instance here is how you would run the glue example on the mrpc task from the root of the repo accelerate launch example nlp example py this cli tool is optional and you can still use python my script py or python m torch distributed launch my script py at your convenance launching multi cpu run using mpi here is another way to launch multi cpu run using mpi you can learn how to install open mpi on this page you can use intel mpi or mvapich a well once you have mpi setup on your cluster just run mpirun np python example nlp example py launching training using deepspeed accelerate support training on single multiple gpus using deepspeed to use it you don t need to change anything in your training code you can set everything using just accelerate config however if you desire to tweak your deepspeed related args from your python script we provide you the deepspeedplugin from accelerator import accelerator deepspeedplugin deepspeed need to know your gradient accumulation step before hand so don t forget to pas it remember you still need to do gradient accumulation by yourself just like you would have done without deepspeed deepspeed plugin deepspeedplugin zero stage gradient accumulation step accelerator accelerator fp true deepspeed plugin deepspeed plugin how to save your transformer accelerator wait for everyone unwrapped model accelerator unwrap model model unwrapped model save pretrained save dir save function accelerator save state dict accelerator get state dict model note deepspeed support is experimental for now in case you get into some problem please open an issue launching your training from a notebook accelerate also provides a notebook launcher function you can use in a notebook to launch a distributed training this is especially useful for colab or kaggle notebook with a tpu backend just define your training loop in a training function then in your last cell add from accelerate import notebook launcher notebook launcher training function an example can be found in this notebook why should i use accelerate you should use accelerate when you want to easily run your training script in a distributed environment without having to renounce full control over your training loop this is not a high level framework above pytorch just a thin wrapper so you don t have to learn a new library in fact the whole api of accelerate is in one class the accelerator object why shouldn t i use accelerate you shouldn t use accelerate if you don t want to write a training loop yourself there are plenty of high level library above pytorch that will offer you that accelerate is not one of them framework using accelerate if you like the simplicity of accelerate but would prefer a higher level abstraction around your training loop some framework that are built on top of accelerate are listed below animus is a minimalistic framework to run machine learning experiment animus highlight common breakpoints in ml experiment and provides a unified interface for them within iexperiment catalyst is a pytorch framework for deep learning research and development it focus on reproducibility rapid experimentation and codebase reuse so you can create something new rather than write yet another train loop catalyst provides a runner to connect all part of the experiment hardware backend data transformation model train and inference logic fastai is a pytorch framework for deep learning that simplifies training fast and accurate neural net using modern best practice fastai provides a learner to handle the training fine tuning and inference of deep learning algorithm kornia is a differentiable library that allows classical computer vision to be integrated into deep learning model kornia provides a trainer with the specific purpose to train and fine tune the supported deep learning algorithm within the library pytorch accelerated is a lightweight training library with a streamlined feature set centred around a general purpose trainer that place a huge emphasis on simplicity and transparency enabling user to understand exactly what is going on under the hood but without having to write and maintain the boilerplate themselves installation this repository is tested on python and pytorch you should install accelerate in a virtual environment if you re unfamiliar with python virtual environment check out the user guide first create a virtual environment with the version of python you re going to use and activate it then you will need to install pytorch refer to the official installation page regarding the specific install command for your platform then accelerate can be installed using pip a follows pip install accelerate supported integration cpu only multi cpu on one node machine multi cpu on several node machine single gpu multi gpu on one node machine multi gpu on several node machine tpu fp with native amp apex on the roadmap deepspeed support experimental pytorch fully sharded data parallel fsdp support experimental citing accelerate if you use accelerate in your publication please cite it by using the following bibtex entry misc accelerate title accelerate training and inference at scale made simple efficient and adaptable author sylvain gugger lysandre debut thomas wolf philipp schmid zachary mueller sourab mangrulkar howpublished url http github com huggingface accelerate year run your raw pytorch training script on any kind of device accelerate wa created for pytorch user who like to write the training loop of pytorch model but are reluctant to write and maintain the boilerplate code needed to use multi gpus tpu fp accelerate abstract exactly and only the boilerplate code related to multi gpus tpu fp and leaf the rest of your code unchanged here is an example a you can see in this example by adding line to any standard pytorch training script you can now run on any kind of single or distributed node setting single cpu single gpu multi gpus and tpus a well a with or without mixed precision fp in particular the same code can then be run without modification on your local machine for debugging or your training environment accelerate even handle the device placement for you which requires a few more change to your code but is safer in general so you can even simplify your training loop further want to learn more check out the documentation or have look at our example accelerate also provides an optional cli tool that allows you to quickly configure and test your training environment before launching the script no need to remember how to use torch distributed launch or to write a specific launcher for tpu training on your machine s just run and answer the question asked this will generate a config file that will be used automatically to properly set the default option when doingfor instance here is how you would run the glue example on the mrpc task from the root of the repo this cli tool is optional and you can still use python my script py or python m torch distributed launch my script py at your convenance here is another way to launch multi cpu run using mpi you can learn how to install open mpi on this page you can use intel mpi or mvapich a well once you have mpi setup on your cluster just run accelerate support training on single multiple gpus using deepspeed to use it you don t need to change anything in your training code you can set everything using just accelerate config however if you desire to tweak your deepspeed related args from your python script we provide you the deepspeedplugin note deepspeed support is experimental for now in case you get into some problem please open an issue accelerate also provides a notebook launcher function you can use in a notebook to launch a distributed training this is especially useful for colab or kaggle notebook with a tpu backend just define your training loop in a training function then in your last cell add an example can be found in this notebook you should use accelerate when you want to easily run your training script in a distributed environment without having to renounce full control over your training loop this is not a high level framework above pytorch just a thin wrapper so you don t have to learn a new library in fact the whole api of accelerate is in one class the accelerator object you shouldn t use accelerate if you don t want to write a training loop yourself there are plenty of high level library above pytorch that will offer you that accelerate is not one of them if you like the simplicity of accelerate but would prefer a higher level abstraction around your training loop some framework that are built on top of accelerate are listed below this repository is tested on python and pytorch you should install accelerate in a virtual environment if you re unfamiliar with python virtual environment check out the user guide first create a virtual environment with the version of python you re going to use and activate it then you will need to install pytorch refer to the official installation page regarding the specific install command for your platform then accelerate can be installed using pip a follows if you use accelerate in your publication please cite it by using the following bibtex entry,"[('standard pytorch training script', 0.6555), ('raw pytorch training script', 0.6159), ('pytorch framework', 0.5947), ('import accelerator device cpu accelerator accelerator model torch', 0.5719), ('library pytorch', 0.5236), ('experimental pytorch', 0.5149), ('pytorch model', 0.5044), ('example import torch import torch', 0.4915), ('pytorch user', 0.479), ('pytorch', 0.4708)]","[-8.80489871e-02 -9.05062705e-02 -6.37551472e-02  2.27018278e-02
  3.15024927e-02 -2.22241902e-03 -2.37293858e-02  5.93818016e-02
 -9.27098840e-02 -6.57061487e-02  1.28839165e-02  6.02171151e-03
 -7.18109161e-02 -1.38997110e-02  2.00773426e-03  1.58404838e-03
 -4.29297462e-02  5.73466867e-02 -6.38773739e-02 -1.43488184e-01
  2.61607878e-02 -8.05196315e-02  6.65360987e-02  1.80924516e-02
 -4.84503284e-02 -2.06386875e-02  1.86645854e-02 -1.31591419e-02
  3.18226144e-02  1.57149546e-02  2.41481587e-02 -4.99687232e-02
  4.74802218e-02  2.37813070e-02  2.23979689e-02  1.55450329e-02
 -3.66331935e-02 -4.29015011e-02 -1.92018561e-02  2.52101719e-02
  1.50727183e-02 -3.10357567e-02 -5.29784337e-02 -2.63050944e-03
 -1.24050807e-02 -2.51605734e-02 -1.50186906e-03 -6.04189821e-02
 -6.98653385e-02 -6.26307800e-02 -4.43573780e-02 -2.81664412e-02
 -2.48397496e-02  3.52082737e-02  2.11269613e-02  2.15802193e-02
  4.95694503e-02 -7.53114978e-03  2.79495642e-02 -4.43351083e-02
 -4.81396317e-02  5.49819088e-03 -4.64982241e-02 -8.68473016e-03
 -1.37336422e-02  5.53105436e-02 -2.32911780e-02  2.92202234e-02
  1.46268502e-01 -1.12211831e-01 -8.46964493e-02 -3.80516276e-02
 -6.16419800e-02  4.22443189e-02 -4.69026975e-02 -9.00528207e-02
  2.65152603e-02  8.80039334e-02  2.50054412e-02 -1.29384622e-01
 -8.55853707e-02 -4.89277318e-02  6.58983439e-02  1.45151820e-02
  6.25226572e-02  1.09071366e-01 -1.59266889e-02  8.85733664e-02
  1.41029740e-02  1.89039800e-02  1.44437114e-02 -7.04544559e-02
 -2.45500961e-03  4.72733676e-02 -1.80383194e-02  5.09020276e-02
  1.28243375e-03 -2.27707475e-02 -4.37378101e-02  2.45681237e-02
 -2.79485937e-02 -2.00557057e-02 -7.93772191e-03  5.20070605e-02
 -3.94980274e-02  5.68355434e-02 -6.41349703e-03  5.08697517e-02
  3.74102108e-02 -1.27695957e-02  3.60248014e-02  5.13031483e-02
 -4.07144353e-02 -7.80099854e-02  9.31361318e-02 -2.43539978e-02
 -5.38868345e-02  1.49125345e-02  1.70712993e-02  7.86206871e-02
 -7.40624592e-02  3.97568196e-02 -3.76554541e-02  4.04844806e-02
 -1.74796116e-02 -3.94268669e-02 -5.87048084e-02  8.76133409e-33
 -8.88682622e-03  5.12182526e-02 -2.38522906e-02 -3.14715691e-02
 -1.53926602e-02 -5.03150821e-02  1.08903289e-01  5.16760685e-02
 -1.23687470e-02 -6.57888653e-04  2.03200616e-02  2.18889322e-02
 -2.90105287e-02  9.49896276e-02 -1.01191081e-01 -1.23825371e-02
  2.32924148e-02  4.29933518e-02  7.26111755e-02  1.77558176e-02
  8.82908776e-02  4.65420447e-02 -3.32922377e-02  1.12289958e-01
 -3.45183164e-02  1.13155335e-01 -4.87007350e-02 -4.21495270e-03
 -3.44955549e-02  1.26093077e-02 -1.87056307e-02 -2.73728929e-03
  2.75787581e-02 -2.13068780e-02 -1.76465120e-02 -1.33355362e-02
  8.88674799e-03 -2.92515438e-02 -1.92159391e-03 -1.82258524e-02
 -5.44545315e-02  1.37725426e-02 -1.81283858e-02 -2.85953283e-02
 -2.45330227e-03 -4.69270023e-03 -1.97874680e-02  5.64030260e-02
  5.99259809e-02 -3.31740007e-02 -5.95626496e-02  4.66924571e-02
  5.70296012e-02  4.10261378e-02  5.25845587e-02  4.56416495e-02
  2.73259655e-02  2.85366345e-02  1.23397924e-01 -7.84688815e-03
  6.62270486e-02  5.14822714e-02  3.58033925e-02  4.88741472e-02
  7.97629915e-03 -3.74131314e-02 -9.02580172e-02 -2.00735796e-02
 -7.35906139e-02  9.20155793e-02 -9.37978849e-02  4.94362637e-02
 -4.50289100e-02  1.87699199e-02  7.86252096e-02 -6.51004761e-02
 -1.52518796e-02 -3.16124074e-02 -3.03670559e-02  1.83011331e-02
 -8.66871923e-02  1.41423931e-02  2.49699578e-02 -7.51545951e-02
 -5.49443811e-02 -1.79374125e-02 -7.23103760e-03 -3.36450227e-02
  3.33881527e-02 -1.73751600e-02 -1.78618301e-02 -7.73027912e-02
  1.39540443e-02  4.51497026e-02  2.26775762e-02 -8.15531385e-33
  7.37964138e-02  1.21523581e-01 -4.90542762e-02  5.92290163e-02
 -3.14978585e-02  8.48134980e-03 -5.47509491e-02 -5.26301339e-02
  7.68986763e-03  2.38507311e-03  4.24682745e-04 -2.77660601e-02
  8.66258750e-04 -4.67927344e-02  7.07415491e-02  2.22581886e-02
 -3.80132981e-02  4.03168201e-02  3.40441689e-02  1.95490625e-02
 -7.66027942e-02  7.42539912e-02 -6.98680207e-02 -1.73554905e-02
 -1.88176371e-02 -4.93673831e-02  1.75124723e-02  3.37200426e-02
  1.46396188e-02 -2.65703239e-02 -1.39528280e-02  2.56230067e-02
 -3.23856808e-02  1.23388218e-02 -7.10178018e-02  3.65631171e-02
  4.80306521e-02 -6.31814972e-02  1.31631829e-02  2.65743118e-02
  9.09081921e-02  5.83941452e-02 -5.02701811e-02  4.08132188e-02
 -6.63112551e-02  5.51870875e-02 -1.16107166e-01 -5.90808270e-03
  7.27914926e-03 -4.27320264e-02  1.50443958e-02 -3.57197784e-02
 -4.15993407e-02 -4.36119512e-02 -4.30690758e-02 -1.00513911e-02
  1.13832727e-01  1.69842374e-02 -1.41192321e-02 -3.64716575e-02
 -6.49409816e-02 -8.34387392e-02 -1.42969042e-02  4.96776104e-02
 -5.95539622e-02 -3.65179330e-02 -7.84718618e-02  8.16390440e-02
  2.48685461e-02 -5.04274257e-02 -2.28138268e-02  2.61167064e-02
  1.02891468e-01  1.18442625e-01 -1.46373007e-02  2.26078201e-02
  2.40237042e-02  2.89312825e-02  8.59651435e-03  4.15568333e-03
  5.31214885e-02  4.61364612e-02  3.65878008e-02  1.52501103e-03
  2.27076467e-02  1.03018917e-01  4.89283130e-02  1.55094191e-02
 -4.06806692e-02 -8.01847056e-02 -1.11368662e-02  3.05928495e-02
  1.01987518e-01  7.39421099e-02  5.24910055e-02 -3.79289844e-08
 -6.00815229e-02  5.26209436e-02 -3.61202867e-03  4.45959605e-02
  1.51408110e-02  4.70382944e-02  1.23934087e-03  8.42214748e-02
 -5.72932363e-02  2.74621230e-02 -2.62618121e-02 -5.49558029e-02
  4.29357737e-02  2.01420728e-02  2.64219474e-02  1.46330491e-01
 -1.60889849e-02  3.08853909e-02  3.29276435e-02  1.56149976e-02
  2.97538680e-03  2.49673743e-02  5.48914261e-02 -2.52969004e-03
 -1.22445375e-02  4.71411971e-03 -7.47283595e-03  2.44865753e-02
 -4.31923009e-02  1.14716496e-02 -2.40226053e-02 -4.41311337e-02
  8.54340866e-02 -5.73951788e-02  1.58216938e-01  9.79625732e-02
 -9.53816622e-03 -4.64138761e-02  2.29745684e-03 -9.38416109e-04
 -5.10578156e-02 -3.45202796e-02 -3.93479355e-02 -3.26919928e-02
  2.54344754e-02 -8.74857418e-03 -7.31862262e-02 -1.04464784e-01
 -7.74958506e-02  3.30816060e-02  3.55239175e-02  1.53791867e-02
 -4.36675549e-03 -5.11146896e-03 -1.64417513e-02  9.47659165e-02
 -2.45847329e-02 -6.82948977e-02 -1.87358651e-02 -8.24048892e-02
 -6.42444985e-03  9.56596881e-02 -8.12701788e-03 -9.47906449e-02]",2,2
pytorch_lightning,8,the lightweight pytorch wrapper for high performance ai research scale your model not the boilerplate website key feature how to use doc example community lightning ai license lightning disentangles pytorch code to decouple the science from the engineering lightning structure pytorch code with these principle lightning force the following structure to your code which make it reusable and shareable once you do this you can train on multiple gpus tpus cpu ipus hpus and even in bit precision without changing your code get started in just minuteslightning is rigorously tested across multiple cpu gpus tpus ipus and hpus and against major python and pytorch version simple installation from pypia lightningmodule defines a full system ie a gan autoencoder bert or a simple image classifier note training step defines the training loop forward defines how the lightningmodule behaves during inference prediction lightning ha over advanced feature designed for professional ai research at scale here are some example for complex professional level work you have optional full control of the training loop and optimizers in the pytorch lightning release lightninglite now enables you to leverage all the capability of pytorch lightning accelerator without any refactoring to your training loop check out the blogpost and doc for more info the pytorch lightning community is maintained bywant to help u build lightning and reduce boilerplate for thousand of researcher learn how to make your first contribution herepytorch lightning is also part of the pytorch ecosystem which requires project to have solid testing documentation and support if you have any question please,"[('pytorch lightning accelerator', 0.6975), ('engineering lightning structure pytorch code', 0.6372), ('pytorch lightning community', 0.6332), ('pytorch version', 0.5853), ('lightweight pytorch wrapper', 0.5834), ('pytorch lightning release lightninglite', 0.583), ('pytorch code', 0.5418), ('pytorch ecosystem', 0.4342), ('doc example community lightning ai license lightning', 0.4227), ('pypia lightningmodule', 0.4066)]","[-1.36237606e-01 -5.96479662e-02 -7.20883254e-03  3.50001454e-02
  4.90150452e-02 -1.83729604e-02  1.19804107e-02  2.88201254e-02
 -5.70988581e-02 -3.35030444e-02 -7.35382130e-03 -1.37939183e-02
 -1.30524755e-01  1.11762695e-02  5.24368547e-02  5.84295020e-02
 -5.21169007e-02  8.45203400e-02 -2.08749566e-02 -7.46618062e-02
  8.09209701e-03 -2.69158222e-02 -3.72771127e-03  5.70764095e-02
  1.51644740e-02  5.35012856e-02 -1.68512613e-02 -4.83596921e-02
  6.21895790e-02 -4.46417443e-02 -3.64176445e-02 -2.06544288e-02
  7.47447610e-02  5.42918332e-02 -7.43492460e-03 -7.80318864e-04
 -5.74604422e-02  1.56902075e-02  1.84966046e-02 -1.60653703e-02
  2.40191445e-02 -3.04892678e-02 -8.06323066e-02  4.08330932e-02
  2.65110168e-03  1.48268128e-02  2.43648104e-02 -7.46045336e-02
 -4.80353050e-02 -6.45070076e-02 -6.01348607e-03 -5.54704443e-02
 -5.74549213e-02  2.77069770e-02 -1.16339680e-02 -4.80477959e-02
  1.06623638e-02 -4.15469194e-03  7.95856863e-02 -4.07444593e-03
  6.00590277e-03  2.16756258e-02 -3.77891436e-02  5.01318835e-02
 -2.29626019e-02  4.69745435e-02 -2.80872881e-02  8.59201998e-02
  7.09878728e-02 -1.20082147e-01  2.19892934e-02 -4.07039896e-02
 -2.65871529e-02  6.86424747e-02 -2.12806221e-02 -2.64600385e-02
  4.28347327e-02  3.93951964e-03 -4.52127382e-02 -1.02991782e-01
 -2.20070872e-03 -3.34611372e-03 -2.47130319e-02  8.84105638e-02
 -1.10860355e-03  7.07159638e-02 -2.83970740e-02  5.03011160e-02
  6.25392469e-03 -2.80242451e-02  8.97157267e-02 -2.36628801e-02
  7.14477152e-02  7.52186924e-02 -3.58107733e-03  6.20960779e-02
  3.83671634e-02 -1.42711863e-01 -9.09866169e-02  3.37330140e-02
 -5.04218154e-02 -1.64453518e-02 -1.27480784e-03 -2.51190849e-02
 -3.84275950e-02  3.45852524e-02 -1.76211670e-02 -3.22454842e-03
  7.52247423e-02  8.11644867e-02  5.05156294e-02 -3.16320993e-02
 -6.63348511e-02 -8.30279961e-02  7.15770423e-02 -7.68088084e-03
 -6.77732378e-02  1.17080152e-01  1.12845443e-01  4.12396118e-02
 -4.55224067e-02  2.32071113e-02  1.27387419e-02  2.06225701e-02
 -3.27252485e-02 -6.77668815e-03 -7.52597675e-02  1.00482646e-32
 -4.31964286e-02 -2.02982016e-02 -8.87762234e-02  8.19381792e-04
  6.44388795e-02 -1.16331249e-01  9.45436731e-02  6.42874977e-03
 -2.96088904e-02 -3.57274227e-02  2.58450788e-02  1.76995154e-02
 -4.09142710e-02  7.63696954e-02  1.01408269e-02 -5.44025339e-02
 -7.35658184e-02  3.83034833e-02  3.94063368e-02  7.09756240e-02
  1.33495312e-02 -3.08978930e-02 -4.12506424e-03  1.50342301e-01
 -3.29949036e-02 -4.24547074e-03  4.76021692e-03 -3.65091711e-02
  1.90256741e-02  2.21941434e-02 -1.64906047e-02  4.37791348e-02
  5.30950800e-02 -1.99077204e-02 -3.97718064e-02  3.16444486e-02
 -5.73376194e-02 -6.35346845e-02 -2.40267944e-02 -6.08270168e-02
 -1.20348804e-01  1.14664175e-02 -8.57789889e-02 -2.37296466e-02
  9.66817513e-02 -5.85488752e-02 -3.36763263e-02  4.71708253e-02
  4.74816263e-02 -5.36243096e-02 -6.06220439e-02  5.10852337e-02
 -1.95372105e-02  5.88133037e-02  4.41198908e-02 -2.04113405e-02
  8.14023521e-03  6.14563785e-02  1.04261331e-01  2.39743888e-02
 -3.35845212e-03  6.40519187e-02  2.86765378e-02 -5.58129400e-02
  1.49700716e-02  3.67168114e-02 -3.99536565e-02 -4.16657003e-03
 -7.55711421e-02  7.79602155e-02 -5.38894795e-02  2.34873779e-02
 -5.76727986e-02  1.20379496e-02  4.96079139e-02 -2.81703677e-02
 -3.22818682e-02  1.28080035e-02  2.32095923e-02 -2.97858734e-02
 -1.08650230e-01  2.64925733e-02  5.09442128e-02  3.10222544e-02
 -3.74707989e-02 -2.40919236e-02  6.84788032e-03  4.55439389e-02
 -5.27180322e-02 -3.20627762e-04  2.27978695e-02 -3.08187306e-02
  2.88009979e-02  1.51299434e-02 -1.67519581e-02 -8.68462500e-33
  2.36276798e-02  3.04504838e-02 -9.03431550e-02  7.49783069e-02
  2.25285813e-02 -2.13283184e-03 -1.01600818e-01 -7.48478994e-02
  6.67087454e-03  7.40650669e-02  1.81673970e-02  5.22141755e-02
 -8.84093624e-03 -3.08151590e-03  6.31374419e-02 -1.74370576e-02
 -1.17375627e-02 -1.20520107e-02 -1.03636310e-02  3.63702103e-02
 -2.63226405e-03  7.08243102e-02 -8.04393291e-02 -1.76619471e-03
 -4.24772361e-03 -2.92762704e-02 -2.40471903e-02  1.03277005e-02
  6.18859529e-02 -4.96882349e-02  3.07367109e-02  6.69293553e-02
 -2.77469773e-02 -2.78370678e-02 -5.03349267e-02 -1.30351996e-02
  3.74260359e-02 -4.12375107e-02 -4.10418399e-02 -6.01431020e-02
  1.50460854e-01  1.75472163e-02  5.72013110e-02 -3.28772068e-02
 -2.65277270e-02  1.33537678e-02 -6.86468929e-02  6.69349125e-03
  2.64357738e-02  7.58446380e-03  1.47142680e-02  2.42451970e-02
 -2.92198062e-02 -2.80172992e-02 -2.71556657e-02 -3.25700231e-02
  7.47949705e-02  5.23114502e-02 -2.87967501e-03  1.09459059e-02
 -8.60072374e-02 -7.02532753e-02  3.69527116e-02  2.84026340e-02
 -3.27093080e-02 -5.45119010e-02  3.88463098e-03  4.29839641e-02
 -5.38807102e-02 -3.42779383e-02  6.33307397e-02  2.12188680e-02
  2.30394173e-02  3.12188752e-02 -7.97749236e-02  3.83007303e-02
  9.23484117e-02  2.66403910e-02  1.25269433e-02 -8.40329900e-02
  1.37769744e-01  5.80294542e-02  2.05680430e-02  2.72972067e-03
  1.24167940e-02  2.86585465e-02  3.59637924e-02  2.32644882e-02
  3.45856212e-02  2.21941974e-02 -3.80101008e-03 -4.10598842e-03
  4.78001162e-02  5.63548580e-02  6.10228553e-02 -3.98613018e-08
 -1.63460672e-02  7.37487823e-02 -1.79118086e-02 -6.31063199e-03
  6.07689358e-02  3.20465639e-02 -3.23464745e-03  1.19301245e-01
 -4.40964177e-02 -2.07624082e-02  1.99133325e-02 -5.75341880e-02
 -4.16320749e-02  4.45426069e-02  5.24837784e-02  8.13962892e-02
 -2.72047948e-02 -9.52450791e-04  9.53070167e-03 -1.78113598e-02
 -3.89667414e-02  6.99409097e-03 -3.52316946e-02 -1.88385341e-02
 -7.46680200e-02  4.33663512e-03  4.30380814e-02 -2.10654624e-02
  1.30471941e-02 -4.18116637e-02 -1.70064755e-02  1.20183378e-02
  9.31732431e-02 -6.41005337e-02  1.27941221e-01  5.53220399e-02
 -5.26235029e-02 -5.72030097e-02 -9.96446796e-03  8.39177892e-02
 -6.25635087e-02 -8.88343975e-02  6.36397535e-03 -4.15910222e-02
 -2.71570720e-02  2.42304355e-02 -6.52566999e-02 -1.10950775e-01
 -4.10919972e-02  5.41251861e-02  2.32003964e-02 -1.61266793e-02
 -9.10549890e-03 -6.19133040e-02 -2.68770214e-02  1.48988679e-01
 -5.41118383e-02 -3.59486639e-02 -1.61606614e-02 -8.95180926e-02
  3.61197479e-02  1.43720675e-03  2.88909953e-02  3.07584852e-02]",2,2
gluonnlp,7,gluonnlp your choice of deep learning for nlpgluonnlp is a toolkit that enables easy text preprocessing datasets loading and neural model building to help you speed up your natural language processing nlp research quick start guideresourcestutorial proposal for gluonnlp is accepted at emnlp hong kong gluonnlp wa featured in kdd alaska check out our tutorial from shallow to deep language representation pre training fine tuning and beyond jsalt in montreal checkout http jsalt mxnet io aws re invent in la vega checkout detail pydata nyc checkout the awesome talk by sneha jha kdd london apache mxnet gluon tutorial check out http kdd mxnet io make sure you have python or newer and a recent version of mxnet our ci server run the testsuite with python you can install mxnet and gluonnlp using pip gluonnlp is based on the most recent version of mxnet in particular if you want to install the most recent mxnet release else if you want to install the most recent mxnet nightly build then you can install gluonnlp please check more installation detail gluonnlp documentation is available at our website gluonnlp is a community that belief in sharing for question comment and bug report github issue is the best way to reach u we now have a new slack channel here register gluonnlp community welcome contribution from anyone there are lot of opportunity for you to become our contributor ask or answer question on github issue propose idea or review proposed design idea on github issue improve the documentation contribute bug report github issue write new script to reproduce state of the art result write new example to explain key idea in nlp method and model write new public datasets license permitting most importantly if you have an idea of how to contribute then do it for a list of open starter task check good first issue also see our contributing guide on simple how tos contribution guideline and more check out how to use gluonnlp for your own research or project if you are new to gluon please check out our minute crash course for getting started quickly refer to notebook runnable example at example for advanced example check out our script for experienced user check out our api note load the wikitext dataset for example build vocabulary based on the above dataset for example from the model package apply a standard rnn language model to the above dataset for example load a glove word embedding one of the state of the art english word embeddings the bibtex entry for the reference paper of gluonnlp is for background knowledge of deep learning or nlp please refer to the open source book dive into deep learning,"[('nlpgluonnlp', 0.5443), ('gluonnlp documentation', 0.5081), ('website gluonnlp', 0.48), ('pip gluonnlp', 0.4798), ('deep language representation', 0.4703), ('gluonnlp', 0.4593), ('mxnet', 0.4465), ('natural language processing nlp research', 0.437), ('deep learning', 0.432), ('recent mxnet', 0.4312)]","[-8.76904577e-02 -9.05465260e-02  7.67915323e-02 -6.16334053e-03
  5.37997670e-02  1.61244385e-02 -2.81323344e-02  6.31520078e-02
 -5.94500639e-02 -6.36382997e-02 -5.18721603e-02  7.20527023e-02
 -4.77314629e-02  6.05696402e-02  2.63113678e-02 -3.87709355e-03
  2.01040767e-02  5.90604171e-02 -8.83900523e-02 -8.73504728e-02
  5.61441109e-02  2.79116333e-02  1.93535667e-02 -2.19242238e-02
  1.82145797e-02 -2.92455107e-02 -2.69172825e-02 -1.02874681e-01
  5.44968955e-02 -4.86229267e-03 -3.18354592e-02  5.86709343e-02
 -7.54315872e-03  5.63927107e-02 -3.36424820e-02  1.70056969e-02
 -8.57905205e-03  2.24421456e-04 -6.36370154e-03 -4.88939248e-02
 -3.64537276e-02 -5.28127626e-02 -4.68053808e-03 -2.74417587e-02
  1.59374565e-01  1.37751726e-02 -3.47830206e-02 -7.47949407e-02
 -2.40612272e-02  3.01074758e-02 -5.59146293e-02 -2.94470396e-02
  1.11952620e-02  5.90667687e-02  3.81732127e-03 -2.32041590e-02
 -6.28379285e-02 -4.00475375e-02 -1.82322618e-02 -6.72703236e-02
  1.87679902e-02 -4.26389789e-03 -5.97744994e-02  2.89447457e-02
 -2.69152392e-02  4.23386917e-02  2.29665358e-02  2.99401837e-03
  8.56610611e-02 -3.25635746e-02 -4.76614200e-02  4.71822545e-02
 -4.00287583e-02  3.06085385e-02 -3.32157165e-02  1.59238838e-02
  7.68740252e-02 -3.09129898e-02  6.67917430e-02 -9.77332518e-02
  5.91260977e-02  5.72468862e-02  8.74007642e-02  3.00906431e-02
  5.09874616e-03 -6.22736737e-02 -4.48721796e-02  4.69049439e-02
 -6.64925426e-02 -1.99847072e-02  1.88688822e-02 -1.10074557e-01
  3.70821320e-02  3.18307541e-02  1.33127184e-03  2.85368450e-02
 -4.80308942e-02 -1.28505275e-01 -4.04390208e-02  3.88340764e-02
 -3.11742816e-02  5.39760292e-03  9.14257467e-02 -2.52478905e-02
 -5.78177199e-02  5.75414635e-02 -7.74044124e-03 -1.68695990e-02
  8.89345407e-02 -3.04229055e-02 -6.28813263e-03  5.67964092e-02
 -4.00616303e-02 -6.78905994e-02  5.11382557e-02 -3.04074567e-02
 -1.73239503e-02 -3.13889491e-03  6.80347309e-02  1.40973508e-01
 -1.30471170e-01  5.33474758e-02 -3.57463621e-02  1.41787166e-02
 -5.18008731e-02  2.88453163e-03 -1.05452184e-02  6.10684303e-33
  1.52668199e-02 -1.58867938e-03 -3.69704887e-02 -6.93575814e-02
  4.56440672e-02 -3.53650451e-02  6.28088117e-02  1.43333562e-02
 -4.94893640e-02 -8.52233917e-02 -4.04572263e-02  1.03632640e-02
 -7.27713108e-02  8.65719318e-02 -5.16131707e-02  1.89398155e-02
  5.17796911e-03  2.20576320e-02  3.18521671e-02 -2.91796736e-02
  2.19755364e-03  1.08751558e-01  5.72749749e-02  5.84192947e-02
 -1.31103850e-03 -1.45885879e-02 -2.69335248e-02 -4.50337455e-02
  6.56221309e-05  1.97619572e-02 -3.06168124e-02 -3.11303604e-02
  2.54575871e-02  4.00479399e-02  3.46580558e-02  2.16520000e-02
 -3.03870179e-02 -5.48555963e-02  1.30130807e-02 -5.96715994e-02
 -8.62526596e-02  4.22493033e-02 -9.98006463e-02 -2.25561988e-02
  2.23328490e-02 -2.93099992e-02 -4.75985706e-02 -3.28606218e-02
  4.34038155e-02 -5.53238876e-02  1.02585778e-02 -5.27774962e-03
 -1.03589945e-01 -1.64728344e-03  5.45523874e-02 -1.38697671e-02
  2.66676396e-03  6.58839643e-02  9.43679586e-02  4.47329842e-02
  5.45510352e-02  5.32643199e-02  2.97882650e-02 -4.33952138e-02
 -1.36207615e-03  3.70394774e-02 -1.76923722e-02 -1.94814503e-02
  2.53512263e-02 -2.60120109e-02 -1.25612244e-02  3.68095450e-02
 -2.43586209e-02 -6.52919756e-03  3.78595083e-03 -9.59924515e-03
  1.85193599e-03 -1.33095354e-01 -1.84084550e-02  9.66087580e-02
 -3.97703499e-02  1.16375918e-02  7.13256793e-03  1.22377360e-02
 -6.73861504e-02 -7.72631681e-03  1.78153291e-02 -3.45280357e-02
  9.25040245e-02 -1.14808558e-03 -2.16558561e-04 -4.97428002e-03
 -6.83439989e-03  3.38430628e-02  3.65512148e-02 -5.82228082e-33
 -4.09255102e-02  6.84107989e-02 -1.28977910e-01  7.68311471e-02
 -5.16615473e-02 -2.01824913e-03 -1.32786231e-02 -8.26719105e-02
  1.52922003e-02  3.92696075e-02  1.40545762e-03 -6.62337989e-02
 -1.15145762e-02 -2.01058593e-02 -6.93601975e-03 -9.53442380e-02
 -4.42983694e-02 -4.80432697e-02 -5.63615700e-03  1.30663812e-01
 -3.04061212e-02  3.17685269e-02 -1.32379621e-01  4.86455522e-02
  7.05942810e-02 -3.73696163e-02  1.00549515e-02 -1.66424103e-02
 -7.17553031e-03 -4.53601852e-02 -9.65127139e-04  3.79370153e-02
 -5.21502458e-02  1.00162094e-02  4.46880721e-02 -9.75454692e-03
  5.65928929e-02  2.05137115e-02  1.12316255e-02 -6.75750300e-02
  1.66387960e-01  2.81874072e-02  4.04493660e-02 -5.02851419e-02
 -3.86848412e-02  9.53627226e-04 -1.48298651e-01  6.43333141e-03
  1.95444729e-02 -1.67785939e-02  8.26566014e-03 -5.23484983e-02
 -3.00480854e-02 -3.94107439e-02  8.81317817e-03 -1.17382286e-02
  7.83837959e-02 -2.04405617e-02 -1.22463349e-02  2.30002515e-02
 -1.14807211e-01 -1.54046938e-02  2.35035848e-02  2.38451492e-02
  6.10402562e-02 -1.79408472e-02 -4.31744754e-02  5.90642542e-02
 -2.77802125e-02 -5.64931110e-02  7.92753696e-03  1.59252193e-02
  5.72253056e-02  8.17598030e-02 -5.67498170e-02 -1.93307102e-02
  1.03537226e-03 -3.57078835e-02  3.47425626e-03 -8.89010504e-02
  9.87175778e-02  4.57923599e-02 -9.47064348e-03  5.55465072e-02
  4.29980308e-02  6.34477735e-02  3.29095982e-02  7.74467960e-02
  4.06105556e-02 -4.58232779e-03 -2.95157428e-03  3.90330050e-03
 -5.33753037e-02  5.18944114e-02  1.12260608e-02 -3.11322239e-08
 -4.82338183e-02 -1.05525658e-03  9.84532759e-03  2.63809413e-02
  4.26181108e-02 -2.66423505e-02  4.53378446e-02  1.03757001e-01
 -1.84234641e-02  5.30814677e-02  5.76045625e-02  1.69798974e-02
 -8.50189030e-02 -5.53022474e-02  1.61673166e-02  4.01471592e-02
  3.06171626e-02 -2.95681544e-02  6.80979267e-02 -2.62934193e-02
  3.86130922e-02  3.44556458e-02 -4.60241139e-02  3.95001238e-03
  2.77697593e-02 -4.16034199e-02 -6.36380690e-04  9.27154794e-02
  1.21052163e-02 -6.26047626e-02 -1.31392591e-02  3.31660882e-02
  2.43775435e-02 -8.88389349e-02  9.18674916e-02  1.14998706e-01
  2.59005185e-02 -4.19559442e-02 -1.00184130e-02 -3.89074832e-02
  1.69603415e-02  8.93095434e-02  3.00697144e-02 -8.35266486e-02
  4.76551801e-02 -1.36337467e-02  1.48305914e-03 -7.30259940e-02
 -1.13654360e-02  3.49551551e-02 -5.18203378e-02 -5.51617231e-05
 -1.23930222e-03  7.84712806e-02  9.63847805e-03  4.24784943e-02
 -1.55029744e-02 -8.57068598e-02 -5.06128417e-03 -3.98635026e-03
 -4.66020666e-02  1.06444925e-01  4.16912921e-02  7.83939809e-02]",2,2
gin-config,6,gin provides a lightweight configuration framework for python based on dependency injection function or class can be decorated with gin configurable allowing default parameter value to be supplied from a config file or passed via the command line using a simple but powerful syntax this remove the need to define and maintain configuration object e g protos or write boilerplate parameter plumbing and factory code while often dramatically expanding a project s flexibility and configurability gin is particularly well suited for machine learning experiment e g using tensorflow which tend to have many parameter often nested in complex way author dan holtmann rice sergio guadarrama nathan silberman contributor oscar ramirez marek fiser,"[('configurability gin', 0.6092), ('lightweight configuration framework', 0.5736), ('configuration object e g', 0.5103), ('boilerplate parameter plumbing', 0.4383), ('config file', 0.3559), ('python', 0.3546), ('default parameter value', 0.3387), ('gin', 0.3055), ('dependency injection function', 0.2999), ('tensorflow', 0.2746)]","[-4.44448888e-02 -5.44754118e-02 -1.26446867e-02  1.86961871e-02
 -1.17948698e-02 -1.87135674e-02  1.46179795e-02  1.20065831e-01
 -1.29189312e-01 -4.85184826e-02 -6.13248385e-02 -1.08889481e-02
 -5.66401854e-02  2.31089815e-02  3.60824689e-02  4.97505069e-03
  1.03934687e-02 -1.70758925e-02 -2.07767673e-02 -6.70207962e-02
 -3.54573838e-02 -3.02347802e-02 -3.40358317e-02  8.74059647e-03
  9.27681383e-03  1.90456081e-02 -3.64353657e-02 -5.68926260e-02
  7.65561126e-03 -2.24469174e-02 -1.94179714e-02  1.08758971e-01
  5.18361758e-03 -3.35983783e-02 -1.43553447e-02  5.61391003e-02
  7.07394183e-02 -9.52842683e-02  6.63153420e-04  1.34366518e-02
 -5.95888756e-02  2.22638417e-02 -2.13529374e-02 -1.68697331e-02
  3.85907218e-02  2.54088119e-02  1.76465344e-02 -5.09075895e-02
 -5.14197769e-03 -5.83878681e-02 -7.16104209e-02 -5.23398370e-02
 -6.19318634e-02  1.02260732e-03  3.49925496e-02 -2.67013125e-02
  3.21311429e-02 -1.17137758e-02  3.44377719e-02 -1.91127192e-02
 -2.87434217e-02  1.72106307e-02 -3.90634425e-02  1.13624170e-01
  1.65554304e-02  1.14624780e-02 -1.88511051e-02  4.55249958e-02
  1.41304672e-01 -4.54710722e-02 -9.30761695e-02 -2.05288529e-02
 -2.87218969e-02  1.86429601e-02 -1.85182784e-02 -6.05624542e-03
  5.60927652e-02 -1.35206536e-03 -2.92737912e-02 -7.09364787e-02
  3.60897451e-04  6.73165023e-02  4.74803597e-02  8.76126438e-02
  1.63084567e-02  2.98219454e-02  6.58118725e-02  6.13845438e-02
  5.81547953e-02  8.96012504e-03  2.52321400e-02 -2.59576160e-02
  2.31213756e-02 -5.76972999e-02  6.14698231e-02  7.21945986e-02
  1.93368476e-02 -5.53525724e-02 -1.06657699e-01  2.90580951e-02
 -6.33544028e-02 -4.47446555e-02  1.50084287e-01  5.31300157e-03
 -3.48514412e-04  2.22622696e-02 -6.91448012e-03 -1.71938241e-02
  3.65105197e-02  3.34823355e-02 -5.41081391e-02  9.45566315e-03
  2.89578009e-02 -1.26185179e-01  4.80975658e-02 -2.65331082e-02
 -2.75178626e-02  1.21069737e-02  2.46902034e-02  1.16300799e-01
 -7.60975108e-02 -2.51167989e-03  1.26625085e-02 -2.47336850e-02
 -1.11560598e-02  5.60233891e-02 -4.00109626e-02  3.30671410e-33
 -9.07848775e-03 -6.81392998e-02 -3.85683663e-02  6.30062670e-02
  3.60945649e-02 -3.12918685e-02  7.68012479e-02 -1.52526889e-03
 -1.26656136e-02  1.86590813e-02  1.19296424e-02  1.30707577e-01
 -5.86279072e-02  1.27171755e-01  4.67307679e-02 -6.01804480e-02
 -1.24831488e-02  1.01555757e-01 -2.44923905e-02  9.95193329e-03
  2.74563897e-02  7.29013830e-02 -2.98957657e-02  3.54149900e-02
  9.03937519e-02  1.33087393e-02  6.66271448e-02  1.26780001e-02
 -1.20963514e-01  1.88662249e-04 -4.45746295e-02 -7.42174163e-02
  9.58935940e-04  2.56769918e-02 -3.16603221e-02 -7.29714110e-02
 -6.54013380e-02 -5.57837412e-02 -1.23626180e-02 -5.56027070e-02
 -3.04310657e-02  6.26880601e-02 -3.82113121e-02  6.96865551e-04
  2.45886128e-02 -3.72210741e-02 -5.98303089e-03  4.44267839e-02
  4.84341010e-02 -2.75737494e-02 -5.61818257e-02  2.53568366e-02
  2.95256302e-02  4.31826264e-02 -4.29945439e-03 -2.19013095e-02
  4.79746871e-02 -5.55781601e-03 -1.26131997e-03  5.92867620e-02
 -1.14012614e-01  3.66481580e-02 -4.88246195e-02 -2.31078677e-02
  8.00868645e-02 -4.37987857e-02 -9.95373353e-02 -1.15479240e-02
  9.50577036e-02  6.25550672e-02 -1.15200832e-01 -6.50208024e-03
 -8.82012770e-02  1.81384720e-02 -1.89693831e-02 -2.95290891e-02
  5.01791164e-02 -4.46797237e-02 -6.24756813e-02  7.60635408e-03
 -1.08903371e-01  7.77544305e-02  2.97955219e-02  8.31077024e-02
 -1.77107044e-02 -3.79044488e-02  5.08947223e-02  2.86164861e-02
 -4.98524867e-02 -6.09314926e-02 -2.57437546e-02  8.89332406e-03
  5.80959804e-02 -3.30225728e-03 -4.57472838e-02 -3.04205301e-33
  3.97535600e-02 -4.05925252e-02 -2.87210066e-02  9.65659767e-02
  1.86854787e-02  4.01886553e-02  1.75765827e-02 -6.51406571e-02
  6.01604134e-02  2.37066150e-02  4.14571576e-02 -6.14740793e-03
  6.32951558e-02 -3.36283110e-02 -2.41277553e-02  2.05566920e-02
 -8.83948281e-02 -1.14482880e-01  1.82952899e-02  7.69760041e-03
 -2.14309804e-02  1.04008630e-01 -2.16435045e-02 -4.76252325e-02
 -5.57073057e-02 -3.99925746e-02 -9.37465802e-02 -5.59274815e-02
 -1.41851027e-02  5.86637184e-02  3.17506306e-02  3.45209651e-02
  5.68884367e-04  5.49119897e-02 -1.03516439e-02 -4.17007506e-02
  1.00174420e-01  3.40387337e-02 -4.40061465e-02  4.87053692e-02
  1.01097830e-01  8.00911263e-02  2.69210599e-02  2.56256182e-02
 -3.37610468e-02  4.09963578e-02 -1.04633480e-01 -5.30683659e-02
 -3.33576389e-02 -3.87359448e-02 -3.87121364e-02 -3.42124924e-02
 -1.16625791e-02 -3.33919376e-02 -6.00681677e-02 -5.26659787e-02
  1.00279011e-01 -1.30653540e-02  7.30290730e-03  7.23383352e-02
  2.66199410e-02 -5.39730769e-04  7.21686631e-02 -3.52318920e-02
 -7.01600909e-02  6.01503858e-03 -1.11907542e-01 -4.22820076e-02
 -8.43001306e-02 -4.60149162e-02 -3.94698158e-02 -1.02941349e-01
  6.35719150e-02  4.68356982e-02 -1.68679915e-02 -1.98452435e-02
  2.48274375e-02 -5.06342109e-03  2.08359994e-02  9.02844593e-03
  6.85227513e-02  2.47914679e-02 -2.58042291e-02  6.62448779e-02
 -1.02271978e-02  5.32072522e-02 -2.67309556e-03  1.34535562e-02
  8.41647834e-02  5.32840565e-02 -2.50310861e-02  4.37578857e-02
 -2.13097129e-02  1.14000924e-01  5.34816878e-03 -3.06060919e-08
 -4.62601520e-02  5.20509034e-02  4.33571860e-02  6.23160116e-02
 -3.20987590e-02  3.08215357e-02  6.86451932e-03  9.19983685e-02
  3.64020728e-02  8.40524882e-02  3.95514742e-02  1.22808870e-02
 -1.10974811e-01  4.94565442e-03  2.52257194e-02  4.34064642e-02
 -4.97942604e-02  4.19221893e-02 -3.79991420e-02 -4.31168973e-02
  8.61167088e-02  1.17186252e-02 -3.32703367e-02  3.78280990e-02
  1.76608283e-02 -5.29670529e-02  6.60698861e-02  3.18105668e-02
  1.69071332e-02  6.97916076e-02 -6.97577652e-03  2.06831936e-02
  2.17299219e-02 -2.59799929e-03 -5.01391850e-02  9.20473486e-02
 -5.86121120e-02 -2.08827872e-02 -5.40875969e-03  6.55044466e-02
 -3.71275172e-02  3.01050488e-02 -3.02533600e-02 -5.41122742e-02
 -1.15120178e-02 -2.76654363e-02  2.43156608e-02 -4.44012275e-03
 -3.28553431e-02 -2.49304269e-02 -2.70535517e-02  4.52901833e-02
 -1.98898837e-02  4.00425866e-02 -4.28964704e-04  5.94620667e-02
 -2.51737516e-02 -4.62548137e-02  2.71193162e-02 -4.93866429e-02
 -4.91770022e-02  1.83187295e-02  5.10191210e-02 -2.21102126e-02]",2,6
tensorflow-addons,6,tensorflow addons is a repository of contribution that conform to well established api pattern but implement new functionality not available in core tensorflow tensorflow natively support a large number of operator layer metric loss and optimizers however in a fast moving field like ml there are many interesting new development that cannot be integrated into core tensorflow because their broad applicability is not yet clear or it is mostly used by a smaller subset of the community,"[('tensorflow addons', 0.7577), ('core tensorflow tensorflow', 0.6679), ('core tensorflow', 0.6664), ('operator layer', 0.3046), ('many interesting new development', 0.2981), ('new functionality', 0.2745), ('api pattern', 0.2641), ('optimizers', 0.26), ('broad applicability', 0.2379), ('metric loss', 0.1614)]","[-8.97862688e-02 -1.18038781e-01  3.16391475e-02 -4.33627144e-02
  2.53619216e-02 -2.02324782e-02 -2.57384777e-02  1.83385331e-02
 -5.81522770e-02 -6.88153505e-02 -5.19493669e-02 -3.10901441e-02
 -8.85608718e-02  1.76101588e-02 -5.95074939e-03 -3.21294949e-03
  3.92010715e-03  4.97659519e-02 -6.57572523e-02 -1.30550966e-01
 -2.68512554e-02  5.42681664e-04 -4.33825254e-02  3.12073473e-02
 -6.91145658e-03 -1.05579169e-02 -2.04007290e-02 -8.65406841e-02
  5.74872009e-02 -1.67435047e-03 -8.23836308e-03  7.07201511e-02
  4.79811579e-02  3.24241221e-02 -6.72125742e-02  1.48587301e-02
 -4.51599248e-04 -6.25291467e-02 -3.13824192e-02 -5.94467595e-02
 -3.74892205e-02 -4.23398353e-02  2.67893937e-03  1.40540833e-02
  2.35674996e-02 -4.55066636e-02  5.19453958e-02 -9.77498144e-02
  4.31866013e-02  2.51911264e-02 -4.70772199e-02 -7.73889273e-02
 -6.55976534e-02 -4.40465845e-03  3.16754729e-02 -4.63077910e-02
  2.95551145e-03 -1.21315550e-02 -4.01561484e-02 -3.97619344e-02
  5.15281931e-02 -6.68541044e-02 -4.53533158e-02  2.26086527e-02
  8.98065791e-03 -1.00910263e-02 -6.51421538e-03  7.60303298e-03
  1.43566564e-01 -7.02192560e-02 -3.99390906e-02  1.30615896e-02
 -7.31174797e-02  2.47125588e-02 -3.24466941e-03 -7.23953312e-03
  9.04564857e-02 -2.24864949e-02  8.57826136e-03 -7.33851790e-02
  1.21282237e-02  1.60371177e-02  2.69263256e-02 -1.19217820e-02
  2.84280516e-02  1.27448309e-02  1.49827795e-02  1.39713194e-02
 -9.23442934e-03 -5.31784967e-02  5.07230423e-02 -1.97882000e-02
 -2.44070403e-02 -4.59287874e-02  6.58166781e-02  4.79791984e-02
 -7.16299564e-02 -3.96243744e-02 -6.81035444e-02  4.78538014e-02
 -6.75906762e-02 -2.03151684e-02 -1.71734374e-02  2.17417087e-02
  2.30948273e-02  6.04007505e-02  6.85646087e-02 -6.89302292e-03
 -4.94035613e-03  9.55159217e-03  1.61367357e-02  4.23238687e-02
  4.80575562e-02 -9.01112482e-02  5.87622859e-02  3.04652117e-02
 -1.67199783e-02  3.84754166e-02  1.14962291e-02  1.57751665e-01
 -4.98025157e-02  4.28242795e-02 -3.89041603e-02 -1.27980849e-02
 -6.87029585e-02  5.20303100e-02 -7.98806101e-02  3.57061074e-33
 -2.97458991e-02  2.95454096e-02 -4.15322324e-03  3.63416113e-02
  2.42048223e-02 -4.09483947e-02  3.79373953e-02  4.16373536e-02
  1.37505513e-02 -3.44363786e-02 -5.28889112e-02  1.30500659e-01
 -1.06891673e-02  1.03696562e-01  3.56455594e-02 -1.30566180e-01
 -1.80167872e-02  2.31414922e-02  6.50603101e-02  2.20194627e-02
  1.85122304e-02  3.53577137e-02  2.29679476e-02  6.28408864e-02
  5.46789579e-02  3.49638169e-04  9.22258198e-03 -2.63234065e-03
 -3.52427736e-02  2.61538476e-02 -1.00986406e-01 -1.74671821e-02
 -3.25042196e-02  1.03499582e-02 -3.10013853e-02  2.20676754e-02
 -1.43605247e-02 -5.82874753e-02  1.25837838e-02 -5.65766059e-02
  7.62841012e-03  4.33492213e-02 -1.05761193e-01 -3.93863954e-02
 -5.76675124e-02 -3.24962512e-02 -1.37139140e-02  1.20690027e-02
  4.34219502e-02 -8.88004601e-02 -4.17978428e-02 -1.06062526e-02
 -4.94124554e-03  3.78986297e-04  3.73635665e-02 -1.79445744e-02
  7.65942363e-03  6.94785267e-03  9.91830230e-02  1.02389365e-01
 -3.02649941e-02  1.50350640e-02  1.05823968e-02  3.57300714e-02
  1.00405319e-02  4.01730370e-03 -7.82236084e-03  3.46812122e-02
  3.04140598e-02  6.83226138e-02 -1.18852697e-01  1.15998186e-01
 -2.70475224e-02  6.66844542e-04  2.76115574e-02 -5.26361838e-02
  1.32765602e-02 -1.36603847e-01  1.36075541e-02 -4.35813423e-03
 -6.83725476e-02  9.72760320e-02  3.59404422e-02  4.98092081e-03
 -3.92098390e-02  5.00845723e-03 -3.37681733e-02  2.37528980e-02
 -3.91326062e-02 -4.96176071e-02 -8.09078738e-02 -7.15844054e-03
  7.62598664e-02  5.30059412e-02 -4.12220135e-02 -3.34973903e-33
 -6.49891421e-02  2.87495833e-02 -9.78386849e-02  5.50833121e-02
 -2.20227689e-02  1.72437392e-02  6.72312528e-02  1.03464331e-02
  1.83243994e-02  4.23783734e-02  5.73254898e-02  1.03104732e-03
  4.33737636e-02 -9.66299698e-03  2.80367862e-02 -4.54225950e-02
 -1.10735759e-01 -9.19594690e-02  3.81037071e-02 -2.61781327e-02
 -3.24452966e-02  9.07430947e-02 -7.91312605e-02 -4.18965742e-02
  5.29780146e-03  1.62965897e-02 -1.06994696e-01 -2.75125392e-02
  4.31830063e-02 -5.33793587e-03  5.39815566e-03 -5.21762297e-02
  9.82902292e-03  7.14953914e-02  3.75400260e-02  2.92390883e-02
  9.22522545e-02  1.68651361e-02 -7.16431532e-03  2.11891737e-02
  7.80999959e-02  2.59150546e-02  8.53769034e-02  4.16120216e-02
  1.37825741e-03 -1.70019027e-02 -2.58489419e-02  3.62407677e-02
 -7.76955336e-02 -1.12484261e-01  2.12810598e-02 -7.05539212e-02
 -2.88562682e-02 -5.94380684e-02 -3.25454958e-02  8.79102573e-02
  7.26112276e-02 -8.80222395e-03  6.53270483e-02  3.55740711e-02
 -8.64138547e-03 -5.13032414e-02  4.80309911e-02 -6.88992208e-03
  7.50101032e-03  7.09023923e-02 -9.25012305e-02  2.57168580e-02
 -9.98829827e-02  1.06188152e-02  2.61452086e-02  1.41642313e-03
 -5.99419996e-02  4.66827266e-02 -4.74914722e-02 -1.37416925e-03
  7.79261142e-02  7.28263427e-03  2.18421370e-02  3.76907364e-02
 -6.77363249e-04  1.83496960e-02  5.05678058e-02  8.50512758e-02
  7.40702376e-02  7.90369511e-02  2.00538449e-02 -2.15097032e-02
  2.36571021e-02  2.38116342e-03 -7.36448094e-02  3.96364108e-02
 -1.36429155e-02  6.64068982e-02 -1.63139980e-02 -2.63610502e-08
 -2.39212010e-02  4.16458882e-02  5.06317802e-02  8.03190283e-04
  2.41923574e-02 -1.78833480e-03  1.18812814e-03  8.61851275e-02
  1.33452518e-02 -1.17589599e-02 -6.34663105e-02 -2.38291938e-02
 -8.83849338e-02  2.40428001e-02  8.52790922e-02  6.88007325e-02
 -2.59696860e-02  5.48097752e-02  3.45540862e-03 -8.60182941e-02
  4.71636653e-02  1.01587966e-01  3.37017700e-02 -8.75876285e-03
 -8.38143826e-02 -6.49951547e-02  7.81789571e-02  4.11231816e-03
  1.86090916e-02  2.79773045e-02 -3.36758532e-02  9.78821702e-03
  7.42930099e-02 -3.19101512e-02  2.51054019e-02  9.56631079e-02
  3.36963795e-02 -2.09642947e-02 -4.75622453e-02  6.00609481e-02
 -6.50160834e-02  5.16024530e-02  2.75333822e-02 -3.26147862e-02
  5.83665781e-02 -5.15660569e-02 -7.39329215e-03 -8.95538181e-02
 -1.46772126e-02  2.66767368e-02  1.21054742e-02  7.15145320e-02
 -3.29045169e-02  9.73509327e-02  6.20394871e-02  3.83154899e-02
 -7.32194781e-02 -1.34220466e-01  1.67954937e-02  3.09934523e-02
 -1.96856773e-03  1.72408596e-02  5.59351072e-02  4.87198271e-02]",2,2
tensorflow_addons,5,tensorflow addons is a repository of contribution that conform to well established api pattern but implement new functionality not available in core tensorflow tensorflow natively support a large number of operator layer metric loss and optimizers however in a fast moving field like ml there are many interesting new development that cannot be integrated into core tensorflow because their broad applicability is not yet clear or it is mostly used by a smaller subset of the community,"[('tensorflow addons', 0.7577), ('core tensorflow tensorflow', 0.6679), ('core tensorflow', 0.6664), ('operator layer', 0.3046), ('many interesting new development', 0.2981), ('new functionality', 0.2745), ('api pattern', 0.2641), ('optimizers', 0.26), ('broad applicability', 0.2379), ('metric loss', 0.1614)]","[-8.97862688e-02 -1.18038781e-01  3.16391475e-02 -4.33627144e-02
  2.53619216e-02 -2.02324782e-02 -2.57384777e-02  1.83385331e-02
 -5.81522770e-02 -6.88153505e-02 -5.19493669e-02 -3.10901441e-02
 -8.85608718e-02  1.76101588e-02 -5.95074939e-03 -3.21294949e-03
  3.92010715e-03  4.97659519e-02 -6.57572523e-02 -1.30550966e-01
 -2.68512554e-02  5.42681664e-04 -4.33825254e-02  3.12073473e-02
 -6.91145658e-03 -1.05579169e-02 -2.04007290e-02 -8.65406841e-02
  5.74872009e-02 -1.67435047e-03 -8.23836308e-03  7.07201511e-02
  4.79811579e-02  3.24241221e-02 -6.72125742e-02  1.48587301e-02
 -4.51599248e-04 -6.25291467e-02 -3.13824192e-02 -5.94467595e-02
 -3.74892205e-02 -4.23398353e-02  2.67893937e-03  1.40540833e-02
  2.35674996e-02 -4.55066636e-02  5.19453958e-02 -9.77498144e-02
  4.31866013e-02  2.51911264e-02 -4.70772199e-02 -7.73889273e-02
 -6.55976534e-02 -4.40465845e-03  3.16754729e-02 -4.63077910e-02
  2.95551145e-03 -1.21315550e-02 -4.01561484e-02 -3.97619344e-02
  5.15281931e-02 -6.68541044e-02 -4.53533158e-02  2.26086527e-02
  8.98065791e-03 -1.00910263e-02 -6.51421538e-03  7.60303298e-03
  1.43566564e-01 -7.02192560e-02 -3.99390906e-02  1.30615896e-02
 -7.31174797e-02  2.47125588e-02 -3.24466941e-03 -7.23953312e-03
  9.04564857e-02 -2.24864949e-02  8.57826136e-03 -7.33851790e-02
  1.21282237e-02  1.60371177e-02  2.69263256e-02 -1.19217820e-02
  2.84280516e-02  1.27448309e-02  1.49827795e-02  1.39713194e-02
 -9.23442934e-03 -5.31784967e-02  5.07230423e-02 -1.97882000e-02
 -2.44070403e-02 -4.59287874e-02  6.58166781e-02  4.79791984e-02
 -7.16299564e-02 -3.96243744e-02 -6.81035444e-02  4.78538014e-02
 -6.75906762e-02 -2.03151684e-02 -1.71734374e-02  2.17417087e-02
  2.30948273e-02  6.04007505e-02  6.85646087e-02 -6.89302292e-03
 -4.94035613e-03  9.55159217e-03  1.61367357e-02  4.23238687e-02
  4.80575562e-02 -9.01112482e-02  5.87622859e-02  3.04652117e-02
 -1.67199783e-02  3.84754166e-02  1.14962291e-02  1.57751665e-01
 -4.98025157e-02  4.28242795e-02 -3.89041603e-02 -1.27980849e-02
 -6.87029585e-02  5.20303100e-02 -7.98806101e-02  3.57061074e-33
 -2.97458991e-02  2.95454096e-02 -4.15322324e-03  3.63416113e-02
  2.42048223e-02 -4.09483947e-02  3.79373953e-02  4.16373536e-02
  1.37505513e-02 -3.44363786e-02 -5.28889112e-02  1.30500659e-01
 -1.06891673e-02  1.03696562e-01  3.56455594e-02 -1.30566180e-01
 -1.80167872e-02  2.31414922e-02  6.50603101e-02  2.20194627e-02
  1.85122304e-02  3.53577137e-02  2.29679476e-02  6.28408864e-02
  5.46789579e-02  3.49638169e-04  9.22258198e-03 -2.63234065e-03
 -3.52427736e-02  2.61538476e-02 -1.00986406e-01 -1.74671821e-02
 -3.25042196e-02  1.03499582e-02 -3.10013853e-02  2.20676754e-02
 -1.43605247e-02 -5.82874753e-02  1.25837838e-02 -5.65766059e-02
  7.62841012e-03  4.33492213e-02 -1.05761193e-01 -3.93863954e-02
 -5.76675124e-02 -3.24962512e-02 -1.37139140e-02  1.20690027e-02
  4.34219502e-02 -8.88004601e-02 -4.17978428e-02 -1.06062526e-02
 -4.94124554e-03  3.78986297e-04  3.73635665e-02 -1.79445744e-02
  7.65942363e-03  6.94785267e-03  9.91830230e-02  1.02389365e-01
 -3.02649941e-02  1.50350640e-02  1.05823968e-02  3.57300714e-02
  1.00405319e-02  4.01730370e-03 -7.82236084e-03  3.46812122e-02
  3.04140598e-02  6.83226138e-02 -1.18852697e-01  1.15998186e-01
 -2.70475224e-02  6.66844542e-04  2.76115574e-02 -5.26361838e-02
  1.32765602e-02 -1.36603847e-01  1.36075541e-02 -4.35813423e-03
 -6.83725476e-02  9.72760320e-02  3.59404422e-02  4.98092081e-03
 -3.92098390e-02  5.00845723e-03 -3.37681733e-02  2.37528980e-02
 -3.91326062e-02 -4.96176071e-02 -8.09078738e-02 -7.15844054e-03
  7.62598664e-02  5.30059412e-02 -4.12220135e-02 -3.34973903e-33
 -6.49891421e-02  2.87495833e-02 -9.78386849e-02  5.50833121e-02
 -2.20227689e-02  1.72437392e-02  6.72312528e-02  1.03464331e-02
  1.83243994e-02  4.23783734e-02  5.73254898e-02  1.03104732e-03
  4.33737636e-02 -9.66299698e-03  2.80367862e-02 -4.54225950e-02
 -1.10735759e-01 -9.19594690e-02  3.81037071e-02 -2.61781327e-02
 -3.24452966e-02  9.07430947e-02 -7.91312605e-02 -4.18965742e-02
  5.29780146e-03  1.62965897e-02 -1.06994696e-01 -2.75125392e-02
  4.31830063e-02 -5.33793587e-03  5.39815566e-03 -5.21762297e-02
  9.82902292e-03  7.14953914e-02  3.75400260e-02  2.92390883e-02
  9.22522545e-02  1.68651361e-02 -7.16431532e-03  2.11891737e-02
  7.80999959e-02  2.59150546e-02  8.53769034e-02  4.16120216e-02
  1.37825741e-03 -1.70019027e-02 -2.58489419e-02  3.62407677e-02
 -7.76955336e-02 -1.12484261e-01  2.12810598e-02 -7.05539212e-02
 -2.88562682e-02 -5.94380684e-02 -3.25454958e-02  8.79102573e-02
  7.26112276e-02 -8.80222395e-03  6.53270483e-02  3.55740711e-02
 -8.64138547e-03 -5.13032414e-02  4.80309911e-02 -6.88992208e-03
  7.50101032e-03  7.09023923e-02 -9.25012305e-02  2.57168580e-02
 -9.98829827e-02  1.06188152e-02  2.61452086e-02  1.41642313e-03
 -5.99419996e-02  4.66827266e-02 -4.74914722e-02 -1.37416925e-03
  7.79261142e-02  7.28263427e-03  2.18421370e-02  3.76907364e-02
 -6.77363249e-04  1.83496960e-02  5.05678058e-02  8.50512758e-02
  7.40702376e-02  7.90369511e-02  2.00538449e-02 -2.15097032e-02
  2.36571021e-02  2.38116342e-03 -7.36448094e-02  3.96364108e-02
 -1.36429155e-02  6.64068982e-02 -1.63139980e-02 -2.63610502e-08
 -2.39212010e-02  4.16458882e-02  5.06317802e-02  8.03190283e-04
  2.41923574e-02 -1.78833480e-03  1.18812814e-03  8.61851275e-02
  1.33452518e-02 -1.17589599e-02 -6.34663105e-02 -2.38291938e-02
 -8.83849338e-02  2.40428001e-02  8.52790922e-02  6.88007325e-02
 -2.59696860e-02  5.48097752e-02  3.45540862e-03 -8.60182941e-02
  4.71636653e-02  1.01587966e-01  3.37017700e-02 -8.75876285e-03
 -8.38143826e-02 -6.49951547e-02  7.81789571e-02  4.11231816e-03
  1.86090916e-02  2.79773045e-02 -3.36758532e-02  9.78821702e-03
  7.42930099e-02 -3.19101512e-02  2.51054019e-02  9.56631079e-02
  3.36963795e-02 -2.09642947e-02 -4.75622453e-02  6.00609481e-02
 -6.50160834e-02  5.16024530e-02  2.75333822e-02 -3.26147862e-02
  5.83665781e-02 -5.15660569e-02 -7.39329215e-03 -8.95538181e-02
 -1.46772126e-02  2.66767368e-02  1.21054742e-02  7.15145320e-02
 -3.29045169e-02  9.73509327e-02  6.20394871e-02  3.83154899e-02
 -7.32194781e-02 -1.34220466e-01  1.67954937e-02  3.09934523e-02
 -1.96856773e-03  1.72408596e-02  5.59351072e-02  4.87198271e-02]",2,2
opt_einsum,5,optimized einsum can significantly reduce the overall execution time of einsum like expression e g np einsum dask array einsum pytorch einsum tensorflow einsum by optimizing the expression s contraction order and dispatching many operation to canonical blas cublas or other specialized routine optimized einsum is agnostic to the backend and can handle numpy dask pytorch tensorflow cupy sparse theano jax and autograd array a well a potentially any library which conforms to a standard api see the documentation for more information the opt einsum contract function can often act a a drop in replacement for einsum function without futher change to the code while providing superior performance here a tensor contraction is preformed with and without optimization in this particular example we see a x performance improvement which is not uncommon when compared against unoptimized contraction see the backend example for more information on using other backends the algorithm found in this repository often power the einsum optimization in many of the above project for example the optimization of np einsum ha been passed upstream and most of the same feature that can be found in this repository can be enabled with np einsum optimize true however this repository often ha more up to date algorithm for complex contraction the following capability are enabled by opt einsum please see the documentation for more feature opt einsum can either be installed via pip install opt einsum or from conda conda install opt einsum c conda forge see the installation documenation for further method if this code ha benefited your research please support u by citing daniel g a smith and johnnie gray opt einsum a python package for optimizing contraction order for einsum like expression journal of open source software doi http doi org joss all contribution bug report bug fix documentation improvement enhancement and idea are welcome a detailed overview on how to contribute can be found in the contributing guide,"[('einsum optimization', 0.5924), ('more feature opt einsum', 0.549), ('numpy dask pytorch tensorflow', 0.4508), ('opt einsum', 0.4391), ('opt einsum contract function', 0.4272), ('np einsum', 0.3897), ('einsum', 0.378), ('pip install opt einsum', 0.3759), ('einsum function', 0.3729), ('conda conda install opt einsum', 0.3713)]","[-4.98463921e-02 -1.32788550e-02 -1.59783643e-02 -1.38680283e-02
  2.16168463e-02 -6.34169504e-02  2.02649347e-02  5.79896905e-02
 -7.29189366e-02 -1.16005829e-02 -1.84624828e-02 -3.82967070e-02
 -5.61413690e-02  8.65499489e-04 -1.23706320e-02  3.92319709e-02
  9.89296241e-04  3.61860730e-02 -1.97827090e-02 -1.49880424e-01
 -4.23450992e-02 -2.17884197e-03 -5.09330211e-03 -3.30121517e-02
  3.52483504e-02 -2.49176752e-03  5.33982506e-03  5.41259348e-02
  5.53966276e-02 -7.89640658e-03  3.28913443e-02  7.88678527e-02
  1.20151982e-01 -2.33933739e-02  1.57361070e-03 -3.85166751e-03
 -5.97085580e-02 -3.86435017e-02 -7.84291625e-02  1.19081587e-02
  2.20971815e-02 -7.78682949e-03 -6.16297126e-02  2.57211141e-02
  5.21477610e-02  5.36858849e-02  1.62410568e-02 -2.10996382e-02
  7.38889351e-02 -1.17358016e-02 -8.05507451e-02  5.27025340e-03
 -1.34920165e-01  1.18823834e-02  1.98501330e-02 -4.97155376e-02
  5.22194430e-02 -1.60180014e-02  1.01755103e-02 -5.79429939e-02
  2.53695194e-02 -4.25163470e-02  1.13779875e-02  2.59760879e-02
  5.02031855e-02 -6.43077167e-03  1.92758732e-03  1.41481636e-02
  4.45218571e-02  3.53763551e-02  1.53680157e-04 -5.81549155e-03
 -9.89813432e-02  7.87640177e-03  5.97614702e-03 -4.47227014e-03
  1.56258881e-01 -5.77903762e-02 -2.34617162e-02 -2.33731233e-02
 -8.89504775e-02  2.80464943e-02  7.77215324e-03 -3.02810352e-02
  1.64684914e-02 -1.86021645e-02  7.47856684e-03  1.21778231e-02
  4.83519100e-02 -5.52516878e-02  4.66098115e-02 -5.24953566e-02
 -9.16015133e-02 -1.17737185e-02 -2.21322048e-02  2.52354313e-02
  1.80658186e-03 -1.55809410e-02 -1.87168345e-02  4.98627275e-02
  2.07175408e-02 -1.02250427e-01  5.63803874e-02 -2.74546295e-02
 -1.55697018e-02  2.82746181e-02  1.25564024e-01 -1.29534649e-02
  6.56423438e-03  2.65311766e-02 -5.11875423e-03 -5.66351116e-02
 -1.81203838e-02 -5.92988692e-02  5.54142557e-02  3.65786664e-02
  1.08609581e-02  2.78625004e-02  6.42193630e-02  2.61981934e-02
 -7.10635353e-03 -8.73498211e-04  2.28541549e-02  5.53597845e-02
 -5.29987253e-02  8.03832989e-03 -3.92956175e-02  7.94698760e-33
 -8.42269138e-02 -6.88025281e-02 -3.58431507e-03 -2.98892800e-02
  9.81764495e-03 -2.24681813e-02  8.60089585e-02  3.98572944e-02
 -3.21378447e-02 -5.73812984e-03 -1.28982767e-01  6.48244247e-02
 -4.53966707e-02  5.21126911e-02  2.09652465e-02 -8.01857561e-02
  1.22879166e-02  6.17508292e-02  1.90295223e-02  5.75925224e-02
  8.11239406e-02 -5.46835922e-02  7.27862641e-02  3.46518233e-02
  3.85215282e-02 -2.83141788e-02  3.72672230e-02 -3.61671150e-02
 -3.39034945e-02  4.31873457e-04 -9.55794975e-02 -1.09146601e-02
 -3.09851281e-02  8.55051726e-03 -2.89338306e-02  2.95467228e-02
 -4.17723060e-02 -1.79781287e-03 -3.94559279e-02 -8.55502337e-02
 -2.09799204e-02  1.05973095e-01  1.67715328e-03 -2.93185171e-02
 -2.73897182e-02  6.95217475e-02  1.41566927e-02  6.13646097e-02
  2.62696203e-02  3.85199748e-02 -9.89342630e-02 -1.66701060e-02
  1.06643848e-02  1.72057245e-02 -1.81025043e-02 -1.99219249e-02
  4.28068377e-02 -2.49192752e-02  8.51936713e-02  8.90052784e-03
 -1.07322551e-01 -5.44810155e-03  2.69167665e-02  7.11906934e-03
  9.81928315e-03  1.45607665e-02  4.13407013e-03  7.39577040e-02
  4.12582010e-02  8.25378150e-02 -1.42003149e-01  5.83895408e-02
 -5.75050823e-02 -4.93996069e-02  4.64863842e-03 -1.58095807e-02
  4.35244404e-02 -6.75949082e-02 -3.69264632e-02 -2.76516769e-02
 -9.61874574e-02  6.60981387e-02  1.56103475e-02 -1.00011297e-01
 -3.27599682e-02  1.17421639e-03 -3.96345882e-03 -2.18586321e-03
 -1.78859960e-02 -8.29687119e-02 -2.41911560e-02  5.96190244e-02
  3.00704092e-02  1.45039186e-02 -3.93401086e-02 -7.15218811e-33
 -3.95702608e-02  1.67245325e-02 -3.62644754e-02  1.10371932e-01
  1.17800124e-02  4.67882976e-02 -2.47578435e-02 -5.17783202e-02
  3.50271016e-02  4.27062511e-02  3.18572037e-02 -8.93436670e-02
  2.93995198e-02 -5.96229956e-02  5.78423636e-03  6.73573138e-03
 -5.85820340e-02 -4.73903539e-03  4.78908233e-02  2.07063854e-02
 -9.01666507e-02  1.14404015e-01 -1.82233024e-02 -6.34969175e-02
 -9.86946821e-02  3.13941797e-04 -6.76303357e-02 -3.05485334e-02
 -5.15220053e-02 -6.70944015e-03 -3.16392072e-02  1.53984008e-02
 -1.31059349e-01  5.12378812e-02 -1.20843872e-02 -3.58979143e-02
  8.90667364e-02 -4.09224420e-04 -1.88904069e-02  6.31109551e-02
  1.36648253e-01  1.19887538e-01  1.38688218e-02  6.60495460e-02
 -3.93684283e-02  9.80281737e-03 -1.85373072e-02 -1.99533366e-02
 -9.59375966e-03 -2.29708832e-02  1.17379921e-02 -7.36605050e-03
 -5.06780967e-02 -7.98444636e-03 -4.21186164e-02  5.77029809e-02
  3.15730050e-02  3.68485190e-02  2.00708918e-02 -1.26183936e-02
 -8.82615522e-02 -1.01733170e-01  3.40831727e-02 -5.02613969e-02
 -1.75711587e-02 -1.27974628e-02 -4.77207303e-02  4.15965505e-02
 -1.95100326e-02  1.60424411e-02 -2.99229659e-03  5.88751025e-02
 -6.20903308e-03  5.58108790e-03 -1.00816503e-01  2.69368924e-02
  5.43426722e-02 -8.08093697e-03 -1.08051542e-02  5.74204419e-03
  1.92223154e-02 -4.26549166e-02  8.71949196e-02  9.77290943e-02
  9.99957090e-04  4.66719344e-02  3.39530893e-02  2.36036871e-02
  3.78999971e-02  4.68964390e-02 -2.39562411e-02  3.26051004e-02
  1.45796109e-02  1.04298569e-01  6.41864073e-03 -3.30817862e-08
 -1.07671767e-01  4.64620860e-03  1.41524551e-02 -1.02826431e-02
  5.99301159e-02 -7.92685151e-02  2.00625695e-02  1.58310920e-01
 -1.71964616e-02 -1.90432165e-02  7.45969499e-03  1.58070084e-02
 -1.10186204e-01  2.19600610e-02  1.40168648e-02  1.41402319e-01
 -3.83064002e-02  5.72107770e-02  4.14446183e-03 -6.98072314e-02
  6.52869418e-02  8.99847969e-03  1.37233045e-02  2.07596701e-02
 -3.98691222e-02 -4.57553007e-02  6.82583377e-02  5.81333674e-02
  6.97691441e-02  3.83647671e-03 -3.06724068e-02 -2.54873820e-02
  2.92248800e-02 -1.75503567e-02  9.92281139e-02  1.62174236e-02
  2.26395726e-02 -1.95548851e-02 -9.83700901e-02  2.47555207e-02
 -1.05622932e-01  7.31362775e-02 -1.27599947e-02 -4.82290015e-02
  6.68638274e-02  3.38420756e-02 -1.47112850e-02 -7.63888732e-02
 -5.61558455e-02 -1.87431220e-02  6.74458370e-02  2.62606088e-02
 -3.28529568e-04  7.34342113e-02  6.95929676e-02  1.94750428e-02
 -6.49808198e-02 -6.38274699e-02  2.65939143e-02 -1.60373151e-02
 -6.23267959e-04  3.10298596e-02  6.78851157e-02 -4.30941023e-02]",2,2
tensorflow_probability,5,tensorflow probability is a library for probabilistic reasoning and statistical analysis in tensorflow a part of the tensorflow ecosystem tensorflow probability provides integration of probabilistic method with deep network gradient based inference via automatic differentiation and scalability to large datasets and model via hardware acceleration e g gpus and distributed computation our probabilistic machine learning tool are structured a follows layer tensorflow numerical operation in particular the linearoperator class enables matrix free implementation that can exploit special structure diagonal low rank etc for efficient computation it is built and maintained by the tensorflow probability team and is now part of tf linalg in core tf layer statistical building blockslayer model buildinglayer probabilistic inferencetensorflow probability is under active development interface may change at any time see tensorflow probability example for end to end example it includes tutorial notebook such a it also includes example script such a representation learning with a latent code and variational inference for additional detail on installing tensorflow guidance installing prerequisite and optionally setting up virtual environment see the tensorflow installation guide to install the latest stable version run the following for cpu only usage and a smaller install install with tensorflow cpu to use a pre version of tensorflow run note since tensorflow is not included a a dependency of the tensorflow probability package in setup py you must explicitly install the tensorflow package tensorflow or tensorflow cpu this allows u to maintain one package instead of separate package for cpu and gpu enabled tensorflow see the tfp release note for more detail about dependency between tensorflow and tensorflow probability there are also nightly build of tensorflow probability under the pip package tfp nightly which depends on one of tf nightly or tf nightly cpu nightly build include newer feature but may be le stable than the versioned release both stable and nightly doc are available here you can also install from source this requires the bazel build system it is highly recommended that you install the nightly build of tensorflow tf nightly before trying to build tensorflow probability from source a part of tensorflow we re committed to fostering an open and welcoming environment see the tensorflow community page for more detail check out our latest publicity here we re eager to collaborate with you see contributing md for a guide on how to contribute this project adheres to tensorflow s code of conduct by participating you are expected to uphold this code if you use tensorflow probability in a paper please cite we re aware there s a lot more to tensorflow probability than distribution but the distribution paper lay out our vision and is a fine thing to cite for now,"[('tensorflow probability package', 0.738), ('tensorflow probability team', 0.7113), ('tensorflow probability', 0.6695), ('tensorflow ecosystem tensorflow probability', 0.6658), ('tensorflow', 0.6618), ('tensorflow probability example', 0.6436), ('tensorflow package tensorflow', 0.6174), ('tensorflow cpu', 0.5865), ('tensorflow installation guide', 0.5349), ('tensorflow guidance', 0.5328)]","[ 9.79672838e-03 -7.66210780e-02  5.59374131e-02 -1.44364983e-02
  4.11125161e-02 -7.91378040e-03  3.97070125e-02  5.40270424e-03
 -3.76961157e-02 -3.21845375e-02 -2.32717171e-02 -4.01458405e-02
 -2.90208384e-02  2.36592093e-03  1.32573759e-02  2.39661924e-04
 -4.63506468e-02  5.60573749e-02 -3.82299572e-02 -6.25100955e-02
 -7.22466558e-02 -4.14525010e-02  4.67450358e-02 -5.11685275e-02
 -2.78163329e-02 -6.70635253e-02  1.56614110e-02 -2.01312429e-03
 -2.64377147e-02 -2.11430211e-02 -3.81344967e-02  9.78407171e-03
  1.23873122e-01 -5.09057306e-02  4.61958069e-03 -5.35770580e-02
 -7.92234465e-02 -4.30544242e-02 -1.62295569e-02  5.18310741e-02
 -2.35614590e-02  2.25517750e-02 -6.40649721e-02 -9.96949431e-03
  2.01567411e-02  1.23280426e-03  5.31343259e-02 -1.09158397e-01
  1.81321389e-04 -1.01864827e-03 -6.43929690e-02 -7.83040002e-02
 -5.75212389e-02  1.54180359e-02  1.63700953e-02 -3.95728499e-02
  7.98176825e-02 -5.05827880e-03 -2.40439754e-02 -7.83586726e-02
 -3.78692220e-03 -5.14354669e-02 -9.21745673e-02  3.70807424e-02
  5.19806752e-03  2.94553414e-02 -1.29144993e-02  1.00687779e-01
  1.50479034e-01 -1.05385497e-01 -1.40000358e-01  1.87609084e-02
 -4.52482589e-02  5.05827889e-02 -1.22636175e-02 -1.63865671e-03
  3.11506316e-02 -2.29313001e-02 -1.77768636e-02 -3.94548774e-02
 -2.89022662e-02 -5.36640026e-02  1.00851506e-01 -6.52830303e-02
  3.48979123e-02  3.09668984e-02 -2.22628135e-02  4.48866002e-02
 -1.33337239e-02 -3.90255339e-02  1.59834623e-02 -3.67321596e-02
 -1.52517846e-02  2.72694249e-02  3.15965563e-02  1.00503951e-01
 -1.20686278e-01 -9.96984020e-02 -1.96400546e-02 -3.21010202e-02
 -3.94203365e-02 -8.35065320e-02  4.68204282e-02  3.05894427e-02
 -5.11386357e-02  5.51136434e-02 -3.02332062e-02  4.00197394e-02
  4.64906543e-02  5.80401309e-02  4.52186260e-03  2.77584791e-02
  7.13449121e-02 -5.98328896e-02 -4.11422807e-04  2.92246193e-02
 -9.91292521e-02  6.16961308e-02 -1.51460581e-02  9.17256251e-02
 -1.15396433e-01  5.99631816e-02  4.65785787e-02 -8.41575488e-02
 -2.02776138e-02  2.57174056e-02 -7.32153095e-03  4.26799297e-33
  8.81964341e-03 -2.46802215e-02 -5.40086888e-02 -5.82715916e-03
  7.74532035e-02 -5.17136343e-02  3.81127037e-02  1.17382826e-02
  2.74002980e-02 -3.70271988e-02 -4.61875321e-03  8.48762840e-02
 -2.30359938e-02  3.75448912e-02 -7.54540190e-02 -5.40743880e-02
  2.21247580e-02  3.78101505e-02 -1.40992925e-02  5.26937917e-02
  2.99383624e-04  3.30810472e-02  1.21924076e-02  8.35283250e-02
  7.59862214e-02  6.55165985e-02  5.05593047e-02  4.50558960e-02
  3.13759185e-02  8.28519557e-03 -6.47735596e-02 -2.49410439e-02
 -8.56664404e-02 -7.50719979e-02  3.13174278e-02  3.49857993e-02
 -2.59110797e-02 -1.92849431e-02 -7.50729516e-02 -7.81845972e-02
 -1.98411699e-02  4.91238199e-03 -5.74655607e-02  1.24259200e-02
 -3.21340896e-02 -4.81382422e-02 -4.51507699e-03  7.84414355e-03
  1.42062172e-01 -9.70333442e-02 -2.12436598e-02 -9.06432793e-02
  5.21400236e-02  1.93797406e-02 -8.57864134e-03  1.90901998e-02
  1.75323095e-02  4.30935249e-02  2.45299432e-02  6.57220930e-02
 -6.11255504e-02  1.61297992e-02  8.32022205e-02 -5.73056713e-02
  4.39760042e-03 -1.45760849e-02 -1.41101167e-01  3.52858417e-02
  3.04570701e-02  3.17545049e-02 -4.54675555e-02  7.83667564e-02
  4.44152877e-02  6.45332830e-03 -1.58720613e-02  3.52202877e-02
 -4.46165614e-02 -3.75898108e-02  2.33126171e-02  2.97722965e-02
 -1.02090128e-01  2.47534774e-02 -8.31990037e-03  3.04864328e-02
 -2.34158877e-02  5.53222373e-02  3.46466675e-02  3.15422714e-02
 -1.72815230e-02  4.08925116e-02 -1.51241412e-02 -2.01248843e-02
  5.56829907e-02  6.88262582e-02 -3.92196216e-02 -4.35710279e-33
 -6.26766384e-02  3.16091813e-02 -4.42827195e-02  6.30143434e-02
  3.63348089e-02  5.51117361e-02  8.58191960e-03 -4.79645059e-02
  1.54175133e-01  1.96403880e-02 -3.62742543e-02  4.71096188e-02
  6.00817911e-02 -5.36977239e-02 -4.61125467e-03 -4.21084277e-02
 -7.03449324e-02  4.60884115e-03  2.02712547e-02 -9.35053686e-04
 -3.49059887e-02  2.17702258e-02 -5.01822643e-02 -7.34557491e-03
  2.67534163e-02  1.31499507e-02  5.73853552e-02 -7.53113925e-02
  8.96190107e-02 -4.51435000e-02 -1.16513697e-02 -2.98552751e-03
 -2.72314977e-02  8.51464421e-02 -1.82543043e-02 -4.33553569e-02
  7.08925202e-02  4.33637248e-03 -1.26940226e-02  2.53086463e-02
  2.08049659e-02  7.24695995e-02  1.63950976e-02 -4.92653660e-02
 -4.06515896e-02  2.52452381e-02 -1.15843946e-02  1.55352126e-03
 -1.77543552e-03 -4.07275483e-02 -3.65112200e-02  2.09164899e-02
 -4.78843376e-02 -1.08476393e-02 -6.80373982e-02  4.09754887e-02
  4.25681956e-02  4.88106012e-02  1.39550403e-01  3.69922705e-02
 -3.05626020e-02  1.24401855e-03  2.15246100e-02 -5.75244240e-03
 -1.92173272e-02  5.09075867e-03 -9.84760523e-02 -9.96015515e-05
 -2.25854176e-03  1.50978556e-02 -9.44237560e-02  7.13035977e-03
 -3.76118557e-03  4.93061617e-02 -1.00929486e-02  2.96065845e-02
  5.22994883e-02  4.55125347e-02  4.34301794e-02 -3.20811830e-02
  5.18394262e-02 -2.60738637e-02 -5.62190898e-02  1.10210560e-01
  9.59593356e-02  1.84527989e-02  3.59861143e-02 -9.20021385e-02
  5.06870672e-02 -7.38095166e-03 -1.00216186e-02 -8.22551362e-03
  4.18205261e-02  7.47425556e-02 -3.72342952e-02 -2.44472567e-08
  1.80648062e-02  5.83028533e-02  1.04483493e-01  1.94734186e-02
  8.96431878e-02  5.95194399e-02  2.58982889e-02  8.27322453e-02
 -1.73212364e-02  1.09900720e-03 -4.06035781e-03  6.41013682e-02
 -5.70257679e-02 -4.59415056e-02  1.50579605e-02  6.32105097e-02
 -4.68424521e-02  6.42288253e-02  2.29841135e-02 -6.86318949e-02
  5.45531362e-02 -5.53905964e-03 -3.20510417e-02  1.30765969e-02
  4.12552338e-03 -3.45529243e-02  1.51286777e-02  2.31627934e-02
 -4.27494273e-02 -4.19598259e-02 -6.48573935e-02 -4.35772426e-02
  6.37281686e-02 -7.64805377e-02  9.64181796e-02  6.99362457e-02
 -9.96940304e-03 -8.31136927e-02  9.50930547e-03  6.42674565e-02
 -1.90949515e-02 -2.96128970e-02  5.37340008e-02  6.07494323e-04
  3.57843079e-02 -4.11328953e-03 -3.08738835e-02 -2.62078829e-02
 -6.58444781e-03  1.46274641e-02 -2.13329010e-02  5.39793335e-02
 -6.31865263e-02  7.31907710e-02 -3.91540825e-02  1.13054663e-01
  5.13191074e-02 -7.14649931e-02 -1.19699938e-02 -4.62325588e-02
 -3.08358837e-02  7.90426508e-02  2.70450264e-02 -8.75769332e-02]",2,2
fastai,5,you can use fastai without any installation by using google colab in fact every page of this documentation is also available a an interactive notebook click open in colab at the top of any page to open it be sure to change the colab runtime to gpu to have it run fast see the fast ai documentation on using colab for more information you can install fastai on your own machine with conda highly recommended a long a you re running linux or window nb mac is not supported for window please see the running on window for important note if you re using miniconda recommended then run note that if you replace conda with mamba the install process will be much faster and more reliable or if you re using anaconda then run to install with pip use pip install fastai if you install with pip you should install pytorch first by following the pytorch installation instruction if you plan to develop fastai yourself or want to be on the cutting edge you can use an editable install if you do this you should also use an editable install of fastcore to go with it first install pytorch and then the best way to get started with fastai and deep learning is to read the book and complete the free course to see what s possible with fastai take a look at the quick start which show how to use around line of code to build an image classifier an image segmentation model a text sentiment model a recommendation system and a tabular model for each of the application the code is much the same read through the tutorial to learn how to train your own model on your own datasets use the navigation sidebar to look through the fastai documentation every class function and method is documented here to learn about the design and motivation of the library read the peer reviewed paper fastai is a deep learning library which provides practitioner with high level component that can quickly and easily provide state of the art result in standard deep learning domain and provides researcher with low level component that can be mixed and matched to build new approach it aim to do both thing without substantial compromise in ease of use flexibility or performance this is possible thanks to a carefully layered architecture which express common underlying pattern of many deep learning and data processing technique in term of decoupled abstraction these abstraction can be expressed concisely and clearly by leveraging the dynamism of the underlying python language and the flexibility of the pytorch library fastai includes fastai is organized around two main design goal to be approachable and rapidly productive while also being deeply hackable and configurable it is built on top of a hierarchy of lower level apis which provide composable building block this way a user wanting to rewrite part of the high level api or add particular behavior to suit their need doe not have to learn how to use the lowest level it s very easy to migrate from plain pytorch ignite or any other pytorch based library or even to use fastai in conjunction with other library generally you ll be able to use all your existing data processing code but will be able to reduce the amount of code you require for training and more easily take advantage of modern best practice here are migration guide from some popular library to help you on your way when installing with mamba or conda replace c fastchan in the installation with c pytorch c nvidia c fastai since fastchan is not currently supported on window due to python multiprocessing issue on jupyter and window num worker of dataloader is reset to automatically to avoid jupyter hanging this make task such a computer vision in jupyter on window many time slower than on linux this limitation doesn t exist if you use fastai from a script see this example to fully leverage the fastai api on window to run the test in parallel launch nbdev testfor all the test to pas you ll need to install the dependency specified a part of dev requirement in setting inipip install e dev test are written using nbdev for example see the documentation for test eq after you clone this repository make sure you have run nbdev install hook in your terminal this install jupyter and git hook to automatically clean trust and fix merge conflict in notebook after making change in the repo you should run nbdev prepare and make additional and necessary change in order to pas all the test for those interested in official docker container for this project they can be found here,"[('pytorch library fastai', 0.5603), ('c pytorch c nvidia c fastai', 0.4951), ('pytorch installation instruction', 0.4898), ('fastai api', 0.4691), ('fast ai documentation', 0.4587), ('fastai documentation', 0.4521), ('colab runtime', 0.4392), ('pip use pip', 0.4179), ('deep learning library', 0.3818), ('fastai', 0.3755)]","[-1.14903301e-01 -6.06574453e-02 -3.33504863e-02  2.14504190e-02
  4.30319719e-02 -3.90747096e-03 -3.99334542e-02  2.10152436e-02
 -2.48280317e-02 -7.14624748e-02 -1.04066804e-02 -3.67643125e-02
 -1.04611218e-01  2.51438860e-02  2.93428618e-02 -2.01829579e-02
 -6.39011525e-03  1.01353027e-01 -4.77273241e-02 -1.57866716e-01
 -2.97240280e-02 -3.69015671e-02  8.60593095e-03 -1.16575593e-02
  6.85520470e-03  6.40299544e-03  1.57806128e-02 -4.89727631e-02
  4.87874597e-02 -1.00109745e-02 -3.86584811e-02 -1.59265902e-02
  7.62751028e-02  2.52096262e-02 -4.67325114e-02  3.78783159e-02
 -4.11749966e-02 -5.00896350e-02 -1.01432577e-02 -4.01575565e-02
 -5.85946441e-03 -1.12425033e-02 -2.65413839e-02 -2.01237518e-02
  6.57891706e-02 -3.83461732e-03  1.48199755e-03 -4.57933582e-02
  2.11412162e-02 -4.27581295e-02 -9.51825455e-02 -1.00033823e-02
 -4.09723334e-02  9.19585675e-03 -1.42160440e-02  3.60983461e-02
  5.51138958e-03  4.60524391e-03  4.59730029e-02 -2.90044248e-02
  4.15857183e-03 -4.74749692e-02  3.03897262e-02  6.53180704e-02
 -9.30505246e-03  4.83660921e-02 -3.08271218e-02  6.68910565e-03
  1.51428819e-01 -1.15857027e-01 -3.20425890e-02  7.79715413e-03
 -5.08912429e-02  2.40686629e-02 -5.89148030e-02 -4.17910181e-02
  2.73723062e-02  4.56265733e-03 -1.83833465e-02 -9.61546153e-02
 -1.90630052e-02  1.10729346e-02  2.94546993e-03  6.97706044e-02
  2.93830857e-02  6.50137961e-02 -6.13657804e-03  7.05405250e-02
 -2.80079269e-03 -8.11506212e-02  1.31452724e-01 -4.54194210e-02
  4.98561822e-02  3.77651118e-02  6.95502153e-03  3.32164504e-02
  5.80285266e-02 -1.08127065e-01 -1.07513666e-01  2.80408300e-02
 -8.37787613e-02 -6.56528845e-02  9.56989732e-03  4.32139337e-02
 -2.81569790e-02  3.78064550e-02  1.93472542e-02  3.43461819e-02
  8.55137482e-02  2.88994592e-02 -1.75572422e-05  1.43099427e-02
 -4.39446941e-02 -3.44241783e-02  6.04324602e-02  2.36375351e-02
 -4.19913009e-02  3.50330994e-02  5.46496920e-02  7.80194998e-02
 -1.41143203e-01  8.11805483e-03 -8.32838472e-03  1.33785224e-02
  3.49266976e-02 -1.21566262e-02 -8.43034014e-02  8.54928372e-33
  1.97213404e-02 -1.29310973e-02  1.33894570e-03 -6.59453273e-02
  6.69443905e-02 -8.07232037e-02  1.22325949e-01  7.76249683e-03
 -1.85527578e-02 -2.39696074e-02 -1.10337008e-02  5.71426004e-02
 -7.33689740e-02  1.09670661e-01 -1.22445477e-02 -6.50466606e-02
 -3.93785462e-02  5.45778610e-02  1.15451561e-02  3.12542357e-02
  5.44381551e-02  1.75892375e-02  1.84404049e-02  1.00554936e-01
  6.15485260e-05  3.90252359e-02  3.47170904e-02 -2.87537519e-02
  5.83474711e-02  2.47716885e-02 -7.31087476e-02 -1.25544434e-02
  8.53911135e-03 -8.97414610e-03  1.06469905e-02 -4.72251959e-02
 -9.87479314e-02  2.71954830e-03 -1.34174284e-02  2.41270624e-02
 -7.47321174e-02  3.84920500e-02 -3.01227458e-02 -1.06187463e-01
 -6.59218570e-03  3.38864699e-03 -1.80388056e-02  1.06134444e-01
  6.88944478e-03 -4.25599143e-02 -6.85070232e-02  3.28128375e-02
 -1.90394670e-02  4.32996787e-02  2.61328463e-02 -7.46029615e-02
  1.12053547e-02  7.21748099e-02  7.23266676e-02  3.84175628e-02
  2.51891203e-02  5.18697612e-02 -2.15464886e-02  5.56712486e-02
 -3.80996875e-02  4.74719703e-03 -3.46318185e-02  4.87332270e-02
  2.42633768e-03  6.35127872e-02 -6.50555491e-02  6.40436709e-02
 -3.04148719e-02 -2.07460783e-02  3.37682031e-02 -2.01595109e-02
 -2.76639052e-02 -5.92445582e-02 -1.04572196e-02  1.08122779e-02
 -9.22044590e-02  4.13258448e-02  3.49680297e-02  8.78430874e-05
 -2.19867378e-02 -6.12588460e-03 -5.27899247e-03  1.36929136e-02
  1.21252947e-02 -5.20199277e-02 -6.51925057e-02 -1.58019904e-02
  2.01547774e-03  1.18469598e-03 -1.75740756e-02 -5.90132364e-33
  7.66066685e-02 -4.27294336e-02 -2.00531650e-02  1.02178998e-01
  4.18378189e-02  2.33906135e-03 -9.24186409e-02 -9.11680609e-02
  5.77670038e-02  4.39369306e-03 -4.03370848e-03  5.20707399e-04
  1.71478894e-02  1.21299177e-02  2.90962625e-02  2.32113954e-02
  4.93998360e-03 -2.59659458e-02 -2.80893464e-02 -1.39995748e-02
 -1.04340769e-01  4.03450429e-02 -5.65049909e-02 -6.30225390e-02
 -2.68463185e-03 -1.68931391e-02  5.02662687e-03 -1.74857620e-02
 -1.87815949e-02  2.18756292e-02 -5.34238853e-03  5.96821867e-02
 -5.48899993e-02  1.67757235e-02  2.37409258e-03  5.11364597e-05
  6.19798191e-02 -2.02624220e-02  5.47573250e-03  1.22071989e-02
  1.74249500e-01  3.75604257e-03 -2.22650450e-02 -2.30730139e-02
 -3.51435393e-02  6.06982224e-02 -9.45246294e-02  4.79750931e-02
 -3.54463346e-02 -7.04103336e-03  5.71590615e-03 -8.94407369e-03
 -1.48267411e-02 -6.40160264e-03 -1.05109392e-02 -1.28072184e-02
  9.19868574e-02  6.10770471e-02 -2.56949067e-02 -3.28545645e-02
 -1.17830761e-01 -8.04189146e-02  1.23806216e-03 -8.15812778e-03
 -3.89324278e-02  4.34064452e-04 -3.39191891e-02  5.19142449e-02
  6.07529795e-03 -6.30476475e-02  2.18596887e-02 -2.29311492e-02
  8.35094750e-02  1.06660075e-01 -9.55835953e-02  2.48380750e-02
  1.24036996e-02  4.08241265e-02  5.29332906e-02 -7.49465358e-03
  3.32171507e-02 -1.03590013e-02  3.62960771e-02  6.09626807e-02
  1.28532872e-02  7.60590658e-02  7.01148808e-02 -2.52552480e-02
  9.02002454e-02  2.41177939e-02 -1.24394139e-02  6.77055120e-02
  6.06455728e-02  8.40822905e-02  2.66275238e-02 -3.21024949e-08
  1.88533925e-02  5.72902150e-03  1.69310328e-02  8.22489560e-02
  2.98982561e-02  4.88936864e-02 -1.45519841e-02  1.56273142e-01
 -1.91992596e-02  2.63436697e-02  2.77039241e-02 -7.27635920e-02
 -4.11917605e-02 -5.04416972e-02  7.33087817e-03  9.31746662e-02
  3.48817222e-02  1.44191319e-02  3.05125825e-02 -3.49768661e-02
  1.56543329e-02  3.44304554e-02  2.32823379e-02 -4.13319170e-02
 -6.18077330e-02 -7.59737939e-02  3.65882702e-02 -4.15271744e-02
 -3.14883403e-02 -6.33840784e-02 -3.91610675e-02 -1.78617425e-02
  5.77359274e-02 -7.02167228e-02  1.65382430e-01  3.00677493e-02
  1.97003200e-03 -3.71789075e-02  4.34766673e-02  4.62809019e-02
 -5.14572524e-02  4.87861596e-02  5.47265820e-03 -8.11612085e-02
  4.89128046e-02 -1.40258092e-02 -5.82946576e-02 -8.87144133e-02
 -1.63392853e-02  2.02102624e-02 -2.74492539e-02  2.28771325e-02
  1.43072605e-02  6.40792102e-02  2.53614821e-02  1.20217673e-01
 -2.67112516e-02 -1.15836598e-01 -3.06007378e-02 -2.46862881e-02
  1.80989709e-02  7.70588517e-02  4.80489805e-02  1.65839083e-02]",2,2
sagemaker-training,4,train machine learning model within a docker container using amazon sagemaker amazon sagemaker is a fully managed service for data science and machine learning ml workflow you can use amazon sagemaker to simplify the process of building training and deploying ml model to train a model you can include your training script and dependency in a docker container that run your training code a container provides an effectively isolated environment ensuring a consistent runtime and reliable training process the sagemaker training toolkit can be easily added to any docker container making it compatible with sagemaker for training model if you use a prebuilt sagemaker docker image for training this library may already be included for more information see the amazon sagemaker developer guide section on using docker container for training to install this library in your docker image add the following line to your dockerfile the following are brief how to guide for complete working example of custom training container built with the sagemaker training toolkit please see the example notebook write a training script eg train py define a container with a dockerfile that includes the training script and any dependency the training script must be located in the opt ml code directory the environment variable sagemaker program defines which file inside the opt ml code directory to use a the training entry point when training start the interpreter executes the entry point defined by sagemaker program python and shell script are both supported build and tag the docker image use the docker image to start a training job using the sagemaker python sdk to train a model using the image on sagemaker push the image to ecr and start a sagemaker training job with the image uri any hyperparameters provided by the training job are passed to the entry point a script argument the sagemaker python sdk us this feature to pas special hyperparameters to the training job including sagemaker program and sagemaker submit directory the complete list of sagemaker hyperparameters is available here implement an argument parser in the entry point script for example in a python script start a training job with hyperparameters an entry point often need additional information not available in hyperparameters the sagemaker training toolkit writes this information a environment variable that are available from within the script for example this training job includes the channel training and testing the environment variable sm channel training and sm channel testing provide the path to the channel when training start sagemaker training toolkit will print all available environment variable please see the reference on environment variable for a full list of provided environment variable to get information about the container environment initialize an environment object environment provides access to aspect of the environment relevant to training job including hyperparameters system characteristic filesystem location environment variable and configuration setting it is a read only snapshot of the container environment during training and it doesn t contain any form of state to execute the entry point call entry point run if the entry point execution fails trainer train will write the error message to opt ml output failure otherwise it will write to the file opt ml success this library is licensed under the apache license for more detail please take a look at the license file contribution are welcome please read our contributing guideline if you d like to open an issue or submit a pull request,"[('sagemaker training toolkit', 0.7163), ('prebuilt sagemaker docker image', 0.6596), ('sagemaker training job', 0.6474), ('sagemaker program', 0.6318), ('sagemaker python', 0.5924), ('sagemaker python sdk', 0.5901), ('sagemaker program python', 0.5888), ('custom training container', 0.5703), ('amazon sagemaker developer guide section', 0.5694), ('amazon sagemaker amazon sagemaker', 0.5405)]","[-5.70723005e-02 -5.64359240e-02  2.57573295e-02  2.13970151e-02
  9.21455994e-02 -3.93685997e-02 -1.84947047e-02  2.20812298e-02
 -1.25996262e-01  3.97554971e-03 -3.98725155e-04  1.71596708e-04
  2.28600726e-02 -1.25862546e-02  9.24291015e-02  1.54182315e-03
 -6.07829131e-02 -9.60205495e-03  1.00024175e-02 -4.25063856e-02
 -9.80880335e-02  3.16228047e-02  1.90463085e-02  5.38924448e-02
 -2.48079430e-02  1.96829112e-03 -6.19586976e-03  1.16225367e-03
 -5.63359261e-03 -5.39791919e-02 -1.09696323e-02  1.77466255e-02
  1.14581376e-01  1.62756313e-02 -2.14295387e-02  4.96753976e-02
  1.76477514e-03 -5.29868295e-03  1.50124505e-02 -5.63590508e-03
 -4.27904837e-02 -3.47159170e-02 -9.84108597e-02  2.04027593e-02
  6.06687441e-02 -1.15212798e-02  2.92776749e-02 -8.02139938e-02
  3.46453488e-02  2.29163393e-02 -6.64581507e-02 -1.35848284e-01
 -5.71522526e-02  3.64056267e-02 -2.32397411e-02  7.08306441e-03
  2.98490636e-02 -1.34150414e-02  2.48748418e-02  2.00856440e-02
 -4.38389704e-02 -9.37777199e-03 -2.74417736e-02  6.81079645e-03
 -3.72529291e-02  3.93357724e-02 -3.47867534e-02  5.87287955e-02
  1.14373557e-01 -7.37984627e-02 -7.45937154e-02 -3.45268138e-02
 -4.35189717e-02  9.58336070e-02 -5.18656485e-02 -3.41464654e-02
  4.10780823e-03 -1.48765265e-03 -4.15911451e-02 -1.05733268e-01
 -4.07995284e-02  3.98479141e-02 -1.81496181e-02  1.39439190e-02
 -7.44435638e-02  3.94588560e-02  2.06364729e-02  4.52717133e-02
  4.79208604e-02 -5.02626114e-02  4.25037146e-02 -8.45184699e-02
  1.42758302e-02 -2.52134241e-02 -6.66635036e-02  4.68545556e-02
  6.10396313e-03 -3.56039666e-02  1.78559702e-02  3.13446671e-02
 -6.14897208e-03 -8.03359449e-02  9.99781583e-03 -1.92149319e-02
 -3.58631536e-02  8.13685432e-02 -1.80919226e-02 -3.37887332e-02
  3.15219276e-02  3.82559858e-02  2.43992452e-02 -2.46595796e-02
 -1.00412019e-01 -8.00625980e-02  3.32696959e-02  5.25474101e-02
 -6.41483143e-02  1.73009131e-02 -3.02771088e-02  1.07914314e-01
 -1.36239231e-02  2.05266699e-02  1.33958086e-01 -1.67342052e-02
 -4.00111005e-02 -1.24763027e-02 -9.94428396e-02  6.65930186e-33
  2.42258068e-02 -6.30207658e-02 -1.98895652e-02 -1.66806299e-02
  1.03550524e-01 -6.41038567e-02  8.70507285e-02  7.34836832e-02
  7.99701456e-03 -5.81998751e-02  7.66710490e-02  3.85141931e-02
 -5.87844290e-02  1.17260449e-01 -2.73169614e-02 -4.96001309e-03
  2.54758894e-02  5.04165404e-02 -6.17307192e-03 -3.10656391e-02
 -1.65782031e-02 -2.57767197e-02 -3.53902876e-02  9.13577676e-02
  2.85434742e-02  1.27454726e-02  3.77605297e-02 -1.39420126e-02
  5.46332784e-02  5.47353039e-03 -5.05611785e-02  1.15202432e-02
  4.45268378e-02 -1.83593184e-02 -3.63275176e-03  1.06625967e-02
 -2.59024035e-02 -6.05300441e-02  4.75348271e-02 -1.81719586e-02
 -7.54117891e-02  6.42229319e-02  2.53653470e-02  1.81459151e-02
  5.55693917e-02 -2.99567264e-02 -1.90962981e-02  3.04087102e-02
  6.67765439e-02  5.19168451e-02 -8.09837058e-02 -2.58075818e-02
  6.98872805e-02  3.37513052e-02 -4.42180969e-02 -3.17386873e-02
  2.05787756e-02 -2.75013689e-02  8.31960328e-03 -2.84142103e-02
 -5.68868704e-02  1.12141122e-03 -5.02627436e-03  4.42996342e-03
 -6.69393390e-02 -1.53775746e-02  7.88553208e-02  6.79814741e-02
  2.42363997e-02  6.24910146e-02 -1.26370028e-01  8.14499259e-02
 -2.08535269e-02  3.53707955e-03  2.49189157e-02 -1.08912261e-02
 -2.64096800e-02 -4.64364626e-02 -4.87208553e-02 -1.95280705e-02
 -1.08377926e-01  6.29717186e-02 -1.11164022e-02  4.74330001e-02
 -2.65860837e-02 -3.54421847e-02  6.93613142e-02  1.14152553e-02
 -5.41876070e-02 -4.93955053e-02 -5.18948063e-02  9.57706049e-02
 -9.82329343e-03  1.92887224e-02 -4.47199307e-02 -6.77502047e-33
  4.65763882e-02  5.98760843e-02 -8.49411637e-02  7.00473711e-02
  5.07210866e-02  4.25895080e-02  5.00687435e-02 -4.28319871e-02
 -1.61391478e-02 -5.53402863e-03  1.41603639e-03 -2.06368212e-02
 -4.52862829e-02  3.99431447e-03 -2.79633030e-02 -3.45132459e-04
 -1.00734375e-01  8.98430124e-03  2.12086104e-02  2.04403959e-02
 -1.24612376e-02 -1.84818041e-02 -2.57876050e-02 -6.81467587e-03
 -5.75104635e-03 -3.11655086e-02  1.36087835e-02 -1.93740847e-03
  8.88862312e-02  6.99355230e-02  6.30170852e-02  7.06038997e-03
 -1.96911898e-02  8.46866891e-03  1.02048637e-02 -2.52129938e-02
  3.40551250e-02 -1.13156838e-02 -4.94094975e-02 -1.87292304e-02
  1.72483668e-01 -6.74172677e-03  4.69434634e-02  8.36456753e-03
 -1.54225051e-01 -4.29131575e-02 -7.62248412e-02  7.97322392e-03
 -1.14762057e-02 -6.53908253e-02 -4.21148874e-02 -7.30499253e-02
 -6.34370372e-02 -2.57369168e-02 -2.36554723e-02 -2.63842265e-03
  7.50063285e-02  7.92754814e-02  4.08544242e-02 -3.37821655e-02
 -2.98959669e-02 -6.59347977e-03  3.00675873e-02  2.96909716e-02
 -8.19453150e-02 -5.32065667e-02  2.30810459e-04  2.94630527e-02
 -1.01860568e-01 -2.11129133e-02 -1.37781203e-02  3.93412299e-02
  2.00988892e-02  3.27646397e-02 -6.44082800e-02  6.32869005e-02
  6.60509476e-03 -1.89437363e-02  1.11923600e-02 -4.79319766e-02
  1.19447686e-01 -1.62131172e-02 -1.67031903e-02  8.07224140e-02
 -1.86305176e-02  9.24956352e-02  1.65049601e-02  2.94884369e-02
  1.61356851e-03 -2.34479327e-02 -1.54731497e-02  5.60481288e-02
  3.18142585e-02  8.39873627e-02  4.31987979e-02 -3.32887922e-08
 -9.32278205e-03  1.19311819e-02  1.46499742e-02  7.52979591e-02
  4.68777865e-02  3.08648068e-02 -9.57280863e-03  1.44792065e-01
 -4.72569354e-02  6.16733767e-02  1.30046364e-02 -5.34268282e-02
 -8.59768167e-02 -3.68819833e-02  1.72819966e-03  4.88562360e-02
  1.45816738e-02  5.72126843e-02  1.00777959e-02 -7.45245144e-02
 -3.03834025e-02  2.82905251e-02  2.60663852e-02 -2.67719384e-02
 -2.16727965e-02 -3.95220239e-03 -4.13090214e-02  8.83710906e-02
  2.13670940e-03 -5.63342730e-03  9.09021720e-02 -5.47743663e-02
  4.98384796e-02 -1.26712499e-02  1.50904179e-01  2.68584602e-02
 -1.27793297e-01 -5.92762120e-02  2.00443948e-03  3.17256451e-02
 -9.98697206e-02 -3.75981592e-02  3.82367671e-02 -6.34387583e-02
 -1.57189427e-03 -4.62293290e-02 -1.84107218e-02 -1.94520541e-02
 -6.77545592e-02 -9.72176529e-03 -2.88650673e-02 -1.52145224e-02
  3.99035178e-02 -2.26796921e-02  6.18365481e-02  1.13646522e-01
 -3.11020892e-02 -8.74944553e-02 -1.76031236e-02  4.70620207e-03
  3.65449451e-02  6.21438101e-02  4.87904623e-02  4.73185703e-02]",2,1
nipype,4,current neuroimaging software offer user an incredible opportunity to analyze data using a variety of different algorithm however this ha resulted in a heterogeneous collection of specialized application without transparent interoperability or a uniform operating interface nipype an open source community developed initiative under the umbrella of nipy is a python project that provides a uniform interface to existing neuroimaging software and facilitates interaction between these package within a single workflow nipype provides an environment that encourages interactive exploration of algorithm from different package e g afni ant brain brainsuite camino freesurfer fsl mne mrtrix mne nipy slicer spm eas the design of workflow within and between package and reduces the learning curve necessary to use different package nipype is creating a collaborative platform for neuroimaging software development in a high level language and addressing limitation of existing pipeline system nipype allows you to easily interact with tool from different software packagescombine processing step from different software packagesdevelop new workflow faster by reusing common step from old onesprocess data faster by running it in parallel on many core machinesmake your research easily reproducibleshare your processing workflow with the community,"[('current neuroimaging software', 0.6622), ('neuroimaging software', 0.6564), ('single workflow nipype', 0.5691), ('processing workflow', 0.4804), ('pipeline system nipype', 0.4682), ('different package nipype', 0.4539), ('uniform operating interface nipype', 0.4125), ('nipy', 0.394), ('python project', 0.3885), ('workflow', 0.388)]","[-5.77134639e-02 -5.63931875e-02  5.27489334e-02 -5.40140383e-02
 -1.53696816e-02 -7.79422000e-02  3.39358374e-02  5.39110713e-02
 -2.57060975e-02 -1.35520827e-02 -7.01159835e-02 -1.84829012e-02
  1.18233112e-03  4.74126041e-02  1.76369082e-02 -4.04774444e-04
 -9.60527062e-02  4.91544895e-04  1.30939549e-02 -7.78960139e-02
 -4.07264382e-02  2.16612574e-02 -2.48792283e-02 -2.60446705e-02
  4.03532432e-03  2.48059276e-02 -4.87330258e-02 -7.62719959e-02
  3.38723175e-02 -6.76555187e-02 -8.20409227e-03  2.39510816e-02
  5.68780536e-03 -2.28666328e-02  4.34488207e-02  2.09789611e-02
 -2.59548035e-02 -2.32019182e-02 -7.44003579e-02 -6.46329112e-03
 -9.42928996e-03  2.33931355e-02 -2.98940763e-02 -9.21550952e-03
  5.02061881e-02  2.53988542e-02  6.10997900e-04 -1.05454944e-01
 -1.71663538e-02 -2.54118517e-02 -9.79458764e-02 -9.91368964e-02
 -2.42583305e-02  1.06269024e-01  7.31179044e-02 -7.63728283e-03
  1.57726072e-02 -2.28754152e-02 -2.14208383e-02  3.24016884e-02
 -5.45783788e-02  3.93137187e-02 -4.51225862e-02 -3.43841240e-02
  2.35160515e-02  7.05578923e-02  4.47393348e-03 -3.79291438e-02
  9.69186425e-02 -8.24239776e-02 -6.41677380e-02 -3.19230482e-02
  1.72670577e-02  1.50762927e-02 -8.66687819e-02 -4.65484224e-02
  4.66879308e-02 -4.51332964e-02  7.19059557e-02 -1.71241373e-01
  4.42807898e-02  5.28881922e-02  2.77813151e-02  1.91628747e-02
  6.00113757e-02  3.99656072e-02 -4.52171378e-02  9.57239866e-02
  2.04113871e-02  1.53250080e-02  2.00447012e-02 -7.42694959e-02
 -4.20848047e-03 -3.85947786e-02 -2.71111671e-02 -2.42607314e-02
 -4.93639782e-02 -4.72477116e-02 -3.65980715e-02  5.74848354e-02
 -1.93696301e-02 -6.70770332e-02  6.10070638e-02  3.35392244e-02
 -2.83678551e-03  7.46304318e-02  1.21941887e-01 -6.68321103e-02
  8.25054124e-02  1.91630237e-02 -8.72329175e-02 -2.91963145e-02
 -5.57272211e-02 -1.53753310e-02  3.49961333e-02 -3.42901088e-02
 -5.83615154e-02  1.20698690e-01 -1.10470899e-03  7.95057416e-02
 -6.58914894e-02 -4.71434137e-03  1.31645473e-02 -6.55782670e-02
 -3.26798484e-02 -6.49309892e-04 -8.93436074e-02  4.82096902e-33
 -1.39485775e-02  5.49043342e-03 -1.14654703e-02  5.16412547e-04
  4.28085811e-02 -4.12671678e-02  1.19753042e-02  1.17633268e-02
  4.13980372e-02 -4.07370180e-02 -4.15845551e-02  1.15174621e-01
 -7.51482323e-02  1.47839546e-01  3.00728567e-02 -5.20097166e-02
 -3.96824107e-02  1.20185129e-01  3.37855965e-02  4.30291705e-02
  7.18710050e-02  6.59773648e-02 -3.58367488e-02  7.63178095e-02
 -2.71075387e-02 -3.10883950e-02 -5.44347651e-02 -1.98835693e-03
  6.16653301e-02 -4.44575539e-03 -8.30185637e-02  8.26886026e-05
  5.11036031e-02 -3.23487855e-02 -2.88710501e-02  1.08963633e-02
  8.42048600e-03 -5.30433729e-02  1.54460659e-02 -8.93026358e-04
 -4.24268609e-03  7.18014091e-02 -1.02378419e-02 -2.93244533e-02
  4.32364009e-02  3.03068403e-02 -3.67354080e-02  6.43711537e-02
  7.22648799e-02 -1.31669864e-02 -3.18945711e-03 -2.12775655e-02
  3.66942324e-02 -6.07416667e-02 -3.84567268e-02 -1.27776749e-02
 -1.41757214e-02  1.58646666e-02  1.27343267e-01  2.38384251e-02
 -5.31455595e-03 -9.94720869e-03  1.25087583e-02 -3.94971482e-02
  8.97846669e-02 -1.00709526e-02  2.04780977e-02 -2.85223592e-03
 -2.07316093e-02  4.77912091e-03 -1.10622920e-01  6.11427315e-02
 -1.16286455e-02  1.96365006e-02  8.33066739e-03 -5.12382686e-02
  7.51672452e-03 -4.23400439e-02 -7.92026743e-02 -1.18053388e-02
 -1.23271681e-01  3.52604240e-02 -6.14630654e-02 -3.11732106e-02
  6.15819283e-02  3.59478071e-02 -1.59631111e-03  7.34729767e-02
 -7.22467974e-02 -2.83279968e-03  1.72625529e-03  9.43496544e-03
  2.09570825e-02  3.60428542e-02 -1.43475654e-02 -3.91766480e-33
 -2.77721919e-02  4.74639870e-02 -9.68134925e-02  4.87188138e-02
  1.28224110e-02  4.62736227e-02  7.75020570e-02 -5.90336621e-02
 -1.13575775e-02  3.15235375e-04  6.89039528e-02 -4.26805243e-02
  4.99657281e-02 -1.07963588e-02 -9.12719814e-04 -1.09964065e-01
 -5.66343367e-02 -1.46512156e-02  2.05801781e-02  3.55674922e-02
 -5.16634434e-02  1.22363701e-01 -8.42669681e-02  7.40898121e-03
 -2.10406687e-02  5.82196601e-02 -1.85908563e-02 -1.73686333e-02
  5.08218631e-02  2.45715026e-02 -5.21658473e-02  4.10686582e-02
 -6.87093660e-02 -2.94594392e-02  9.13362131e-02 -1.60763075e-03
  2.41563804e-02 -7.62214884e-02 -4.12286706e-02 -9.72889587e-02
  1.67118683e-01  5.14740013e-02  7.52405003e-02  3.91642712e-02
 -8.64380319e-03  3.84377409e-03 -1.20778047e-01  4.98551615e-02
  1.31766247e-02 -1.22822411e-02 -2.74615344e-02 -3.21935453e-02
 -7.72722214e-02 -3.28368656e-02 -2.25190837e-02  1.95408519e-02
  6.57133684e-02  2.29157638e-02 -4.29934151e-02  5.03357779e-03
 -9.39541459e-02 -1.99156608e-02  1.35235582e-02 -4.95645180e-02
 -5.10135256e-02  1.06139995e-01 -5.85465506e-03  4.19644266e-02
 -3.80112492e-02 -3.77843790e-02  2.03893315e-02  8.85065868e-02
  7.41935298e-02  2.45119706e-02 -3.86237428e-02  1.73075181e-02
  4.49057259e-02 -2.79732961e-02  1.01301642e-02 -7.15380907e-02
  5.88523559e-02 -4.19840496e-03 -8.55743513e-03  6.70408085e-02
 -3.35966721e-02  9.50943232e-02  9.15234536e-03 -6.96744770e-02
  8.60420763e-02 -6.40265122e-02 -3.81416082e-02  2.41522789e-02
  4.77043018e-02  2.41632238e-02 -2.20822301e-02 -2.97088221e-08
  2.51397267e-02 -2.21719351e-02  8.17734599e-02 -5.46857808e-03
  4.78730127e-02 -1.07822269e-02 -2.37349570e-02  4.65281978e-02
 -1.38046974e-02  7.40609225e-03  8.38331208e-02 -2.31548026e-02
 -4.57047336e-02 -4.01976109e-02  3.90520692e-02  6.60435334e-02
  3.43887769e-02  7.33444393e-02  9.73410066e-03 -1.06013671e-01
  8.26608948e-03  1.99127272e-02  3.64960963e-03  1.39698554e-02
 -9.49861202e-03 -1.11571848e-02  2.64985990e-02 -2.47939471e-02
  4.05089138e-03 -5.70505336e-02  7.62869716e-02  3.57940514e-03
  9.14354548e-02 -3.08657680e-02  1.02604888e-01 -5.39840795e-02
 -3.63355153e-03  3.26055586e-02 -1.88506430e-03 -5.81721356e-03
 -2.01961640e-02  3.78687517e-03  5.01619279e-02 -4.52674404e-02
 -3.45572010e-02 -2.01540105e-02 -2.66860407e-02 -6.07732646e-02
 -1.82495173e-02  3.00872326e-02 -3.59703451e-02  4.00496498e-02
  1.86311137e-02  6.74581081e-02 -5.80343045e-03  6.57725334e-02
  2.13080626e-02 -5.09039909e-02  1.78291835e-02  5.88539504e-02
  6.52165338e-03  7.01589044e-03  5.83567321e-02  7.19637275e-02]",2,2
nilearn,4,nilearn enables approachable and versatile analysis of brain volume it provides statistical and machine learning tool with instructive documentation friendly community it support general linear model glm based analysis and leverage the scikit learn python toolbox for multivariate statistic with application such a predictive modelling classification decoding or connectivity analysis official source code repo http github com nilearn nilearn html documentation stable release http nilearn github io first make sure you have installed all the dependency listed below then you can install nilearn by running the following command in a command prompt more detailed instruction are available at http nilearn github io stable introduction html installation the nilearn team organizes regular online office hour to answer question discus feature request or have any nilearn related discussion nilearn office hour occur every friday from pm to pm utc and we make sure that at least one member of the core developer team is available these event are held on our on discord server and are fully open anyone is welcome to join for more information and way to engage with the nilearn team see how to get help the required dependency to use the software are listed in the file nilearn setup cfg if you are using nilearn plotting functionality or running the example matplotlib is required some plotting function in nilearn support both matplotlib and plotly a plotting engine in order to use the plotly engine in these function you will need to install both plotly and kaleido which can both be installed with pip and anaconda if you want to run the test you need pytest and pytest cov for coverage reporting detailed instruction on how to contribute are available at http nilearn github io stable development html,"[('nilearn support', 0.5077), ('nilearn', 0.4975), ('http nilearn github', 0.4908), ('discussion nilearn office hour', 0.4779), ('nilearn team', 0.4751), ('http nilearn github io', 0.4585), ('http github com nilearn nilearn html documentation', 0.4577), ('file nilearn setup cfg', 0.4366), ('stable release http nilearn github io', 0.4349), ('machine learning tool', 0.3903)]","[-1.46761641e-01 -5.23916520e-02 -3.45946662e-02 -1.07955770e-03
 -1.07559906e-02 -4.70603630e-02 -4.24746424e-02 -3.07155331e-03
 -8.21244121e-02 -3.01033072e-02 -3.57531123e-02 -2.03846209e-03
 -5.82481287e-02 -5.02559431e-02  3.14915515e-02  2.31619328e-02
 -3.63638848e-02  4.94932942e-02 -9.01734829e-03 -8.92467424e-02
  3.85310641e-03 -1.76824871e-02  1.03077609e-02 -3.28022316e-02
  3.10352631e-03  5.01923673e-02 -3.62701789e-02  3.86534221e-02
  8.82149562e-02 -6.26744255e-02  1.77014004e-02  2.46013049e-03
 -1.90310983e-03  4.99889925e-02 -4.52270322e-02  1.15886100e-01
  5.59723824e-02 -8.95276573e-03 -9.75228287e-03  2.60505099e-02
 -3.02868709e-02  2.20764745e-02  7.50579173e-03 -3.87596488e-02
  4.82831635e-02 -2.19131839e-02  3.74268405e-02 -6.65899739e-02
 -1.56661067e-02  1.62477717e-02 -9.49525833e-02 -1.15219973e-01
  1.81180667e-02  1.03706084e-02 -2.18278524e-02 -6.84475377e-02
 -9.08276998e-03 -1.07554095e-02 -2.54044048e-02 -4.02934849e-02
  2.94134077e-02  4.29223515e-02 -1.06839523e-01  3.84459011e-02
  4.40385044e-02  5.31809889e-02 -1.26893846e-02  8.13707933e-02
  1.61187481e-02 -1.04236804e-01 -8.83704051e-02 -2.43154783e-02
 -1.13837175e-01  8.03703219e-02 -6.24803603e-02  9.37009510e-03
  6.71388805e-02 -1.20904390e-02  1.02544084e-01 -1.28480688e-01
  2.43294835e-02 -1.50638372e-02  5.43683097e-02  6.80776983e-02
 -1.15186255e-03  2.62126955e-03  3.19251940e-02  1.12525858e-01
 -6.74235309e-03 -1.29651837e-02  5.68461046e-02  2.99660135e-02
  1.02212690e-01 -2.34925505e-02  3.67904431e-03  5.34240343e-02
  2.33252551e-02  2.81745270e-02 -5.99765070e-02  7.52435774e-02
 -8.41524750e-02 -6.17176034e-02  5.38380593e-02 -4.91764769e-02
  1.33694429e-03  4.44226116e-02  4.36513647e-02  8.67610872e-02
  1.53580099e-01 -4.90407050e-02 -4.87078689e-02  6.16745045e-03
 -9.84592363e-02 -1.22435063e-01 -2.40936149e-02  2.07898542e-02
 -9.38571766e-02 -3.20261642e-02  3.78546342e-02  4.63249162e-02
 -6.73392788e-02  1.64363384e-02 -2.62829941e-02 -1.33549497e-02
  3.75747569e-02 -3.18900263e-03 -3.50021683e-02  1.13696907e-32
  5.23009971e-02 -6.66164160e-02 -2.32958682e-02 -9.66794342e-02
  1.31023988e-01 -8.07375535e-02  3.15265022e-02 -5.48141748e-02
 -8.53554010e-02 -5.04368544e-02  5.96733093e-02 -3.86272594e-02
 -5.93342632e-02  9.45696309e-02 -7.07373908e-03 -7.51789734e-02
  3.04657295e-02 -9.43802204e-03  3.16853523e-02 -1.60523672e-02
 -9.19834059e-03  1.10330991e-02  5.08562587e-02  6.26212656e-02
  2.77931113e-02  3.44263352e-02  2.44165417e-02  3.17005962e-02
  4.64159399e-02  4.02680561e-02  6.02485389e-02 -9.14329439e-02
 -2.28571314e-02 -1.25205601e-02 -8.60290509e-03 -6.09975047e-02
 -1.10618249e-01 -4.28652838e-02  9.61943343e-03  1.01613458e-02
 -4.37327176e-02 -3.27687599e-02 -3.91043723e-02  1.36978850e-02
  2.58124806e-02 -9.02680587e-03  4.80909977e-04 -4.00388017e-02
  1.40781477e-01 -6.86690137e-02  1.16534214e-02  2.92379991e-03
 -9.84466868e-04  1.19973803e-02  2.04038084e-03  6.73531666e-02
 -2.59402897e-02  5.75690251e-03  3.56830545e-02  8.90207663e-03
  1.49679882e-03 -4.75285985e-02 -1.13209430e-03 -5.39826304e-02
  1.62588116e-02 -6.19466715e-02  2.55977102e-02 -2.22457033e-02
  7.29370266e-02 -3.34017202e-02 -5.67890564e-03 -1.41203962e-02
  4.62562218e-02  7.05651194e-02 -4.36424613e-02  3.57016502e-03
  2.52212863e-03 -1.23963533e-02 -2.92400494e-02 -1.11271616e-03
 -5.80059066e-02 -7.01883016e-03  7.82711282e-02  1.50455348e-02
  4.00765389e-02 -6.21617131e-04  4.89583388e-02 -3.90728517e-03
  1.82464700e-02  3.27656046e-02 -2.51452308e-02  4.59594652e-02
  6.84777228e-03 -2.80391052e-02  1.60509758e-02 -1.01714571e-32
 -5.49620874e-02  1.92231927e-02 -1.26412198e-01  1.00170195e-01
 -5.11757620e-02  5.33831939e-02  2.37125736e-02  7.21132616e-03
  3.07898112e-02  4.70648259e-02  1.40366917e-02  1.99484937e-02
 -2.17196830e-02 -4.36173640e-02 -8.73262156e-03 -3.53268869e-02
 -4.33945321e-02 -5.87858930e-02 -8.80781189e-03  4.19343188e-02
  1.64433534e-03  3.12419403e-02  1.83872189e-02  3.75159681e-02
  6.00758120e-02 -6.56905174e-02 -1.31675722e-02  3.82990353e-02
  5.98266087e-02 -3.76634933e-02  3.71488817e-02  1.04480321e-02
 -6.36840537e-02 -4.02764045e-03 -1.25975376e-02  8.35609622e-03
  5.19706905e-02  3.85473520e-02 -2.52063684e-02  5.57810217e-02
  1.33929163e-01  2.55321972e-02 -5.35033755e-02 -5.98153584e-02
 -1.32080223e-02  6.40590563e-02 -6.13065809e-02  3.53159234e-02
 -9.49267223e-02 -2.14067940e-02  3.68175358e-02 -3.27588171e-02
  7.82292243e-03 -6.46987259e-02  4.80825901e-02  2.41876999e-03
  9.10068378e-02 -7.74215534e-03 -1.77196432e-02  2.68162247e-02
 -1.05058424e-01 -8.55186507e-02  8.62807333e-02 -3.82703147e-03
  4.19356935e-02  3.47155705e-02 -8.97710174e-02  2.97748223e-02
 -8.12402219e-02 -2.70325150e-02  1.92434732e-02 -3.72335017e-02
 -3.04888226e-02  1.73843969e-02 -1.45845544e-02  6.93915635e-02
 -2.66773533e-02  3.41336094e-02 -6.50349781e-02 -3.12396344e-02
  3.78948823e-02  6.51499778e-02 -2.22780257e-02 -3.01636377e-04
 -5.63495792e-03  6.59367815e-02  6.75849393e-02  1.94501143e-03
  1.50070814e-02 -3.34845372e-02 -4.85553127e-03  2.79411562e-02
  2.63527478e-03  1.03187181e-01  7.86262564e-03 -4.40067396e-08
 -5.72632402e-02  2.72774138e-03  2.64076274e-02 -3.16148289e-02
  5.28944619e-02  3.08055207e-02  7.13551641e-02  9.76749975e-03
 -2.70785508e-03  3.10551953e-02  7.81102031e-02 -2.45246924e-02
 -2.26491410e-02  2.59096045e-02 -1.50647052e-02 -2.78205029e-03
 -5.19169606e-02  2.91671194e-02 -1.09537570e-02 -6.66162372e-02
  6.43379986e-03  2.22758204e-02 -1.55168399e-02  3.37774009e-02
  5.54772578e-02 -2.12943684e-02 -3.31875905e-02  8.88022706e-02
 -2.78080571e-02 -4.55715656e-02  2.11848021e-02  5.56750707e-02
  3.11608566e-03 -1.28867075e-01  9.79422480e-02  2.49425154e-02
 -1.16309360e-01 -4.76435870e-02 -5.70805324e-03 -4.59891707e-02
  6.67766854e-02  4.13685059e-03  6.03860505e-02 -3.96848582e-02
  7.03752562e-02  3.27310674e-02  4.55578901e-02 -9.25470665e-02
 -4.42889072e-02 -8.78137052e-02  4.78264317e-02 -2.59015691e-02
  2.44236048e-02  3.68323810e-02  5.97398877e-02  7.85265043e-02
 -3.32369097e-02 -5.63825518e-02  9.00039747e-02 -3.92853608e-03
 -2.41026073e-03  4.27571647e-02 -1.28331687e-02  9.94400005e-04]",2,0
torch-geometric,4,documentation paper colab notebook and video tutorial external resource ogb examplespyg pytorch geometric is a library built upon pytorch to easily write and train graph neural network gnns for a wide range of application related to structured data it consists of various method for deep learning on graph and other irregular structure also known a geometric deep learning from a variety of published paper in addition it consists of easy to use mini batch loader for operating on many small and single giant graph multi gpu support datapipe support distributed graph learning via quiver a large number of common benchmark datasets based on simple interface to create your own the graphgym experiment manager and helpful transforms both for learning on arbitrary graph a well a on d mesh or point cloud click here to join our slack community whether you are a machine learning researcher or first time user of machine learning toolkits here are some reason to try out pyg for machine learning on graph structured data in this quick tour we highlight the ease of creating and training a gnn model with only a few line of code in the first glimpse of pyg we implement the training of a gnn for classifying paper in a citation graph for this we load the cora dataset and create a simple layer gcn model using the pre defined gcnconv more information about evaluating final model performance can be found in the corresponding example in addition to the easy application of existing gnns pyg make it simple to implement custom graph neural network see here for the accompanying tutorial for example this is all it take to implement the edge convolutional layer from wang et al x i prime max j in mathcal n i textrm mlp theta left x i x j x i right graphgym allows you to manage and launch gnn experiment using a highly modularized pipeline see here for the accompanying tutorial user are highly encouraged to check out the documentation which contains additional tutorial on the essential functionality of pyg including data handling creation of datasets and a full list of implemented method transforms and datasets for a quick start check out our example in example pyg provides a multi layer framework that enables user to build graph neural network solution on both low and high level it comprises of the following component we list currently supported pyg model layer and operator according to category gnn layer all graph neural network layer are implemented via the nn messagepassing interface a gnn layer specifies how to perform message passing i e by designing different message aggregation and update function a defined here these gnn layer can be stacked together to create graph neural network model pooling layer graph pooling layer combine the vectorial representation of a set of node in a graph or a subgraph into a single vector representation that summarizes it property of node it is commonly applied to graph level task which require combining node feature into a single graph representation gnn model our supported gnn model incorporate multiple message passing layer and user can directly use these pre defined model to make prediction on graph unlike simple stacking of gnn layer these model could involve pre processing additional learnable parameter skip connection graph coarsening etc gnn operator and utility pyg come with a rich set of neural network operator that are commonly used in many gnn model they follow an extensible design it is easy to apply these operator and graph utility to existing gnn layer and model to further enhance model performance scalable gnns pyg support the implementation of graph neural network that can scale to large scale graph such application is challenging since the entire graph it associated feature and the gnn parameter cannot fit into gpu memory many state of the art scalability approach tackle this challenge by sampling neighborhood for mini batch training graph clustering and partitioning or by using simplified gnn model these approach have been implemented in pyg and can benefit from the above gnn layer operator and model pyg is available for python to python update you can now install pyg via anaconda for all major o pytorch cuda combination given that you have pytorch installed simply runnote conda package are not published for pytorch yet we alternatively provide pip wheel for all major o pytorch cuda combination see here to install the binary for pytorch simply runwhere cuda should be replaced by either cpu cu cu or cu depending on your pytorch installation for additional but optional functionality runto install the binary for pytorch simply runwhere cuda should be replaced by either cpu cu cu or cu depending on your pytorch installation torch version cuda for additional but optional functionality runnote binary of older version are also provided for pytorch pytorch pytorch pytorch pytorch pytorch and pytorch following the same procedure for older version you might need to explicitly specify the latest supported version number in order to prevent a manual installation from source you can look up the latest supported version number here in case you want to experiment with the latest pyg feature which are not fully released yet ensure that torch scatter and torch sparse are installed by following the step mentioned above and install either the nightly version of pyg viaor install pyg from master viaplease cite our paper and the respective paper of the method used if you use this code in your own work feel free to email u if you wish your work to be listed in the external resource if you notice anything unexpected please open an issue and let u know if you have any question or are missing a specific feature feel free to discus them with u we are motivated to constantly make pyg even better,"[('train graph neural network gnns', 0.6163), ('single graph representation gnn model', 0.601), ('tutorial external resource ogb examplespyg pytorch', 0.5958), ('custom graph neural network', 0.5479), ('gnn model', 0.5404), ('geometric deep learning', 0.5347), ('graphgym experiment manager', 0.5284), ('many gnn model', 0.5277), ('graph neural network model', 0.5244), ('graph neural network layer', 0.4995)]","[-1.26604766e-01 -4.70866039e-02  3.90014574e-02 -3.92921327e-04
  2.71205699e-05 -3.11806016e-02 -5.19718230e-02  4.45435755e-02
 -1.09546088e-01 -2.16815509e-02  9.14476160e-03 -3.76754478e-02
  1.44257965e-02  1.04885790e-02 -2.09322833e-02 -4.56225760e-02
  9.06291306e-02  5.16009964e-02 -3.71705107e-02 -1.02417223e-01
 -1.29796546e-02 -3.07763787e-03  2.58555654e-02 -7.96227343e-03
  2.13779379e-02 -4.80144285e-02  3.26110376e-03 -3.90827190e-03
  8.56911242e-02 -3.22966576e-02  2.88871471e-02 -2.63709389e-02
  1.91432536e-02  5.21198697e-02 -5.39952293e-02 -1.61870793e-02
 -3.89384478e-02 -3.78085226e-02  1.17138866e-02 -2.22393852e-02
 -3.42139453e-02 -6.85902359e-03 -3.21210325e-02  1.75304618e-02
  9.35228094e-02  1.62906814e-02 -1.77912861e-02 -3.03506367e-02
 -1.86688770e-02 -8.45861621e-03 -3.66957821e-02 -1.30996287e-01
 -6.32687807e-02  3.85018997e-03  4.67561185e-02 -7.82262087e-02
 -6.58858288e-03  5.15831169e-03  4.72238800e-03 -1.44728748e-02
  1.80765465e-02 -2.48407722e-02 -6.32897019e-02 -2.01801099e-02
 -2.12693233e-02  3.78859453e-02  1.40832523e-02  9.21586230e-02
  5.39191552e-02 -5.75036034e-02  2.53017563e-02 -2.35772654e-02
 -7.41740391e-02  4.98480303e-03 -3.43937539e-02  2.96023227e-02
 -3.80086363e-03  4.88689207e-02  4.70761806e-02 -7.98506886e-02
 -6.61524385e-03 -3.84089723e-02  6.13651946e-02  9.02544055e-03
 -3.47982012e-02 -1.21673793e-02 -8.85682404e-02  4.45331670e-02
 -2.18263157e-02 -2.64418535e-02  3.62804420e-02 -3.52954888e-03
 -4.80327904e-02  1.21009937e-02  1.53513392e-02  2.42665764e-02
  2.81278929e-03 -1.69576332e-03 -3.73333655e-02  2.26556025e-02
 -2.59910375e-02 -2.23647207e-02  8.09929594e-02  1.02657890e-02
 -3.69020216e-02  8.19183215e-02  3.54306027e-02  7.05674812e-02
  6.35258481e-02 -1.81566142e-02 -4.79972810e-02  7.11227879e-02
 -9.06790271e-02 -8.02196041e-02  3.28186862e-02 -6.84387842e-03
 -6.65990561e-02 -8.08707718e-03  1.89165827e-02  8.03962275e-02
 -1.31439120e-01  7.06465691e-02  2.13386165e-03 -2.02069920e-03
 -7.50981122e-02 -5.04317868e-04 -9.97146666e-02  6.28969117e-33
  6.39572293e-02  4.79780883e-03 -2.14313548e-02 -8.80353600e-02
  1.11568347e-01 -1.88774932e-02 -4.50518448e-04  3.92986573e-02
  3.37609425e-02 -2.79816873e-02 -6.55945912e-02  4.77556325e-02
 -2.36712173e-02  1.06668517e-01 -2.40393784e-02  1.18425600e-02
 -1.84928086e-02 -3.01109552e-02  6.26533404e-02 -1.06019685e-02
  7.00850040e-02 -1.39996866e-02  3.65717188e-02  1.01730853e-01
  5.42340986e-02  9.31815431e-02 -6.60362374e-03 -1.19587118e-02
  1.16900569e-02  9.60354600e-03 -8.34455527e-03  5.67778610e-02
 -1.42643582e-02 -1.43180881e-03 -3.32360938e-02 -5.70317172e-03
 -2.01145392e-02 -5.32015450e-02  3.84786278e-02 -7.53748044e-02
 -2.33043246e-02  1.62063166e-02 -1.46597214e-02 -4.91811857e-02
 -2.56676599e-02 -4.04731818e-02  1.04797082e-02  2.16886890e-03
  7.10911974e-02 -2.19152514e-02 -2.17113458e-02 -6.98506832e-03
 -5.86010218e-02  4.24137451e-02  6.68534487e-02  2.07944755e-02
  9.03384089e-02  6.40809238e-02 -1.77111896e-03  4.57925647e-02
  3.11673824e-02  8.65261257e-02  5.94939329e-02 -2.34327577e-02
 -1.45712839e-02 -3.74445952e-02 -6.73098564e-02 -2.25967495e-03
  8.41420330e-03 -2.61080824e-02 -4.19447720e-02  1.67682357e-02
  2.24929256e-03 -3.84011194e-02 -2.13743020e-02 -3.14125270e-02
 -2.23122574e-02 -1.07814386e-01 -9.98064876e-02  7.29449168e-02
 -6.51889071e-02 -5.19816689e-02  1.56801734e-02 -6.68452773e-03
 -4.35386039e-02 -1.53521569e-02  2.89274361e-02 -1.88671257e-02
  8.48061293e-02  1.78953763e-02  9.49309580e-03 -1.98116787e-02
  6.07961789e-03  4.42341678e-02 -3.45701240e-02 -5.13767402e-33
 -8.84543732e-03  2.01177374e-01 -3.51315439e-02  6.36016056e-02
  1.17977239e-01  2.85082357e-03  8.67324229e-03 -5.95614351e-02
 -3.44559997e-02  7.90199414e-02 -1.25295483e-02 -4.09794673e-02
  5.96134178e-02  6.04191050e-02 -1.81683432e-02 -2.64778566e-02
 -9.11859795e-02 -5.07372878e-02 -2.99061183e-02  8.10777536e-04
 -5.77429086e-02  7.63762891e-02 -1.18189193e-01  1.96512882e-03
  7.40822256e-02 -2.96519976e-02 -5.98376393e-02 -1.95449069e-02
  5.85487187e-02  6.05222583e-02 -2.30058115e-02  8.80522840e-03
  3.19294888e-03  3.55486554e-04  8.47316310e-02  3.71054746e-02
  9.73996222e-02 -5.21877743e-02  2.85696741e-02 -6.86247721e-02
  6.99097514e-02 -9.20652132e-03  3.28821577e-02 -1.10921171e-02
 -2.23945733e-02  3.72867882e-02 -1.02153473e-01  4.77487519e-02
 -1.24419160e-01 -3.48741598e-02 -5.91623830e-03  2.27647237e-02
  2.18379833e-02 -3.36292796e-02 -2.34669279e-02 -4.25668694e-02
  1.10565186e-01  4.65484969e-02 -8.22960213e-03 -3.27236690e-02
 -3.72015014e-02 -1.03430435e-01 -6.94123749e-03  2.18816753e-02
 -5.49337380e-02 -1.79635771e-02 -2.77329683e-02  8.34034290e-03
  1.23068579e-02  3.44679994e-03 -6.53143181e-03  6.55082464e-02
  1.09520974e-03  5.02508283e-02 -1.14276046e-02  5.13309687e-02
 -3.91721688e-02  3.52719501e-02 -2.36849561e-02 -4.05728444e-02
  7.00003728e-02  4.24260534e-02 -4.27213497e-03  8.26210380e-02
  6.46088123e-02  8.14716518e-02  5.83836390e-03  8.33923742e-02
 -8.17716180e-04  1.55959046e-02 -6.27649352e-02  3.14671360e-02
 -2.78677419e-02  1.68121278e-01  9.90810245e-03 -2.79304793e-08
 -1.03623591e-01  1.87370349e-02  8.83182138e-02  4.61338600e-03
  6.09034896e-02  2.84024922e-04  9.32705849e-02  5.68993799e-02
 -2.77881287e-02  1.12349868e-01 -4.95452657e-02  2.28530671e-02
 -4.26356532e-02 -1.11737428e-02  2.97519602e-02 -1.12537527e-03
 -4.46766242e-03 -3.04292534e-02  8.28981847e-02 -2.44509839e-02
  4.08452190e-02  1.51605103e-02 -4.08557132e-02  6.12759404e-02
  3.86759192e-02 -6.68868646e-02 -1.09114479e-02  7.41380826e-02
 -6.79942816e-02  2.55630314e-02  2.43850388e-02 -1.34420488e-03
  1.40209153e-01 -9.60883424e-02  4.39769030e-02  8.11166391e-02
  1.34515455e-02  1.62677828e-03  1.17301662e-02  4.08790968e-02
 -4.88145351e-02  5.46572320e-02  1.00973239e-02 -3.82262357e-02
 -4.33830619e-02  7.06194947e-03  1.67876277e-02 -7.63133913e-02
 -7.14874547e-03  3.10621597e-02 -6.44288957e-02  2.79994812e-02
 -9.60699320e-02 -1.83793297e-03  2.97579188e-02  7.25633353e-02
 -2.74866838e-02 -5.69678359e-02  9.20536369e-03  3.26085202e-02
 -7.32787997e-02  3.47090028e-02 -7.65703246e-02 -9.51861404e-03]",2,2
asreview,4,systematically screening large amount of textual data is time consuming and often tiresome the rapidly evolving field of artificial intelligence ai ha allowed the development of ai aided pipeline that assist in finding relevant text for search task a well established approach to increasing efficiency is screening prioritization via active learning the active learning for systematic review asreview project published in nature machine intelligence implement different machine learning algorithm that interactively query the researcher asreview lab is designed to accelerate the step of screening textual data with a minimum of record to be read by a human with no or very few false negative asreview lab will save time increase the quality of output and strengthen the transparency of work when screening large amount of textual data to retrieve relevant information active learning will support decision making in any discipline or industry asreview software implement three different mode the asreview software requires python detailed step by step instruction to install python and asreview are available for window and macos user upgrade asreview with the following command to install asreview lab with docker see install with docker getting started with asreview lab the following publication in nature machine intelligence can be used to cite the project van de schoot r de bruin j schram r et al an open source machine learning framework for efficient and transparent systematic review nat mach intell http doi org s for citing the software please refer to the specific release of the asreview software on zenodo http doi org zenodo the menu on the right can be used to find the citation format of prevalence for more scientific publication on the asreview software go to asreview ai paper for an overview of the team working on asreview see asreview research team asreview lab is maintained by jonathan de bruin and yongchao terry ma the best resource to find an answer to your question or way to get in contact with the team are the asreview software ha an apache license the asreview team accepts no responsibility or liability for the use of the asreview tool or any direct or indirect damage arising out of the application of the tool,"[('active learning', 0.5231), ('industry asreview software', 0.4717), ('open source machine learning framework', 0.4605), ('asreview lab', 0.4484), ('nature machine intelligence', 0.4404), ('asreview software', 0.4346), ('researcher asreview lab', 0.422), ('textual data', 0.4146), ('systematic review asreview project', 0.4081), ('artificial intelligence', 0.4012)]","[-2.16343976e-03 -1.30252331e-01 -3.32960747e-02  8.46167803e-02
  7.72979259e-02 -2.51851846e-02 -2.37216745e-02  2.48034764e-02
 -6.52033240e-02  2.42643524e-02 -7.64689296e-02  1.35472296e-02
 -2.36483980e-02 -2.29451749e-02 -1.81963667e-02  8.00103322e-02
 -1.82033926e-02 -1.34934159e-02  5.41305216e-03 -5.54487035e-02
  2.41998155e-02  6.38947040e-02  5.78125380e-02 -4.05906438e-04
 -4.01396155e-02 -4.13819961e-02 -1.72053147e-02 -4.58362550e-02
  7.58077670e-03 -6.85701743e-02 -3.88942957e-02  2.67653223e-02
  9.18280035e-02 -1.48848277e-02 -4.30922881e-02  5.63411303e-02
 -3.60947736e-02  4.36445046e-03  3.91159905e-03 -2.88619921e-02
 -5.63893802e-02 -1.29300719e-02  3.16234902e-02 -4.10484150e-02
  8.67135003e-02 -1.35194240e-02 -4.03359458e-02 -1.14890873e-01
  6.43287301e-02 -1.54304886e-02 -1.75070256e-01 -9.10740495e-02
 -5.51229157e-02  2.00101063e-02 -1.12107113e-01 -2.08699778e-02
 -2.34889705e-03 -8.12421218e-02 -1.19303977e-02 -4.04418074e-02
  6.00190461e-02 -3.48556116e-02 -3.41932029e-02 -2.54074484e-03
 -6.12323582e-02  7.11968094e-02 -3.90583947e-02  3.56195755e-02
  7.17527717e-02 -1.16690643e-01 -3.20035517e-02  3.38543765e-02
  5.10001481e-02  4.13251743e-02  1.92554314e-02 -8.50563869e-03
  1.92195121e-02 -6.27809204e-04  1.25323132e-01 -1.05657808e-01
  2.99218879e-03  2.87704766e-02  1.68046658e-03  3.14679146e-02
 -1.91475656e-02  3.74139915e-03 -4.80476879e-02  4.87093292e-02
  7.19597749e-03  3.70678380e-02 -1.39137113e-03 -5.05346619e-02
  4.87209521e-02  1.25676282e-02 -8.66400637e-03 -2.58599613e-02
  2.34044455e-02 -5.05989268e-02  4.64814678e-02  2.34890562e-02
 -5.92778884e-02  6.38919771e-02  1.62804388e-02  3.01580876e-02
 -4.87031713e-02  5.30161485e-02  2.57071219e-02 -4.25814763e-02
  1.88726068e-01  3.09643857e-02 -3.98553759e-02 -8.29166733e-03
 -3.92477587e-02 -9.75334719e-02  2.94912769e-03 -1.00175175e-03
 -6.11935370e-02  3.43169570e-02  4.93162544e-03  5.12385331e-02
 -8.65795389e-02  3.78334150e-02 -7.47506740e-03 -1.81181617e-02
  1.02098532e-01 -3.91371511e-02 -7.27493390e-02  4.81109781e-33
 -3.72498780e-02 -1.85555704e-02  2.90999655e-02 -7.38429744e-03
  3.19756046e-02 -1.22649439e-01 -1.20947119e-02  1.45073850e-02
 -2.33969502e-02 -6.42902330e-02  3.24854776e-02  1.14504784e-01
  1.16888350e-02  1.10125244e-01  2.65949816e-02 -6.40759766e-02
  3.51237296e-03  8.32577273e-02 -2.78289281e-02 -4.33739536e-02
 -1.96624324e-02 -7.32586756e-02  4.54697348e-02  7.19118714e-02
  1.81188043e-02 -2.95402836e-02 -3.20284255e-02 -2.02309601e-02
 -1.25571517e-02  9.63798445e-03  3.55282389e-02  5.11173606e-02
 -3.75676900e-02  3.63587812e-02 -1.05167730e-02  2.50812117e-02
 -1.04563618e-02 -2.46148743e-02  4.68589440e-02  5.95668480e-02
 -8.05294588e-02  7.05482215e-02  4.50771302e-02 -3.89760220e-03
  1.30730504e-02 -4.31870222e-02  4.12224084e-02 -3.72314965e-03
  8.67861584e-02  4.27038223e-03 -2.47616954e-02 -1.82778686e-02
  3.90937440e-02 -3.06427423e-02 -1.39879950e-04  6.72519952e-02
 -2.80581368e-03  9.60149895e-03  1.67591386e-02  6.78783059e-02
  2.24032998e-03  3.13143469e-02  1.02134412e-02 -5.44198975e-02
 -2.31686030e-02  4.78901677e-02  6.43565208e-02 -4.09440398e-02
  1.24141118e-02  5.70108220e-02 -2.28729416e-02  3.59599590e-02
 -1.88823845e-02  1.97238047e-02 -3.89964283e-02 -2.89116763e-02
 -3.02076377e-02 -7.26196840e-02 -2.27197353e-02  5.42397425e-02
 -9.64334086e-02  3.33471559e-02 -2.62422822e-02  1.78772006e-02
 -2.82708462e-02  6.96927123e-03  3.68653499e-02  2.72327010e-03
 -4.47725086e-03 -4.42253165e-02 -1.28462657e-01 -7.50591606e-03
 -7.47799650e-02  6.41871989e-03  7.05992244e-03 -4.43495431e-33
 -6.59569651e-02 -9.13586188e-03 -6.59302548e-02  5.88526614e-02
  3.50063108e-02  7.40431473e-02 -4.54965644e-02 -4.67167376e-03
  1.59484465e-02  3.01979668e-02  5.01472950e-02 -2.53074802e-02
  5.37162721e-02  4.29529771e-02 -2.42085923e-02 -6.73760474e-02
 -7.80299455e-02 -7.08948597e-02 -3.72105874e-02  1.05517291e-01
  9.69795417e-03  1.05175078e-01 -2.76324861e-02  3.92301343e-02
  1.03761889e-01  1.96982492e-02 -4.24109139e-02 -9.92110372e-03
  9.62690488e-02  3.07990406e-02  2.01196708e-02  1.27728134e-02
  1.28202187e-02 -1.70189850e-02 -1.37852272e-02  3.53072248e-02
  7.96868056e-02 -7.40093738e-02 -2.48013088e-03  1.44631334e-03
  1.35939091e-01  1.39848497e-02 -2.69418154e-02 -4.71743532e-02
 -2.37470530e-02 -3.34571376e-02 -8.92759636e-02  6.87812865e-02
 -1.69822294e-02 -7.50679225e-02  1.57365464e-02 -3.60696716e-03
 -1.86250862e-02 -6.74413443e-02 -3.79691049e-02  2.30273996e-02
  3.77231240e-02  1.92377660e-02 -1.56183895e-02  1.23486193e-02
 -3.22799049e-02 -4.00440069e-04  5.86713664e-02  3.19063999e-02
 -5.36660738e-02 -1.74143407e-02  1.20065846e-02 -9.68061388e-03
 -6.15533292e-02 -9.43029299e-02 -1.31995520e-02  6.95547312e-02
 -3.41740623e-02 -1.03674247e-04 -5.58194444e-02 -6.57491107e-03
 -5.82254343e-02  3.11470516e-02 -4.07274850e-02  1.72616169e-02
  9.01543275e-02 -7.87564814e-02  1.02051459e-01  1.13166593e-01
  3.57071147e-03  6.57450110e-02  6.21645339e-03 -6.69715181e-02
 -1.07900868e-03 -3.63225825e-02 -8.59005228e-02 -1.70668755e-02
  1.69147626e-02  9.15496349e-02 -3.32534425e-02 -2.86079249e-08
  9.71600786e-03  6.97326288e-02  1.42549545e-01 -7.73649616e-03
  4.44619991e-02  4.23391685e-02 -7.31192306e-02  6.16667941e-02
 -1.06302321e-01  1.10430410e-02 -4.21525948e-02  8.67985014e-04
 -3.92314717e-02  1.55877415e-02  1.76887475e-02 -2.46337671e-02
  2.70069130e-02  2.43100002e-02  2.46159416e-02 -2.66228133e-04
  8.71708542e-02  3.60455811e-02 -1.62973199e-02  2.24785283e-02
  5.47870845e-02 -6.14647716e-02 -1.30715799e-02 -6.81254491e-02
  5.76173072e-04 -1.00544449e-02  1.12431375e-02  4.59746197e-02
  6.79652318e-02 -7.80148730e-02  8.07046071e-02  2.72339163e-03
  9.35982391e-02 -8.34861696e-02 -2.56233532e-02  3.27430442e-02
 -2.00281423e-02  5.58341667e-02 -8.01033061e-03 -1.71124116e-02
 -1.12868454e-02  2.99756397e-02  1.19741587e-02 -7.57173747e-02
  3.66894752e-02 -5.22634462e-02 -3.42832096e-02  1.32670663e-02
 -1.72427818e-02  7.66772870e-03  3.22589725e-02  7.56696463e-02
  3.25452387e-02 -8.66858736e-02 -2.14381162e-02  7.53554106e-02
  4.82289791e-02 -1.42919486e-02  3.66770439e-02  2.56503094e-02]",2,0
tensorflow-text,4,tf text is a tensorflow library of text related ops module and subgraphs the library can perform the preprocessing regularly required by text based model and includes other feature useful for sequence modeling not provided by core tensorflow see the readme on github for further documentation http github com tensorflow text,"[('further documentation http github com tensorflow text', 0.559), ('text', 0.4706), ('tensorflow library', 0.4389), ('core tensorflow', 0.4303), ('sequence modeling', 0.3388), ('subgraphs', 0.288), ('preprocessing', 0.2776), ('library', 0.2558), ('ops module', 0.2491), ('readme', 0.193)]","[-8.48985240e-02 -1.05365530e-01 -2.26196144e-02 -1.05914893e-02
  4.02279012e-02 -7.90699199e-03 -2.24151853e-02  3.96282412e-02
 -7.36452863e-02 -6.08053990e-02 -6.14273027e-02  5.87629564e-02
 -6.45800456e-02 -2.52954531e-02  1.64023321e-02  2.04044934e-02
 -3.05992793e-02  8.04893002e-02  1.60978809e-02 -1.45018756e-01
  6.18530586e-02  3.94132175e-02  4.41720197e-03 -2.83468654e-03
 -2.32700501e-02 -4.36668564e-03 -2.65639089e-02 -5.85029572e-02
  5.75005114e-02  4.44354527e-02  2.06325389e-02  6.96103647e-02
  1.29024372e-01  4.29735556e-02 -5.81215359e-02  5.07187061e-02
 -9.61325224e-03 -1.15030289e-01 -1.53248676e-03  6.60149008e-02
  9.87338927e-03  2.67634913e-02 -3.85966226e-02 -6.40958687e-03
  7.33082145e-02  2.28303485e-02 -2.09361110e-02 -6.46867752e-02
 -3.10473107e-02 -2.58882414e-03 -1.23353995e-01 -7.76234344e-02
 -7.77672157e-02  3.73150930e-02  1.60757136e-02  1.24545703e-02
  2.25971080e-02 -6.62993565e-02  1.29392464e-02 -6.30979836e-02
  4.74762060e-02 -3.98416221e-02 -7.68114999e-02  1.75736547e-02
  3.00437864e-02  2.20965855e-02  5.42285964e-02  4.23491895e-02
  1.15291074e-01 -6.95756078e-02 -4.92071472e-02 -1.90124810e-02
 -3.94352563e-02  2.28108484e-02 -7.56324595e-03  2.48388387e-03
  7.65044689e-02  1.93357319e-02  1.42379468e-02 -1.00360177e-01
 -4.84811440e-02  3.52357253e-02  1.18696772e-01 -1.59305464e-02
 -1.50134712e-02  4.24404405e-02  5.08847609e-02  6.70480803e-02
  2.29059681e-02  2.61820704e-02  2.63209715e-02 -4.99210991e-02
 -3.87867391e-02 -7.89504685e-03 -2.25057453e-03  8.40033963e-02
 -1.68305449e-02 -2.31032055e-02 -2.81669991e-03  2.99679115e-02
 -2.83267479e-02 -5.94985485e-02  8.65981579e-02  3.22601236e-02
 -7.28764758e-02  3.12721469e-02  2.20259149e-02  2.27361824e-02
  1.78539660e-02  4.63538943e-03 -3.60444840e-03  7.75577053e-02
  2.47070789e-02 -6.12333454e-02  3.65333706e-02  1.56086050e-02
 -3.05752316e-03  1.89898349e-02  4.68293019e-02  6.96335360e-02
 -6.81881309e-02  4.25342433e-02 -5.32145612e-03  2.87305955e-02
 -7.25677162e-02 -4.56277169e-02 -6.11312129e-02  2.01010471e-33
  6.78360462e-02 -9.65752639e-03  2.68010292e-02  2.22445372e-02
  9.50646028e-03 -6.57446459e-02  1.26342764e-02 -4.07799408e-02
 -3.82464565e-02  2.71693673e-02 -2.41369791e-02  7.72598535e-02
  4.63170279e-03  1.13284819e-01 -1.06646046e-01 -1.25824094e-01
 -1.60665978e-02 -2.37110164e-03  3.99017222e-02  2.99518481e-02
  4.51562479e-02  8.20028596e-03  1.48051884e-02  8.66448134e-02
  5.79570793e-02  9.65103656e-02  1.21240299e-02 -5.98894432e-02
  1.53553206e-03  1.00750821e-02 -7.40851611e-02  7.32028950e-03
 -1.15783876e-02  1.28034353e-02 -6.65193237e-03  1.14351232e-02
 -5.97907640e-02 -4.47006449e-02 -3.08425240e-02 -6.56276569e-02
 -7.85598066e-03  3.28840688e-02 -7.11362436e-02 -7.80922398e-02
 -2.47050859e-02 -1.86127163e-02 -1.68774854e-02  4.09597792e-02
  1.40465155e-01 -5.00275567e-02 -2.16988232e-02 -2.92568356e-02
  1.06983632e-02  4.45671519e-03  1.14143118e-02 -8.30012001e-03
  1.99419018e-02  2.86656227e-02  6.98233843e-02  8.27607960e-02
 -1.46799013e-02 -1.44965323e-02  4.48489226e-02  1.85923837e-02
 -1.94109324e-02 -1.96813941e-02 -1.77294761e-01  2.11668555e-02
  8.46393183e-02 -1.24089979e-02 -9.17555466e-02  9.55243558e-02
 -5.79055846e-02  1.31970700e-02 -5.75639419e-02 -2.83754859e-02
 -1.23671945e-02 -8.65295380e-02 -4.66112942e-02  5.26901819e-02
 -1.21129073e-01  1.81473121e-02  5.23943491e-02 -1.45291844e-02
 -2.84083132e-02 -1.14237284e-02  1.88558903e-02  3.52330357e-02
  1.51574425e-03 -8.99408683e-02 -1.16440766e-02  3.82197765e-03
  9.94860288e-03 -6.08237786e-03  1.86291691e-02 -3.03906120e-33
 -1.73382033e-02  1.64860934e-02 -7.95922950e-02  9.29418430e-02
 -3.19303870e-02  1.62205845e-02  1.34604918e-02  4.74516340e-02
  5.79234622e-02  3.88973244e-02  1.10949576e-02  2.67097075e-02
  5.09126224e-02 -4.92060222e-02  1.08617730e-02 -7.55175576e-02
 -2.71697864e-02 -6.72359616e-02 -4.91666375e-03  2.14215759e-02
 -4.11514677e-02  4.79920693e-02 -5.36247008e-02 -5.60225211e-02
  2.92650964e-02 -1.98274441e-02 -2.15952974e-02 -2.14837808e-02
  2.28660759e-02 -3.64243612e-02  1.75866242e-02 -1.30590880e-02
 -4.16914485e-02  4.87025529e-02 -1.07602244e-02 -2.10655555e-02
  8.10260400e-02  5.31812869e-02 -6.20319648e-03  2.67206784e-02
  1.16984151e-01  8.35786238e-02 -1.12596322e-02  1.00098515e-03
 -3.67891751e-02  1.02308933e-02 -3.83844711e-02  6.51200339e-02
 -7.10810199e-02 -7.23461285e-02 -2.29883823e-03 -6.01146184e-02
 -2.14312002e-02 -1.03402019e-01 -5.20458594e-02 -3.04004885e-02
  1.00513600e-01  1.20695578e-02  7.54725412e-02 -4.35346141e-02
 -5.40511087e-02 -2.30508614e-02  1.91291682e-02 -1.83677413e-02
 -3.94274406e-02 -2.88652815e-02 -1.00297809e-01 -2.34646536e-02
 -2.81460453e-02 -2.10267901e-02 -2.15697158e-02  2.32489668e-02
 -5.96008357e-03  7.06593841e-02  3.36855836e-02 -2.92830300e-02
  3.72389262e-03  4.31591533e-02 -5.60529996e-03 -2.25698035e-02
  2.18177028e-02  3.91292274e-02  2.63611116e-02  1.15663499e-01
  4.40573692e-02  5.19867353e-02  2.58590579e-02  1.30071864e-03
  2.00312473e-02  8.85597616e-03 -7.74160251e-02  1.14318205e-03
  1.26658417e-02  1.33485883e-01 -5.60378330e-03 -2.56059156e-08
 -8.67137164e-02  1.57764480e-02  1.83874890e-02  2.01362604e-03
 -6.69274665e-03  5.90407923e-02  1.15585983e-01  6.55702427e-02
  8.28954764e-03  4.97089094e-03 -9.48142819e-03 -9.79637355e-03
 -4.82813045e-02 -4.43749353e-02  4.08345722e-02  1.03686139e-01
  2.79569793e-02  2.95524057e-02  3.58590088e-03 -2.20971480e-02
  2.68536154e-02  3.04921833e-03  1.03922440e-02  4.54922169e-02
  3.22784856e-02 -4.82628755e-02  3.55274342e-02  6.85337409e-02
 -4.02265303e-02 -9.11148861e-02 -3.73591408e-02  1.04066571e-02
  5.73336817e-02 -8.70135203e-02  4.44007106e-02  8.89365822e-02
  3.40273008e-02 -4.76862080e-02  1.84010919e-02  5.93688041e-02
 -7.60680139e-02  4.46265191e-02  1.05360877e-02 -4.55872491e-02
  1.17996205e-02  1.54704843e-02 -5.54527938e-02 -6.01120591e-02
 -4.91762012e-02 -2.89404858e-03 -4.92895662e-04  3.27010192e-02
 -5.40999062e-02  7.66129047e-02 -4.14291620e-02  8.48612860e-02
 -1.99942160e-02 -9.02475715e-02  4.01321389e-02 -6.29222617e-02
 -6.49786219e-02  8.14921185e-02  1.88473389e-02 -6.06327131e-02]",2,2
efficientnet,3,this repository contains a kera and tensorflow kera reimplementation of efficientnet a lightweight convolutional neural network architecture achieving the state of the art accuracy with an order of magnitude fewer parameter and flop on both imagenet and five other commonly used transfer learning datasets the codebase is heavily inspired by the tensorflow implementation there wa a huge library update of july now efficintnet work with both framework kera and tensorflow kera if you have model trained before that date to load them please use efficientnet of version pypi you can roll back using pip install u efficientnet efficientnets rely on automl and compound scaling to achieve superior performance without compromising resource efficiency the automl mobile framework ha helped develop a mobile size baseline network efficientnet b which is then improved by the compound scaling method to obtain efficientnet b to b efficientnets achieve state of the art accuracy on imagenet with an order of magnitude better efficiency in high accuracy regime efficientnet b achieves the state of the art top top accuracy on imagenet with m parameter and b flop at the same time the model is x smaller and x faster on cpu inference than the former leader gpipe in middle accuracy regime efficientnet b is x smaller and x faster on cpu inference than resnet with similar imagenet accuracy compared to the widely used resnet efficientnet b improves the top accuracy from of resnet to under similar flop constraint see the complete example of loading the model and making an inference in the jupyter notebook here the performance of each model variant using the pre trained weight converted from checkpoint provided by the author is a follows topk accuracy score for converted model imagenet val set pypi stable releasepypi latest release with kera and tf kera support pick the target directory like dist and run the converter script from the repo directory a follows you can also optionally create the virtual environment with all the dependency installed by adding make venv true and operate in a self destructing temporary location instead of the target directory by setting tmp working dir true i would like to thanks community member who actively contribute to this repository,"[('high accuracy regime efficientnet b', 0.5738), ('imagenet', 0.56), ('similar imagenet accuracy', 0.5585), ('efficientnet', 0.5532), ('tensorflow kera', 0.5399), ('tensorflow implementation', 0.5332), ('middle accuracy regime efficientnet b', 0.5322), ('efficientnet b', 0.5211), ('lightweight convolutional neural network architecture', 0.4995), ('mobile size baseline network efficientnet b', 0.494)]","[-3.02949138e-02 -4.26765345e-02  4.71873283e-02 -3.60298343e-02
  3.36314328e-02  2.27551330e-02 -9.63080022e-03 -4.96807732e-02
 -4.24348116e-02 -5.75806201e-02 -1.35233365e-02  1.64937731e-02
  1.86797678e-02  6.36529410e-03 -4.37843055e-02  9.66706313e-03
  3.94803248e-02  3.59592326e-02 -6.74012825e-02 -8.21095407e-02
 -5.64035028e-03  5.24532935e-03  2.46319324e-02 -2.02462655e-02
  6.89820051e-02 -2.92734094e-02 -4.74697240e-02 -5.60563430e-02
  9.74315591e-03 -3.10085975e-02  3.93638648e-02  2.82924529e-02
 -2.81256833e-03  1.39514990e-02 -5.73953837e-02 -3.86262536e-02
 -4.53979336e-02 -4.25209701e-02  3.19525450e-02 -4.78873812e-02
 -1.47015462e-02 -1.66007802e-02  1.65681578e-02 -4.73977532e-03
  8.49284679e-02  2.64733229e-02  7.86694046e-03 -5.39933965e-02
 -8.31788871e-03 -1.78921465e-02 -3.55706885e-02 -5.34996018e-02
 -2.61526741e-02  7.88332522e-02 -3.12080402e-02 -8.75527970e-03
 -8.57560486e-02  3.62225734e-02  2.31548846e-02 -1.16931880e-02
  7.58434925e-03 -2.63157226e-02 -7.36579075e-02  2.61124335e-02
  1.78219378e-02  5.36735989e-02  3.38494256e-02 -4.69293222e-02
  1.02662534e-01 -7.16919899e-02  3.67008299e-02  3.74285839e-02
 -5.16200587e-02  4.27086465e-02 -2.23927051e-02 -8.03120900e-03
  1.25011608e-01  3.80846076e-02 -1.97952427e-02 -1.08648553e-01
  3.51860635e-02 -8.69666506e-03  1.66592076e-02 -1.17347678e-02
  6.98522925e-02 -3.92951071e-02 -8.76317173e-02  4.94998693e-02
 -2.76270378e-02 -8.17405507e-02 -5.87663706e-03  1.28960600e-02
  1.18650701e-02 -2.20285561e-02  7.61804059e-02 -4.74502769e-04
 -4.02992070e-02 -9.73171294e-02 -1.17440827e-01  9.56500024e-02
 -1.74712203e-02 -7.82901645e-02 -1.64351985e-02  3.72893699e-02
  4.22888473e-02  2.75272336e-02  9.43555906e-02  7.46150268e-03
  9.11198705e-02  1.26110734e-02  1.48544703e-02  1.60379689e-02
 -1.08976737e-02 -8.27009082e-02  8.21471810e-02 -1.17992936e-02
 -5.29854707e-02  5.19859232e-02  7.40728676e-02  1.00540496e-01
 -1.12209074e-01 -6.56911638e-03 -1.82441995e-02 -5.39775901e-02
 -2.18338352e-02  3.15460227e-02 -8.42458606e-02  7.39727428e-33
 -4.31646891e-02 -1.13457879e-02  3.41487713e-02 -3.87857519e-02
  7.23911077e-02 -3.67298871e-02  1.73376631e-02  7.88559578e-03
  5.95927471e-03 -2.87145237e-03 -1.54208988e-01  5.87663054e-02
 -2.57405285e-02  1.38519257e-01  9.53304693e-02 -4.62237895e-02
  2.14637984e-02  6.99922740e-02  7.87860379e-02  3.54237519e-02
  2.02468149e-02  6.49001496e-03  5.41752689e-02  3.93818580e-02
  1.97592303e-02 -2.80398875e-02  2.92531140e-02  3.18430439e-02
 -1.71262138e-02  1.46488044e-02 -3.58809642e-02  1.05898536e-03
  6.53690770e-02 -5.83366342e-02 -1.09054549e-02 -8.63407627e-02
 -7.27517456e-02 -9.39343590e-03  7.58355949e-03 -2.46334933e-02
 -1.30475452e-03  7.96394646e-02 -1.38289444e-02  6.28977548e-03
 -1.68056674e-02 -4.85250764e-02 -7.86117837e-02  5.60116954e-02
  1.42698819e-02  2.51510981e-02  1.97774097e-02 -1.87044702e-02
 -5.16893268e-02 -7.17958212e-02 -6.11367356e-03  3.49626015e-03
  6.08588085e-02  9.15856734e-02  5.68009093e-02  8.92490745e-02
 -7.21663004e-03 -1.15549238e-03 -7.58950859e-02  5.04700691e-02
  8.88553448e-03 -2.39480045e-02 -9.51066520e-03 -6.29985298e-04
  1.17093762e-02 -1.74931288e-02 -3.38530988e-02  4.38126549e-02
  2.56031789e-02 -1.41674578e-02  2.10963972e-02 -1.00130495e-02
 -5.98095777e-03 -1.11722283e-01 -4.68220338e-02  1.07744806e-01
 -1.20692194e-01  7.12141693e-02  1.44388189e-03  1.29089644e-02
 -5.52844964e-02  1.40182124e-02 -4.17237589e-03 -2.96565127e-02
  9.56510231e-02  5.11010438e-02 -5.98790757e-02 -1.76248811e-02
  4.28642742e-02  1.01210838e-02 -4.49742973e-02 -4.55430225e-33
  2.88597890e-03  1.35600582e-01 -6.32861853e-02  1.06075443e-01
  3.43670063e-02 -4.90484061e-03  5.19452542e-02 -3.36994268e-02
 -3.95108648e-02  2.71527059e-02  5.13771325e-02 -2.89969682e-03
  4.99597788e-02 -7.30054229e-02  7.21287131e-02 -8.28216076e-02
 -2.70912498e-02 -1.07907213e-01  2.84039639e-02  4.40337928e-03
  4.61448207e-02  2.68717408e-02 -3.77471447e-02 -3.10324132e-03
  4.02227556e-03 -3.31543162e-02 -9.54478085e-02  6.74910843e-02
  3.00885513e-02 -3.92442159e-02 -5.29564582e-02 -8.86082351e-02
  4.91658263e-02  1.51264435e-02  7.20089152e-02  1.23850312e-02
  7.79160336e-02 -6.42056987e-02 -2.01214547e-03  2.31811684e-03
  6.34248108e-02  1.39637466e-03  1.42408889e-02  5.11910729e-02
 -5.95464464e-03 -1.62489079e-02 -7.49860927e-02  1.70681532e-03
 -3.24096461e-03  1.26972364e-03 -1.54648814e-02 -1.02623170e-02
 -8.51473361e-02  2.94613130e-02  3.21673229e-02 -7.09351385e-03
 -7.49816559e-03  2.41219867e-02  6.01893105e-02  1.61478184e-02
  1.78447012e-02 -1.27116680e-01 -2.42021084e-02  7.21493512e-02
  2.71104407e-02  6.47775158e-02 -6.89411014e-02  4.09465581e-02
 -7.13490881e-03  1.06205756e-04 -4.05286215e-02 -2.14892207e-03
  1.19332755e-02  7.97095746e-02 -9.66515690e-02 -4.59310412e-02
  5.67985699e-02 -1.48713849e-02  3.26530561e-02 -3.93250063e-02
 -1.59136988e-02  6.28231019e-02 -4.85252962e-02  9.75872800e-02
  6.97692335e-02  1.28209099e-01  2.46566217e-02 -9.90008339e-02
  6.69655055e-02  4.91048507e-02  3.35706063e-02  2.51826234e-02
  2.99578942e-02  1.99304018e-02 -5.16339056e-02 -3.14868700e-08
 -4.29441594e-02  3.44115719e-02  6.54975623e-02 -1.27405776e-02
  6.31017238e-02 -4.32792678e-02  9.58757568e-03  5.07630222e-02
  7.06601143e-02 -4.13283855e-02 -3.78897972e-02  1.93490721e-02
 -7.16373920e-02 -8.70913863e-02  4.09495085e-02  3.93487029e-02
 -3.31390575e-02  2.06020405e-03  3.59619968e-02 -5.54323308e-02
  1.40852416e-02  5.29116802e-02  1.50030786e-02 -1.16744917e-02
 -6.97561167e-03 -3.92670892e-02 -7.52673000e-02  5.90733029e-02
  9.23818815e-03  1.87582970e-02 -8.66294373e-03 -2.47814041e-02
  4.00134809e-02 -1.16053961e-01  3.08503844e-02  5.88669963e-02
 -6.37939593e-05  1.38291083e-02  1.26383845e-02  2.69445404e-02
  5.23503497e-03  4.10897657e-02  1.43797288e-03 -4.81909178e-02
  4.14385162e-02 -1.18311428e-01 -1.84987094e-02 -8.47382843e-02
 -2.16861684e-02 -1.81309823e-02  3.10110878e-02  4.37778272e-02
 -4.34492230e-02  8.13048556e-02 -8.11565574e-03 -1.48925968e-02
  4.26255874e-02 -1.47274226e-01  1.24657439e-04  9.85083804e-02
  6.31030202e-02  6.35531619e-02 -1.38046425e-02 -1.29321842e-02]",2,2
mxnet,3,apache mxnet is a deep learning framework designed for both efficiency and flexibility it allows you to mix the flavour of deep learning program together to maximize the efficiency and your productivity this package support linux mac osx and window platform you may also want to check to use this package on linux you need the libquadmath so shared library on debian based system including ubuntu run sudo apt install libquadmath to install the shared library on rhel based system including centos run sudo yum install libquadmath to install the shared library a libquadmath so is a gpl library and mxnet part of the apache software foundation mxnet must not redistribute libquadmath so a part of the pypi package and user must manually install it to install for other platform e g window raspberry pi arm or other version check installing mxnet for instruction on building from source to install use,"[('apache software foundation mxnet', 0.6215), ('apache mxnet', 0.5674), ('mxnet', 0.4958), ('mxnet part', 0.4734), ('package support linux mac osx', 0.4244), ('deep learning framework', 0.3965), ('deep learning program', 0.3558), ('pypi package', 0.3225), ('other platform e g window raspberry', 0.3107), ('linux', 0.3027)]","[-1.66534670e-02 -1.04744330e-01  8.79987702e-02 -1.08676568e-01
  1.32461369e-01 -1.66125074e-02 -3.02309468e-02  7.50988210e-03
 -3.39999981e-02 -4.31076139e-02  3.09749730e-02  3.21690962e-02
 -6.93150237e-02 -9.88664571e-04  5.50997965e-02  9.41910222e-03
  1.26906885e-02  3.36717255e-02  5.76712331e-03 -1.40596181e-02
  1.53087638e-02 -1.36936652e-02  9.08474531e-03 -2.20358819e-02
 -4.71156016e-02  4.39951606e-02 -8.71999376e-03 -2.53194738e-02
  1.69563685e-02 -6.91213533e-02 -5.70782879e-03  3.05425171e-02
  1.35069443e-02 -3.09055503e-02 -4.06342223e-02  3.00723109e-02
  3.08061279e-02  3.13900178e-03 -9.38137770e-02 -4.96159531e-02
 -6.49477467e-02  2.74199080e-02  8.60190243e-02 -9.12032183e-03
  8.76213983e-02 -2.69906875e-02  6.32159561e-02 -1.00898989e-01
  3.10445484e-02 -4.91306768e-04 -5.58067765e-03 -8.01027417e-02
  2.02977192e-02  9.32154879e-02  2.99725644e-02 -2.15541888e-02
 -1.29045263e-01 -4.90489453e-02 -9.87859792e-04 -4.76507330e-03
  2.63219401e-02  3.47087346e-02 -9.32164937e-02  5.20432964e-02
 -7.44475722e-02  2.74954680e-02  9.02300642e-04 -3.47099751e-02
  8.58025327e-02 -1.18161693e-01 -2.44904850e-02 -2.17712857e-02
  1.21518765e-02  7.61379078e-02 -7.79190511e-02  8.02287236e-02
  1.37851760e-01 -9.25699435e-03  5.19760251e-02 -2.04701461e-02
  3.26765180e-02  8.12846273e-02 -2.21043685e-03  2.99867205e-02
  3.97081710e-02 -7.23329335e-02 -3.46649787e-03  7.15605766e-02
 -6.24631867e-02  3.53574082e-02  1.29732331e-02  3.10010789e-03
  3.29944864e-02  5.00525460e-02 -4.15121317e-02 -2.19124760e-02
  3.89418856e-04 -4.01846021e-02 -8.66517946e-02  1.30088449e-01
 -8.74296725e-02 -2.85898410e-02  4.47355323e-02 -2.86526866e-02
 -5.10467179e-02  4.35981899e-02  2.29876228e-02  4.38031666e-02
  1.41691670e-01 -2.35384628e-02  7.68625457e-03 -5.05662113e-02
 -5.64955845e-02 -2.72029061e-02  3.17098224e-03 -9.44613814e-02
  4.55397414e-03  4.18779999e-02  5.10257669e-02  4.68373708e-02
 -5.70489019e-02  2.77829580e-02 -3.38932946e-02 -7.30180815e-02
  2.70319674e-02 -2.31024600e-03 -4.70131449e-02  5.04049994e-33
 -3.53603414e-03 -3.34976763e-02 -6.36619627e-02 -7.42347986e-02
  6.83426708e-02 -1.21291630e-01  9.95434299e-02 -1.72591694e-02
 -4.13787216e-02 -3.25884111e-02  5.62376436e-03  3.11148074e-02
 -5.02694175e-02  9.54207405e-02  9.04739951e-04 -4.43593599e-02
  2.48369537e-02 -2.99068983e-03  8.38646740e-02  1.87378115e-04
  2.10590716e-02  4.97509129e-02  4.29935679e-02  1.68559216e-02
  2.37328410e-02 -5.01560122e-02  3.08811329e-02 -7.03675821e-02
  4.96049747e-02  4.10771891e-02 -3.22127305e-02  3.04016541e-03
  7.14942440e-03 -5.44163305e-03 -3.46794575e-02 -4.64091217e-03
 -2.39673723e-02 -7.15470016e-02 -2.25188565e-02 -8.88960029e-04
 -1.32978231e-01  1.43270930e-02 -1.24152265e-02 -2.15993021e-02
  1.55658852e-02 -9.04134437e-02 -3.55991237e-02  1.59748346e-02
  6.68400200e-03 -2.10465491e-02 -5.52514121e-02 -7.48861805e-02
  4.28455397e-02 -4.96697100e-03 -8.10662750e-03  7.41022602e-02
 -1.96018908e-02  7.32566193e-02  2.03384645e-02  4.69398685e-02
 -6.02363348e-02 -5.90209030e-02 -3.36030498e-02 -1.81048624e-02
  6.01562522e-02 -2.63094506e-03  5.88703007e-02 -1.07935712e-01
  5.52174752e-04  6.30526319e-02 -8.23767707e-02 -1.82306615e-03
  5.95107079e-02  6.09831139e-02 -5.07089235e-02 -1.52680632e-02
 -2.53327768e-02 -2.22132821e-02  1.14750611e-02  8.08619112e-02
 -9.14754272e-02  6.59929663e-02  4.98909242e-02  7.02991411e-02
  5.27385529e-03  8.34564418e-02  5.03285117e-02  7.64106438e-02
  4.77027334e-02  7.40499003e-03 -1.85386632e-02 -2.60250326e-02
  4.54930142e-02 -3.49421520e-04 -5.61250858e-02 -4.01331220e-33
 -5.54440580e-02  2.28862390e-02 -1.25492975e-01  3.82208750e-02
 -1.54281678e-02  1.05362341e-01 -6.52223825e-04 -2.03437060e-02
 -8.91510323e-02 -2.71116886e-02  9.97239444e-03 -1.58715602e-02
  6.02262132e-02 -4.03084010e-02 -4.06460837e-02 -8.46513808e-02
 -7.42672337e-03 -7.09861889e-02  2.19751075e-02 -2.33996902e-02
 -6.88822418e-02  6.85536191e-02  2.83529572e-02  4.66100406e-03
  1.44028505e-02 -5.22316583e-02 -6.15742384e-03  1.09078260e-02
 -4.08601612e-02 -8.05778708e-03 -1.46835521e-02  2.36571692e-02
  8.44911858e-03 -5.51332161e-02  5.38555980e-02  5.92389442e-02
  2.62337979e-02 -1.38381552e-02 -1.55216036e-02  2.04703249e-02
  1.14293039e-01 -1.80969648e-02 -1.06248492e-02 -1.04406325e-03
 -7.28921639e-03 -1.28917545e-02 -1.02329411e-01  5.91930076e-02
  5.82525283e-02 -5.70582375e-02 -2.86051445e-02 -3.27740684e-02
  1.00989658e-02 -2.12694537e-02  6.22803979e-02  6.35032542e-03
  2.95922216e-02  3.81486341e-02  4.22584713e-02  6.58824071e-02
 -6.48649633e-02 -4.89871725e-02  2.05727899e-03 -2.18088040e-03
  9.46781319e-03  7.46323913e-02 -8.65128785e-02  3.57325189e-02
 -8.40592459e-02  1.50164207e-02  3.84988114e-02  4.83317161e-03
  6.83697686e-02  1.25921872e-02 -2.73518804e-02 -3.32212858e-02
  2.67538168e-02  1.56770889e-02 -1.83485039e-02 -7.96095352e-04
  1.10800639e-01  1.02908738e-01  2.26878747e-03  5.96970282e-02
 -2.00759750e-02  1.71403252e-02  2.64996011e-02 -3.09452284e-02
  4.98600267e-02 -1.25780459e-02  5.08337803e-02  5.33148907e-02
 -3.31050679e-02  2.26965193e-02 -1.73344426e-02 -3.02270955e-08
  4.39157113e-02 -2.17199535e-03 -4.70412001e-02 -2.57749036e-02
  6.73585106e-03  7.18478560e-02  1.00970969e-01  2.83027142e-02
  4.09500003e-02  5.12930863e-02  1.03603721e-01 -4.24391925e-02
 -9.96718332e-02 -3.87577713e-02 -3.06865778e-02  8.17529112e-02
  6.90098992e-03  9.17962790e-02  1.26631362e-02 -4.21804599e-02
 -6.36975560e-03  5.90633675e-02 -2.90638022e-02 -7.20726838e-03
  1.81410741e-02 -2.36784089e-02  3.22921202e-02  3.44617814e-02
 -1.83809511e-02  1.55406278e-02 -9.00957957e-02 -1.44714722e-02
 -1.29303653e-02 -6.31999895e-02  7.81876072e-02  4.49006855e-02
 -3.44643816e-02 -3.52110947e-03  1.93539653e-02 -3.90329175e-02
 -4.37734090e-02  2.20659282e-02  4.93710935e-02 -8.38555247e-02
 -1.13495290e-02 -1.24331675e-02 -5.48070744e-02 -6.82323286e-03
 -2.85516288e-02 -1.63958389e-02  5.11466414e-02  3.03223282e-02
  2.55965982e-02  3.12191453e-02 -9.76150855e-03  3.86315845e-02
  4.74888422e-02 -1.66026726e-01 -1.05921607e-02 -2.74855141e-02
 -3.02085709e-02 -2.57993974e-02  7.04609379e-02  2.83447243e-02]",2,2
comet_ml,3,full documentation and additional training example are available on http www comet com doc sign up free on comet com and obtain an api key at http www comet comthe core class of comet ml is an experiment a specific run of a script that generated a result such a training a model on a single set of hyper parameter an experiment will automatically log script output stdout stderr code and command line argument on any script and for the supported library will also log hyper parameter metric and model configuration here is the experiment object we all strive to be data driven and yet every day valuable experiment result are just lost and forgotten comet ml provides a dead simple way of fixing that work with any workflow any ml task any machine and any piece of code for a more in depth tutorial about comet ml you can check out or doc http www comet com doc copyright c comet ml inc this package can not be copied and or distributed without the express permission of comet ml inc,"[('http www comet comthe core class', 0.6267), ('comet ml', 0.5808), ('comet ml inc', 0.5406), ('http www comet com', 0.5103), ('comet com', 0.5084), ('full documentation', 0.4515), ('ml task', 0.3404), ('additional training example', 0.3181), ('code', 0.2745), ('depth tutorial', 0.2715)]","[-7.07992464e-02 -9.19485763e-02 -9.93403234e-03 -3.48289646e-02
 -8.88121780e-03 -4.59308177e-02 -6.10990450e-02  8.24666172e-02
 -1.09051824e-01  3.78495865e-02 -7.45140482e-03  2.42694169e-02
  8.46101064e-03 -8.87489393e-02  2.08634380e-02  6.50916109e-03
  5.51089607e-02 -5.93339978e-03 -1.35400612e-02 -1.27305180e-01
  2.48035695e-02 -3.81838866e-02 -2.96927001e-02  9.03706998e-03
  1.68390218e-02  6.69594631e-02  1.33582475e-02 -1.76083893e-02
  7.95435384e-02 -9.80369300e-02 -1.72626134e-02 -2.70741805e-03
  5.12904711e-02  3.26400772e-02  2.76203249e-02  4.03087065e-02
 -9.37024131e-03 -6.08060621e-02 -1.11751156e-02 -1.15105668e-02
 -2.49859504e-02 -8.03378373e-02  6.62236288e-03 -3.35190706e-02
  7.76875615e-02  1.50819924e-02 -6.30046204e-02 -8.90705287e-02
  2.79905424e-02  4.08475585e-02 -6.56695291e-02 -1.21377163e-01
  1.22751447e-03 -3.46585661e-02 -6.35524020e-02  3.10626458e-02
  2.69788355e-02  5.28943464e-02  2.96155401e-02 -5.20188995e-02
  2.63775582e-03 -4.71938550e-02 -8.45373869e-02  5.21958731e-02
  2.33963598e-02 -2.26475131e-02 -1.58651255e-03  4.25640568e-02
  7.47231841e-02 -3.79693359e-02 -9.44904462e-02  5.18125407e-02
 -7.09941909e-02  7.07942024e-02  1.33851487e-02  7.24142790e-03
  4.96381447e-02 -2.55034808e-02 -1.19350143e-02  2.50993539e-02
 -5.54876681e-03  3.93683240e-02  2.16932874e-02 -1.29517894e-02
  1.67756435e-02  2.07578344e-03  3.91272344e-02  9.50141326e-02
  1.78082194e-02 -5.53680882e-02  3.66812348e-02 -7.14218989e-02
 -2.42987722e-02  5.33799715e-02 -1.01936921e-01  7.10861310e-02
  5.80660887e-02 -7.70285279e-02  2.11145394e-02  4.92493249e-02
 -3.04995812e-02 -2.14845743e-02  8.71945638e-03 -9.43312887e-03
 -8.51919726e-02 -7.43128499e-03  1.42735587e-02  9.11358446e-02
  1.29343763e-01 -4.73923795e-02 -6.69615790e-02 -3.19896005e-02
 -4.86634597e-02 -1.33461818e-01  3.39607382e-03 -2.01037582e-02
 -4.80629271e-03 -8.81518982e-03  2.58701928e-02  1.91420428e-02
  2.95243878e-02 -2.55163852e-02  2.05965303e-02 -2.75337510e-02
 -5.36065176e-03 -4.23021577e-02 -7.63280988e-02  1.50001234e-33
  8.02003369e-02  5.14050834e-02 -2.26141363e-02 -2.23140102e-02
  4.76048179e-02 -6.02765717e-02  3.43938209e-02  8.32741931e-02
 -6.35377169e-02 -3.08437441e-02  1.83972530e-02  2.88620256e-02
 -2.11025737e-02  7.18054771e-02 -4.70865332e-02 -2.99648307e-02
  1.43159898e-02  5.08315824e-02 -1.62493847e-02 -2.30318066e-02
 -2.80054901e-02  9.53577459e-03  4.51682881e-02  3.78405824e-02
  4.18843180e-02  1.58903986e-01  4.62295189e-02  4.14149724e-02
  3.79792415e-02  4.84398119e-02  6.94884360e-02 -4.93510775e-02
 -4.81020845e-02  4.13201861e-02  6.01282232e-02  6.45643771e-02
 -6.61523044e-02 -3.72617431e-02 -1.34845376e-02 -1.99569315e-02
 -6.44303933e-02  1.24773141e-02 -5.43138869e-02 -3.21853384e-02
 -6.47856214e-04 -1.17473520e-01  1.24096368e-02 -7.00286180e-02
  7.01194778e-02 -4.29666825e-02 -4.68625017e-02  1.02403192e-02
  1.77704114e-02  1.11018904e-02  9.52105075e-02  3.74358706e-02
  1.30935451e-02  7.24075288e-02  2.41260906e-03  5.79582974e-02
  1.12162465e-02 -4.09140531e-03 -4.67204116e-02  2.22557299e-02
 -2.79896408e-02  4.32941802e-02 -6.77745938e-02 -3.85388099e-02
  1.17851220e-01 -4.38401476e-03 -1.08005805e-02  5.33389822e-02
  5.36379628e-02 -1.62852760e-02 -4.31755669e-02  6.36097230e-03
  8.00626352e-03 -2.78881919e-02 -5.33366241e-02  2.03699269e-03
 -4.55903783e-02 -4.28179167e-02  6.83250427e-02  2.12441217e-02
 -6.15867041e-02 -5.59798107e-02  3.03592812e-02  1.15388306e-02
  3.16578932e-02 -4.36139964e-02 -1.75467934e-02 -2.58384366e-03
  5.72587457e-03  9.00770631e-03  3.34335342e-02 -1.02760678e-33
 -1.76915433e-02  1.10440873e-01 -8.50661397e-02  7.04157948e-02
  3.02703902e-02  6.14287630e-02  6.33549914e-02  2.25128196e-02
 -7.34793767e-02  5.67333624e-02 -3.11428327e-02  1.49257090e-02
 -4.95975129e-02 -3.16082202e-02 -5.51974261e-03 -3.52982506e-02
  1.91479188e-03 -7.43133798e-02 -3.12749296e-02 -2.99483468e-03
 -1.26732942e-02  8.15709680e-02 -3.44386585e-02 -7.74906874e-02
  4.83478568e-02 -2.87516005e-02 -2.38417578e-03 -1.15894182e-02
  3.37948799e-02 -1.78481769e-02 -3.21920998e-02  2.75721736e-02
 -3.00722998e-02  1.18509801e-02 -5.12081422e-02 -2.56408675e-04
  1.00601673e-01  9.80021432e-02 -3.66188288e-02 -1.98480971e-02
  9.65265036e-02 -3.95680182e-02  4.84246155e-03 -1.00265317e-01
  3.90338637e-02  1.06728384e-02 -2.26281304e-02  3.42419967e-02
  4.41886000e-02  2.76720095e-02 -2.47799195e-02 -7.22961053e-02
  6.40643900e-03 -3.75079811e-02  1.68169308e-02 -5.28216809e-02
  6.29481627e-03  9.77984164e-03  3.82541269e-02 -1.53907062e-02
 -4.03758623e-02 -6.26025200e-02 -2.17925869e-02  1.05972677e-01
  5.59662562e-03 -7.50716962e-03 -5.52668050e-02  8.85461792e-02
 -9.38758478e-02  2.25281697e-02 -1.22369165e-02 -9.49281640e-03
  4.28895541e-02 -1.57311913e-02  3.00937071e-02 -4.14967090e-02
 -5.00682145e-02  2.84559894e-02 -2.57679000e-02 -2.87547559e-02
  2.41267085e-02 -8.01210571e-03  4.08562226e-03  6.26866147e-02
  5.89139648e-02  7.38240331e-02  5.81583530e-02 -2.61641797e-02
 -1.09869093e-01 -3.29516120e-02 -1.14719473e-01  9.30428579e-02
  7.16972165e-03  9.50004235e-02 -3.66165489e-02 -2.31696298e-08
  3.71591449e-02  8.13949946e-03 -8.70279968e-03  1.94094926e-02
  5.40984757e-02  5.23283221e-02 -1.75565537e-02  8.20020214e-02
  2.11357754e-02  8.48407522e-02 -3.26922834e-02 -3.29940990e-02
  1.57340206e-02  2.65801493e-02  6.04446754e-02  8.20514038e-02
 -3.44955572e-03 -1.07891038e-02 -3.13367173e-02 -4.89871241e-02
  3.98519076e-02  5.81164546e-02  4.86698933e-02 -2.89034285e-02
  2.98867039e-02 -1.35866441e-02  3.88030596e-02  1.13468342e-01
 -6.07454265e-03 -9.66961086e-02 -1.02501005e-01  8.60165209e-02
  2.20259186e-02 -1.30143702e-01  3.77798118e-02  6.29250407e-02
 -6.13320270e-04  2.64404882e-02  2.09143143e-02  1.18754804e-02
 -1.26186963e-02  4.22818176e-02  2.87053008e-02  3.50544299e-03
  1.24839075e-01  7.21343234e-02 -5.88790653e-03 -9.41199288e-02
 -4.68130857e-02 -2.87751108e-02 -4.80962694e-02 -2.20460091e-02
 -3.71888354e-02 -3.67903486e-02  1.26132872e-02  1.06618032e-01
  5.94631359e-02 -8.84676203e-02 -4.86237416e-03  3.62885781e-02
 -1.94628425e-02  8.12118202e-02 -3.06016300e-02 -3.55431752e-04]",2,0
imbalanced-learn,3,imbalanced learn is a python package offering a number of re sampling technique commonly used in datasets showing strong between class imbalance it is compatible with scikit learn and is part of scikit learn contrib project installation documentation api documentation and example can be found on the documentation imbalanced learn requires the following dependency python numpy scipy scikit learn additionally imbalanced learn requires the following optional dependency panda for dealing with dataframestensorflow for dealing with tensorflow modelskeras for dealing with kera modelsthe example will requires the following additional dependency matplotlib seaborn imbalanced learn is currently available on the pypi s repository and you can install it via pip the package is release also in anaconda cloud platform if you prefer you can clone it and run the setup py file use the following command to get a copy from github and install all dependency be aware that you can install in developer mode with if you wish to make pull request on github we advise you to install pre commit after installation you can use pytest to run the test suite the development of this scikit learn contrib is in line with the one of the scikit learn community therefore you can refer to their development guide if you use imbalanced learn in a scientific publication we would appreciate citation to the following paper most classification algorithm will only perform optimally when the number of sample of each class is roughly the same highly skewed datasets where the minority is heavily outnumbered by one or more class have proven to be a challenge while at the same time becoming more and more common one way of addressing this issue is by re sampling the dataset a to offset this imbalance with the hope of arriving at a more robust and fair decision boundary than you would otherwise under sampling the majority class e over sampling the minority class combining over and under sampling create ensemble balanced set below is a list of the method currently implemented in this module random majority under sampling with replacementextraction of majority minority tomek link under sampling with cluster centroidsnearmiss condensed nearest neighbour one sided selection neighboorhood cleaning rule edited nearest neighbour instance hardness threshold repeated edited nearest neighbour allknn random minority over sampling with replacementsmote synthetic minority over sampling technique smotenc smote for nominal and continuous smoten smote for nominal bsmote borderline smote of type and svm smote support vector smote adasyn adaptive synthetic sampling approach for imbalanced learning kmeans smote rose random oversampling example smote tomek link smote enn easy ensemble classifier balanced random forest balanced baggingrusboost mini batch resampling for kera and tensorflowthe different algorithm are presented in the sphinx gallery i tomek two modification of cnn ieee transaction on system man and cybernetics vol pp i mani j zhang knn approach to unbalanced data distribution a case study involving information extraction in proceeding of the workshop on learning from imbalanced data set pp p e hart the condensed nearest neighbor rule ieee transaction on information theory vol pp m kubat s matwin addressing the curse of imbalanced training set one sided selection in proceeding of the th international conference on machine learning vol pp j laurikkala improving identification of difficult small class by balancing class distribution proceeding of the th conference on artificial intelligence in medicine in europe pp d wilson asymptotic property of nearest neighbor rule using edited data ieee transaction on system man and cybernetrics vol pp m r smith t martinez c giraud carrier an instance level analysis of data complexity machine learning vol pp n v chawla k w bowyer l o hall w p kegelmeyer smote synthetic minority over sampling technique journal of artificial intelligence research vol pp h han w y wang b h mao borderline smote a new over sampling method in imbalanced data set learning in proceeding of the st international conference on intelligent computing pp h m nguyen e w cooper k kamei borderline over sampling for imbalanced data classification in proceeding of the th international workshop on computational intelligence and application pp g e a p a batista r c prati m c monard a study of the behavior of several method for balancing machine learning training data acm sigkdd exploration newsletter vol pp g e a p a batista a l c bazzan m c monard balancing training data for automated annotation of keywords a case study in proceeding of the nd brazilian workshop on bioinformatics pp x y liu j wu and z h zhou exploratory undersampling for class imbalance learning ieee transaction on system man and cybernetics vol pp i tomek an experiment with the edited nearest neighbor rule ieee transaction on system man and cybernetics vol pp h he y bai e a garcia s li adasyn adaptive synthetic sampling approach for imbalanced learning in proceeding of the th ieee international joint conference on neural network pp c chao a liaw and l breiman using random forest to learn imbalanced data university of california berkeley felix last georgios douzas fernando bacao oversampling for imbalanced learning based on k mean and smote seiffert c khoshgoftaar t m van hulse j napolitano a rusboost a hybrid approach to alleviating class imbalance ieee transaction on system man and cybernetics part a system and human menardi g torelli n training and assessing classification rule with unbalanced data data mining and knowledge discovery,"[('imbalanced data classification', 0.5116), ('imbalanced data university', 0.5011), ('imbalanced learn', 0.4994), ('imbalanced training', 0.4831), ('imbalanced learning', 0.4778), ('scikit', 0.4614), ('imbalanced data', 0.4553), ('class imbalance', 0.4526), ('tensorflow modelskeras', 0.4076), ('tensorflowthe', 0.4062)]","[-5.08030318e-02 -8.03081915e-02  3.76982056e-02 -4.58917357e-02
  6.08450435e-02 -7.15286434e-02 -1.67584736e-02  4.09809425e-02
 -7.94456378e-02 -8.43170751e-03 -5.23268580e-02 -5.67832068e-02
 -2.94088274e-02 -1.22979181e-02 -4.09379639e-02  5.12222410e-04
 -3.24251801e-02  8.94565284e-02 -3.13719809e-02 -1.08713722e-02
 -4.51363660e-02  4.82278923e-03  2.29433160e-02  7.45453686e-02
 -2.79985853e-02 -2.67581511e-02  3.65659632e-02 -6.67859539e-02
 -3.16733271e-02 -7.46704638e-02 -5.24927229e-02  3.63485068e-02
  4.69660424e-02  2.94711604e-03 -9.28529128e-02 -6.66116849e-02
  3.86612006e-02 -2.78345589e-02 -3.32406834e-02 -1.10781947e-02
 -1.08055789e-02 -4.02211808e-02  2.21306048e-02  3.71730849e-02
  4.26550172e-02  2.69768909e-02 -5.55414846e-03 -9.76583585e-02
  3.39557193e-02  4.57728654e-02 -8.53163376e-02 -8.85676742e-02
 -1.05567642e-01  8.76145139e-02 -2.40210071e-02 -9.00993049e-02
  4.26532961e-02 -2.96854023e-02 -1.74816083e-02  7.49902241e-03
  4.35224473e-02 -3.67814116e-02 -8.63054860e-03  5.29175960e-02
 -1.90649070e-02 -1.24475583e-02  4.62934654e-03  8.75040591e-02
  6.99558929e-02 -5.09313345e-02  1.81339458e-02 -2.20994148e-02
 -9.00687277e-02  1.03715565e-02  7.59682730e-02  5.59467711e-02
  8.31140652e-02 -8.13498721e-03  8.13843384e-02 -2.63840612e-03
 -8.29673558e-02 -1.22736357e-02  1.65380053e-02 -5.14504462e-02
 -1.16157755e-02 -4.27329391e-02 -6.08136952e-02  4.17345688e-02
 -5.90669969e-03  1.10519547e-02  6.44664690e-02  9.55806021e-03
  8.18995014e-03  3.73988003e-02 -9.80889797e-03  7.29356930e-02
 -7.42536038e-02 -5.05174883e-02  2.87656784e-02  3.09149567e-02
 -8.28707516e-02  2.76824385e-02 -1.69935245e-02  9.45685878e-02
 -7.19994903e-02 -2.86008324e-02  5.24857938e-02 -6.31220043e-02
  8.26114714e-02 -1.02603277e-02  5.78789599e-03  4.81689312e-02
 -2.62342487e-03 -9.96124893e-02  4.74002585e-02  4.50913571e-02
 -3.46998200e-02  4.62451465e-02 -2.28334256e-02  1.43114358e-01
 -1.30613789e-01  2.84354594e-02 -2.52397805e-02  1.94252450e-02
 -6.78986078e-03 -2.90923975e-02 -1.38475776e-01  4.34136696e-33
  1.18740676e-02 -6.05867691e-02  6.95593134e-02 -2.83594374e-02
  8.48767348e-03 -8.46220031e-02 -4.25117873e-02  2.20545884e-02
 -5.40082157e-02 -5.53948283e-02 -3.20357047e-02  1.04981892e-01
 -1.20333470e-02  5.85897118e-02  1.22465612e-02 -5.84457852e-02
  3.04565951e-02  6.57486171e-02 -1.19936811e-02  4.32858951e-02
  7.75362253e-02 -7.11695338e-03  9.63882077e-03  4.70097587e-02
  2.89512668e-02 -8.23272392e-03 -8.72980356e-02  4.21265922e-02
  1.49061354e-02  2.26291083e-02 -4.47613411e-02 -7.02157570e-03
 -1.03020743e-01 -3.52329202e-02 -3.03626992e-02 -1.84822013e-03
 -2.73441779e-03  3.86210680e-02 -7.96012878e-02 -7.09799603e-02
 -5.12042455e-02  2.39117090e-02 -4.47514914e-02 -4.12379242e-02
  2.49207336e-02 -3.88278700e-02  1.12314643e-02  1.04029421e-02
  7.94739798e-02 -1.28642870e-02 -4.03643176e-02 -5.66980988e-02
 -2.39409097e-02 -2.73772590e-02  2.82551460e-02  7.41687864e-02
  7.11762998e-03 -4.33302531e-03 -9.49268788e-03  5.49598336e-02
 -7.02388957e-02 -2.36871801e-02  6.54088259e-02 -8.84237811e-02
 -7.02579841e-02 -3.84343639e-02 -2.80808192e-02  7.05869985e-04
 -3.81317921e-02  4.05408219e-02 -5.85734881e-02  4.65517119e-02
 -9.51915011e-02  5.48259988e-02 -1.65785961e-02  4.29598195e-03
  1.13279708e-02 -4.01394162e-03 -4.07405831e-02  6.97582290e-02
 -4.71393615e-02  1.10849189e-02 -2.36823615e-02 -1.21291745e-02
 -5.82050458e-02  6.84081092e-02  2.14035865e-02 -4.59343642e-02
  2.79413331e-02  2.70013269e-02 -1.24015495e-01 -1.60815082e-02
  4.97018732e-02  3.86164039e-02 -5.29026464e-02 -5.43411128e-33
 -6.47686869e-02  5.37268370e-02 -1.28202960e-01  8.75796154e-02
  2.49528252e-02  1.19108886e-01  5.20436577e-02  4.77631129e-02
  3.14733647e-02  5.63695207e-02  3.59607562e-02 -7.07092870e-04
 -4.72179987e-02 -5.35970442e-02 -6.09209277e-02 -4.96204980e-02
 -3.15963365e-02 -2.53647510e-02 -2.50171088e-02  6.20728396e-02
 -5.05873486e-02  9.84671786e-02 -5.08324392e-02  1.07798073e-02
  1.13271270e-02  5.13451779e-03 -7.89959803e-02  3.48382704e-02
  8.41905102e-02  1.66338235e-02 -1.91558916e-02 -2.63166837e-02
 -3.65959667e-03  8.30557421e-02 -2.29339171e-02 -1.28134340e-02
  9.55842212e-02 -2.03425940e-02  3.52043472e-02  5.59570044e-02
  1.02281056e-01  1.05028354e-01  8.98369122e-03  5.41803613e-02
 -6.82596564e-02 -5.68575040e-02 -6.47994876e-02  6.59973472e-02
  1.33274514e-02 -2.69594695e-02 -3.68039012e-02  8.65390245e-03
 -8.21252912e-03  2.68439949e-03 -1.29335490e-03 -9.02918540e-03
  8.24602768e-02  2.38653887e-02  1.57849427e-04  9.41281095e-02
 -5.25807589e-02  1.91508532e-02 -2.39676591e-02 -5.07736728e-02
 -2.18441095e-02 -3.17616202e-02 -2.59120706e-02  1.70688238e-02
  1.88052244e-02  6.76935539e-03  2.49455813e-02  8.01093802e-02
  3.72490436e-02 -5.20370388e-03  5.23846373e-02 -1.06098792e-02
  3.96877155e-02  6.34606034e-02 -3.34964041e-03 -4.31591794e-02
  5.27397133e-02 -2.80149351e-03  1.04605285e-02  1.14980049e-01
  1.72782000e-02  7.69902766e-02  8.46430808e-02 -4.97206785e-02
  9.33557302e-02 -3.84352095e-02 -5.78566007e-02  1.46282054e-02
  3.22297998e-02  6.03881478e-02  5.87953022e-04 -2.60869744e-08
 -1.34788183e-02  5.88736311e-03  1.13510601e-01  5.09506091e-02
  5.46800019e-03  3.15277874e-02 -9.22614783e-02  1.11304916e-01
 -3.91804688e-02  6.10685758e-02 -2.06537060e-02  2.20070612e-02
 -1.00619376e-01 -2.04980373e-02  7.61685371e-02 -3.33974883e-03
 -2.13771332e-02  2.31513288e-02  1.81652997e-02 -4.85832250e-04
  1.84953061e-03 -2.49894429e-02 -2.61388104e-02 -6.66375365e-03
  4.38284129e-02 -6.47101626e-02  2.97083464e-02  1.76385287e-02
 -1.03094545e-03  2.80521568e-02 -6.28299341e-02 -2.90452018e-02
  3.13194171e-02 -9.59117264e-02  6.71421587e-02  4.97577824e-02
  2.09438633e-02 -7.64032304e-02 -6.96936324e-02  6.37163743e-02
 -2.93969214e-02  3.32591198e-02  3.67766656e-02 -2.61182971e-02
  6.37346432e-02 -2.63877660e-02 -1.84289049e-02 -4.30876436e-03
  1.98583417e-02 -3.13863829e-02 -3.91654447e-02  3.04140802e-02
 -2.17002667e-02  6.70040399e-02  1.86494309e-02  6.95758089e-02
  1.49764661e-02 -8.10174048e-02 -3.10824402e-02 -2.58859880e-02
  8.75714943e-02  1.46109331e-03 -3.49335782e-02  2.80615557e-02]",2,0
tensorflow-transform,3,tensorflow transform is a library for preprocessing data with tensorflow tf transform is useful for data that requires a full pas such a tensorflow ha built in support for manipulation on a single example or a batch of example tf transform extends these capability to support full pass over the example data the output of tf transform is exported a a tensorflow graph to use for training and serving using the same graph for both training and serving can prevent skew since the same transformation are applied in both stage for an introduction to tf transform see the tf transform section of the tfx dev summit talk on tfx link caution tf transform may be backwards incompatible before version the tensorflow transform pypi package is the recommended way to install tf transform to build from source follow the following step create a virtual environment by running the commandsthis will build the tft wheel in the dist directory to install the wheel from dist directory run the commandstft also host nightly package at http pypi nightly tensorflow org on google cloud to install the latest nightly package please use the following command this will install the nightly package for the major dependency of tft such a tensorflow metadata tfmd tfx basic shared library tfx bsl tensorflow is required apache beam is required it s the way that efficient distributed computation is supported by default apache beam run in local mode but can also run in distributed mode using google cloud dataflow and other apache beam runner apache arrow is also required tft us arrow to represent data internally in order to make use of vectorized numpy function the following table is the tf transform package version that are compatible with each other this is determined by our testing framework but other untested combination may also work please direct any question about working with tf transform to stack overflow using the tensorflow transform tag,"[('tf transform package version', 0.7093), ('library tfx bsl tensorflow', 0.6915), ('tensorflow transform', 0.6459), ('tensorflow metadata tfmd tfx', 0.6402), ('tensorflow transform tag', 0.5831), ('tensorflow', 0.5676), ('tf transform section', 0.5522), ('tensorflow org', 0.5438), ('tensorflow graph', 0.5053), ('tfx dev summit talk', 0.4772)]","[-3.52381766e-02 -6.82597682e-02  7.41812226e-04 -7.11350068e-02
  9.39757600e-02 -2.61346158e-02 -2.43642777e-02  5.21681784e-03
 -3.19568142e-02 -1.32400803e-02 -5.85773438e-02 -4.26875241e-03
 -7.87274688e-02  3.16872522e-02  4.29103449e-02  9.33394581e-03
 -8.93938318e-02  5.35055511e-02  1.28609324e-02 -9.22747701e-02
 -5.64186238e-02  6.48146495e-02  2.78496370e-02  5.39151579e-02
  1.14466585e-02 -4.71308315e-03 -5.25063202e-02 -6.98752850e-02
  1.78319048e-02  1.81638673e-02 -5.74342869e-02  1.01133347e-01
  3.10069160e-03 -2.34475229e-02 -3.22822630e-02 -1.67700946e-02
 -1.40876882e-02 -2.88787521e-02 -3.91825996e-02  1.18979625e-02
 -3.33245210e-02 -1.99732725e-02 -5.03592938e-02  6.34838315e-03
  1.47523079e-02 -1.06909210e-02  6.00387827e-02 -5.14495075e-02
 -1.66549794e-02  3.92321832e-02 -4.27154824e-02 -5.89738004e-02
 -6.34657443e-02  5.12342304e-02  1.06073907e-02  4.46783528e-02
  6.36681989e-02 -4.06975336e-02 -8.87015928e-03  4.27847244e-02
 -2.88196895e-02 -4.16527539e-02 -3.78333293e-02  2.03490276e-02
 -1.08643780e-02  1.05709761e-01 -7.35277906e-02  7.23107252e-04
  5.03397547e-02 -1.26884237e-01 -1.10635683e-01 -2.76407991e-02
  1.69650055e-02  1.24818375e-02 -2.90663857e-02  2.26707626e-02
  1.26963302e-01 -8.62637460e-02  5.40784672e-02 -8.35831240e-02
 -5.98487584e-03 -1.53847327e-02  6.10706545e-02 -4.02020961e-02
  1.96341444e-02 -1.32893212e-02  2.26659663e-02  2.49766428e-02
 -2.44044289e-02  2.68992651e-02  2.25259196e-02 -1.29350303e-02
  1.26996767e-02  1.88646615e-02  4.62939143e-02  2.03315336e-02
 -8.77946466e-02 -5.12087904e-03 -1.98930819e-02  2.23981645e-02
 -9.93329659e-02 -8.80266577e-02  7.25554377e-02  3.96611877e-02
 -8.18257406e-02  6.72737285e-02  2.70665791e-02  1.09439110e-02
  2.18566842e-02  2.03129165e-02  6.94164401e-03  9.30500478e-02
  4.26335596e-02 -9.60914195e-02  4.88986112e-02 -1.20875321e-03
 -2.88273897e-02  3.03048883e-02  2.90618930e-02  5.08069880e-02
 -5.22755161e-02  4.32600901e-02 -6.21218467e-04  9.86324623e-03
 -5.04345000e-02  3.24813202e-02 -3.99858430e-02  6.83193129e-33
  3.81328575e-02  4.90121841e-02  1.66383032e-02  5.18028140e-02
  2.96652950e-02 -3.82020883e-02  2.86900960e-02 -2.73426026e-02
 -8.13744497e-03  8.59990157e-03 -2.39083115e-02  8.75524804e-02
 -1.02711879e-02  5.91697209e-02 -9.62760597e-02 -1.56228259e-01
  3.01655065e-02  3.54254693e-02  4.13851440e-02  2.75449436e-02
  4.45263237e-02  7.97949731e-02  6.79150829e-03  9.09360498e-02
  6.41525015e-02  5.65654784e-02 -1.50856292e-02 -1.75885931e-02
  9.88887530e-03  2.39144452e-02 -1.05154984e-01 -3.19447070e-02
 -2.12937742e-02 -6.34994656e-02  2.61138435e-02 -2.69845985e-02
 -4.82752919e-02  3.29779158e-03 -4.45471630e-02 -1.03617191e-01
  2.29016226e-02  4.27201726e-02 -3.36949788e-02 -8.06389078e-02
  7.93139171e-03 -1.65056624e-02  3.54199670e-02  2.94247344e-02
  1.30877331e-01 -5.83443269e-02 -1.98196061e-02 -4.35265675e-02
 -5.39275222e-02 -1.36614926e-02  6.26354069e-02 -2.41170358e-02
  6.50095865e-02 -1.62789691e-02  7.61382729e-02  6.17777780e-02
 -1.07890209e-02 -2.21175067e-02 -1.89519674e-02 -5.64927384e-02
  5.67844361e-02 -4.79802974e-02 -4.09689061e-02 -2.83813383e-02
 -1.32150901e-02 -5.30590527e-02 -7.94230327e-02  1.16608545e-01
 -1.72947273e-02  7.16595864e-03 -2.00562980e-02 -2.67161429e-02
  1.21720871e-02 -8.65170266e-03 -2.86514661e-03 -2.98269168e-02
 -1.88633263e-01 -1.85007434e-02  4.19557393e-02 -5.22556715e-04
  2.19168905e-02  4.00334178e-03 -1.28693385e-02  3.25465314e-02
  2.70939060e-02 -5.97997122e-02 -4.06719111e-02 -1.84076633e-02
  3.81047255e-03  4.94908094e-02 -9.87760350e-03 -6.06609121e-33
 -3.22913341e-02  6.18354790e-03 -5.92196472e-02  6.52871281e-02
 -1.09474258e-02  4.48219292e-02  5.20753972e-02  2.15699282e-02
  7.71860257e-02  5.68790920e-02  6.25760853e-02 -5.16225249e-02
 -2.85150558e-02 -9.23601016e-02  4.62432690e-02 -1.12892181e-01
 -2.06037946e-02 -7.09312186e-02 -6.54102787e-02 -7.83279538e-03
 -2.19258741e-02  5.62099405e-02 -5.68458065e-02  1.08561502e-03
  2.57497262e-02  2.68815439e-02 -2.01200377e-02 -2.17907541e-02
  2.92901229e-02 -8.72812979e-03 -1.80418473e-02 -4.44651656e-02
 -2.80143861e-02  8.15918297e-02  3.62494327e-02  5.04376693e-03
  3.39114480e-02  3.22517427e-03  3.81216519e-02  3.99870239e-03
  2.33682785e-02  5.97356483e-02  4.74406034e-02  4.41224948e-02
 -8.68555382e-02  2.42873840e-02 -5.52852452e-02  3.77189778e-02
 -3.32125165e-02 -6.95561245e-02 -9.08759516e-03 -8.78709182e-03
  1.54773025e-02 -1.41607404e-01 -3.93506810e-02  1.64887104e-02
  1.29244074e-01 -9.41979047e-03  7.02707693e-02  1.28644183e-02
  2.02239044e-02  2.94782035e-02  3.71611305e-02 -5.41555434e-02
 -6.49605179e-03  9.99915320e-03 -1.30281329e-01 -1.83771935e-03
  5.82249183e-03  5.64626269e-02 -2.96920128e-02  3.55063714e-02
 -3.37880440e-02  2.19186917e-02  1.72605179e-02  1.53820459e-02
  6.90490901e-02  4.60564643e-02  5.75350933e-02 -2.69105248e-02
  4.61968370e-02  3.90586704e-02  1.51621392e-02  1.23154782e-01
  5.62529676e-02  6.34724125e-02  5.79050779e-02 -6.81659505e-02
  6.66591153e-02  1.01093436e-02 -4.21988443e-02 -1.97717771e-02
 -1.36502003e-02  9.08247828e-02  7.60917217e-02 -3.11480939e-08
 -1.61859572e-01  6.66771010e-02 -1.92288756e-02 -2.74946894e-02
  2.93048192e-02  8.59760419e-02  7.89343193e-02  1.18562974e-01
  5.24137588e-03  4.98890579e-02 -1.55887064e-02  5.75260166e-03
 -3.35775279e-02  1.87696815e-02 -3.17729567e-03 -1.40167484e-02
 -8.50078315e-02  2.71538016e-03  1.05413888e-02 -6.68707937e-02
 -2.32906211e-02  3.63415331e-02  1.00694792e-02 -2.92595234e-02
  9.63834021e-03 -5.36142588e-02  1.66873727e-02  5.10346293e-02
  3.22913914e-03 -5.29485010e-03 -4.86301258e-02 -1.90168824e-02
 -1.87460880e-03 -3.11626550e-02  1.01299971e-01  6.26417100e-02
 -4.91255941e-03 -2.34156288e-02 -3.54981795e-02  1.01699859e-01
 -3.84626091e-02  6.37915134e-02  2.81326678e-02 -2.64731254e-02
 -2.55479570e-02  1.49799157e-02 -7.38351494e-02  5.48865460e-03
 -2.87342798e-02 -1.59476139e-02  3.03415395e-03  3.57295126e-02
 -8.15619230e-02  9.47929546e-02 -2.91220322e-02  9.38850567e-02
 -3.12388334e-02 -6.51658028e-02  5.12400232e-02 -5.04278764e-02
 -1.67754740e-02  4.19597216e-02 -2.78252475e-02 -5.66780157e-02]",2,2
tensorpack,3,tensorpack is a neural network training interface based on tensorflow it s yet another tf high level api with speed and flexibility built together focus on training speed speed come for free with tensorpack it us tensorflow in the efficient way with no extra overhead on common cnns it run training x faster than the equivalent kera code your training can probably get faster if written with tensorpack data parallel multi gpu distributed training strategy is off the shelf to use it scale a well a google s official benchmark see tensorpack benchmark for some benchmark script focus on large datasets it s not a model wrapper see tutorial and documentation to know more about these feature we refuse toy example instead of showing tiny cnns trained on mnist cifar we provide training script that reproduce well known paper we refuse low quality implementation unlike most open source repos which only implement paper tensorpack example faithfully reproduce paper demonstrating it flexibility for actual research dependency please note that tensorpack is not yet stable if you use tensorpack in your code remember to mark the exact version of tensorpack you use a your dependency if you use tensorpack in your research or wish to refer to the example please cite with,"[('tensorpack benchmark', 0.6425), ('tensorflow', 0.5815), ('tensorpack', 0.5169), ('paper tensorpack example', 0.4818), ('neural network training interface', 0.4607), ('tensorpack data', 0.4587), ('official benchmark', 0.4154), ('tiny cnns', 0.4099), ('training speed speed', 0.3941), ('high level api', 0.368)]","[-1.21966362e-01 -5.46309538e-02  5.18880561e-02  2.69604232e-02
  6.66108206e-02 -8.94626603e-03 -2.95768213e-02  4.08665873e-02
 -1.26360893e-01 -4.65559103e-02 -2.83406284e-02  1.43159851e-02
 -5.00980206e-02  2.03053746e-02 -2.64670681e-02 -5.27496561e-02
  1.97551008e-02  9.76162180e-02 -6.82481332e-03 -8.29170346e-02
 -5.85468486e-02 -4.69456874e-02  5.11247106e-02  2.05152631e-02
  8.15363005e-02 -9.28722974e-03 -1.79927349e-02 -6.52557537e-02
  1.90583579e-02 -4.37249988e-02 -1.34949414e-02 -1.30324187e-02
  2.27869600e-02  2.95343362e-02 -7.60271102e-02 -3.35758179e-03
  1.64578911e-02 -7.90211633e-02  6.69094622e-02  6.41941151e-05
 -2.89625432e-02 -2.81507410e-02 -1.42982453e-02 -3.32135335e-02
  9.70375016e-02 -9.81103443e-03  9.38468240e-03 -6.29643202e-02
 -3.42844389e-02  5.89970592e-03 -1.18465222e-01 -9.47502553e-02
 -7.00273961e-02  4.18356694e-02  2.19175946e-02 -6.07535057e-02
  1.78038254e-02  1.36481384e-02 -3.63375917e-02  8.62223376e-03
  1.86838843e-02 -8.61117691e-02 -5.84484898e-02  8.59203842e-03
  1.51054692e-02  4.43827286e-02  1.80457272e-02  1.99914202e-02
  1.48605287e-01 -9.30748284e-02 -5.82390055e-02  5.02918921e-02
 -3.41469981e-02  8.48074928e-02  1.42120393e-02 -3.22096758e-02
  3.38552035e-02  1.97144598e-02  5.04018292e-02 -8.62197429e-02
 -4.44960855e-02 -3.48019972e-02  6.65612286e-03 -3.15858610e-02
  3.15962024e-02 -1.20855691e-02  2.49149147e-02  6.92388043e-02
 -1.02208424e-02 -6.26370013e-02  6.11027144e-02  4.37667640e-03
 -2.35138256e-02  2.99786199e-02  4.23818268e-02  4.80867699e-02
 -1.86756197e-02 -4.35671769e-02 -7.52594098e-02  1.87286828e-02
 -5.52766658e-02 -1.63719002e-02 -1.70274507e-02  7.74109960e-02
  3.05168536e-02  5.15888371e-02  5.55972718e-02 -4.94486019e-02
  2.61902511e-02  6.16121814e-02  5.04769618e-03  7.84173384e-02
 -1.83877849e-03 -1.22512341e-01  3.07721794e-02  2.51379777e-02
 -6.47861436e-02  1.37870610e-02  3.22932974e-02  7.97097906e-02
 -9.38818678e-02  6.43995926e-02 -2.66308617e-03 -2.79397517e-02
 -2.47596409e-02  2.29218882e-02 -2.63852254e-02  3.46181029e-33
  1.46911051e-02  1.05273249e-02 -3.57223209e-03  3.14943492e-02
  2.62270737e-02 -1.06630996e-01  3.01737990e-02  9.33828764e-03
 -9.42540355e-03  1.29449219e-02 -3.71592194e-02  9.26257968e-02
 -2.10490581e-02  1.24700502e-01  7.63037661e-03 -4.97364737e-02
  2.97611672e-03  1.42732658e-03  5.46364188e-02  6.47282973e-02
  6.24691583e-02 -2.15414097e-03  2.89048441e-02  8.62556994e-02
  4.17322181e-02  5.04628196e-02 -8.69506039e-03 -1.33686187e-02
  8.69903620e-03  1.78102050e-02 -4.13169675e-02 -6.79409280e-02
 -1.29241473e-03 -5.55559024e-02  4.08548340e-02 -8.06447770e-03
 -7.10803717e-02 -4.53706048e-02 -4.86809239e-02 -5.11272298e-03
 -4.85456921e-02  4.75923903e-02 -6.71476871e-02 -2.31020134e-02
 -5.93185276e-02 -5.08553013e-02  1.60409957e-02  1.80303119e-02
  7.42188171e-02 -8.59105289e-02 -6.76420033e-02  1.69686060e-02
 -3.04716974e-02 -4.00778688e-02 -1.09835295e-03  7.50289811e-03
  7.76736289e-02  3.45925577e-02  2.34308951e-02  1.00357018e-01
 -4.56939638e-02 -1.42984455e-02  3.15477699e-02  4.58160322e-03
 -5.27125448e-02 -1.43733956e-02 -1.08870916e-01  4.65205386e-02
 -1.08584296e-02  5.27316406e-02 -4.48189862e-02  6.29725307e-02
  5.56366052e-03 -4.67906222e-02  2.67923577e-03 -7.56340334e-03
  3.01754680e-02 -9.65583250e-02 -2.54481155e-02 -2.28277482e-02
 -5.47715314e-02  1.29954815e-02  2.34675296e-02 -1.89181697e-02
 -3.55659537e-02 -3.33281159e-02 -3.67019065e-02  9.77639575e-03
  4.33398895e-02 -5.20865247e-02 -6.83289245e-02  1.58855971e-02
  5.54884523e-02 -1.53087545e-02 -3.02277748e-02 -1.89516248e-33
  5.04268259e-02  1.17651364e-02 -6.61653802e-02  8.81944075e-02
 -2.65636873e-02  5.24434224e-02  2.65976470e-02  7.13077113e-02
  4.19185273e-02  1.34639189e-01  6.67582154e-02 -5.82584217e-02
  6.28492907e-02 -1.26397302e-02  1.24570364e-02 -3.29228342e-02
 -1.08879983e-01 -9.42462087e-02  4.57427688e-02 -1.63062215e-02
 -7.04270452e-02  1.13313250e-01 -4.22251001e-02  2.91901384e-03
 -1.34413913e-02  1.48023665e-02 -1.04602955e-01 -2.43809596e-02
  2.62731984e-02 -5.69039993e-02  3.08414735e-02 -6.04108684e-02
 -1.75386872e-02  5.85811362e-02  1.77303590e-02  7.58446828e-02
  8.49176645e-02  4.23890948e-02 -6.27324264e-03  7.88989943e-03
  9.31100249e-02  2.17977837e-02  5.59270531e-02 -1.51964892e-02
 -3.50966193e-02 -1.29786171e-02 -1.11657143e-01  2.89134048e-02
 -9.10351276e-02 -9.34317242e-03 -8.92993063e-03 -7.49318860e-03
 -6.50120759e-03 -7.84914121e-02 -2.60614268e-02 -2.07873471e-02
 -1.98022686e-02  1.83205642e-02  9.37425196e-02  9.76127398e-04
  2.05334779e-02  2.90756114e-03 -1.37258358e-02  5.04166000e-02
 -7.44322687e-02 -1.02743432e-02 -1.09643750e-01 -7.48098455e-03
  1.81722473e-02  1.04064988e-02  1.58903636e-02  1.27334958e-02
  4.89966534e-02  4.64040712e-02 -3.23533118e-02  5.74284233e-02
  2.27539185e-02  8.61666277e-02  4.06047106e-02  9.78981890e-03
  9.79202613e-02  1.95029415e-02  1.66992377e-02  8.75462145e-02
  2.84105819e-02  1.29477620e-01  4.78237309e-02 -1.41432900e-02
  2.85192132e-02 -1.95638998e-03 -1.32567063e-02 -8.81416444e-03
  2.52890140e-02  8.51602778e-02  1.80855915e-02 -2.54465622e-08
 -5.81741221e-02  2.99550723e-02  6.63625151e-02 -1.01050073e-02
 -5.56530245e-03  2.68742759e-02  8.80303211e-04  1.09745838e-01
 -7.23298499e-03  4.53411527e-02  1.76577305e-03 -8.17461591e-03
 -1.01842470e-01 -2.05371273e-03  2.82693040e-02  5.13304658e-02
 -5.74923530e-02  2.81247161e-02  2.27394588e-02 -5.42643294e-02
  1.22014200e-03  1.05957821e-01  4.64726388e-02 -2.71679182e-02
  2.61005238e-02 -1.71802100e-02  6.38118610e-02  4.90098037e-02
 -9.23758280e-03 -6.88942522e-02 -6.77887276e-02 -6.43019145e-03
  9.02934447e-02 -1.21304102e-01  1.04035288e-01  1.03935868e-01
  5.69212362e-02  8.54134187e-03 -1.40379108e-02  6.20626621e-02
 -1.13980258e-02 -7.31762149e-04  1.62458960e-02 -6.76778406e-02
  3.26500759e-02 -8.08411464e-03 -3.19864266e-02 -2.44694408e-02
  8.90051248e-04 -7.62519008e-03 -2.62756296e-03 -8.15668085e-04
 -7.46430606e-02  3.96728590e-02  1.88250300e-02  6.88881278e-02
 -7.90584534e-02 -8.43566507e-02 -2.85147480e-03 -4.34106998e-02
  2.25182325e-02  7.78215900e-02  2.86409892e-02  1.08343055e-02]",2,2
thinc,3,thinc is a lightweight deep learning library that offer an elegant type checked functional programming api for composing model with support for layer defined in other framework such a pytorch tensorflow and mxnet you can use thinc a an interface layer a standalone toolkit or a flexible way to develop new model previous version of thinc have been running quietly in production in thousand of company via both spacy and prodigy we wrote the new version to let user compose configure and deploy custom model built with their favorite framework thinc is compatible with python and run on linux macos and window the latest release with binary wheel are available from pip before you install thinc and it dependency make sure that your pip setuptools and wheel are up to date for the most recent release pip or newer is recommended see the extended installation doc for detail on optional dependency for different backends and gpu you might also want to set up static type checking to take advantage of thinc s type system if you have installed pytorch and you are using python uninstall the package dataclasses with pip uninstall dataclasses since it may have been installed by pytorch and is incompatible with python also see the example directory and usage documentation for more example most example are jupyter notebook to launch them on google colab with gpu support click on the button next to the notebook name view more thinc us black for auto formatting flake for linting and mypy for type checking all code is written compatible with python with type hint wherever possible see the type reference for more detail on thinc s custom type building thinc from source requires the full dependency listed in requirement txt to be installed you ll also need a compiler to build the c extension alternatively install in editable mode or by setting pythonpath thinc come with an extensive test suite the following should all pas and not report any warning or error to view test coverage you can run python m pytest thinc cov thinc we aim for a test coverage this doesn t mean that we meticulously write test for every single line we ignore block that are not relevant or difficult to test and make sure that the test execute all code path,"[('pythonpath thinc', 0.6267), ('favorite framework thinc', 0.5735), ('thinc', 0.5225), ('lightweight deep learning library', 0.4606), ('pytest thinc cov thinc', 0.4444), ('recent release pip', 0.4345), ('pytorch tensorflow', 0.4228), ('pip', 0.4036), ('standalone toolkit', 0.3828), ('pip setuptools', 0.334)]","[-1.03012457e-01 -7.58949518e-02  1.50847742e-02  3.95749100e-02
  4.89863195e-02 -4.24997248e-02 -1.36613632e-02  7.45961964e-02
 -7.63743371e-02 -3.26047949e-02 -4.19284292e-02 -5.15225530e-02
 -9.80280116e-02  5.65989576e-02  3.78894880e-02 -4.63383412e-03
  1.43778939e-02  4.84072566e-02  3.56109887e-02 -7.33928531e-02
 -7.70355910e-02 -1.23383850e-02  1.57161392e-02  5.52103929e-02
  1.40919292e-04 -2.36523170e-02 -1.05222138e-02 -1.75473967e-03
 -1.91640221e-02 -9.32308659e-02 -7.37519655e-03 -1.21415704e-02
 -1.80013210e-03 -5.97222149e-02  2.85952520e-02  8.56625754e-03
  7.96693116e-02 -7.97547251e-02 -3.46357264e-02 -4.45314646e-02
  5.08092763e-03 -1.43473898e-03 -4.54814881e-02  2.44897120e-02
 -2.05627829e-02  5.57383709e-03  1.34307826e-02 -5.09375893e-02
 -1.98345375e-03 -1.49010271e-02  1.28272660e-02 -3.53761502e-02
 -8.44713002e-02  1.60624404e-02 -3.05626448e-02 -1.83494259e-02
 -3.38725820e-02  2.57397573e-02  4.79078144e-02  4.57529537e-03
 -2.49967799e-02 -4.51109856e-02  3.04171611e-02  8.99361968e-02
  7.55578605e-03  3.15182023e-02  2.06682719e-02  4.73025776e-02
  1.29388779e-01 -9.91356149e-02 -4.57364358e-02 -1.77988857e-02
 -3.01434621e-02  3.95707637e-02 -2.41757482e-02 -3.53666097e-02
  3.74325924e-02 -3.20336446e-02 -6.35460615e-02 -4.82711792e-02
 -8.39506090e-02  9.98136029e-03 -4.90010120e-02  4.71670367e-02
 -1.01803045e-04 -4.05636802e-02 -1.24979633e-04  7.06006289e-02
 -2.49391198e-02 -5.12624010e-02  1.56126976e-01 -5.00445999e-03
  4.60636765e-02  7.03895316e-02 -3.72160086e-03  4.31935629e-03
  4.56081852e-02 -3.73717844e-02 -1.23130970e-01  6.37567863e-02
 -1.13465302e-01 -6.16757050e-02  5.87144010e-02  8.78642648e-02
  3.21238488e-02  5.78998961e-02  4.10518423e-02 -3.49255875e-02
  9.16442275e-02  2.93890461e-02 -4.68964763e-02 -5.06118573e-02
 -3.42051610e-02 -7.53358155e-02  7.91122690e-02 -3.20869237e-02
  2.62005301e-03  5.34728579e-02  3.89395542e-02  1.15734011e-01
 -1.00925013e-01 -4.66382056e-02 -2.23521003e-03 -5.45564108e-02
  2.74778111e-03 -1.85533625e-03 -4.35404256e-02  6.36360920e-33
 -1.39468713e-02  1.87305287e-02 -1.89658143e-02  3.63945067e-02
  4.92711589e-02 -9.05722752e-02  1.07170619e-01 -3.21476534e-02
 -9.91809517e-02 -4.21591662e-02 -2.40965951e-02  6.79051951e-02
 -3.14668193e-02  1.27591640e-01  8.68369918e-03 -7.08231255e-02
 -6.59857551e-03  8.36281329e-02  3.19910459e-02  1.65742915e-02
 -4.65641497e-03  7.81349093e-02  5.40879443e-02  3.73399630e-02
  6.54985905e-02 -9.81389210e-02  2.49411203e-02 -1.56465713e-02
  2.83066202e-02  2.76494771e-02 -7.75524750e-02  6.60176529e-03
  6.46104887e-02 -5.02132950e-03 -2.27870774e-02  3.41018587e-02
 -1.09460860e-01 -4.47120219e-02 -1.49446204e-02 -1.08941225e-02
 -6.75961226e-02  2.41279043e-02  2.13188864e-02 -8.31599683e-02
  1.06059359e-02 -4.42285351e-02 -1.65260881e-02  1.07272379e-01
 -3.33794542e-02 -1.97598264e-02 -2.11781431e-02 -2.74061803e-02
 -1.93046406e-02  2.66723335e-02 -8.63817520e-03 -7.86693767e-02
  6.98525272e-03  4.45665931e-03  7.92720541e-02  5.04566319e-02
  8.01460743e-02  8.43709186e-02 -2.44998559e-02  2.63084122e-03
  4.73666303e-02 -6.77533895e-02 -1.80554129e-02  2.63530239e-02
 -3.05178761e-02  2.82560308e-02 -8.51433873e-02  7.11947605e-02
 -1.71528142e-02 -1.25001837e-02  8.09338838e-02 -2.56385989e-02
  7.06407726e-02 -1.28243044e-01  3.59685235e-02 -1.54446168e-02
 -9.11038667e-02  5.28502278e-02  6.84148073e-02 -2.08837222e-02
 -7.17759132e-02 -1.92150064e-02 -1.55798187e-02  2.03780853e-03
 -6.63040951e-02 -5.34039922e-02 -5.10870218e-02 -8.02195668e-02
  6.94290474e-02  2.09803805e-02 -8.14655498e-02 -3.99366638e-33
  3.07393596e-02 -1.71098299e-02 -8.65841191e-03  9.21201706e-02
  9.02620982e-03  2.07702611e-02 -1.79158826e-03 -4.16040719e-02
  4.45718654e-02  1.83480103e-02  1.69196390e-02 -2.43302900e-02
  6.90902695e-02 -2.90027424e-03  5.29569909e-02  2.60902271e-02
 -4.66539189e-02 -1.14878930e-01  3.20824161e-02 -2.03892253e-02
 -5.54277860e-02  4.29779701e-02 -4.73302491e-02 -5.86743280e-02
 -2.67959610e-02 -3.45797464e-02 -4.17009965e-02 -1.83521062e-02
 -1.48130413e-02  2.21275017e-02 -5.16478084e-02  3.41221020e-02
 -5.30981496e-02 -5.79461129e-03 -1.65798538e-03  1.07838279e-02
  3.02883796e-02 -2.97240075e-02  2.75160074e-02 -6.75235987e-02
  8.87571350e-02  6.40299991e-02 -5.32539487e-02 -4.42958511e-02
 -4.74691987e-02  2.02231016e-02 -5.42633869e-02  8.75729546e-02
  8.79221316e-03  5.11576608e-03 -6.27426896e-03  2.19919831e-02
 -6.57103024e-04  7.06452429e-02 -2.08633044e-03  2.86984965e-02
  5.69604188e-02  5.74927628e-02  8.79938435e-03 -1.00663127e-02
 -6.44879937e-02  6.46686880e-03 -1.49190621e-02  4.07486642e-03
 -4.49188007e-03  4.26220633e-02 -2.40796413e-02  5.98991066e-02
 -5.36606535e-02 -6.58056065e-02 -1.79836806e-02  7.87801575e-03
  5.19992150e-02  6.31552469e-03 -4.47368957e-02 -7.87796453e-03
  2.20107567e-03  8.22041743e-03 -1.30579844e-02  3.98397446e-02
  8.58868882e-02  6.84178760e-03  9.17767733e-03  2.68536471e-02
  9.11542401e-02  1.03432015e-01  2.40917280e-02 -6.07525837e-03
  7.71801695e-02  8.17071870e-02  4.26185653e-02  1.06676277e-02
  9.15957913e-02  8.81912410e-02 -2.68482091e-03 -2.88951210e-08
  3.71056050e-02  6.47096187e-02  3.07394136e-02  2.34726537e-02
  5.35843559e-02  2.03064252e-02  1.42815784e-02  7.77454227e-02
 -5.16516156e-03  6.46817610e-02  3.23534794e-02 -6.18713461e-02
 -6.68660924e-02  2.05992870e-02  3.85483913e-02  1.28012463e-01
 -1.33928340e-02  5.70597351e-02  4.54677595e-03 -6.15450144e-02
  7.99587741e-03  8.55416246e-03  3.94312218e-02  1.56102441e-02
 -3.24980430e-02 -7.13698715e-02  1.65954940e-02  2.97623500e-02
  2.14706734e-02 -2.54639350e-02 -3.11300904e-02 -4.42257244e-03
  2.42352970e-02 -5.70613518e-02  9.25543308e-02  7.77510777e-02
  2.32593585e-02 -6.48871483e-03 -3.59848328e-02  1.50679603e-01
 -4.78369258e-02  2.25170590e-02  4.31513414e-02 -6.85258880e-02
  2.90722474e-02 -6.91951737e-02 -5.49154840e-02 -4.06459458e-02
 -5.21464236e-02 -3.50238532e-02  4.08649165e-03  1.33892754e-02
 -4.28293161e-02  1.97531693e-02  4.74452935e-02  8.47993344e-02
 -4.37326431e-02 -5.52758127e-02 -4.01125476e-02  1.01628602e-02
  2.52920240e-02  2.46079434e-02  6.72959536e-02  5.71245141e-02]",2,2
pytorch-transformers,3,pytorch transformer formerly known a pytorch pretrained bert is a library of state of the art pre trained model for natural language processing nlp the library currently contains pytorch implementation pre trained model weight usage script and conversion utility for the following model these implementation have been tested on several datasets see the example script and should match the performance of the original implementation e g f on squad for bert whole word masking f on rocstories for openai gpt perplexity on wikitext for transformer xl peason r coefficient on sts b for xlnet you can find more detail on the performance in the example section of the documentation this repo is tested on python and example are tested only on python and pytorch pytorch transformer can be installed by pip a follows clone the repository and run a series of test is included for the library and the example script library test can be found in the test folder and example test in the example folder these test can be run using pytest install pytest if needed with pip install pytest you can run the test from the root of the cloned repository with the command you should check out our swift coreml transformer repo it contains an example of a conversion script from a pytorch trained transformer model here gpt to a coreml model that run on io device at some point in the future you ll be able to seamlessly move from pre training or fine tuning model in pytorch to productizing them in coreml or prototype a model or an app in coreml then research it hyperparameters or architecture from pytorch super exciting let s do a very quick overview of pytorch transformer detailed example for each model architecture bert gpt gpt transformer xl xlnet and xlm can be found in the full documentation the library comprises several example script with sota performance for nlu and nlg task here are three quick usage example for these script the general language understanding evaluation glue benchmark is a collection of nine sentence or sentence pair language understanding task for evaluating and analyzing natural language understanding system before running anyone of these glue task you should download the glue data by running this script and unpack it to some directory glue dir you should also install the additional package required by the example where task name can be one of cola sst mrpc sts b qqp mnli qnli rte wnli the dev set result will be present within the text file eval result txt in the specified output dir in case of mnli since there are two separate dev set matched and mismatched there will be a separate output folder called tmp mnli mm in addition to tmp mnli this example code fine tune xlnet on the sts b corpus using parallel training on a server with v gpus parallel training is a simple way to use several gpus but is slower and le flexible than distributed training see below on this machine we thus have a batch size of please increase gradient accumulation step to reach the same batch size if you have a smaller machine these hyper parameter should result in a pearson correlation coefficient of on the development set this example code fine tune the bert whole word masking model on the microsoft research paraphrase corpus mrpc corpus using distributed training on v gpus to reach a f training with these hyper parameter gave u the following result this example code fine tune bert on the squad dataset using distributed training on v gpus and bert whole word masking uncased model to reach a f on squad training with these hyper parameter gave u the following result this is the model provided a bert large uncased whole word masking finetuned squad a conditional generation script is also included to generate text from a prompt the generation script includes the trick proposed by by aman rusia to get high quality generation with memory model like transformer xl and xlnet include a predefined text to make short input longer here is how to run the script with the small version of openai gpt model here is a quick summary of what you should take care of when migrating from pytorch pretrained bert to pytorch transformersthe main breaking change when migrating from pytorch pretrained bert to pytorch transformer is that the model forward method always output a tuple with various element depending on the model and the configuration parameter the exact content of the tuples for each model are detailed in the model docstrings and the documentation in pretty much every case you will be fine by taking the first element of the output a the output you previously used in pytorch pretrained bert here is a pytorch pretrained bert to pytorch transformer conversion example for a bertforsequenceclassification classification model breaking change in the from pretrained method model are now set in evaluation mode by default when instantiated with the from pretrained method to train them don t forget to set them back in training mode model train to activate the dropout module the additional input and kwargs argument supplied to the from pretrained method used to be directly passed to the underlying model s class init method they are now used to update the model configuration attribute instead which can break derived model class build based on the previous bertforsequenceclassification example we are working on a way to mitigate this breaking change in by forwarding the the model init method i the provided positional argument and ii the keyword argument which do not match any configuration class attribute also while not a breaking change the serialization method have been standardized and you probably should switch to the new method save pretrained save directory if you were using any other serialization method before here is an example the two optimizers previously included bertadam and openaiadam have been replaced by a single adamw optimizer which ha a few difference the new optimizer adamw match pytorch adam optimizer api and let you use standard pytorch or apex method for the schedule and clipping the schedule are now standard pytorch learning rate scheduler and not part of the optimizer anymore here is a conversion example from bertadam with a linear warmup and decay schedule to adamw and the same schedule at the moment there is no paper associated to pytorch transformer but we are working on preparing one in the meantime please include a mention of the library and a link to the present repository if you use this work in a published or open source project,"[('pytorch pytorch transformer', 0.6359), ('pytorch implementation pre', 0.6145), ('model architecture bert gpt gpt transformer xl xlnet', 0.6131), ('pytorch transformer', 0.604), ('pytorch transformer conversion example', 0.5936), ('new optimizer adamw match pytorch adam optimizer api', 0.5114), ('standard pytorch', 0.502), ('standard pytorch learning rate scheduler', 0.4774), ('pytorch', 0.4609), ('previous bertforsequenceclassification example', 0.4191)]","[-1.95601001e-01 -3.72205079e-02  1.93720143e-02 -2.42585950e-02
  2.66116578e-02  2.36737896e-02 -3.99126038e-02  7.97008649e-02
 -4.16627228e-02 -3.50275151e-02 -6.37718663e-02  3.93529162e-02
 -8.93933401e-02  3.91461514e-02  2.17485782e-02  5.47688380e-02
  5.84165857e-04  5.82702607e-02 -4.83181551e-02 -9.86978635e-02
  4.42808419e-02  3.68649289e-02  2.07884330e-02 -5.78679107e-02
  5.44689223e-02 -6.27402291e-02  2.00782418e-02 -7.85642564e-02
  7.47198910e-02  9.36352611e-02  3.63821201e-02 -5.42224273e-02
  1.34452479e-02  9.59555209e-02 -5.15060537e-02 -9.46279732e-04
 -1.23679386e-02 -3.46382968e-02  4.49338602e-03  3.87415923e-02
  2.77099106e-02 -3.85972112e-02 -6.98968396e-02 -4.47425433e-02
  5.98238185e-02 -2.58972999e-02  2.25227065e-02 -5.91751970e-02
 -2.91487370e-02 -5.86963631e-02  4.39734431e-03 -2.56214403e-02
 -1.05977422e-02  9.01318062e-03  1.16154635e-02  8.88332278e-02
  3.49662751e-02 -3.62666398e-02  4.70082313e-02 -1.16686359e-01
 -4.30338383e-02 -2.73602959e-02 -1.19115701e-02  5.93584310e-03
 -1.08263738e-01  3.67133580e-02  3.45653528e-03  9.10093822e-03
  7.60218054e-02 -1.26501158e-01 -5.93511090e-02  5.03881983e-02
 -4.94260080e-02  6.25006258e-02  3.43950205e-02 -5.02834506e-02
  6.56586289e-02 -5.94314048e-03 -3.14759538e-02 -9.32281390e-02
  3.80885531e-03 -2.76452731e-02  5.03085516e-02  7.60460794e-02
  7.12716356e-02 -5.67568420e-03 -5.83995283e-02  6.72642142e-02
  4.23859246e-03 -8.09723628e-04  7.11028138e-03 -9.06190425e-02
  2.67694127e-02  2.04306077e-02  2.90658139e-02  3.95321799e-03
 -7.17653835e-04 -9.76634845e-02 -4.07988839e-02  3.78810801e-02
 -1.01700583e-02 -4.23965231e-02  1.50058987e-02 -1.27594676e-02
 -2.06843298e-03  3.29833403e-02 -1.41785201e-03  4.67159078e-02
  4.03334238e-02 -1.39367739e-02  4.69342954e-02 -4.79554608e-02
  1.41692059e-02 -8.85906219e-02  4.72497530e-02  1.15634073e-02
 -6.29568398e-02  5.36528304e-02  6.08990118e-02  7.21318498e-02
 -2.46437322e-02  6.17252067e-02 -4.08297181e-02  4.88332771e-02
 -6.93035647e-02  5.34425266e-02 -6.78024814e-02  1.28511684e-32
 -5.06713800e-02 -1.07473796e-02  1.85408052e-02 -7.98168406e-02
 -1.97316874e-02  5.55155333e-03  5.56701869e-02  4.86346520e-02
  5.03238142e-02 -8.69334191e-02 -5.30236587e-02 -6.91787573e-03
 -9.21273157e-02  5.36125340e-02 -7.40215555e-02 -2.60732882e-02
 -6.77226260e-02  1.04113914e-01  7.01651797e-02  3.72442305e-02
  3.71119976e-02  2.48984229e-02 -1.97372679e-02 -6.46297447e-03
 -8.40277150e-02  4.80385944e-02  3.55740115e-02 -3.11991666e-02
  3.19199674e-02 -2.42076558e-03 -1.59447029e-01  5.82584031e-02
 -2.42820270e-02 -4.64856662e-02 -1.10976249e-02  3.75684840e-03
  2.14878563e-02 -3.77800316e-02 -1.97048802e-02 -5.57574406e-02
 -8.74584392e-02  4.22464199e-02 -1.20332455e-02 -4.24362868e-02
  5.08243367e-02  3.22710141e-03  6.60548033e-03  8.46550837e-02
  8.39504823e-02 -1.62487887e-02 -4.72426154e-02  4.37111557e-02
 -7.47478306e-02  5.76655455e-02  6.49166554e-02 -3.05845961e-02
  8.30509663e-02  5.22506051e-02  8.51563662e-02  4.66740057e-02
  1.44253615e-02  5.14424741e-02  1.98906604e-02  4.76808567e-03
 -2.38655936e-02 -7.16865510e-02 -1.31783234e-02 -3.76429111e-02
 -5.66206500e-02  7.03735128e-02 -3.40055884e-03  2.92225592e-02
 -2.76063755e-02 -3.47967558e-02  9.50942859e-02 -4.22831252e-02
  3.58727574e-02  1.99090987e-02 -7.31562525e-02 -4.08076011e-02
 -6.26953170e-02  5.98310456e-02  3.45546170e-04 -4.99343053e-02
 -8.21698010e-02 -1.01696402e-02  4.97717932e-02 -3.25101614e-02
  1.79441087e-02  2.37677936e-02 -1.61763169e-02 -5.83648495e-02
 -3.61898504e-02  8.28764439e-02 -6.00395317e-04 -1.03809897e-32
  3.34236436e-02  7.19733089e-02 -9.81100947e-02  6.20790236e-02
  1.76485657e-04 -7.16834292e-02 -3.08561325e-02  1.73133630e-02
  4.39012460e-02 -3.92592466e-03  2.20224578e-02 -6.26150519e-02
 -3.70765361e-03 -5.77191599e-02  6.97326139e-02 -2.69840993e-02
 -2.31836438e-02  4.20706579e-03 -1.90414432e-02  2.37042177e-02
 -2.04768050e-02  9.47854370e-02 -9.39276963e-02 -2.58797314e-02
 -3.82832214e-02 -2.31819246e-02 -6.58104792e-02  8.64676535e-02
  1.08116372e-02 -1.08003486e-02 -7.44785881e-04  1.59247890e-02
  3.58478655e-03  6.55972809e-02 -2.81562209e-02  6.09116890e-02
  5.64618856e-02 -5.08054998e-03 -6.47712033e-03  1.13484142e-02
  1.35661915e-01 -3.51096876e-02 -1.52028287e-02  8.58656988e-02
 -4.06586081e-02  5.09442911e-02 -6.07558712e-02 -1.21045141e-02
 -1.16929468e-02  1.94172878e-02 -2.71786167e-03  4.97915223e-03
 -7.11272731e-02 -1.53120961e-02 -5.14859625e-04 -2.19061840e-02
  1.03184335e-01 -1.83222292e-03 -2.10121702e-02  1.59124788e-02
 -4.39935066e-02 -5.83705828e-02  6.46075979e-02 -2.31225472e-02
 -2.04740930e-02 -8.48642178e-03 -2.59422213e-02  8.75160024e-02
  1.74235518e-03  1.01275399e-01 -9.83475428e-03  4.68677189e-03
  9.39450860e-02  5.64853400e-02 -2.75981072e-02  3.41331102e-02
  4.17202376e-02 -2.86724102e-02 -3.34011540e-02 -3.40222195e-02
 -4.93014082e-02  1.07194697e-02  4.89965081e-02  3.78367677e-02
 -1.46876378e-02  6.33619949e-02  1.07860282e-01  7.23400386e-03
  3.21170315e-02  4.35559377e-02 -3.97743620e-02 -6.52848626e-04
  9.02084447e-03  8.80823508e-02  2.39081550e-02 -4.06691925e-08
 -6.92775622e-02  6.17742948e-02 -3.79921161e-02  1.87798440e-02
 -6.45225111e-04 -7.75499567e-02 -4.80411202e-03  1.29666016e-01
 -5.31225950e-02  5.37566189e-03 -1.76678896e-02 -4.02784608e-02
 -2.55548265e-02  1.72189213e-02  7.64227286e-02  5.02443165e-02
 -5.02158189e-04  9.96553060e-03  1.68646574e-02 -2.74244212e-02
 -3.85762788e-02  3.73448730e-02 -4.11108434e-02 -6.30684718e-02
 -1.68497674e-02 -4.01903056e-02 -3.86183411e-02  8.21034983e-02
  3.46611701e-02  2.32498292e-02 -9.12342872e-03  4.77981418e-02
 -3.79781425e-02 -7.02114254e-02  1.21398225e-01  4.21694331e-02
 -4.17234637e-02  8.65722820e-03  2.73717027e-02  7.29306368e-03
 -2.34711263e-02  6.93155045e-04 -1.08416304e-01 -9.41134058e-03
  5.31047024e-02 -2.17331648e-02 -3.81564163e-02 -9.74908993e-02
 -2.20845938e-02  4.35814597e-02  5.30869998e-02 -1.53792137e-02
  5.11949509e-03  3.40964156e-03 -2.88845841e-02  2.54927743e-02
 -8.17222968e-02 -6.49839267e-02 -1.45575302e-02 -7.71102356e-03
 -6.94510667e-03  8.40878561e-02  2.66275164e-02  6.89378157e-02]",2,2
opt-einsum,3,optimized einsum can significantly reduce the overall execution time of einsum like expression e g np einsum dask array einsum pytorch einsum tensorflow einsum by optimizing the expression s contraction order and dispatching many operation to canonical blas cublas or other specialized routine optimized einsum is agnostic to the backend and can handle numpy dask pytorch tensorflow cupy sparse theano jax and autograd array a well a potentially any library which conforms to a standard api see the documentation for more information the opt einsum contract function can often act a a drop in replacement for einsum function without futher change to the code while providing superior performance here a tensor contraction is preformed with and without optimization in this particular example we see a x performance improvement which is not uncommon when compared against unoptimized contraction see the backend example for more information on using other backends the algorithm found in this repository often power the einsum optimization in many of the above project for example the optimization of np einsum ha been passed upstream and most of the same feature that can be found in this repository can be enabled with np einsum optimize true however this repository often ha more up to date algorithm for complex contraction the following capability are enabled by opt einsum please see the documentation for more feature opt einsum can either be installed via pip install opt einsum or from conda conda install opt einsum c conda forge see the installation documenation for further method if this code ha benefited your research please support u by citing daniel g a smith and johnnie gray opt einsum a python package for optimizing contraction order for einsum like expression journal of open source software doi http doi org joss all contribution bug report bug fix documentation improvement enhancement and idea are welcome a detailed overview on how to contribute can be found in the contributing guide,"[('einsum optimization', 0.5924), ('more feature opt einsum', 0.549), ('numpy dask pytorch tensorflow', 0.4508), ('opt einsum', 0.4391), ('opt einsum contract function', 0.4272), ('np einsum', 0.3897), ('einsum', 0.378), ('pip install opt einsum', 0.3759), ('einsum function', 0.3729), ('conda conda install opt einsum', 0.3713)]","[-4.98463921e-02 -1.32788550e-02 -1.59783643e-02 -1.38680283e-02
  2.16168463e-02 -6.34169504e-02  2.02649347e-02  5.79896905e-02
 -7.29189366e-02 -1.16005829e-02 -1.84624828e-02 -3.82967070e-02
 -5.61413690e-02  8.65499489e-04 -1.23706320e-02  3.92319709e-02
  9.89296241e-04  3.61860730e-02 -1.97827090e-02 -1.49880424e-01
 -4.23450992e-02 -2.17884197e-03 -5.09330211e-03 -3.30121517e-02
  3.52483504e-02 -2.49176752e-03  5.33982506e-03  5.41259348e-02
  5.53966276e-02 -7.89640658e-03  3.28913443e-02  7.88678527e-02
  1.20151982e-01 -2.33933739e-02  1.57361070e-03 -3.85166751e-03
 -5.97085580e-02 -3.86435017e-02 -7.84291625e-02  1.19081587e-02
  2.20971815e-02 -7.78682949e-03 -6.16297126e-02  2.57211141e-02
  5.21477610e-02  5.36858849e-02  1.62410568e-02 -2.10996382e-02
  7.38889351e-02 -1.17358016e-02 -8.05507451e-02  5.27025340e-03
 -1.34920165e-01  1.18823834e-02  1.98501330e-02 -4.97155376e-02
  5.22194430e-02 -1.60180014e-02  1.01755103e-02 -5.79429939e-02
  2.53695194e-02 -4.25163470e-02  1.13779875e-02  2.59760879e-02
  5.02031855e-02 -6.43077167e-03  1.92758732e-03  1.41481636e-02
  4.45218571e-02  3.53763551e-02  1.53680157e-04 -5.81549155e-03
 -9.89813432e-02  7.87640177e-03  5.97614702e-03 -4.47227014e-03
  1.56258881e-01 -5.77903762e-02 -2.34617162e-02 -2.33731233e-02
 -8.89504775e-02  2.80464943e-02  7.77215324e-03 -3.02810352e-02
  1.64684914e-02 -1.86021645e-02  7.47856684e-03  1.21778231e-02
  4.83519100e-02 -5.52516878e-02  4.66098115e-02 -5.24953566e-02
 -9.16015133e-02 -1.17737185e-02 -2.21322048e-02  2.52354313e-02
  1.80658186e-03 -1.55809410e-02 -1.87168345e-02  4.98627275e-02
  2.07175408e-02 -1.02250427e-01  5.63803874e-02 -2.74546295e-02
 -1.55697018e-02  2.82746181e-02  1.25564024e-01 -1.29534649e-02
  6.56423438e-03  2.65311766e-02 -5.11875423e-03 -5.66351116e-02
 -1.81203838e-02 -5.92988692e-02  5.54142557e-02  3.65786664e-02
  1.08609581e-02  2.78625004e-02  6.42193630e-02  2.61981934e-02
 -7.10635353e-03 -8.73498211e-04  2.28541549e-02  5.53597845e-02
 -5.29987253e-02  8.03832989e-03 -3.92956175e-02  7.94698760e-33
 -8.42269138e-02 -6.88025281e-02 -3.58431507e-03 -2.98892800e-02
  9.81764495e-03 -2.24681813e-02  8.60089585e-02  3.98572944e-02
 -3.21378447e-02 -5.73812984e-03 -1.28982767e-01  6.48244247e-02
 -4.53966707e-02  5.21126911e-02  2.09652465e-02 -8.01857561e-02
  1.22879166e-02  6.17508292e-02  1.90295223e-02  5.75925224e-02
  8.11239406e-02 -5.46835922e-02  7.27862641e-02  3.46518233e-02
  3.85215282e-02 -2.83141788e-02  3.72672230e-02 -3.61671150e-02
 -3.39034945e-02  4.31873457e-04 -9.55794975e-02 -1.09146601e-02
 -3.09851281e-02  8.55051726e-03 -2.89338306e-02  2.95467228e-02
 -4.17723060e-02 -1.79781287e-03 -3.94559279e-02 -8.55502337e-02
 -2.09799204e-02  1.05973095e-01  1.67715328e-03 -2.93185171e-02
 -2.73897182e-02  6.95217475e-02  1.41566927e-02  6.13646097e-02
  2.62696203e-02  3.85199748e-02 -9.89342630e-02 -1.66701060e-02
  1.06643848e-02  1.72057245e-02 -1.81025043e-02 -1.99219249e-02
  4.28068377e-02 -2.49192752e-02  8.51936713e-02  8.90052784e-03
 -1.07322551e-01 -5.44810155e-03  2.69167665e-02  7.11906934e-03
  9.81928315e-03  1.45607665e-02  4.13407013e-03  7.39577040e-02
  4.12582010e-02  8.25378150e-02 -1.42003149e-01  5.83895408e-02
 -5.75050823e-02 -4.93996069e-02  4.64863842e-03 -1.58095807e-02
  4.35244404e-02 -6.75949082e-02 -3.69264632e-02 -2.76516769e-02
 -9.61874574e-02  6.60981387e-02  1.56103475e-02 -1.00011297e-01
 -3.27599682e-02  1.17421639e-03 -3.96345882e-03 -2.18586321e-03
 -1.78859960e-02 -8.29687119e-02 -2.41911560e-02  5.96190244e-02
  3.00704092e-02  1.45039186e-02 -3.93401086e-02 -7.15218811e-33
 -3.95702608e-02  1.67245325e-02 -3.62644754e-02  1.10371932e-01
  1.17800124e-02  4.67882976e-02 -2.47578435e-02 -5.17783202e-02
  3.50271016e-02  4.27062511e-02  3.18572037e-02 -8.93436670e-02
  2.93995198e-02 -5.96229956e-02  5.78423636e-03  6.73573138e-03
 -5.85820340e-02 -4.73903539e-03  4.78908233e-02  2.07063854e-02
 -9.01666507e-02  1.14404015e-01 -1.82233024e-02 -6.34969175e-02
 -9.86946821e-02  3.13941797e-04 -6.76303357e-02 -3.05485334e-02
 -5.15220053e-02 -6.70944015e-03 -3.16392072e-02  1.53984008e-02
 -1.31059349e-01  5.12378812e-02 -1.20843872e-02 -3.58979143e-02
  8.90667364e-02 -4.09224420e-04 -1.88904069e-02  6.31109551e-02
  1.36648253e-01  1.19887538e-01  1.38688218e-02  6.60495460e-02
 -3.93684283e-02  9.80281737e-03 -1.85373072e-02 -1.99533366e-02
 -9.59375966e-03 -2.29708832e-02  1.17379921e-02 -7.36605050e-03
 -5.06780967e-02 -7.98444636e-03 -4.21186164e-02  5.77029809e-02
  3.15730050e-02  3.68485190e-02  2.00708918e-02 -1.26183936e-02
 -8.82615522e-02 -1.01733170e-01  3.40831727e-02 -5.02613969e-02
 -1.75711587e-02 -1.27974628e-02 -4.77207303e-02  4.15965505e-02
 -1.95100326e-02  1.60424411e-02 -2.99229659e-03  5.88751025e-02
 -6.20903308e-03  5.58108790e-03 -1.00816503e-01  2.69368924e-02
  5.43426722e-02 -8.08093697e-03 -1.08051542e-02  5.74204419e-03
  1.92223154e-02 -4.26549166e-02  8.71949196e-02  9.77290943e-02
  9.99957090e-04  4.66719344e-02  3.39530893e-02  2.36036871e-02
  3.78999971e-02  4.68964390e-02 -2.39562411e-02  3.26051004e-02
  1.45796109e-02  1.04298569e-01  6.41864073e-03 -3.30817862e-08
 -1.07671767e-01  4.64620860e-03  1.41524551e-02 -1.02826431e-02
  5.99301159e-02 -7.92685151e-02  2.00625695e-02  1.58310920e-01
 -1.71964616e-02 -1.90432165e-02  7.45969499e-03  1.58070084e-02
 -1.10186204e-01  2.19600610e-02  1.40168648e-02  1.41402319e-01
 -3.83064002e-02  5.72107770e-02  4.14446183e-03 -6.98072314e-02
  6.52869418e-02  8.99847969e-03  1.37233045e-02  2.07596701e-02
 -3.98691222e-02 -4.57553007e-02  6.82583377e-02  5.81333674e-02
  6.97691441e-02  3.83647671e-03 -3.06724068e-02 -2.54873820e-02
  2.92248800e-02 -1.75503567e-02  9.92281139e-02  1.62174236e-02
  2.26395726e-02 -1.95548851e-02 -9.83700901e-02  2.47555207e-02
 -1.05622932e-01  7.31362775e-02 -1.27599947e-02 -4.82290015e-02
  6.68638274e-02  3.38420756e-02 -1.47112850e-02 -7.63888732e-02
 -5.61558455e-02 -1.87431220e-02  6.74458370e-02  2.62606088e-02
 -3.28529568e-04  7.34342113e-02  6.95929676e-02  1.94750428e-02
 -6.49808198e-02 -6.38274699e-02  2.65939143e-02 -1.60373151e-02
 -6.23267959e-04  3.10298596e-02  6.78851157e-02 -4.30941023e-02]",2,2
skipthoughts,3,skip thought torch is a lightweight porting of skip thought pretrained model from theano to pytorch it us the nn gru layer from torch with the cudnn backend it is the fastest implementation but the dropout is sampled after each time step in the cudnn implementation equal bad regularization it us the nn grucell layer from torch with the cudnn backend it is slightly slower than uniskip however the dropout is sampled once for all time step in a sequence good regularization it us a custom gru layer with a torch backend it is at least two time slower than uniskip however the dropout is sampled once for all time step for each linear best regularization equivalent to uniskip but with a bi sequential gru,"[('cudnn implementation', 0.4938), ('nn gru layer', 0.4878), ('sequential gru', 0.4798), ('nn grucell layer', 0.4593), ('skip thought', 0.4045), ('skip', 0.4009), ('torch', 0.4002), ('torch backend', 0.39), ('fastest implementation', 0.3843), ('custom gru layer', 0.3488)]","[-1.15042083e-01 -7.08624795e-02  2.66538430e-02 -4.49765585e-02
 -3.17716710e-02  4.93230931e-02 -7.84658547e-03  8.36151012e-04
 -3.61798629e-02 -1.00210786e-01  2.41544880e-02 -9.16460156e-03
 -1.02341309e-01 -1.28918178e-02 -6.46994114e-02 -6.81988820e-02
  4.19830643e-02  1.19423307e-02  1.74071584e-02 -1.03717133e-01
 -3.10711883e-04 -4.17086557e-02  5.19290939e-02 -1.72685906e-02
  1.42486896e-02 -1.28729660e-02  3.29460105e-04  5.93855605e-03
  1.26762658e-01 -4.58394103e-02  5.46838976e-02  4.12699878e-02
 -5.81267215e-02  7.01954439e-02 -7.53011927e-02  1.28229996e-02
 -5.23580089e-02  1.10015199e-02 -6.83633760e-02  1.31652383e-02
 -2.39756722e-02 -1.28585184e-02 -2.37942394e-02  1.89449154e-02
  8.32646266e-02 -6.87568262e-02  5.13541587e-02 -5.91879571e-03
 -6.61496073e-02 -6.85648322e-02  6.79398552e-02 -3.21509801e-02
 -1.42006408e-02  8.15704316e-02  1.04070760e-01 -4.27445993e-02
 -4.42250781e-02 -6.08923621e-02 -5.53526403e-03 -5.54951243e-02
  6.30868375e-02 -5.67884222e-02 -5.44871129e-02 -2.22760122e-02
  1.14406198e-02  3.03795859e-02 -8.56084470e-03 -5.18530654e-03
  1.05429456e-01 -9.49498266e-03  5.66548184e-02  6.27806187e-02
 -2.25589685e-02  6.45968020e-02 -3.98056544e-02  5.94606474e-02
  9.03544500e-02  2.69103162e-02  4.05976037e-03 -7.74977058e-02
  1.13220112e-02  8.90158191e-02  7.34303445e-02  2.66788993e-02
  7.85992667e-02 -2.98103932e-02  3.01929400e-03  8.40433314e-03
 -4.06661108e-02 -7.48429149e-02  8.53713509e-03 -3.32054123e-02
  3.10974475e-03 -2.83141211e-02  3.97456959e-02  4.81990166e-02
 -7.21115172e-02 -9.08347964e-02 -8.76588076e-02  3.89172845e-02
  2.23834701e-02 -1.27220396e-02  8.39309394e-03  3.17207612e-02
 -2.94613317e-02  5.77724576e-02  2.03041900e-02  9.36979875e-02
 -5.19845188e-02 -1.13189798e-02  3.19818547e-03  5.24589270e-02
  4.83924104e-03 -7.95689449e-02  7.04066223e-03 -4.65516262e-02
 -4.00112569e-03  1.88861936e-02  3.80912982e-02  1.09282605e-01
 -8.82947147e-02  6.34375885e-02 -7.52035901e-02  4.62478511e-02
 -3.60756256e-02  3.47259529e-02  1.54658109e-02  3.78205352e-33
  4.49640006e-02  1.06312754e-02 -8.78764763e-02 -9.57493857e-02
  2.69037392e-02  4.21986403e-03  3.71898860e-02 -6.44384325e-03
 -3.26016769e-02 -2.21333280e-02 -3.59909758e-02 -5.06277615e-03
 -3.73707563e-02  1.06437758e-01  4.98984056e-03 -6.07771054e-02
  1.41278440e-02  2.79220864e-02  2.40617469e-02 -6.02409802e-02
  6.53706538e-03  1.74384955e-02  1.10020274e-02 -7.64121162e-03
 -4.08586338e-02  5.68067320e-02 -2.82815937e-02  1.22122420e-02
  4.48847376e-03 -8.06885120e-03  1.62125714e-02 -6.30271574e-03
  5.93093559e-02  1.34202186e-02 -6.77560968e-03  1.83979038e-03
  3.88101675e-02 -3.91942523e-02  7.87446871e-02 -5.36291674e-02
 -3.76014709e-02  7.36499503e-02  8.45462177e-03 -3.12136915e-02
 -4.01352532e-02 -2.95780897e-02 -3.02429516e-02  2.85004489e-02
  7.06995092e-03 -2.28628796e-02  5.85275441e-02  3.40573117e-02
 -1.01356193e-01  1.66375656e-02  7.37514719e-02 -5.06569333e-02
  8.82078260e-02  3.99244875e-02  9.90517959e-02  5.20703085e-02
  3.82651985e-02  3.93925644e-02 -3.14908363e-02  1.80389676e-02
 -6.71371222e-02 -3.82999494e-03 -4.43102531e-02  1.74152218e-02
  1.84820667e-02 -7.32878149e-02 -9.61217135e-02  3.33027057e-02
  6.76917005e-03 -2.79512834e-02  8.44676644e-02 -3.42739560e-02
 -2.95623280e-02 -1.80584669e-01 -2.11702893e-03  6.31709546e-02
 -9.09070820e-02  6.36688992e-02 -5.88306859e-02 -5.00637405e-02
 -6.65694773e-02 -2.56467685e-02 -1.34675021e-04  2.70591322e-02
  4.85133752e-02 -2.86614615e-02 -6.79730102e-02  6.53289305e-03
  1.12476483e-01 -2.00104434e-02 -3.20504722e-03 -2.40777904e-33
  1.02160743e-03  1.95359159e-02  3.38863494e-04  4.36398834e-02
  6.77559450e-02  2.43947981e-03 -2.01934446e-02 -7.49329776e-02
 -1.66541748e-02  1.83268990e-02  4.66655679e-02 -5.77470362e-02
  2.53573637e-02  2.69990005e-02  5.35741933e-02  2.04768367e-02
 -4.22018506e-02 -7.65504166e-02 -3.07446020e-03 -3.36698629e-02
  1.15220053e-02  7.70154819e-02 -1.15357786e-01 -5.73393237e-03
 -1.13548823e-01  9.13497433e-03  8.83527752e-03  6.89808978e-03
  2.84363311e-02 -4.56684362e-03 -9.56831966e-03 -5.25814109e-02
 -9.61820856e-02  4.01980802e-02  4.46314132e-03  3.95924747e-02
  9.57989693e-02  8.91391784e-02 -5.89668266e-02 -7.58664459e-02
  1.34007588e-01 -3.07486672e-02  3.52052860e-02  5.92009537e-02
 -5.16495295e-02  2.23963521e-02 -5.83983138e-02  4.29681689e-02
 -7.51904026e-02  7.29799792e-02 -3.99322771e-02 -1.09101338e-02
 -1.40671013e-02 -8.85983813e-04  3.74901332e-02 -1.56005202e-02
  4.72242720e-02 -1.11648887e-02 -9.63456463e-03  3.68856229e-02
  1.60630103e-02 -1.15855232e-01  3.94546203e-02 -4.16898653e-02
  2.76330728e-02  1.04488283e-02 -4.22154292e-02  1.25704810e-01
 -2.03006878e-03 -3.75738018e-03 -2.89454274e-02  9.32093188e-02
  3.47056277e-02 -2.90038157e-02 -1.19889351e-02 -2.25973073e-02
  4.26795222e-02  1.52112078e-02  8.02977085e-02 -1.30944354e-02
 -3.72632742e-02  2.81456653e-02 -4.75047454e-02  6.65439814e-02
  6.44882843e-02  7.97805861e-02  1.42323403e-02  3.79269174e-03
  5.46096601e-02  1.49153778e-02 -3.39502422e-03  3.07132602e-02
  8.19984302e-02  1.35669252e-02  5.27676456e-02 -2.56218744e-08
 -3.35631706e-02 -4.07483093e-02  2.77638584e-02  1.33652771e-02
  3.31470445e-02 -3.56341712e-02  3.24180909e-02  5.19352742e-02
  4.76638647e-03 -5.61625771e-02  6.37181923e-02  3.99968736e-02
 -4.01102975e-02 -3.00054103e-02  3.94703485e-02  8.39281231e-02
  7.80744851e-02 -1.40128015e-02  1.75644625e-02 -8.66486728e-02
 -2.41346154e-02  8.02148506e-02 -1.11728320e-02 -2.59238441e-04
 -4.86168601e-02 -2.09659506e-02  5.94114363e-02  5.33915497e-02
 -2.79102046e-02  1.24601955e-02 -2.10723095e-03  3.96656664e-03
  2.98726670e-02 -3.74229401e-02  1.17607079e-01  9.42844748e-02
  2.46723182e-02  6.02479540e-02  1.08957328e-02 -2.27876399e-02
  1.15954352e-03 -2.25299392e-02  3.56717594e-02 -3.37645002e-02
 -5.55442460e-02 -5.69797959e-03 -6.40420765e-02 -1.11314856e-01
 -1.76557135e-02  6.79262206e-02 -3.87543440e-02 -2.25797128e-02
 -1.98819325e-03  6.56612292e-02  4.66042693e-05  2.48549897e-02
 -2.65165884e-02 -1.25602916e-01 -2.08655242e-02  3.60208564e-02
 -2.08782144e-02  2.30755447e-03 -5.84285259e-02  6.83372915e-02]",2,2
torch-sparse,3,this package consists of a small extension library of optimized sparse matrix operation with autograd support this package currently consists of the following method all included operation work on varying data type and are implemented both for cpu and gpu to avoid the hazzle of creating torch sparse coo tensor this package defines operation on sparse tensor by simply passing index and value tensor a argument with same shape a defined in pytorch note that only value come with autograd support a index is discrete and therefore not differentiable update you can now install pytorch sparse via anaconda for all major o pytorch cuda combination given that you have pytorch installed simply runwe alternatively provide pip wheel for all major o pytorch cuda combination see here to install the binary for pytorch simply runwhere cuda should be replaced by either cpu cu cu or cu depending on your pytorch installation to install the binary for pytorch simply runwhere cuda should be replaced by either cpu cu cu or cu depending on your pytorch installation note binary of older version are also provided for pytorch pytorch pytorch pytorch pytorch pytorch and pytorch following the same procedure for older version you might need to explicitly specify the latest supported version number in order to prevent a manual installation from source you can look up the latest supported version number here ensure that at least pytorch is installed and verify that cuda bin and cuda include are in your path and cpath respectively e g if you want to additionally build torch sparse with metis support e g for partioning please download and install the metis library by following the instruction in the install txt file note that metis need to be installed with bit idxtypewidth by changing include metis h afterwards set the environment variable with metis then run when running in a docker container without nvidia driver pytorch need to evaluate the compute capability and may fail in this case ensure that the compute capability are set via torch cuda arch list e g row wise sort index and remove duplicate entry duplicate entry are removed by scattering them together for scattering any operation of torch scatter can be used transpose dimension and of a sparse matrix matrix product of a sparse matrix with a dense matrix matrix product of two sparse tensor both input sparse matrix need to be coalesced use the coalesced attribute to force torch sparse also offer a c api that contains c equivalent of python model for this we need to add torchlib to the dcmake prefix path e g it may exists in conda lib python x x site package torch if installed via conda,"[('pytorch sparse', 0.6934), ('pytorch installation', 0.6223), ('major o pytorch cuda combination', 0.5922), ('torch sparse coo tensor', 0.5868), ('sparse tensor', 0.5372), ('nvidia driver pytorch', 0.5198), ('pytorch pytorch pytorch pytorch pytorch pytorch', 0.5038), ('pytorch', 0.4872), ('least pytorch', 0.4852), ('torch sparse', 0.4706)]","[-7.80796185e-02 -3.83932739e-02 -2.59154588e-02  8.53528362e-03
  2.76532266e-02  1.01607665e-02 -3.41356248e-02  2.40911283e-02
 -9.92982164e-02 -7.82004893e-02 -1.11251436e-02 -1.09381159e-03
 -1.25817895e-01  5.01683243e-02  9.00447462e-03  5.18352874e-02
  1.82639901e-02  5.54157123e-02 -2.10937746e-02 -1.05802417e-01
 -6.18311353e-02 -1.06780022e-01  2.56579891e-02  7.15189986e-03
  1.25794057e-02 -1.17081152e-04  3.91387977e-02 -5.55290170e-02
  5.60030341e-02 -5.35039743e-03 -1.32659096e-02  2.20343620e-02
  3.02276574e-02  2.38650236e-02  2.45719459e-02 -4.68862802e-02
  2.40801740e-03 -2.44449731e-02 -4.95682545e-02  2.62598377e-02
 -2.35636864e-04 -2.86940811e-03 -4.97977212e-02  5.77853024e-02
  9.13673267e-03 -4.91097830e-02  8.18197876e-02  1.45880301e-02
  6.02966128e-03 -1.34483084e-01 -4.55107633e-03 -4.53714579e-02
 -8.33197162e-02  1.43290404e-02 -7.71038560e-03  8.77679791e-03
 -5.10639837e-03 -5.78860268e-02  2.31002383e-02 -7.00801685e-02
  5.00613786e-02 -5.44610508e-02 -3.55535150e-02  6.70525655e-02
  1.55189764e-02  6.97431788e-02  4.65257019e-02 -1.59882568e-02
  1.05365992e-01 -7.33227357e-02 -1.01808803e-02 -8.71488731e-03
 -6.49700165e-02  8.01893026e-02 -1.82506610e-02 -2.39713639e-02
  6.80382028e-02  1.05557060e-02  2.38479860e-02 -5.42363636e-02
 -1.92563999e-02  1.18734157e-02 -2.20104610e-03 -7.56277656e-03
  6.73678964e-02 -1.58613175e-02 -3.65267992e-02  7.33419061e-02
  2.29538040e-04 -4.85042147e-02  7.41439909e-02 -7.57988989e-02
 -3.35795581e-02  7.95059428e-02 -5.58282211e-02 -1.08619023e-03
  9.66139212e-02 -6.44082129e-02 -8.78851414e-02  4.74882089e-02
 -3.10094003e-02 -1.15657218e-01  9.65099968e-03  9.79059637e-02
 -6.22752011e-02  4.33353856e-02  2.53895042e-03  9.35257077e-02
  2.82432586e-02  5.89143969e-02  3.84937599e-02  3.70379500e-02
 -6.85210852e-03 -6.27575740e-02  9.50543862e-03 -2.35432331e-02
 -5.79796098e-02  5.29181287e-02  8.81807506e-02  7.38077983e-02
 -6.04130290e-02  6.62778318e-03 -2.83745956e-02  6.52197283e-03
 -3.14931758e-02  4.38654609e-02 -6.22955747e-02  1.26476653e-32
  3.15882266e-02  8.32259953e-02 -2.32454371e-02 -4.68544513e-02
  1.27918441e-02 -2.91606113e-02  6.77378029e-02  2.00231336e-02
  3.42561565e-02 -1.60624497e-02 -4.01996821e-02  2.99022533e-02
 -1.27323894e-02  7.65623152e-02 -3.52591425e-02 -5.50691374e-02
  9.83303133e-03  7.34158978e-02  2.73429155e-02  8.76321085e-03
  4.17096950e-02  1.39279410e-01 -3.89467254e-02  8.39760602e-02
 -5.51587604e-02  9.54484940e-03 -2.41139140e-02 -7.90308714e-02
 -2.05097301e-03  2.75456831e-02 -7.42513388e-02 -1.89904552e-02
  6.00739755e-02 -2.34375219e-03  1.69307124e-02  2.14999504e-02
 -4.28483337e-02 -6.99685514e-02  9.70328785e-03 -9.44498777e-02
 -6.50948659e-02  5.78131825e-02 -2.03221962e-02 -1.12848490e-01
 -2.15171427e-02  2.66036149e-02 -1.79869344e-03  6.21839464e-02
 -6.18538773e-03 -4.55989987e-02 -1.03369385e-01  6.80325627e-02
 -5.58731370e-02  1.16327725e-01  4.33383472e-02 -6.90641105e-02
  3.30277300e-03 -4.73392494e-02  1.30070865e-01 -9.56061587e-04
  4.75546019e-03  4.91517819e-02  2.20166538e-02 -1.57743897e-02
 -4.10764590e-02 -4.74754199e-02 -3.70678715e-02 -3.45517695e-03
 -3.94187905e-02  6.16994761e-02 -1.11371584e-01  7.05510452e-02
 -4.48998176e-02 -3.09092682e-02  7.42900074e-02 -2.11202707e-02
  5.67740090e-02 -5.27048707e-02 -4.50583212e-02 -1.18743824e-02
 -1.33306131e-01  2.16112640e-02  4.26081941e-02 -1.71965007e-02
 -9.87943336e-02  6.13199286e-02  1.11573853e-03  2.09465344e-02
  7.71571463e-03 -2.73480825e-02 -1.10150702e-01  5.02255885e-03
  8.45498592e-02 -9.27188620e-03  1.46545898e-02 -9.97404753e-33
  1.68226715e-02 -2.45121885e-02  1.74466856e-02  3.57871987e-02
  2.93395147e-02  4.86156940e-02 -5.41041829e-02 -8.16714317e-02
  4.73878831e-02 -2.65726205e-02  3.88243166e-03  3.37908566e-02
  2.90133022e-02 -3.74764726e-02  7.38807907e-03  3.39647941e-02
 -3.62971164e-02  2.29165014e-02 -5.48968418e-03  2.03293301e-02
 -5.48682958e-02  7.08009303e-02 -9.58266109e-02 -5.81430644e-02
 -1.57364123e-02 -1.46158207e-02  4.57395660e-03 -8.85059778e-03
  9.51859402e-04  1.77304167e-02  4.49876003e-02  2.04198584e-02
 -4.65669036e-02  2.39837989e-02 -6.98713288e-02  5.97635135e-02
  6.73007593e-02  1.88273024e-02 -7.45919570e-02  6.20352663e-02
  1.22831389e-01  6.80668429e-02 -3.05880196e-02  1.20358188e-02
 -3.92185152e-02  1.83715597e-02 -1.01173304e-01 -2.45694667e-02
 -3.47357653e-02  5.90715632e-02  2.50729825e-02 -1.44832479e-02
 -3.40563394e-02  6.28738431e-04 -2.06241719e-02  6.17880281e-03
  8.63494202e-02  1.30283302e-02  7.24454690e-03  2.11609956e-02
 -4.47742939e-02 -5.65574467e-02 -1.66224912e-02 -3.67612764e-02
  1.19272824e-02 -3.04716099e-02 -2.30417307e-02  1.24170601e-01
 -1.30442274e-03 -2.44857986e-02 -4.71750014e-02  7.39068985e-02
  5.23379333e-02  6.87237754e-02 -1.12587912e-02  8.12971070e-02
  3.63914110e-02  3.89165580e-02  3.75934392e-02  1.28020123e-02
  4.70044911e-02 -3.47997360e-02  3.65667269e-02  4.25565019e-02
  2.23231222e-02  1.23505041e-01  5.53030223e-02 -3.42985019e-02
  4.90912348e-02 -7.55631132e-03 -3.31048481e-02 -2.27395748e-03
  1.03701003e-01  6.95949793e-02  5.62924258e-02 -3.88196248e-08
 -3.53322960e-02  2.16453802e-02 -1.01978807e-02  2.42574862e-03
  4.37669978e-02 -1.95360295e-02  2.50535626e-02  1.13186568e-01
 -2.72648353e-02 -9.10577364e-03  4.98821437e-02 -5.94912469e-02
 -4.04629521e-02  1.15390532e-02  2.77873408e-02  8.08401555e-02
 -5.17597701e-03 -2.78555858e-03  6.93312613e-03 -3.42085883e-02
  1.33206602e-02  1.45744942e-02  3.71655519e-03  1.85481478e-02
 -8.46172497e-03 -3.48440930e-02  6.41361549e-02  2.93249283e-02
  3.74153219e-02 -4.93523804e-03 -8.12872201e-02 -3.53189334e-02
  8.58314708e-02 -7.41327256e-02  1.23449758e-01  8.45104679e-02
  2.18131170e-02 -3.31037864e-02 -5.39524369e-02  2.08666306e-02
 -6.12845868e-02 -4.87821773e-02  2.87441798e-02 -4.65674512e-02
  2.71882787e-02 -3.99168842e-02 -3.04630809e-02 -8.19202065e-02
 -4.68608774e-02 -6.53168699e-03 -2.12413128e-02  5.22513725e-02
 -2.85675451e-02  7.06746280e-02  3.57314982e-02  6.26387000e-02
 -1.67770702e-02 -3.62345763e-02 -3.82912681e-02 -7.81926066e-02
  5.89753538e-02  1.84906889e-02 -3.75392996e-02 -1.68862101e-02]",2,2
pytorch_pretrained_bert,3,this repository contains op for op pytorch reimplementations pre trained model and fine tuning example for these implementation have been tested on several datasets see the example and should match the performance of the associated tensorflow implementation e g f on squad for bert f on rocstories for openai gpt and perplexity on wikitext for the transformer xl you can find more detail in the example section below here are some information on these model bert wa released together with the paper bert pre training of deep bidirectional transformer for language understanding by jacob devlin ming wei chang kenton lee and kristina toutanova this pytorch implementation of bert is provided with google s pre trained model example notebook and a command line interface to load any pre trained tensorflow checkpoint for bert is also provided openai gpt wa released together with the paper improving language understanding by generative pre training by alec radford karthik narasimhan tim salimans and ilya sutskever this pytorch implementation of openai gpt is an adaptation of the pytorch implementation by huggingface and is provided with openai s pre trained model and a command line interface that wa used to convert the pre trained numpy checkpoint in pytorch google cmu s transformer xl wa released together with the paper transformer xl attentive language model beyond a fixed length context by zihang dai zhilin yang yiming yang jaime carbonell quoc v le ruslan salakhutdinov this pytorch implementation of transformer xl is an adaptation of the original pytorch implementation which ha been slightly modified to match the performance of the tensorflow implementation and allow to re use the pretrained weight a command line interface is provided to convert tensorflow checkpoint in pytorch model openai gpt wa released together with the paper language model are unsupervised multitask learner by alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever this pytorch implementation of openai gpt is an adaptation of the openai s implementation and is provided with openai s pre trained model and a command line interface that wa used to convert the tensorflow checkpoint in pytorch this repo wa tested on python and example are tested only on python and pytorch pytorch pretrained bert can be installed by pip a follows if you want to reproduce the original tokenization process of the openai gpt paper you will need to install ftfy limit to version if you are using python and spacy if you don t install ftfy and spacy the openai gpt tokenizer will default to tokenize using bert s basictokenizer followed by byte pair encoding which should be fine for most usage don t worry clone the repository and run here also if you want to reproduce the original tokenization process of the openai gpt model you will need to install ftfy limit to version if you are using python and spacy again if you don t install ftfy and spacy the openai gpt tokenizer will default to tokenize using bert s basictokenizer followed by byte pair encoding which should be fine for most usage a series of test is included in the test folder and can be run using pytest install pytest if needed pip install pytest you can run the test with the command this package comprises the following class that can be imported in python and are detailed in the doc section of this readme eight bert pytorch model torch nn module with pre trained weight in the modeling py file three openai gpt pytorch model torch nn module with pre trained weight in the modeling openai py file two transformer xl pytorch model torch nn module with pre trained weight in the modeling transfo xl py file three openai gpt pytorch model torch nn module with pre trained weight in the modeling gpt py file tokenizers for bert using word piece in the tokenization py file tokenizer for openai gpt using byte pair encoding in the tokenization openai py file tokenizer for transformer xl word token ordered by frequency for adaptive softmax in the tokenization transfo xl py file tokenizer for openai gpt using byte level byte pair encoding in the tokenization gpt py file optimizer for bert in the optimization py file optimizer for openai gpt in the optimization openai py file configuration class for bert openai gpt and transformer xl in the respective modeling py modeling openai py modeling transfo xl py file the repository further comprises five example on how to use bert in the example folder one example on how to use openai gpt in the example folder one example on how to use transformer xl in the example folder one example on how to use openai gpt in the unconditional and interactive mode in the example folder these example are detailed in the example section of this readme three notebook that were used to check that the tensorflow and pytorch model behave identically in the notebook folder these notebook are detailed in the notebook section of this readme a command line interface to convert tensorflow checkpoint bert transformer xl or numpy checkpoint openai in a pytorch save of the associated pytorch model this cli is detailed in the command line interface section of this readme here is a quick start example using berttokenizer bertmodel and bertformaskedlm class with google ai s pre trained bert base uncased model see the doc section below for all the detail on these class first let s prepare a tokenized input with berttokenizerlet s see how to use bertmodel to get hidden statesand how to use bertformaskedlmhere is a quick start example using openaigpttokenizer openaigptmodel and openaigptlmheadmodel class with openai s pre trained model see the doc section below for all the detail on these class first let s prepare a tokenized input with openaigpttokenizerlet s see how to use openaigptmodel to get hidden statesand how to use openaigptlmheadmodelhere is a quick start example using transfoxltokenizer transfoxlmodel and transfoxlmodellmheadmodel class with the transformer xl model pre trained on wikitext see the doc section below for all the detail on these class first let s prepare a tokenized input with transfoxltokenizerlet s see how to use transfoxlmodel to get hidden statesand how to use transfoxllmheadmodelhere is a quick start example using gpt tokenizer gpt model and gpt lmheadmodel class with openai s pre trained model see the doc section below for all the detail on these class first let s prepare a tokenized input with gpt tokenizerlet s see how to use gpt model to get hidden statesand how to use gpt lmheadmodelhere is a detailed documentation of the class in the package and how to use them to load one of google ai s openai s pre trained model or a pytorch saved model an instance of bertforpretraining saved with torch save the pytorch model class and the tokenizer can be instantiated aswherebert class is either a tokenizer to load the vocabulary berttokenizer or openaigpttokenizer class or one of the eight bert or three openai gpt pytorch model class to load the pre trained weight bertmodel bertformaskedlm bertfornextsentenceprediction bertforpretraining bertforsequenceclassification bertfortokenclassification bertformultiplechoice bertforquestionanswering openaigptmodel openaigptlmheadmodel or openaigptdoubleheadsmodel andpre trained model name or path is either the shortcut name of a google ai s or openai s pre trained model selected in the list a path or url to a pretrained model archive containing if pre trained model name or path is a shortcut name the pre trained weight will be downloaded from aws s see the link here and stored in a cache folder to avoid future download the cache folder can be found at pytorch pretrained bert cache dir can be an optional path to a specific directory to download and cache the pre trained model weight this option is useful in particular when you are using distributed training to avoid concurrent access to the same weight you can set for example cache dir pretrained model format args local rank see the section on distributed training for more information uncased mean that the text ha been lowercased before wordpiece tokenization e g john smith becomes john smith the uncased model also strip out any accent marker cased mean that the true case and accent marker are preserved typically the uncased model is better unless you know that case information is important for your task e g named entity recognition or part of speech tagging for information about the multilingual and chinese model see the multilingual readme or the original tensorflow repository when using an uncased model make sure to pas do lower case to the example training script or pas do lower case true to fulltokenizer if you re using your own script and loading the tokenizer your self example this section explain how you can save and re load a fine tuned model bert gpt gpt and transformer xl there are three type of file you need to save to be able to reload a fine tuned model here is the recommended way of saving the model configuration and vocabulary to an output dir directory and reloading the model and tokenizer afterwards here is another way you can save and reload the model if you want to use specific path for each type of file model bert gpt gpt and transformer xl are defined and build from configuration class which containes the parameter of the model number of layer dimensionality and a few utility to read and write from json configuration file the respective configuration class are these configuration class contains a few utility to load and save configuration bertmodel is the basic bert transformer model with a layer of summed token position and sequence embeddings followed by a series of identical self attention block for bert base for bert large the input and output are identical to the tensorflow model input and output we detail them here this model take a input modeling pythis model output a tuple composed of encoded layer controled by the value of the output encoded layer argument pooled output a torch floattensor of size batch size hidden size which is the output of a classifier pretrained on top of the hidden state associated to the first character of the input clf to train on the next sentence task see bert s paper an example on how to use this class is given in the extract feature py script which can be used to extract the hidden state of the model for a given input bertforpretraining includes the bertmodel transformer followed by the two pre training head input comprises the input of the bertmodel class plus two optional label output if masked lm label and next sentence label are not none output the total loss which is the sum of the masked language modeling loss and the next sentence classification loss if masked lm label or next sentence label is none output a tuple comprisingan example on how to use this class is given in the run lm finetuning py script which can be used to fine tune the bert language model on your specific different text corpus this should improve model performance if the language style is different from the original bert training corpus wiki bookcorpus bertformaskedlm includes the bertmodel transformer followed by the possibly pre trained masked language modeling head input comprises the input of the bertmodel class plus optional label output bertfornextsentenceprediction includes the bertmodel transformer followed by the next sentence classification head input comprises the input of the bertmodel class plus an optional label output bertforsequenceclassification is a fine tuning model that includes bertmodel and a sequence level sequence or pair of sequence classifier on top of the bertmodel the sequence level classifier is a linear layer that take a input the last hidden state of the first character in the input sequence see figure a and b in the bert paper an example on how to use this class is given in the run classifier py script which can be used to fine tune a single sequence or pair of sequence classifier using bert for example for the mrpc task bertformultiplechoice is a fine tuning model that includes bertmodel and a linear layer on top of the bertmodel the linear layer output a single value for each choice of a multiple choice problem then all the output corresponding to an instance are passed through a softmax to get the model choice this implementation is largely inspired by the work of openai in improving language understanding by generative pre training and the answer of jacob devlin in the following issue an example on how to use this class is given in the run swag py script which can be used to fine tune a multiple choice classifier using bert for example for the swag task bertfortokenclassification is a fine tuning model that includes bertmodel and a token level classifier on top of the bertmodel the token level classifier is a linear layer that take a input the last hidden state of the sequence bertforquestionanswering is a fine tuning model that includes bertmodel with a token level classifier on top of the full sequence of last hidden state the token level classifier take a input the full sequence of the last hidden state and compute several e g two score for each token that can for example respectively be the score that a given token is a start span and a end span token see figure c and d in the bert paper an example on how to use this class is given in the run squad py script which can be used to fine tune a token classifier using bert for example for the squad task openaigptmodel is the basic openai gpt transformer model with a layer of summed token and position embeddings followed by a series of identical self attention block openai gpt use a single embedding matrix to store the word and special embeddings special token embeddings are additional token that are not pre trained sep cl special token need to be trained during the fine tuning if you use them the number of special embeddings can be controled using the set num special token num special token function the embeddings are ordered a follow in the token embeddings matrice where total token embeddings can be obtained a config total token embeddings and is total token embeddings config vocab size config n special you should use the associate index to index the embeddings the input and output are identical to the tensorflow model input and output we detail them here this model take a input modeling openai pythis model output openaigptlmheadmodel includes the openaigptmodel transformer followed by a language modeling head with weight tied to the input embeddings no additional parameter input are the same a the input of the openaigptmodel class plus optional label output openaigptdoubleheadsmodel includes the openaigptmodel transformer followed by two head input are the same a the input of the openaigptmodel class plus a classification mask and two optional label output the transformer xl model is described in transformer xl attentive language model beyond a fixed length context transformer xl use a relative positioning with sinusiodal pattern and adaptive softmax input which mean that this model take a input modeling transfo xl pythis model output a tuple of last hidden state new mem the new mem contain all the hidden state plus the output of the embeddings new mem new mem is the output of the hidden state of the layer below the last layer and last hidden state is the output of the last layer i e the input of the softmax when we have a language modeling head on top there are two difference between the shape of new mem and last hidden state new mem have transposed first dimension and are longer of size self config mem len here is how to extract the full list of hidden state from the model output transfoxllmheadmodel includes the transfoxlmodel transformer followed by an adaptive softmax head with weight tied to the input embeddings input are the same a the input of the transfoxlmodel class plus optional label output a tuple of last hidden state new mem gpt model is the openai gpt transformer model with a layer of summed token and position embeddings followed by a series of identical self attention block the input and output are identical to the tensorflow model input and output we detail them here this model take a input modeling gpt pythis model output gpt lmheadmodel includes the gpt model transformer followed by a language modeling head with weight tied to the input embeddings no additional parameter input are the same a the input of the gpt model class plus optional label output gpt doubleheadsmodel includes the gpt model transformer followed by two head input are the same a the input of the gpt model class plus a classification mask and two optional label output berttokenizer perform end to end tokenization i e basic tokenization followed by wordpiece tokenization this class ha five argument and three method please refer to the doc string and code in tokenization py for the detail of the basictokenizer and wordpiecetokenizer class in general it is recommended to use berttokenizer unless you know what you are doing openaigpttokenizer perform byte pair encoding bpe tokenization this class ha four argument and five method please refer to the doc string and code in tokenization openai py for the detail of the openaigpttokenizer transfoxltokenizer perform word tokenization this tokenizer can be used for adaptive softmax and ha utility for counting token in a corpus to create a vocabulary ordered by toekn frequency for adaptive softmax see the adaptive softmax paper efficient softmax approximation for gpus for more detail the api is similar to the api of berttokenizer see above please refer to the doc string and code in tokenization transfo xl py for the detail of these additional method in transfoxltokenizer gpt tokenizer perform byte level byte pair encoding bpe tokenization this class ha three argument and two method please refer to tokenization gpt py for more detail on the gpt tokenizer bertadam is a torch optimizer adapted to be closer to the optimizer used in the tensorflow implementation of bert the difference with pytorch adam optimizer are the following the optimizer accepts the following argument openaiadam is similar to bertadam the difference with bertadam is that openaiadam compensate for bias a in the regular adam optimizer openaiadam accepts the same argument a bertadam the optimization module also provides additional schedule in the form of schedule object that inherit from lrschedule all lrschedule subclass accept warmup and t total argument at construction when an lrschedule object is passed into bertadam or openaiadam the warmup and t total argument on the optimizer are ignored and the one in the lrschedule object are used an overview of the implemented schedule bert base and bert large are respectively m and m parameter model and it can be difficult to fine tune them on a single gpu with the recommended batch size for good performance in most case a batch size of to help with fine tuning these model we have included several technique that you can activate in the fine tuning script run classifier py and run squad py gradient accumulation multi gpu training distributed training and bit training for more detail on how to use these technique you can read the tip on training large batch in pytorch that i published earlier this month here is how to use these technique in our script to use bit training and distributed training you need to install nvidia s apex extension a detailed here you will find more information regarding the internals of apex and how to use apex in the doc and the associated repository the result of the test performed on pytorch bert by the nvidia team and my trial at reproducing them can be consulted in the relevant pr of the present repository note to use distributed training you will need to run one training script on each of your machine this can be done for example by running the following command on each server see the above mentioned blog post for more detail where this machine index is an sequential index assigned to each of your machine and the machine with rank ha an ip address and an open port we showcase several fine tuning example based on and extended from the original implementation we get the following result on the dev set of glue benchmark with an uncased bert base model all experiment were run on a p gpu with a batch size of some of these result are significantly different from the one reported on the test set of glue benchmark on the website for qqp and wnli please refer to faq on the webite before running anyone of these glue task you should download the glue data by running this script and unpack it to some directory glue dir where task name can be one of cola sst mrpc sts b qqp mnli qnli rte wnli the dev set result will be present within the text file eval result txt in the specified output dir in case of mnli since there are two separate dev set matched and mismatched there will be a separate output folder called tmp mnli mm in addition to tmp mnli the code ha not been tested with half precision training with apex on any glue task apart from mrpc mnli cola sst the following section provides detail on how to run half precision training with mrpc with that being said there shouldn t be any issue in running half precision training with the remaining glue task a well since the data processor for each task inherits from the base class dataprocessor this example code fine tune bert on the microsoft research paraphrase corpus mrpc corpus and run in le than minute on a single k and in second on single tesla v gb with apex installed before running this example you should download the glue data by running this script and unpack it to some directory glue dir our test ran on a few seed with the original implementation hyper parameter gave evaluation result between and fast run with apex and bit precision fine tuning on mrpc in second first install apex a indicated here then runthis example code fine tune bert on the squad dataset it run in min with bert base or min with bert large on a single tesla v gb the data for squad can be downloaded with the following link and should be saved in a squad dir directory training with the previous hyper parameter gave u the following result the data for swag can be downloaded by cloning the following repositorytraining with the previous hyper parameter on a single gpu gave u the following result the data should be a text file in the same format a sample text txt one sentence per line doc separated by empty line you can download an exemplary training corpus generated from wikipedia article and splitted into k sentence with spacy training one epoch on this corpus take about h on x nvidia tesla p with train batch size and max seq length thank to the work of rocketknight and tholor there are now several script that can be used to fine tune bert using the pretraining objective combination of masked language modeling and next sentence prediction loss these script are detailed in the readme of the example lm finetuning folder we provide three example of script for openai gpt transformer xl and openai gpt based on and extended from the respective original implementation this example code fine tune openai gpt on the rocstories dataset before running this example you should download the rocstories dataset and unpack it to some directory roc story dir this command run in about min on a single k an give an evaluation accuracy of about the author report a median accuracy with the tensorflow code of and the openai gpt paper report a best single run accuracy of this example code evaluate the pre trained transformer xl on the wikitext dataset this command will download a pre processed version of the wikitext dataset in which the vocabulary ha been computed this command run in about min on a v and give an evaluation perplexity of on wikitext the author report a perplexity of about on this dataset with the tensorflow code this example code is identical to the original unconditional and conditional generation code conditional generation unconditional generation the same option a in the original script are provided please refere to the code of the example and the original repository of openai the option we list above allow to fine tune bert large rather easily on gpu s instead of the tpu used by the original implementation for example fine tuning bert large on squad can be done on a server with k these are pretty old now in hour our result are similar to the tensorflow implementation result actually slightly higher to get these result we used a combination of here is the full list of hyper parameter for this run if you have a recent gpu starting from nvidia volta series you should try bit fine tuning fp here is an example of hyper parameter for a fp run we tried the result were similar to the above fp result actually slightly higher we include three jupyter notebook that can be used to check that the prediction of the pytorch model are identical to the prediction of the original tensorflow model the first notebook comparing tf and pt model ipynb extract the hidden state of a full sequence on each layer of the tensorflow and the pytorch model and computes the standard deviation between them in the given example we get a standard deviation of e to e on the various hidden state of the model the second notebook comparing tf and pt model squad ipynb compare the loss computed by the tensorflow and the pytorch model for identical initialization of the fine tuning layer of the bertforquestionanswering and computes the standard deviation between them in the given example we get a standard deviation of e between the model the third notebook comparing tf and pt model mlm nsp ipynb compare the prediction computed by the tensorflow and the pytorch model for masked token language modeling using the pre trained masked language modeling model please follow the instruction given in the notebook to run and modify them a command line interface is provided to convert a tensorflow checkpoint in a pytorch dump of the bertforpretraining class for bert or numpy checkpoint in a pytorch dump of the openaigptmodel class for openai gpt you can convert any tensorflow checkpoint for bert in particular the pre trained model released by google in a pytorch save file by using the convert tf checkpoint to pytorch py script this cli take a input a tensorflow checkpoint three file starting with bert model ckpt and the associated configuration file bert config json and creates a pytorch model for this configuration load the weight from the tensorflow checkpoint in the pytorch model and save the resulting model in a standard pytorch save file that can be imported using torch load see example in extract feature py run classifier py and run squad py you only need to run this conversion script once to get a pytorch model you can then disregard the tensorflow checkpoint the three file starting with bert model ckpt but be sure to keep the configuration file bert config json and the vocabulary file vocab txt a these are needed for the pytorch model too to run this specific conversion script you will need to have tensorflow and pytorch installed pip install tensorflow the rest of the repository only requires pytorch here is an example of the conversion process for a pre trained bert base uncased model you can download google s pre trained model for the conversion here here is an example of the conversion process for a pre trained openai gpt model assuming that your numpy checkpoint save a the same format than openai pretrained model see here here is an example of the conversion process for a pre trained transformer xl model see here here is an example of the conversion process for a pre trained openai s gpt model tpu support and pretraining scriptstpu are not supported by the current stable release of pytorch however the next version of pytorch v should support training on tpu and is expected to be released soon see the recent official announcement we will add tpu support when this next release is published the original tensorflow code further comprises two script for pre training bert create pretraining data py and run pretraining py since pre training bert is a particularly expensive operation that basically requires one or several tpus to be completed in a reasonable amout of time see detail here we have decided to wait for the inclusion of tpu support in pytorch to convert these pre training script,"[('basic bert transformer model', 0.6374), ('bert openai gpt', 0.6209), ('model bert gpt gpt', 0.6158), ('bert pytorch model torch nn module', 0.6056), ('tensorflow checkpoint bert transformer xl', 0.6052), ('pytorch model openai gpt', 0.6047), ('pytorch bert', 0.5933), ('bert language model', 0.5791), ('openai gpt pytorch model class', 0.5773), ('basic openai gpt transformer model', 0.5703)]","[-1.56760857e-01 -1.04763083e-01  2.75051072e-02  6.86431304e-03
  6.13270625e-02  2.80084810e-03  2.21043383e-03  9.81701985e-02
 -1.95977016e-04 -6.27615005e-02 -1.76243037e-02  5.72458357e-02
 -6.69730455e-02  5.52043989e-02  3.64507176e-02  2.37252861e-02
 -1.57855581e-02  4.13788408e-02 -6.59902319e-02 -6.35639131e-02
  3.41981426e-02  5.87642081e-02  6.55158758e-02 -1.71515364e-02
 -1.84968971e-02 -3.99368405e-02  5.10075912e-02 -4.23665866e-02
  6.30076900e-02  4.89567667e-02  5.61288372e-03 -2.54554860e-02
 -4.97062430e-02  8.06156769e-02  2.22479533e-02 -1.37920873e-02
 -3.96109447e-02 -3.96833159e-02  2.22979486e-02  3.67135480e-02
  6.05692454e-02 -1.90512538e-02 -2.86293998e-02 -1.66374147e-02
  6.44685328e-02 -6.38181195e-02  3.00800819e-02 -7.66972527e-02
 -3.72719802e-02 -7.77855515e-02  5.82213933e-03 -4.59646583e-02
 -3.72111169e-03  8.76708254e-02  1.68059263e-02  5.67967668e-02
  1.17918467e-02 -8.31273943e-02  1.82814766e-02 -1.36311054e-01
 -5.42348661e-02 -1.13908015e-03 -4.33791317e-02 -2.46776138e-02
 -1.16217658e-01  9.06308591e-02 -5.21037430e-02  4.09568399e-02
  5.54249585e-02 -1.07253306e-01 -7.35811815e-02  4.37467285e-02
 -4.56787869e-02  3.64189707e-02 -1.67461007e-03 -9.23845842e-02
  1.01047479e-01 -3.97080556e-03 -3.24825607e-02 -5.46898581e-02
  2.49046460e-02 -5.50255226e-03  8.43271539e-02  2.23744474e-02
  4.11471985e-02 -7.15800142e-03 -5.21930121e-03  6.92278594e-02
  3.17639001e-02 -1.68368611e-02 -2.25455035e-02 -5.64657897e-02
  5.99222891e-02  3.83674959e-03  1.06812259e-02 -2.42792889e-02
  2.55319737e-02 -6.49963915e-02 -3.78730521e-02  4.58375588e-02
 -1.97912417e-02 -4.51134853e-02  3.39895338e-02  2.87199393e-02
 -2.60367449e-02  5.15494533e-02  6.91515720e-03  4.34148647e-02
 -8.44568480e-03 -6.18627155e-03  4.56660837e-02 -5.76185398e-02
  4.39193845e-02 -1.20183416e-01  2.45935451e-02  1.33992522e-03
 -1.20354861e-01  4.49789576e-02 -3.04500349e-02  8.28828216e-02
 -3.43588158e-03  5.15334159e-02  2.78630070e-02  4.19535488e-02
 -7.82054663e-02  4.70642671e-02 -2.95580514e-02  1.00475343e-32
 -1.49319938e-03  1.43798562e-02 -4.21672175e-03 -2.21569370e-02
  1.83496680e-02  5.96240833e-02  6.84990138e-02  1.00344103e-02
  6.58738464e-02 -7.78019726e-02 -3.74784172e-02  3.67058478e-02
 -9.05090570e-02  3.37599330e-02 -8.02285373e-02  4.46148664e-02
 -6.17902204e-02  9.08940583e-02  4.79905605e-02  4.94772643e-02
  8.65426585e-02  8.94292220e-02  1.84588321e-02  2.89609167e-03
 -9.79477614e-02  4.64128777e-02  3.09042120e-03 -6.01398125e-02
 -2.24240121e-05  2.45298017e-02 -1.12234190e-01  5.11105135e-02
 -1.34919528e-02 -1.69717725e-02  8.28410499e-03  2.24332921e-02
  1.05519770e-02 -6.05571270e-02 -2.29985308e-04 -9.50263590e-02
 -7.73754269e-02  5.70429824e-02 -8.80776253e-03 -6.23848625e-02
  2.39295475e-02  6.86372072e-03  2.11480297e-02  6.26460239e-02
  7.90133029e-02  1.17348656e-02 -4.34875265e-02  2.99589653e-02
 -1.33643791e-01  5.74771091e-02  4.50505093e-02 -3.66810709e-02
  6.60987571e-02  3.12175695e-02  5.55722490e-02  1.09868245e-02
  2.06270162e-02  2.33930144e-02  3.37744281e-02 -9.93474852e-03
  1.88836288e-02 -2.35443451e-02 -8.94808546e-02 -1.09727094e-02
 -1.50878401e-03  3.83937992e-02 -3.21621820e-02  3.13292779e-02
 -1.62106976e-02  2.38826331e-02  2.19705515e-02 -3.61287594e-02
  3.37763131e-02 -8.64272751e-03 -6.30287454e-02  1.65406018e-02
 -8.17116052e-02 -7.58324284e-03 -3.19744982e-02 -3.66063751e-02
 -1.01870030e-01 -1.21630859e-02  3.57637033e-02 -5.46495066e-05
  6.94715977e-03 -3.42632597e-03 -6.68290034e-02 -6.73232824e-02
  8.01904069e-04  6.07944690e-02 -3.61571573e-02 -7.41496106e-33
  1.51505275e-02  8.39106813e-02 -6.99087381e-02  5.90682626e-02
 -1.28536206e-02 -9.50758234e-02  2.49161236e-02  8.62922594e-02
  2.05087811e-02  4.02252264e-02  3.00858822e-02 -3.89696509e-02
  4.19579726e-03 -3.43688354e-02  9.58596095e-02 -6.51267618e-02
 -3.37772295e-02  7.21130287e-03 -2.49682534e-02  2.57955678e-02
 -2.00174544e-02  2.07854640e-02 -1.23319529e-01 -1.31898718e-02
 -3.35094854e-02  6.28074184e-02 -7.07420483e-02  4.05191556e-02
  2.93655433e-02  1.13978591e-02  4.15337794e-02  6.05654791e-02
  2.51350156e-03  5.61516434e-02 -5.13864532e-02  6.34992793e-02
  7.66335875e-02 -6.34066155e-03 -1.22814509e-03 -1.27798086e-02
  8.58691037e-02 -2.51116361e-02  2.82137026e-03  7.08743408e-02
 -6.43639341e-02  1.90041233e-02 -7.31112733e-02 -2.38679000e-03
 -2.87633613e-02 -5.21618836e-02 -7.29872892e-03 -1.76198054e-02
 -3.64554115e-02 -2.80817933e-02 -1.40338130e-02 -5.24837673e-02
  1.21696293e-01 -1.28598288e-02 -2.85647083e-02 -5.04027354e-04
 -7.49328407e-03 -3.57659869e-02  7.18691647e-02 -2.04338320e-02
 -2.95922291e-02 -7.06823468e-02 -2.25089621e-02  5.60652241e-02
  4.04497311e-02  6.10051975e-02 -1.17455080e-01 -8.08540080e-03
  1.02700964e-01  2.24329568e-02  2.97051929e-02  3.03369407e-02
  5.73012559e-03 -1.96387693e-02  6.96671978e-02 -2.60033105e-02
 -3.43288891e-02 -5.93519807e-02  3.16971987e-02  2.75193807e-02
  1.29638575e-02  3.30383889e-02  8.72271284e-02  2.25940347e-03
  3.14075798e-02  5.96393421e-02 -7.99197108e-02  2.52373610e-02
  6.85296282e-02  9.70348194e-02  4.49820459e-02 -3.25279998e-08
 -7.42176026e-02 -4.09606751e-03 -2.80481596e-02  5.13317063e-02
 -3.24887745e-02 -4.34013195e-02 -1.27283530e-02  9.78274867e-02
 -5.74011132e-02  3.44519317e-02  1.68694612e-02 -3.05148978e-02
 -6.09170869e-02  7.86318444e-03  1.40303029e-02  8.54080394e-02
 -3.83132300e-03  2.00766511e-02  1.03129242e-02 -5.85900322e-02
 -2.93656960e-02  3.35293226e-02  2.52088048e-02 -4.04343382e-02
 -6.27574651e-03 -3.39768119e-02 -2.48016268e-02  9.57383588e-02
 -5.52710611e-03  3.62538211e-02  2.95867864e-02  6.85007684e-03
 -1.13831013e-02 -2.33965479e-02  8.13965797e-02  7.24046305e-02
  1.36177596e-02 -2.61435024e-02  1.42127192e-02  5.20633720e-02
  1.72421765e-02  4.41052625e-03 -9.38021764e-02 -1.33688739e-02
  5.63864037e-02 -4.43641003e-03 -6.37164637e-02 -1.27973065e-01
 -4.67351563e-02  6.07866459e-02 -2.32237857e-02 -2.62873098e-02
 -3.62876356e-02  5.71360737e-02 -8.68561566e-02  5.05250208e-02
 -5.61719872e-02 -6.73107896e-03 -2.14872062e-02 -1.90842114e-02
 -4.64982577e-02  9.53018889e-02  2.29367595e-02  5.18115014e-02]",2,2
gpytorch,3,gpytorch is a gaussian process library implemented using pytorch gpytorch is designed for creating scalable flexible and modular gaussian process model with ease internally gpytorch differs from many existing approach to gp inference by performing all inference operation using modern numerical linear algebra technique like preconditioned conjugate gradient implementing a scalable gp method is a simple a providing a matrix multiplication routine with the kernel matrix and it derivative via our linearoperator interface or by composing many of our already existing linearoperators this allows not only for easy implementation of popular scalable gp technique but often also for significantly improved utilization of gpu computing compared to solver based on the cholesky decomposition gpytorch provides significant gpu acceleration through mvm based inference state of the art implementation of the latest algorithmic advance for scalability and flexibility ski kiss gp stochastic lanczos expansion love skip stochastic variational deep kernel learning easy integration with deep learning framework see our numerous example and tutorial on how to construct all sort of model in gpytorch requirement install gpytorch using pip or conda to use package globally but install gpytorch a a user only package use pip install user above to upgrade to the latest unstable version runnote experimental aur package for most user we recommend installation by conda or pip gpytorch is also available on the archlinux user repository aur you can install it with an aur helper like yay a follows to discus any issue related to this aur package refer to the comment section of python gpytorch if you use gpytorch please cite the following paper gardner jacob r geoff pleiss david bindel kilian q weinberger and andrew gordon wilson gpytorch blackbox matrix matrix gaussian process inference with gpu acceleration in advance in neural information processing system to run the unit test by default the random seed are locked down for some of the test if you want to run the test without locking down the seed runif you plan on submitting a pull request please make use of our pre commit hook to ensure that your commits adhere to the general style guideline enforced by the repo to do this navigate to your local repository and run from then on this will automatically run flake isort black and other tool over the file you commit each time you commit to gpytorch or a fork of it gpytorch is primarily maintained by we would like to thank our other contributor including but not limited to david arbour eytan bakshy david eriksson jared frank sam stanton bram wallace ke alexander wang ruihan wu development of gpytorch is supported by funding from the bill and melinda gate foundation the national science foundation sap the simon foundation and the gatsby charitable trust,"[('gaussian process library', 0.5968), ('cholesky decomposition gpytorch', 0.596), ('python gpytorch', 0.5689), ('gpu computing', 0.5373), ('gpytorch', 0.5357), ('pytorch gpytorch', 0.5256), ('scalable gp method', 0.5229), ('pip gpytorch', 0.5031), ('popular scalable gp technique', 0.4909), ('gpytorch requirement install gpytorch', 0.4815)]","[-1.20188974e-01 -1.14577048e-01 -2.18396913e-02 -6.05296902e-02
  4.08828370e-02 -6.62640035e-02  2.96843913e-03  4.25454117e-02
 -7.68566132e-02 -2.48807967e-02 -5.93279675e-02  5.86975440e-02
 -5.53973541e-02 -1.70023041e-03 -5.70501946e-03 -1.25565170e-03
  6.87092450e-03  9.27965865e-02 -6.71707094e-02 -1.04430966e-01
 -3.22413072e-02 -6.04874678e-02 -3.04688793e-02  4.65834513e-02
  1.16640897e-02 -4.49502915e-02  6.08801767e-02 -7.83113092e-02
  2.63368636e-02  3.49734947e-02  4.80457544e-02 -1.31102363e-02
  6.36191517e-02 -3.74416634e-02 -5.07266112e-02  3.69717106e-02
  2.07016412e-02 -3.25745009e-02 -1.48795536e-02  1.84102692e-02
 -6.09584665e-03  2.00708187e-03 -5.60298525e-02  2.66303625e-02
  3.43633853e-02 -2.04156507e-02 -1.83375236e-02 -1.31651565e-01
 -5.73352762e-02 -6.09021485e-02 -6.67717382e-02 -1.91940111e-03
 -6.89782053e-02  1.61799528e-02  6.91720564e-03 -6.77135214e-02
  2.75069978e-02 -5.67251183e-02  2.56858636e-02  1.40103977e-02
 -1.76693294e-02 -2.12040097e-02 -3.22140455e-02 -7.00073875e-03
 -4.65223454e-02  5.60561642e-02  4.21396084e-02 -1.48039183e-03
  1.10910580e-01 -8.53445977e-02 -5.08410819e-02  2.48611122e-02
 -1.10191233e-01  5.22110425e-02 -9.21513066e-02 -3.16290408e-02
 -6.57944847e-03  1.73413791e-02  2.51670517e-02 -7.77284428e-02
 -1.63091440e-02 -2.34754756e-02  3.95645993e-03  1.51790660e-02
  2.43764389e-02  3.79292779e-02 -5.47723472e-02  8.59905705e-02
  2.42719576e-02 -1.49949081e-02  6.83136284e-02  2.36549135e-02
 -1.52766304e-02 -8.24764092e-03 -1.50895715e-02 -3.66598368e-02
  5.16380481e-02 -7.00353384e-02 -8.70975200e-04  1.38494847e-02
 -2.96928082e-02 -4.69697043e-02  4.81733307e-02  5.52343763e-02
 -2.24908553e-02  2.05056723e-02  1.25718182e-02  8.35137740e-02
  6.32214844e-02  2.46035364e-02 -2.31515802e-02 -3.33116204e-02
 -1.70121174e-02 -3.18458118e-02  3.62239145e-02  6.21639527e-02
 -3.00642401e-02  1.14193885e-02  1.58767980e-02  1.08377062e-01
 -4.30047028e-02  2.19508037e-02 -3.75328064e-02 -1.66812446e-02
 -8.11746716e-03  1.78607982e-02 -5.08425683e-02  9.77805441e-33
 -3.93561758e-02  1.70271564e-02  1.15931388e-02 -4.72853705e-02
  1.01100709e-02 -2.04509590e-02 -1.14268698e-02 -5.05839959e-02
  2.79008951e-02 -2.06269864e-02  2.51672491e-02  9.14310068e-02
 -6.15483634e-02  2.42388416e-02  1.28542371e-02  1.46960719e-02
 -1.63965598e-02  4.89886068e-02  1.38288466e-02  1.13486797e-02
  7.40857869e-02  2.31261756e-02 -2.50176229e-02  1.03031896e-01
 -5.95103055e-02  2.92028114e-02 -3.76581661e-02 -6.48754761e-02
  4.46798094e-03  1.90179069e-02 -5.79851121e-03 -2.30888210e-04
 -5.92166260e-02  5.71155772e-02 -1.62181072e-02 -3.79664488e-02
 -2.81243995e-02 -8.51698145e-02  2.94566769e-02 -7.39275664e-02
 -4.33708988e-02 -1.76468294e-03  3.00291404e-02 -2.21075621e-02
 -2.30755620e-02  3.13958973e-02  4.67873700e-02  1.07881457e-01
  8.15473795e-02 -8.08061808e-02 -2.45232657e-02  4.05795174e-03
  1.67048741e-02  6.51816130e-02  1.44045502e-02 -2.24248692e-03
  2.87401192e-02 -2.52949893e-02  8.30803066e-02  7.03303590e-02
  3.51726040e-02  4.09841500e-02  1.36502031e-02 -6.86562210e-02
 -2.95671541e-02 -7.60234296e-02 -8.99652615e-02 -3.91155994e-03
 -2.02515721e-02  1.00589849e-01 -8.99154022e-02  1.26935281e-02
 -1.13629308e-02  2.71493904e-02  2.59806421e-02 -2.77895667e-02
  5.86464554e-02 -5.05846664e-02 -4.36811000e-02 -4.23227018e-03
 -1.29824564e-01  5.98293357e-02  2.89976653e-02 -5.73786721e-02
 -7.94117749e-02 -3.40155177e-02 -4.78541143e-02  1.09634651e-02
 -1.04928948e-01 -3.86089943e-02 -1.11562535e-01  1.30293174e-02
  2.17604283e-02  7.85264224e-02 -4.17888388e-02 -9.06031446e-33
 -1.75079424e-02 -3.38547491e-02 -4.23935391e-02  1.17267020e-01
  4.01844382e-02  2.02505365e-02 -4.88980375e-02 -1.45971641e-01
  1.37152895e-01 -1.36836199e-02 -4.71391305e-02  1.89968739e-02
  7.31720850e-02  5.29497154e-02  5.15828282e-02  1.58721339e-02
  1.81496572e-02 -1.66407879e-02 -4.43505086e-02 -1.59983356e-02
 -1.13917150e-01  8.42360035e-02 -2.14576349e-02 -6.48600310e-02
  4.18703333e-02 -9.33125056e-03 -3.23172212e-02 -5.49196489e-02
  3.93216796e-02  4.85291258e-02 -4.79335822e-02  3.06992531e-02
 -4.00087722e-02  3.06298342e-02 -4.54579294e-02 -1.31002292e-02
  8.01250562e-02  2.16914453e-02 -2.85987593e-02 -1.68261677e-02
  1.14742316e-01  3.23530585e-02  3.49285789e-02 -2.05727033e-02
  3.16359922e-02  8.69170949e-02 -6.55311719e-02  5.59108444e-02
 -2.11323779e-02 -2.24989094e-02  2.39308197e-02  4.06532511e-02
 -4.43207733e-02  5.00188470e-02  8.49026069e-03  6.37787133e-02
  9.26699936e-02  5.56796091e-03  7.70933833e-03 -1.29237007e-02
 -4.08120826e-02 -3.24911065e-02  1.53961908e-02 -1.30316149e-02
 -3.97978760e-02  5.10563739e-02  6.61463104e-03  4.32450324e-02
  2.15336047e-02 -7.22198933e-02 -8.15412775e-03  9.33701992e-02
  7.16202408e-02  1.42697766e-01 -3.38537991e-02  6.34447634e-02
  1.62593182e-02 -2.21355278e-02 -4.06739190e-02 -2.37825625e-02
  1.18167363e-01  5.45339696e-02  3.90771776e-02  3.53394188e-02
  1.19128227e-02  7.59022608e-02  2.14135610e-02 -3.91997993e-02
  7.92239085e-02 -1.03585690e-01 -3.19747813e-02 -1.07762506e-02
  9.39867832e-03  1.16617605e-01 -3.65653099e-03 -3.53330876e-08
 -6.41997950e-03  2.83522774e-02  6.36801273e-02 -1.55374138e-02
  2.35122107e-02  4.08215486e-02  1.47250416e-02  1.03862323e-01
 -1.74106378e-02  1.35784000e-02 -4.69620675e-02 -6.11843318e-02
 -4.81800623e-02  1.00142872e-02  5.94668314e-02  3.39260362e-02
  1.72023643e-02  2.84999721e-02  4.54050535e-03 -7.00359270e-02
  3.53779527e-03 -2.22326405e-02 -3.05163451e-02  3.52226906e-02
 -3.51937301e-02 -4.66860794e-02  1.56013668e-02 -6.11942001e-02
  3.29408534e-02  4.54010218e-02  6.73839496e-03 -2.54228916e-02
  7.61985257e-02 -1.16950767e-02  1.27023265e-01  6.69908971e-02
 -1.89008936e-02 -4.78985813e-03  4.49760305e-03  4.18810174e-02
 -4.86836880e-02  4.36905362e-02 -4.31867912e-02 -1.73770729e-02
  3.65452915e-02  2.62611080e-02 -3.22041325e-02 -8.01858976e-02
  2.47493461e-02  1.26428351e-01  1.92518868e-02  4.73993309e-02
 -2.85682250e-02  2.23766062e-02  2.42304374e-02  4.31527905e-02
 -2.86119767e-02 -1.22752912e-01 -4.64370055e-03 -8.37495476e-02
  1.69384629e-02  6.21177778e-02  4.35196608e-02  3.27344239e-02]",2,2
dpu-utils,3,this contains a set of utility used across project of the dpu team stored in the python subdirectory published a the dpu utils package or via the community maintained conda recipe below you can find an overview of the utility included detailed documentation is provided at the docstring of each class unsorted segment operation following tensorflow s unsorted segment sum operation unsorted segment operation following tensorflow s unsorted segment sum operation these model have not been tested with tf you can use the deduplicationcli command to detect duplicate in pre processed source code by invokingwhere data path is a file containing tokenized jsonl gz file and out json is the target output file for more option look at help an exact but usually slower version of this can be found here along with code to tokenize java c python and javascript into the relevant format the resulting html file will be in htmlcov index html stored in the dotnet subdirectory generic utility code related utility this project welcome contribution and suggestion most contribution require you to agree to a contributor license agreement cla declaring that you have the right to and actually do grant u the right to use your contribution for detail visit http cla microsoft com when you submit a pull request a cla bot will automatically determine whether you need to provide a cla and decorate the pr appropriately e g label comment simply follow the instruction provided by the bot you will only need to do this once across all repos using our cla this project ha adopted the microsoft open source code of conduct for more information see the code of conduct faq or contact opencode microsoft com with any additional question or comment,"[('tensorflow', 0.4437), ('deduplicationcli command', 0.4304), ('jsonl', 0.3359), ('source code', 0.3345), ('generic utility code', 0.3304), ('microsoft open source code', 0.3276), ('json', 0.3139), ('detailed documentation', 0.3125), ('file', 0.2801), ('utility', 0.2777)]","[-1.30349904e-01 -8.87143686e-02 -4.16377652e-03 -2.42061615e-02
  2.71636620e-02 -4.23388667e-02 -3.37035246e-02  7.23342299e-02
 -4.33803797e-02 -2.50679459e-02  2.07037982e-02  2.51092184e-02
 -6.57561570e-02 -4.25593108e-02  5.84625192e-02  2.52732411e-02
 -7.92815983e-02  4.29926999e-02 -4.17450108e-02 -9.34595019e-02
  2.78574564e-02  3.53781469e-02 -3.73292752e-02  4.06469777e-02
  1.94660295e-02  5.14956042e-02  2.16625091e-02 -6.64262697e-02
  5.19761220e-02 -4.85198684e-02 -1.98826031e-03  5.39853796e-02
  7.18194470e-02  1.79710966e-02  5.33119123e-03  8.69455859e-02
  1.12327165e-03 -7.33039230e-02 -1.19924853e-02 -3.67603227e-02
 -4.55598161e-02  5.40867485e-02 -6.31147176e-02 -1.50376884e-02
  4.16147970e-02 -8.38106424e-02 -1.90276187e-02 -7.38198087e-02
 -1.38712442e-02  2.11386289e-02 -8.95432681e-02 -4.34283093e-02
 -8.19577277e-02  5.86299691e-04  4.79131714e-02 -9.48126167e-02
 -1.73080117e-02  4.07609902e-03 -3.61859165e-02 -2.17183623e-02
  1.85060389e-02  1.90725233e-02 -9.07829702e-02  7.69612119e-02
 -1.38603977e-03  3.76626067e-02  1.65926199e-02  4.41278182e-02
  1.29496843e-01 -8.82892981e-02 -5.27749248e-02 -1.65890064e-02
 -5.57309948e-02  1.03091560e-01 -5.85499499e-03 -6.96630101e-04
  5.11603393e-02 -2.11319793e-03 -4.49842401e-02 -5.66489995e-02
 -3.90053652e-02 -2.83246990e-02  3.37149501e-02  6.30481914e-02
 -2.45540962e-03  1.94537174e-02  5.07957190e-02  7.06634521e-02
  2.89592315e-02  2.68846918e-02  9.15548205e-03 -8.44070315e-02
  5.95498681e-02 -3.60509604e-02  6.15958460e-02  6.17567338e-02
 -1.96588747e-02 -5.58553971e-02  1.92979872e-02  2.74981279e-02
 -8.72038156e-02 -4.19052951e-02  6.80208728e-02  5.44359870e-02
 -7.33304583e-03  1.08884294e-02  6.51263148e-02 -4.72685769e-02
  3.22833359e-02  3.25335078e-02  4.71335807e-04  5.42080663e-02
 -5.51361442e-02 -1.07233286e-01  3.69226038e-02 -1.36372412e-03
 -1.91180483e-02  5.09238616e-02  3.31967138e-02  8.69726315e-02
 -7.97133427e-03  1.71800461e-02  1.69554539e-02 -1.08461985e-02
 -5.68554699e-02  9.24491882e-03 -6.65100217e-02  2.34755112e-33
  2.74699572e-02 -7.26649864e-03 -2.39218818e-03  9.53507144e-03
  4.61339429e-02 -1.12786904e-01  6.28092885e-02  5.21159805e-02
 -3.91625725e-02 -3.97272706e-02 -1.15375910e-02  1.62175342e-01
 -1.27603831e-02  1.38248384e-01 -5.92456236e-02 -6.83754534e-02
  4.39282693e-02  5.49954660e-02  1.77854653e-02  2.69183479e-02
  5.39165512e-02  3.92907858e-02  1.78908389e-02  8.99832100e-02
  3.65056768e-02  1.35382181e-02  1.78434774e-02  4.47203266e-03
  3.07326093e-02  1.24481404e-02 -3.99489366e-02 -9.86771584e-02
 -3.84386466e-03  4.92613465e-02  3.18762846e-02  7.94165358e-02
 -7.75155053e-02 -7.36493543e-02  2.77733617e-02 -8.03294331e-02
 -1.22220758e-02  1.70391724e-02 -8.06235299e-02 -3.07061188e-02
  1.75344162e-02 -6.86527491e-02 -4.75771911e-02 -1.76601037e-02
  2.06022188e-02 -4.52634878e-02 -2.38454342e-02  7.30787218e-02
  1.78082697e-02 -6.41136523e-03  4.32250788e-03 -1.00793310e-01
  2.43542939e-02 -1.31994556e-03  8.40428099e-02  1.52954012e-01
 -4.17994037e-02  7.34187886e-02  6.23646751e-03  9.15295910e-03
  5.14664724e-02 -2.81366259e-02 -7.22985715e-02 -2.86527090e-02
  1.48183387e-02  3.19020562e-02 -4.99605164e-02  6.24169111e-02
 -2.57412065e-02  8.91462062e-03 -1.66749079e-02 -3.70701402e-02
 -4.18912881e-04 -5.85123673e-02 -1.94754526e-02 -1.80313978e-02
 -1.17873795e-01  4.40948270e-03  7.46488629e-04  2.25977618e-02
 -7.34567121e-02  2.00739354e-02 -6.03726413e-03 -2.09364798e-02
  2.34914999e-02 -5.11059463e-02 -3.04973442e-02  2.62528714e-02
  2.15440430e-02 -4.87649217e-02 -1.74382422e-02 -2.96098707e-33
 -2.18445491e-02 -3.07697300e-02 -2.41497178e-02  6.72594607e-02
 -5.66644333e-02  4.44366895e-02  3.62022314e-04  5.23667783e-04
  3.89506184e-02  5.17509878e-02 -6.16795430e-03 -2.12724265e-02
  1.87779367e-02 -7.66603947e-02  2.13253107e-02 -2.68221851e-02
 -5.39080873e-02 -8.03202689e-02  1.64354239e-02 -4.19196876e-04
 -4.73093614e-02  1.33341983e-01  2.06579976e-02 -2.99508702e-02
  2.79382858e-02 -4.32550199e-02 -1.05662815e-01 -5.03678396e-02
  8.68622959e-03 -1.88418552e-02  2.26500574e-02  1.19114006e-02
 -1.27860550e-02 -4.47563604e-02  3.85598629e-03 -6.53612521e-03
  1.14529289e-01  9.72827002e-02  2.60924716e-02  3.27130407e-02
  7.38717988e-02 -1.16214091e-02  5.92724457e-02  5.24883196e-02
  3.60275316e-03 -2.76045296e-02 -1.00400247e-01  4.54111844e-02
 -1.96154546e-02 -5.53719252e-02  2.53899917e-02 -2.56719980e-02
 -2.67841369e-02 -8.99138600e-02 -1.27481623e-02  1.20347654e-02
  1.03538379e-01 -2.24356446e-02 -1.10372482e-03  5.58251143e-03
 -3.50564942e-02 -6.25178888e-02  3.98658663e-02 -5.92711056e-03
 -3.22045833e-02 -2.81469151e-02 -5.38022667e-02  1.94050407e-03
 -5.84546998e-02 -5.57101183e-02 -3.97286415e-02 -3.22910585e-03
  1.87755167e-03  2.66828667e-02 -3.15635763e-02 -1.49158062e-02
  1.57293323e-02  7.23389164e-03  2.25171391e-02  3.62156294e-02
  1.03320554e-01  2.10115127e-02  2.07245685e-02  6.97054863e-02
  5.35977930e-02  5.69354892e-02  3.62474248e-02 -2.23971009e-02
 -1.66272689e-02  1.01057645e-02 -9.17782038e-02  6.36509135e-02
 -1.39315624e-03  1.48950234e-01  3.84925446e-03 -2.60702837e-08
 -6.21247552e-02  1.46031147e-02  2.43550423e-03  8.48146230e-02
 -1.55485440e-02  1.13500832e-02 -5.48546873e-02  5.73344640e-02
  3.07142157e-02  5.62428730e-03  4.25875448e-02  2.38176305e-02
 -1.71178609e-01 -2.44911313e-02 -9.46897175e-03  5.67288250e-02
 -2.83802319e-02  3.51330154e-02  1.62526499e-02 -1.76315513e-02
  8.83880854e-02  7.47803599e-02 -3.95962549e-03  1.25541957e-02
 -1.81900356e-02 -1.48548828e-02  8.24892595e-02  7.63030574e-02
 -3.11002508e-02 -1.37238456e-02 -5.59123326e-03 -9.98143852e-03
  6.85535446e-02 -7.88562745e-02  2.69020181e-02  2.40573334e-03
  5.28628789e-02 -3.16939130e-02 -1.70935150e-02  5.50710112e-02
 -4.84144222e-03  5.58012053e-02 -1.99240986e-02 -5.02684899e-03
  6.09154254e-02  6.67336676e-03 -2.93241255e-02 -3.90158035e-02
 -5.71576916e-02 -4.60497402e-02 -6.50599971e-02  4.32426929e-02
 -1.70643646e-02  1.20891556e-01 -1.46990670e-02  7.55309388e-02
 -4.98153605e-02 -9.62766185e-02  5.20281121e-02 -1.60096854e-03
  5.91821736e-04  5.66600971e-02  4.91114222e-02  2.33512633e-02]",2,2
pytube,2,have idea for how pytube can be improved feel free to open an issue or a pull request pytube is a genuine lightweight dependency free python library and command line utility for downloading youtube video detailed documentation about the usage of the library can be found at pytube io this is recommended for most case if you want to hastily download a single video the quick start guide below might be what you re looking for youtube is the most popular video sharing platform in the world and a a hacker you may encounter a situation where you want to script something to download video for this i present to you pytube pytube is a lightweight library written in python it ha no third party dependency and aim to be highly reliable pytube also make pipelining easy allowing you to specify callback function for different download event such a on progress or on complete furthermore pytube includes a command line utility allowing you to download video right from the terminal this guide cover the most basic usage of the library for more detailed information please refer to pytube io pytube requires an installation of python or greater a well a pip pip is typically bundled with python installation to install from pypi with pip sometimes the pypi release becomes slightly outdated to install from the source with pip to download a video using the library in a script you ll need to import the youtube class from the library and pas an argument of the video url from there you can access the stream and download them using the cli is remarkably straightforward a well to download a video at the highest progressive quality you can use the following command you can also do the same for a playlist,"[('pull request pytube', 0.5421), ('reliable pytube', 0.5374), ('pytube pytube', 0.4985), ('pytube', 0.4947), ('pytube io pytube', 0.4854), ('youtube class', 0.4288), ('popular video sharing platform', 0.4105), ('youtube', 0.41), ('youtube video', 0.4018), ('video url', 0.3852)]","[-9.37282592e-02 -9.48861539e-02 -3.93155329e-02 -1.40864849e-02
  5.70612550e-02  5.77388005e-03 -3.18597220e-02  5.69379106e-02
 -5.61343655e-02 -5.48385568e-02 -2.68240776e-02  1.41145010e-02
 -7.74984434e-02 -6.82080444e-03  5.46308188e-03  1.79903973e-02
  1.72223039e-02  9.77455378e-02 -7.39452764e-02 -1.00331105e-01
  2.21523196e-02 -3.25830318e-02  3.28623615e-02  1.70102213e-02
  2.24146317e-03 -3.35343368e-02 -1.48816165e-02  1.85403656e-02
  9.98707414e-02 -6.97497129e-02 -3.29186432e-02 -3.75975221e-02
  4.02538339e-03  1.69200860e-02 -1.52638145e-02 -1.36105288e-02
 -4.15558293e-02 -1.21084377e-02 -2.73228772e-02  9.75389965e-03
 -9.23328567e-03 -1.82646327e-02  6.04577223e-03  1.59299523e-02
 -4.57538106e-02 -6.80404855e-03 -1.73099842e-02 -9.90745872e-02
  3.00362464e-02 -2.68049482e-02 -1.03785396e-01 -6.84616715e-02
 -5.99559732e-02 -4.21178341e-03 -2.75670458e-02 -2.92287245e-02
  5.26470728e-02  1.61543209e-02  1.35750219e-01 -3.02829314e-02
  1.71022136e-02 -8.31459612e-02 -2.62539331e-02  3.66003178e-02
 -1.96489431e-02  5.93852252e-02  2.07360275e-02  3.28784101e-02
  8.30462798e-02 -8.01541135e-02 -1.15154445e-01  1.59580279e-02
 -1.43655449e-01  1.12194449e-01 -4.17093523e-02 -2.16065478e-02
 -5.06889075e-02  3.17476317e-02 -8.90239477e-02 -1.03911171e-02
 -1.84072368e-02 -5.69497645e-02  1.76241454e-02  4.41394188e-02
  4.09717895e-02  3.96837816e-02 -6.12971149e-02  9.75113362e-02
  6.58153184e-03 -3.79232727e-02 -6.50400668e-03  4.14375179e-02
  4.97499704e-02  6.96134344e-02  4.96491604e-02  7.59860054e-02
  5.10103293e-02 -1.47893205e-01 -1.88021399e-02  1.27768433e-02
  8.58756434e-03  1.53322881e-02 -3.02408412e-02  2.88568693e-03
 -2.06717551e-02  1.37746362e-02  1.17547233e-02  9.41427499e-02
  9.50585678e-02  2.50517279e-02 -4.98054735e-02 -2.96149813e-02
 -2.51502115e-02 -6.69731051e-02  2.91435439e-02 -3.71628888e-02
  1.85558223e-03  2.91483141e-02  2.42962670e-02  2.42339615e-02
  5.96760511e-02  5.74483201e-02  7.27926716e-02 -4.15985994e-02
 -1.14098946e-02 -4.77328748e-02 -3.46177295e-02  6.20112355e-33
  7.72676095e-02  2.59233695e-02  8.92210810e-04 -9.18451510e-03
  2.60370094e-02 -3.13597620e-02  4.72058207e-02 -9.72810760e-03
  2.75106374e-02 -1.10286653e-01  1.25105046e-02 -5.74455038e-03
 -4.45650667e-02 -2.70030554e-02  1.23489497e-03 -3.97100812e-03
  4.23694728e-03  1.20851127e-02  3.61913666e-02  6.92678392e-02
  3.62202078e-02  3.22104916e-02  3.56096476e-02  1.14510827e-01
 -3.56611349e-02 -2.25789323e-02 -2.90892795e-02  1.80047695e-02
  1.14860870e-02  5.01239393e-03  2.87275333e-02 -6.38914332e-02
 -4.56343405e-02 -7.68421292e-02  8.01576078e-02 -6.70674294e-02
 -4.17868905e-02 -1.77884158e-02 -5.42533286e-02 -4.24448922e-02
 -5.38569987e-02 -2.24379869e-03 -6.06159754e-02  7.98341911e-03
  3.57121266e-02 -5.82073927e-02 -1.40643520e-02  5.03218658e-02
  6.20341599e-02 -2.37373281e-02 -4.69688587e-02  2.51569375e-02
  3.02529596e-02 -3.59043181e-02 -1.98798534e-02  1.87771709e-03
  2.28707045e-02  8.10754821e-02  7.15154642e-03  4.33438234e-02
  7.45043298e-03  9.16928053e-02  7.98200294e-02 -5.40960953e-02
 -9.71081331e-02  3.30489986e-02 -3.93219069e-02 -6.25930820e-03
 -7.01035261e-02  5.53752743e-02 -3.08388341e-02  5.19926772e-02
 -1.08499331e-02 -3.23691815e-02 -3.58631462e-03 -2.55100876e-02
 -6.18225634e-02 -8.23637620e-02  3.45725939e-02 -6.19429760e-02
 -3.28963734e-02 -2.69724131e-02  9.11934674e-02 -2.00515427e-03
 -1.28994798e-02 -1.83594893e-05 -3.34390402e-02 -4.47373688e-02
  2.77043059e-02  2.76577156e-02 -5.35886027e-02  2.88140532e-02
  2.32335296e-03  1.34238256e-02  2.07395870e-02 -5.29966265e-33
 -2.98146289e-02  8.15656558e-02  1.48900477e-02  1.05249301e-01
  6.15892112e-02  5.62485680e-02 -5.34744784e-02 -9.73320529e-02
  9.26217958e-02  5.67210279e-02 -3.66608053e-02 -3.02811097e-02
 -5.13494797e-02 -7.38588870e-02  1.75701212e-02  1.63161177e-02
 -1.56475715e-02 -6.67160079e-02 -3.93943079e-02 -3.78204919e-02
 -1.23522222e-01  5.12034483e-02 -2.03815531e-02 -2.97532436e-02
  3.88029707e-03  3.46039310e-02 -2.05693431e-02 -2.55331304e-02
  5.93276247e-02  1.04309795e-02  6.90703541e-02 -9.59237292e-03
 -5.27521893e-02  8.76839797e-04 -5.08420505e-02  7.36005430e-04
  1.20200239e-01  3.01461276e-02  3.32146556e-05  9.40689445e-03
  1.41547963e-01  6.55011013e-02  3.60354744e-02 -4.66099381e-02
 -2.19253357e-02  5.63401915e-02 -1.17854759e-01  5.45560829e-02
 -4.99171764e-02 -1.91672873e-02  4.77008969e-02  3.58040333e-02
  1.72328763e-02  1.90275777e-02  5.20518944e-02  1.87618043e-02
  4.13588695e-02  1.27815694e-01  1.25064198e-02  3.07647344e-02
 -5.73613979e-02 -2.10539047e-02 -7.44745210e-02  6.42172396e-02
  2.25664303e-03 -4.46477048e-02 -2.88630836e-03  1.18309826e-01
 -9.13730860e-02 -2.90688071e-02  8.03738162e-02 -4.38307266e-04
  4.89443764e-02  2.36914139e-02  4.12927307e-02  9.95114297e-02
  7.47728497e-02  2.38630306e-02  6.02468033e-04 -1.24333589e-03
  2.96735950e-02  7.35813901e-02 -7.88024394e-04 -3.57820913e-02
  2.90166549e-02 -1.05377696e-02  6.34593591e-02  6.00647144e-02
  5.97337587e-03 -9.62907374e-02 -8.82531982e-03  4.10360895e-04
  2.80191097e-02  6.60710484e-02  8.08070526e-02 -3.11324939e-08
  1.05089005e-02  3.16771585e-03  7.34234648e-03  8.21946487e-02
  3.88966650e-02  5.47690541e-02 -2.25398168e-02  9.77371708e-02
  3.25536132e-02  3.20911705e-02 -6.38677701e-02 -6.98737651e-02
 -5.81101980e-03  2.62623634e-02 -8.03456642e-03  3.72162573e-02
 -3.17062018e-03  1.33572044e-02  4.03526239e-02 -3.57495621e-02
 -1.54658332e-02 -6.47611842e-02  6.16747029e-02 -4.42186072e-02
 -6.41973242e-02 -5.15779145e-02  2.79475339e-02  2.38376167e-02
 -1.73039827e-02 -2.06752401e-02 -3.95649597e-02 -2.96697393e-03
  2.55908370e-02 -1.46002546e-01  8.46194252e-02  8.13794658e-02
 -9.11185667e-02 -2.34447662e-02 -1.10872341e-02  4.63566519e-02
 -4.46162634e-02 -1.84704158e-02  8.60708132e-02 -8.12238641e-03
  4.45439927e-02  6.55178279e-02 -3.26694883e-02 -4.14476208e-02
  3.78480135e-03 -8.09343718e-03 -5.99577315e-02 -6.29248098e-02
 -2.91325357e-02  1.27206482e-02  7.31444499e-03  8.09714645e-02
 -3.33984457e-02 -4.82441671e-02 -1.39283007e-02 -2.01928765e-02
  4.46178615e-02  5.96859567e-02  1.75485574e-02  4.30254191e-02]",2,2
keras_applications,2,kera application is the application module of the kera deep learning library it provides model definition and pre trained weight for a number of popular archictures such a vgg resnet xception mobilenet and more read the documentation at http kera io application kera application may be imported directly from an up to date installation of kera from kera import application kera application is compatible with python and is distributed under the mit license,"[('kera application', 0.7674), ('kera deep learning library', 0.7296), ('kera import application kera application', 0.7225), ('http kera io application kera application', 0.676), ('kera', 0.5898), ('application module', 0.4228), ('python', 0.2899), ('resnet xception mobilenet', 0.2336), ('vgg', 0.2168), ('documentation', 0.187)]","[-1.46897957e-01 -6.16764873e-02 -6.53945841e-04 -5.94325401e-02
 -6.19122088e-02 -4.41272408e-02 -3.23167145e-02 -8.68926197e-03
 -6.19193465e-02 -2.56231911e-02 -1.54294143e-03 -2.28091031e-02
 -5.21451049e-02 -5.63356988e-02  1.44641167e-02 -3.26211825e-02
  5.87263331e-02  3.36578190e-02 -3.88734341e-02 -4.51077670e-02
  7.96142593e-02  4.81660441e-02  6.50432426e-03 -1.48907760e-02
 -4.08381335e-02  3.35872662e-03 -1.76257193e-02 -1.32905059e-02
  5.91133386e-02 -1.02516107e-01 -1.05007284e-03  5.55298254e-02
 -4.17950228e-02  2.24114973e-02 -7.07529602e-04  6.42540455e-02
 -5.57738133e-02 -7.96474516e-02 -8.76581073e-02 -7.05578029e-02
  1.67564563e-02 -4.66847681e-02 -4.73667309e-02 -6.75359508e-03
  6.56004772e-02 -7.38460794e-02 -1.39974393e-02 -2.76825782e-02
  4.46877480e-02 -4.65260409e-02 -2.28694510e-02 -7.61850253e-02
  1.38718274e-03  1.76812336e-02 -2.54833847e-02 -8.39978233e-02
 -3.78072374e-02  3.18015926e-02  5.33520989e-02  1.74368694e-02
  5.03041409e-02  5.58260567e-02 -4.05850597e-02  7.37809986e-02
 -9.14676115e-03 -2.66230814e-02  1.24285482e-02  1.31335761e-02
  4.38282900e-02 -1.10316873e-01 -4.81032245e-02 -3.00066993e-02
 -7.52800629e-02  2.90812664e-02 -3.38611677e-02 -4.83080149e-02
  5.08430824e-02  8.20635911e-03  1.19159492e-02 -6.10257871e-02
  6.45859390e-02  3.07996757e-02  1.74572389e-03  7.83696324e-02
  2.31626593e-02 -1.56877767e-02 -5.14095873e-02  1.69056859e-02
  9.39621776e-03 -6.39626607e-02  2.95084994e-02 -4.18235958e-02
 -4.75122128e-03 -5.44548128e-03  3.89171368e-03  3.45592536e-02
 -1.73184536e-02 -4.04434763e-02 -5.81156388e-02  7.50096217e-02
 -1.36297597e-02 -1.30127504e-01  5.25051088e-04  1.48974629e-02
 -2.28849854e-02 -6.47961162e-03  5.93131334e-02 -6.81662560e-03
  4.47976179e-02  9.07381065e-03 -8.18085372e-02 -1.32384440e-02
 -4.39809524e-02 -6.37210980e-02  7.50827938e-02  1.55052058e-02
 -2.64801783e-04  7.00763287e-03  7.53281638e-02  5.90519607e-02
 -1.32363290e-01 -2.68089268e-02  2.69823354e-02 -3.98774520e-02
  1.07374256e-02 -8.04792251e-03 -1.15584157e-01  4.45268297e-33
 -1.03807207e-02 -3.49821225e-02 -3.58640291e-02 -3.88005525e-02
  8.04333016e-02 -1.63388744e-01  6.05043955e-02 -5.84087782e-02
 -1.06193885e-01 -9.92704704e-02 -8.76318663e-02  1.05539106e-01
 -4.23356704e-03  6.73579648e-02  1.29195815e-02  2.66681537e-02
 -4.49088737e-02  4.28136811e-02  6.68337122e-02 -8.22817255e-03
  4.87223566e-02 -5.35469241e-02  8.98470655e-02  5.88500425e-02
  1.28971515e-02  7.87983648e-03  1.43959252e-02 -3.79730985e-02
  1.69594306e-02  1.54206092e-02  2.22533103e-02 -3.35612334e-03
  4.93697561e-02 -2.79169083e-02 -1.70854740e-02  5.72994584e-03
 -1.59992569e-03 -4.63529825e-02 -6.00591935e-02 -7.17004063e-03
 -8.94449204e-02  3.74327004e-02 -3.24856490e-02 -5.66415191e-02
 -2.89873108e-02 -1.92906447e-02  2.08112914e-02  1.96295846e-02
  5.43298125e-02  1.08406637e-02 -1.65236148e-03 -4.85818982e-02
 -2.55701263e-02 -5.30556142e-02 -3.08128484e-02  8.88700038e-02
  9.42037478e-02  6.09749369e-02  7.39406198e-02 -1.35914944e-02
 -3.54919471e-02 -6.63673040e-03 -8.06329306e-03 -2.68360600e-02
  3.42335142e-02 -3.24762538e-02 -3.47964801e-02  2.90339552e-02
  2.64953151e-02  4.64637429e-02 -7.47346729e-02  4.27331813e-02
  5.20393960e-02 -3.45420465e-02  4.32391725e-02 -4.14255867e-03
 -2.33748797e-02 -3.53046432e-02 -8.20889100e-02  9.79905948e-02
 -3.62596079e-03 -1.54822674e-02  4.88735251e-02 -8.84155277e-03
 -2.75304969e-02 -2.76686288e-02  2.65533943e-02 -3.56601588e-02
  4.32773642e-02  3.56808156e-02  1.79430284e-02  1.12002632e-02
  8.40988103e-03 -2.41437890e-02 -4.25539799e-02 -5.43955529e-33
  2.98721194e-02  7.38720447e-02 -1.91024765e-02  6.33713156e-02
  7.80198053e-02  4.41545583e-02 -2.54698806e-02 -3.81031982e-03
  3.67770866e-02  8.18631984e-03  3.61476280e-02 -2.68151760e-02
  1.38912648e-01 -2.25972896e-03  1.63941514e-02 -3.08155287e-02
 -4.46503498e-02 -4.55891788e-02  1.51605543e-03 -2.00294610e-02
 -6.77516684e-02  6.24978580e-02 -4.63156700e-02 -4.65987846e-02
 -6.04888331e-03 -9.62864608e-02 -2.26116031e-02  5.19644544e-02
 -1.73041392e-02 -7.77772814e-02 -3.80231743e-03 -2.18868610e-02
 -8.12677220e-02 -1.27030443e-02  5.06823920e-02  2.06216928e-02
  7.68390223e-02 -2.28297664e-03  5.66139109e-02 -4.30661701e-02
  1.66618377e-01  3.13869044e-02  2.17253761e-03 -1.44736990e-02
 -3.15982252e-02 -4.89978939e-02 -4.02978063e-02  6.36044890e-02
 -1.69841647e-02 -6.39262944e-02  8.09041634e-02 -2.86647212e-02
 -2.02865247e-03 -1.03596300e-02  1.97007675e-02  1.13184713e-01
  1.30369261e-01  9.71635133e-02  4.30268720e-02  7.90453106e-02
 -4.11942266e-02 -1.45543501e-01  1.22937711e-03 -3.58947329e-02
 -1.15186058e-03 -2.76349066e-03 -7.26612061e-02  7.46026961e-03
 -2.83425078e-02 -1.79235707e-03 -6.21280409e-02 -3.87049327e-03
  1.07669123e-02  1.39588276e-02 -6.10372722e-02 -3.52065749e-02
 -3.07238065e-02  4.42816019e-02 -1.80888618e-03 -4.02672589e-02
  3.47607806e-02  6.22771010e-02 -3.25231371e-03  2.64966730e-02
  1.08892836e-01  9.37365666e-02  1.78511497e-02  1.74758602e-02
  4.74212915e-02 -2.30594333e-02 -2.11174972e-02  8.96461159e-02
  2.85187904e-02  5.36444560e-02 -1.21238176e-02 -3.06036299e-08
  2.34951880e-02  6.09241426e-02 -3.82764563e-02  1.36021404e-02
  4.46353331e-02  5.44596948e-02  3.11109852e-02  5.07310070e-02
  5.49848005e-02  2.79072896e-02 -1.34179685e-02  2.81699225e-02
 -1.11673027e-01 -7.05814175e-03 -1.54128000e-02  1.20502099e-01
  2.12134491e-03  7.22417980e-02  4.63270880e-02 -4.68130670e-02
  7.94252306e-02  2.91419569e-02  1.16671659e-02  3.09057795e-02
  1.12329116e-02  1.16798636e-02  3.87799963e-02  1.07497359e-02
  1.08400695e-02 -6.42708763e-02 -7.87414983e-02  5.16993888e-02
  2.58136168e-02 -9.26064253e-02  4.47083563e-02  7.14309514e-02
 -1.10620717e-02  3.22247520e-02  3.14859226e-02  3.10796723e-02
 -3.05703022e-02  1.90733541e-02  2.43682954e-02 -7.54875541e-02
 -4.46997525e-04  8.12750757e-02 -1.84837505e-02 -3.83130983e-02
 -3.06321252e-02 -2.50736699e-02 -1.13133844e-02 -1.49430735e-02
 -1.17528681e-02  3.42387110e-02  5.14969528e-02  9.39996094e-02
  1.29994983e-02 -1.42841473e-01  9.24878102e-03 -1.21167405e-02
  5.92448562e-02  9.04189348e-02  1.56693235e-02  4.38005216e-02]",2,2
keras_preprocessing,2,kera preprocessing is the data preprocessing and data augmentation module of the kera deep learning library it provides utility for working with image data text data and sequence data read the documentation at http kera io kera preprocessing may be imported directly from an up to date installation of kera from kera import preprocessing kera preprocessing is compatible with python and is distributed under the mit license,"[('kera preprocessing', 0.697), ('kera deep learning library', 0.67), ('http kera io kera preprocessing', 0.5876), ('kera import', 0.5566), ('data augmentation module', 0.5248), ('kera', 0.5196), ('data preprocessing', 0.4493), ('image data text data', 0.3971), ('sequence data', 0.3442), ('python', 0.2792)]","[-1.27251223e-01 -5.81196062e-02  3.35413739e-02 -4.28900346e-02
 -8.51503089e-02 -5.86543120e-02 -5.48173822e-02 -7.19404221e-02
 -9.75475460e-02 -1.48517694e-02  7.77857658e-03 -4.39374242e-03
 -6.81467056e-02 -5.43507785e-02 -2.89656501e-03 -1.05466833e-02
 -1.80765092e-02  5.51898777e-02 -5.32964841e-02 -9.81243253e-02
  8.44265707e-03  2.85272226e-02  1.24746524e-02 -3.98220494e-02
 -2.45652488e-03  8.98829699e-02  2.67078485e-02 -5.12172319e-02
  3.91204655e-02 -1.18120283e-01 -7.95480423e-03  6.27308982e-05
  2.67323665e-02  1.12704709e-02 -4.24236665e-03  5.18993698e-02
 -1.84819137e-03 -5.53886481e-02 -1.83753651e-02 -2.04849336e-02
  1.34687526e-02 -2.77962778e-02 -5.66388667e-02 -2.46480592e-02
  8.90725479e-02 -1.63510907e-02 -1.40780555e-02 -7.94918910e-02
  2.90218331e-02 -7.50788823e-02 -6.78369775e-02 -7.65139461e-02
 -5.25410138e-02  3.54217924e-02 -1.22267483e-02 -6.78345114e-02
 -5.81330787e-05  2.06518192e-02  8.17743614e-02  2.02079806e-02
  8.77597742e-03  8.75911117e-03 -1.48612633e-02  4.80384342e-02
 -7.99085572e-03 -4.29854132e-02  7.86516666e-02 -1.68876152e-03
  8.01688209e-02 -9.47469398e-02 -2.52325349e-02 -5.85837988e-03
 -4.82695457e-03  9.22404900e-02 -2.50720885e-02 -1.90871861e-02
  5.17093316e-02 -2.04219762e-02  1.49433231e-02 -1.03163458e-01
  2.93134451e-02 -8.57786043e-04  4.44213897e-02  6.66749999e-02
  3.62332612e-02 -1.78065561e-02 -7.54847303e-02  3.96734253e-02
 -2.06875950e-02 -5.15806861e-02  2.43728850e-02 -6.98349476e-02
  1.81534750e-04 -2.23025270e-02 -4.18351479e-02  7.85158277e-02
 -2.44495161e-02 -5.11509664e-02  1.97874885e-02  3.16284522e-02
 -3.69370542e-02 -9.64180529e-02 -3.21435416e-03  7.00128451e-02
 -5.69403917e-02 -1.86237805e-02  6.58480823e-02  1.21285226e-02
  4.43008021e-02  4.12108488e-02 -2.67528035e-02 -5.14901616e-03
 -5.31201884e-02 -8.23133439e-02  6.83870912e-02 -1.24840215e-02
 -3.40644382e-02 -1.01906443e-02  7.95410722e-02  3.43746617e-02
 -1.10314637e-01 -1.56827848e-02  3.82682420e-02  1.51232705e-02
 -4.33864491e-03 -1.78831425e-02 -1.19364761e-01  5.68499337e-33
  4.55212891e-02 -4.16203253e-02 -1.30179673e-04 -3.74313109e-02
  5.32476231e-02 -1.38476655e-01 -5.85734239e-03 -4.24871780e-02
 -1.02968380e-01 -4.92884256e-02 -3.02462541e-02  1.07544966e-01
 -7.11550340e-02  1.13123506e-01  1.08436653e-02  2.43354216e-02
  1.99657516e-03  4.50452343e-02  4.61600274e-02  3.16955298e-02
  2.74449568e-02 -2.72586532e-02  8.41832459e-02  1.33099496e-01
  2.72764657e-02  1.08238114e-02  1.81709547e-02 -4.08609398e-02
 -1.49151655e-02 -9.72624496e-03 -1.84935313e-02  6.77049626e-04
  5.79140447e-02  9.90684144e-03 -1.59842335e-02  9.31888353e-03
 -1.18192295e-02 -5.71886152e-02 -4.73284423e-02  5.27502783e-02
 -5.25883287e-02  7.35718906e-02 -9.80018638e-03 -6.94132075e-02
 -1.72729455e-02  4.12911959e-02  3.44099253e-02  3.40749100e-02
  6.52977899e-02 -2.17452981e-02  1.59148816e-02 -1.13114612e-02
 -1.12860715e-02 -2.16502082e-02 -5.69921173e-02  8.58400092e-02
  8.56299922e-02 -1.61825065e-02  6.71213642e-02 -6.64784526e-03
 -5.23127019e-02  1.56361796e-02  3.28638852e-02 -2.09369846e-02
 -2.00174656e-02 -1.20339751e-01  1.63890813e-02  5.60564436e-02
  2.07418725e-02  2.30948310e-02 -1.09333120e-01  4.66174632e-02
 -1.05223223e-03 -2.77614407e-02  4.87340651e-02  1.20257922e-02
 -3.06256469e-02 -5.40320985e-02 -6.03493378e-02  1.07929803e-01
  2.83927731e-02  1.88486241e-02  8.73451307e-02 -1.69724356e-02
  7.46002048e-03 -3.18152867e-02  1.98400859e-02  2.56257411e-02
  1.11181894e-02  7.19267875e-03 -1.93455424e-02  1.47406301e-02
 -3.57154347e-02 -2.13208031e-02 -5.55225573e-02 -5.89267200e-33
  5.71968481e-02  3.82206701e-02 -3.39029916e-02  1.04909480e-01
  4.46128398e-02  8.48949477e-02 -1.03719765e-02 -3.46844904e-02
  8.82641003e-02 -2.36413945e-02  5.07467687e-02 -4.20508832e-02
  1.04504168e-01 -8.03216398e-02  3.60356621e-03 -5.60581833e-02
 -5.18758819e-02 -6.80396184e-02 -3.89815345e-02 -9.62825678e-03
 -4.55366224e-02  8.83284584e-02 -2.33576018e-02 -3.26550938e-02
 -2.96351127e-03 -5.47961742e-02 -6.17247038e-02  5.61849810e-02
  7.01520033e-03  2.72339466e-03  1.10713486e-02 -2.36818083e-02
  8.64095148e-03  7.69941416e-03 -1.23864701e-02 -7.30464654e-03
  8.96291360e-02 -1.97343957e-02  9.70994309e-03 -2.04213504e-02
  1.66742325e-01  8.14464316e-02 -1.67437643e-02 -1.56630483e-02
 -8.44751522e-02 -5.66506349e-02 -7.99927488e-02  8.47179145e-02
 -3.17485519e-02 -3.49825509e-02  1.37489242e-02 -7.33043486e-03
 -9.39605571e-03 -1.96487568e-02  3.76188867e-02 -3.50909634e-03
  9.16827917e-02  6.66751042e-02  3.66928540e-02  6.90902844e-02
 -8.70460048e-02 -1.08797461e-01  8.09737518e-02 -4.62849550e-02
 -3.96905504e-02  3.84242111e-03 -6.67770877e-02 -3.31850983e-02
 -4.03818190e-02 -6.84425905e-02 -4.70017781e-03 -4.21663979e-03
  7.98365194e-03  8.00544545e-02 -4.50831614e-02 -5.28338365e-02
 -6.14239313e-02  4.44216914e-02  5.46509363e-02  5.60572743e-03
  1.92613583e-02  1.19206952e-02 -1.92382094e-02  9.36609358e-02
  1.15316123e-01  1.62234247e-01  2.97216456e-02 -1.51950046e-02
  5.74868470e-02 -3.29008959e-02 -1.92107931e-02  7.17532337e-02
  5.72546795e-02  6.20485395e-02 -4.15999293e-02 -3.13670405e-08
  2.71304157e-02 -6.36381470e-03  1.60130747e-02  3.76186445e-02
  5.92159852e-02  1.67472865e-02  1.97257902e-02  1.26968354e-01
  1.96761601e-02 -3.05151790e-02  4.60896939e-02  1.73681583e-02
 -6.74130991e-02 -2.25431770e-02 -5.41842682e-03  7.75324106e-02
  3.22576687e-02  1.19206933e-02  5.45520037e-02 -2.84944451e-03
  1.14000477e-02  4.08124961e-02 -3.99297960e-02  4.67725517e-03
  3.77614386e-02  1.35501120e-02  2.16903351e-03  2.82275062e-02
 -4.42265812e-03 -5.75769134e-02 -4.71036062e-02 -4.22758497e-02
  7.81879574e-02 -7.66144693e-02  6.64125830e-02  1.73455365e-02
  5.36001883e-02 -8.25882424e-03 -6.70279190e-02  3.94955650e-02
  9.12523270e-03  4.91586179e-02 -2.75257640e-02 -5.91551736e-02
 -4.57556453e-03  4.12985645e-02 -8.20829347e-03 -2.41352282e-02
 -4.79479469e-02 -6.10797592e-02  3.46544635e-04 -2.56073140e-02
 -8.54600500e-03  3.23976241e-02  2.91477665e-02  5.16542792e-02
  2.35571060e-02 -6.50681332e-02  8.48842505e-03  4.30618562e-02
  4.04165313e-02  6.38107397e-03  2.26071030e-02 -8.13845266e-03]",2,2
comet-ml,2,full documentation and additional training example are available on http www comet com doc sign up free on comet com and obtain an api key at http www comet comthe core class of comet ml is an experiment a specific run of a script that generated a result such a training a model on a single set of hyper parameter an experiment will automatically log script output stdout stderr code and command line argument on any script and for the supported library will also log hyper parameter metric and model configuration here is the experiment object we all strive to be data driven and yet every day valuable experiment result are just lost and forgotten comet ml provides a dead simple way of fixing that work with any workflow any ml task any machine and any piece of code for a more in depth tutorial about comet ml you can check out or doc http www comet com doc copyright c comet ml inc this package can not be copied and or distributed without the express permission of comet ml inc,"[('http www comet comthe core class', 0.6267), ('comet ml', 0.5808), ('comet ml inc', 0.5406), ('http www comet com', 0.5103), ('comet com', 0.5084), ('full documentation', 0.4515), ('ml task', 0.3404), ('additional training example', 0.3181), ('code', 0.2745), ('depth tutorial', 0.2715)]","[-7.07992464e-02 -9.19485763e-02 -9.93403234e-03 -3.48289646e-02
 -8.88121780e-03 -4.59308177e-02 -6.10990450e-02  8.24666172e-02
 -1.09051824e-01  3.78495865e-02 -7.45140482e-03  2.42694169e-02
  8.46101064e-03 -8.87489393e-02  2.08634380e-02  6.50916109e-03
  5.51089607e-02 -5.93339978e-03 -1.35400612e-02 -1.27305180e-01
  2.48035695e-02 -3.81838866e-02 -2.96927001e-02  9.03706998e-03
  1.68390218e-02  6.69594631e-02  1.33582475e-02 -1.76083893e-02
  7.95435384e-02 -9.80369300e-02 -1.72626134e-02 -2.70741805e-03
  5.12904711e-02  3.26400772e-02  2.76203249e-02  4.03087065e-02
 -9.37024131e-03 -6.08060621e-02 -1.11751156e-02 -1.15105668e-02
 -2.49859504e-02 -8.03378373e-02  6.62236288e-03 -3.35190706e-02
  7.76875615e-02  1.50819924e-02 -6.30046204e-02 -8.90705287e-02
  2.79905424e-02  4.08475585e-02 -6.56695291e-02 -1.21377163e-01
  1.22751447e-03 -3.46585661e-02 -6.35524020e-02  3.10626458e-02
  2.69788355e-02  5.28943464e-02  2.96155401e-02 -5.20188995e-02
  2.63775582e-03 -4.71938550e-02 -8.45373869e-02  5.21958731e-02
  2.33963598e-02 -2.26475131e-02 -1.58651255e-03  4.25640568e-02
  7.47231841e-02 -3.79693359e-02 -9.44904462e-02  5.18125407e-02
 -7.09941909e-02  7.07942024e-02  1.33851487e-02  7.24142790e-03
  4.96381447e-02 -2.55034808e-02 -1.19350143e-02  2.50993539e-02
 -5.54876681e-03  3.93683240e-02  2.16932874e-02 -1.29517894e-02
  1.67756435e-02  2.07578344e-03  3.91272344e-02  9.50141326e-02
  1.78082194e-02 -5.53680882e-02  3.66812348e-02 -7.14218989e-02
 -2.42987722e-02  5.33799715e-02 -1.01936921e-01  7.10861310e-02
  5.80660887e-02 -7.70285279e-02  2.11145394e-02  4.92493249e-02
 -3.04995812e-02 -2.14845743e-02  8.71945638e-03 -9.43312887e-03
 -8.51919726e-02 -7.43128499e-03  1.42735587e-02  9.11358446e-02
  1.29343763e-01 -4.73923795e-02 -6.69615790e-02 -3.19896005e-02
 -4.86634597e-02 -1.33461818e-01  3.39607382e-03 -2.01037582e-02
 -4.80629271e-03 -8.81518982e-03  2.58701928e-02  1.91420428e-02
  2.95243878e-02 -2.55163852e-02  2.05965303e-02 -2.75337510e-02
 -5.36065176e-03 -4.23021577e-02 -7.63280988e-02  1.50001234e-33
  8.02003369e-02  5.14050834e-02 -2.26141363e-02 -2.23140102e-02
  4.76048179e-02 -6.02765717e-02  3.43938209e-02  8.32741931e-02
 -6.35377169e-02 -3.08437441e-02  1.83972530e-02  2.88620256e-02
 -2.11025737e-02  7.18054771e-02 -4.70865332e-02 -2.99648307e-02
  1.43159898e-02  5.08315824e-02 -1.62493847e-02 -2.30318066e-02
 -2.80054901e-02  9.53577459e-03  4.51682881e-02  3.78405824e-02
  4.18843180e-02  1.58903986e-01  4.62295189e-02  4.14149724e-02
  3.79792415e-02  4.84398119e-02  6.94884360e-02 -4.93510775e-02
 -4.81020845e-02  4.13201861e-02  6.01282232e-02  6.45643771e-02
 -6.61523044e-02 -3.72617431e-02 -1.34845376e-02 -1.99569315e-02
 -6.44303933e-02  1.24773141e-02 -5.43138869e-02 -3.21853384e-02
 -6.47856214e-04 -1.17473520e-01  1.24096368e-02 -7.00286180e-02
  7.01194778e-02 -4.29666825e-02 -4.68625017e-02  1.02403192e-02
  1.77704114e-02  1.11018904e-02  9.52105075e-02  3.74358706e-02
  1.30935451e-02  7.24075288e-02  2.41260906e-03  5.79582974e-02
  1.12162465e-02 -4.09140531e-03 -4.67204116e-02  2.22557299e-02
 -2.79896408e-02  4.32941802e-02 -6.77745938e-02 -3.85388099e-02
  1.17851220e-01 -4.38401476e-03 -1.08005805e-02  5.33389822e-02
  5.36379628e-02 -1.62852760e-02 -4.31755669e-02  6.36097230e-03
  8.00626352e-03 -2.78881919e-02 -5.33366241e-02  2.03699269e-03
 -4.55903783e-02 -4.28179167e-02  6.83250427e-02  2.12441217e-02
 -6.15867041e-02 -5.59798107e-02  3.03592812e-02  1.15388306e-02
  3.16578932e-02 -4.36139964e-02 -1.75467934e-02 -2.58384366e-03
  5.72587457e-03  9.00770631e-03  3.34335342e-02 -1.02760678e-33
 -1.76915433e-02  1.10440873e-01 -8.50661397e-02  7.04157948e-02
  3.02703902e-02  6.14287630e-02  6.33549914e-02  2.25128196e-02
 -7.34793767e-02  5.67333624e-02 -3.11428327e-02  1.49257090e-02
 -4.95975129e-02 -3.16082202e-02 -5.51974261e-03 -3.52982506e-02
  1.91479188e-03 -7.43133798e-02 -3.12749296e-02 -2.99483468e-03
 -1.26732942e-02  8.15709680e-02 -3.44386585e-02 -7.74906874e-02
  4.83478568e-02 -2.87516005e-02 -2.38417578e-03 -1.15894182e-02
  3.37948799e-02 -1.78481769e-02 -3.21920998e-02  2.75721736e-02
 -3.00722998e-02  1.18509801e-02 -5.12081422e-02 -2.56408675e-04
  1.00601673e-01  9.80021432e-02 -3.66188288e-02 -1.98480971e-02
  9.65265036e-02 -3.95680182e-02  4.84246155e-03 -1.00265317e-01
  3.90338637e-02  1.06728384e-02 -2.26281304e-02  3.42419967e-02
  4.41886000e-02  2.76720095e-02 -2.47799195e-02 -7.22961053e-02
  6.40643900e-03 -3.75079811e-02  1.68169308e-02 -5.28216809e-02
  6.29481627e-03  9.77984164e-03  3.82541269e-02 -1.53907062e-02
 -4.03758623e-02 -6.26025200e-02 -2.17925869e-02  1.05972677e-01
  5.59662562e-03 -7.50716962e-03 -5.52668050e-02  8.85461792e-02
 -9.38758478e-02  2.25281697e-02 -1.22369165e-02 -9.49281640e-03
  4.28895541e-02 -1.57311913e-02  3.00937071e-02 -4.14967090e-02
 -5.00682145e-02  2.84559894e-02 -2.57679000e-02 -2.87547559e-02
  2.41267085e-02 -8.01210571e-03  4.08562226e-03  6.26866147e-02
  5.89139648e-02  7.38240331e-02  5.81583530e-02 -2.61641797e-02
 -1.09869093e-01 -3.29516120e-02 -1.14719473e-01  9.30428579e-02
  7.16972165e-03  9.50004235e-02 -3.66165489e-02 -2.31696298e-08
  3.71591449e-02  8.13949946e-03 -8.70279968e-03  1.94094926e-02
  5.40984757e-02  5.23283221e-02 -1.75565537e-02  8.20020214e-02
  2.11357754e-02  8.48407522e-02 -3.26922834e-02 -3.29940990e-02
  1.57340206e-02  2.65801493e-02  6.04446754e-02  8.20514038e-02
 -3.44955572e-03 -1.07891038e-02 -3.13367173e-02 -4.89871241e-02
  3.98519076e-02  5.81164546e-02  4.86698933e-02 -2.89034285e-02
  2.98867039e-02 -1.35866441e-02  3.88030596e-02  1.13468342e-01
 -6.07454265e-03 -9.66961086e-02 -1.02501005e-01  8.60165209e-02
  2.20259186e-02 -1.30143702e-01  3.77798118e-02  6.29250407e-02
 -6.13320270e-04  2.64404882e-02  2.09143143e-02  1.18754804e-02
 -1.26186963e-02  4.22818176e-02  2.87053008e-02  3.50544299e-03
  1.24839075e-01  7.21343234e-02 -5.88790653e-03 -9.41199288e-02
 -4.68130857e-02 -2.87751108e-02 -4.80962694e-02 -2.20460091e-02
 -3.71888354e-02 -3.67903486e-02  1.26132872e-02  1.06618032e-01
  5.94631359e-02 -8.84676203e-02 -4.86237416e-03  3.62885781e-02
 -1.94628425e-02  8.12118202e-02 -3.06016300e-02 -3.55431752e-04]",2,0
tpot,2,a python tool that automatically creates and optimizes machine learning pipeline using genetic programming if you have any question or comment about tpot please feel free to contact u via e mail ttle pennmedicine upenn edu or weixuanf pennmedicine upenn eduor twitter http twitter com trang or http twitter com weixuanfuthis project is hosted at http github com epistasislab tpot,"[('machine learning pipeline', 0.6155), ('genetic programming', 0.5263), ('python tool', 0.4543), ('tpot', 0.434), ('http github com epistasislab tpot', 0.396), ('http twitter com weixuanfuthis project', 0.2543), ('upenn eduor twitter http twitter com', 0.2375), ('e mail ttle pennmedicine upenn edu', 0.2004), ('weixuanf', 0.1392), ('question', 0.0899)]","[-8.58256444e-02  2.87575601e-03 -1.13766314e-02 -3.39014493e-02
 -3.91858444e-03 -1.04384013e-01 -3.57676968e-02  5.34879752e-02
 -7.13518485e-02 -2.23142691e-02 -2.35903473e-03 -2.20465995e-02
  2.20752712e-02 -8.75980705e-02 -1.22968117e-02  2.57186722e-02
 -9.29952636e-02  5.43841906e-03 -1.88585985e-02 -2.19376355e-01
  1.29245501e-02  3.88328321e-02  2.45396886e-02  2.62615755e-02
 -5.41786139e-04 -4.46650758e-02  1.34192202e-02 -5.59973111e-03
  2.26098411e-02 -3.34074348e-02 -8.29424262e-02  1.02837449e-02
 -5.71263060e-02  1.06250914e-02 -7.97810853e-02  1.07480042e-01
 -4.22455147e-02  4.43513282e-02 -6.31772075e-03  1.54558790e-03
 -2.04206016e-02 -1.41227558e-01 -2.96692103e-02 -5.99244833e-02
  7.08886161e-02  1.00707747e-02 -5.93292974e-02 -9.43929255e-02
 -7.86051229e-02 -9.13123076e-04 -1.13083430e-01 -1.13278702e-01
 -4.07209694e-02  2.92060841e-02 -6.15536347e-02 -7.79220611e-02
  6.84769750e-02 -7.42585026e-03  6.96625235e-03 -7.11418092e-02
 -6.02126028e-03 -3.07841636e-02 -5.47257997e-02  6.77808300e-02
  4.13406156e-02 -3.20714409e-03 -5.07500255e-03  6.60839602e-02
  8.34668055e-02 -7.15174600e-02  1.43027939e-02  5.40169328e-02
 -7.58989602e-02  1.01291403e-01  2.73679048e-02  9.62499995e-03
  5.64630441e-02  2.58803312e-02  2.58166175e-02 -1.94030926e-02
 -1.33090625e-02 -2.43795514e-02 -2.53654807e-03  5.56896627e-02
  1.12887649e-02 -4.70110681e-03 -2.31561228e-03  8.46975818e-02
 -1.51532730e-02  4.73846421e-02  2.53173094e-02 -5.00769280e-02
  9.95004475e-02  3.42288762e-02 -9.91185196e-03  1.32056186e-02
 -3.10049318e-02 -1.09100901e-01 -1.05056986e-02  4.42917459e-02
 -1.34447709e-01 -4.08835672e-02  8.12313557e-02  8.89183674e-03
 -1.93355978e-02  6.44594505e-02  2.84727011e-02 -3.71751301e-02
  1.29617929e-01 -6.22424968e-02 -7.58361667e-02 -4.14309725e-02
 -2.04113089e-02 -7.44529068e-02  4.94493954e-02  5.21347579e-03
 -1.31566888e-02  4.79589663e-02  1.78152195e-03  5.35160340e-02
 -2.26775203e-02  1.72726121e-02 -4.79873791e-02 -5.12650795e-02
 -3.53820680e-04 -5.58564141e-02 -8.96658897e-02  8.68518850e-33
  4.27302122e-02 -1.07591487e-02  1.16077706e-03 -2.21981555e-02
  7.61580393e-02 -6.85059577e-02  1.86117832e-02  2.08279099e-02
 -9.90617648e-02 -8.06794316e-02 -3.80057432e-02  2.10876334e-02
 -4.45415378e-02  1.06845185e-01 -1.73823778e-02 -1.03402078e-01
  2.14394853e-02  9.87885073e-02  2.97334418e-02  1.20214112e-02
  4.03943993e-02 -5.31112636e-03  2.79582068e-02  5.90165369e-02
  1.66912619e-02  2.90033910e-02  3.74784730e-02 -4.59492058e-02
  2.54512322e-03  4.46294472e-02 -5.08794747e-02 -1.21087506e-02
 -4.67444807e-02  1.91194634e-03  4.89922389e-02 -1.07675549e-02
  4.09164019e-02 -1.73549745e-02  4.26993482e-02  7.59298950e-02
 -6.54943287e-03  5.23438081e-02  5.02624549e-02 -3.73594612e-02
  2.41173394e-02 -2.58364305e-02  3.50793563e-02 -3.18990997e-03
  5.41212745e-02  1.21640479e-02 -1.03968047e-02 -4.08132188e-02
  1.88725870e-02 -2.41515227e-02  6.07232680e-04  5.67969643e-02
 -6.23024441e-02  3.72056738e-02  3.81577164e-02  2.30708439e-02
  4.66296170e-03  2.76710335e-02  1.39855864e-02 -5.04861958e-02
 -4.19025943e-02 -1.46951638e-02  1.51967658e-02 -2.60414295e-02
  7.14281872e-02  6.23216853e-02  1.26683470e-02  3.61081003e-03
 -1.08670592e-02 -1.82174239e-02  3.21760140e-02  2.53099855e-02
  1.76943243e-02 -5.28541692e-02 -2.13396996e-02  1.83781479e-02
 -1.86922867e-02  2.07947399e-02  3.63708027e-02 -5.67451753e-02
  3.08548305e-02 -3.35263386e-02  1.10519920e-02  2.41115093e-02
 -7.06106201e-02 -2.32647303e-02 -6.75581396e-02  2.52495743e-02
 -3.63942632e-03  3.83377373e-02  1.93055812e-02 -8.77109730e-33
 -8.01616907e-02  6.16579503e-03 -7.15687945e-02  1.14229769e-01
  1.71199460e-02  5.17354757e-02 -2.92882621e-02 -7.63819441e-02
  6.57005534e-02  7.68793821e-02  3.73769701e-02 -2.59410311e-02
 -1.65229905e-02 -6.20519258e-02 -2.46310625e-02 -4.87198643e-02
 -3.50025259e-02 -4.53312956e-02 -8.78387690e-02  2.37271264e-02
 -7.98056349e-02  8.58138129e-02 -1.00588985e-01  3.48346308e-02
  2.01048739e-02 -6.25096541e-03 -5.43264709e-02  9.76923015e-03
  3.01979817e-02  1.77560784e-02 -5.73372282e-02 -1.52841974e-02
 -7.18759149e-02  9.29878131e-02  4.95062093e-04  4.67044227e-02
  1.78665873e-02  6.50190189e-02  9.20506194e-02  1.63737815e-02
  1.93353042e-01 -4.57735127e-03 -3.16750705e-02 -3.70636513e-03
 -3.16898115e-02  3.52845713e-02 -8.56650397e-02  6.37210906e-02
  3.86926085e-02 -7.73782004e-03  7.96259940e-02  5.29114865e-02
  3.39747071e-02 -5.85689358e-02  5.85403177e-04 -2.22996175e-02
  8.75717551e-02 -3.56940925e-02 -7.76986331e-02  1.90501567e-02
 -5.65838031e-02 -3.81426066e-02  9.63363424e-02  5.99648803e-02
 -3.94998156e-02 -7.63138011e-02 -5.76521233e-02  5.39210625e-02
 -3.20419185e-02 -9.24821347e-02  3.30658853e-02  3.19904201e-02
  3.65700461e-02 -7.92138502e-02 -1.58603434e-02 -4.61972691e-02
 -6.39597550e-02  4.57381234e-02 -7.20353425e-03  1.05503658e-02
  6.37664422e-02  4.80447225e-02  2.14216728e-02  1.75848752e-02
  5.59881795e-03  4.31570038e-03  7.87846744e-02  4.63876575e-02
  8.40820223e-02 -3.02001033e-02 -1.29643958e-02  1.09959520e-01
 -2.63830740e-02  3.68283838e-02  2.83496245e-03 -4.10390086e-08
  6.71561584e-02 -2.00678054e-02  1.37882587e-02  9.16900486e-02
  1.01971151e-02  5.24974391e-02  1.05851898e-02  6.82769865e-02
 -2.38685254e-02  3.55893001e-02 -1.21164881e-02  3.00098546e-02
 -5.23225702e-02  6.47233874e-02  4.14501131e-02  8.01378582e-03
 -7.36256037e-03  1.90666299e-02  6.10077055e-03 -7.55408332e-02
 -4.43218555e-03  1.11828838e-02 -1.38203269e-02  3.09817828e-02
 -2.77274149e-03 -4.40165438e-02 -5.72981732e-03 -5.48988441e-03
 -2.13832012e-03 -7.01544760e-03 -3.00979614e-02 -5.97421080e-02
 -8.31818208e-02 -1.54474685e-02  1.09798685e-01  3.44820805e-02
 -3.36825736e-02 -4.12775874e-02 -2.60673184e-02  1.87433213e-02
  3.46998242e-03  1.24121308e-02  6.80868477e-02 -3.20018418e-02
 -1.50296558e-02 -1.69615459e-03  1.11565227e-02 -9.56856180e-03
 -4.92821150e-02 -5.61821973e-03 -4.80743684e-02  1.00371032e-03
 -4.06777672e-03 -2.61245687e-02  7.86649287e-02  6.85211048e-02
  2.29118355e-02 -6.54806793e-02  7.47815426e-03  3.62523571e-02
  4.43869159e-02  5.05159199e-02  6.30404726e-02 -2.83458345e-02]",2,0
modelforge,2,modelforge is a foundation for sharing trained machine learning model it is a set of command line tool and a python library modelforge maintains model file in a third party remote storage service cloud using the backend mechanism model metadata download link name description version etc resides in a git repository called the index and documentation is automatically generated there modelforge doe no assumption about the model they can be of any origin such a tensorflow scikit learn or your custom the underlying model storage format advanced scientific data format can wrap any data easily and efficiently but it s the developer s responsibility to convert learn more about you can run modelforge through docker or install it using the python package manager the project expose two interface command line and api contribution are pretty much welcome please follow the contribution guide and the code of conduct if you wish to make your mloncode model available in src d model please fork that repository and run modelforge publish to upload your model on your fork then create a pull request you should provide read access to the model file for everybody if you consider using our google cloud storage bucket feel free to contact u through github issue apache,"[('python library modelforge', 0.5966), ('model file', 0.5393), ('modelforge', 0.5146), ('underlying model storage format', 0.488), ('mloncode model', 0.4637), ('modelforge publish', 0.4603), ('tensorflow scikit', 0.4312), ('modelforge doe', 0.4298), ('model', 0.3662), ('machine learning model', 0.3613)]","[-4.78008278e-02 -1.20020255e-01 -5.71805723e-02 -3.26431799e-03
  1.08440034e-01 -5.23617007e-02 -9.06293765e-02  2.76329815e-02
 -1.82761196e-02 -3.31485644e-02 -3.28050926e-02  3.24235484e-02
 -7.92411622e-03 -2.76054274e-02  1.29534230e-02 -3.51569057e-02
 -7.50801563e-02  7.21364692e-02 -4.12462838e-02 -3.83534767e-02
  7.12015294e-03  4.80080359e-02 -4.77415510e-02  4.43593226e-02
 -1.73489656e-02 -6.21390902e-02  3.73167992e-02 -1.07582072e-02
 -2.21527610e-02 -2.44044848e-02 -2.05265041e-02 -3.15581262e-02
  5.15388176e-02  4.89339940e-02  2.99587045e-02 -4.28843265e-03
  5.38210645e-02 -1.03466608e-01 -5.06313369e-02 -2.45063361e-02
 -1.63641032e-02  9.81741678e-03  3.24041843e-02 -5.96800111e-02
  4.96153943e-02  2.51215976e-02 -1.86968539e-02 -7.40548894e-02
  1.19379899e-02 -2.20596213e-02 -5.88177182e-02 -7.94087350e-02
 -8.31749588e-02  5.58544248e-02  2.32634712e-02 -5.60056902e-02
  3.76863554e-02 -3.38715948e-02 -1.99759845e-02 -1.61302891e-02
 -2.39571091e-02  3.51973549e-02 -8.04852173e-02  6.55829906e-02
 -5.73505685e-02  4.48409468e-02 -3.69492806e-02  5.13465144e-02
  9.32207406e-02 -1.26841471e-01 -1.08029172e-01 -1.84833724e-02
 -4.11436260e-02 -5.72210364e-03  1.35758880e-03 -3.58818360e-02
  1.32528454e-01  6.05748519e-02  1.02291189e-01 -6.33241907e-02
 -5.76302223e-02 -3.30893393e-03  6.34917021e-02  9.40075330e-03
 -2.04766318e-02 -5.64544536e-02  1.40511114e-02  7.54239410e-02
  4.60163839e-02  3.54953762e-03  5.66417873e-02 -3.67589816e-02
 -1.62730943e-02 -1.46774901e-03 -6.25756159e-02  1.05643511e-01
  3.57251503e-02 -2.07684655e-02  1.55144092e-02  1.71686821e-02
 -1.00358605e-01 -1.97024681e-02  7.98678398e-02  3.06749176e-02
  1.93183087e-02  1.26231126e-02  6.00515231e-02  5.08377142e-03
  4.43697944e-02  9.81508289e-03  1.29848495e-02  3.42636742e-02
 -5.03236391e-02 -5.76091744e-02  9.24734697e-02 -2.16291123e-03
 -8.39470476e-02  2.55855378e-02 -7.37536745e-03  3.93097736e-02
 -8.48561525e-02  4.93014939e-02  6.38988391e-02 -3.07804607e-02
 -7.75211006e-02 -4.93283495e-02 -7.60003999e-02  3.88925163e-33
 -1.21412128e-02 -2.22501233e-02  5.61711080e-02  2.13095956e-02
  6.54230639e-02 -9.31637734e-02  2.92241424e-02 -1.42729860e-02
  1.26820998e-02 -1.06038023e-02 -8.90516117e-02  9.78820473e-02
 -5.15162386e-02  7.95886070e-02  9.77275148e-03  1.12779522e-02
 -2.52739787e-02  5.97659275e-02 -2.44352799e-02  7.53862262e-02
  1.26757652e-01  9.78825241e-03  1.20833395e-02  6.59093931e-02
  2.15821639e-02  7.99160302e-02 -1.18076866e-02 -1.88103262e-02
 -4.96075936e-02  4.30711173e-02 -7.49388859e-02 -5.23826294e-02
  1.18496818e-02  3.45327854e-02 -2.63525955e-02  4.19405065e-02
 -1.45706413e-02 -5.89310601e-02 -2.45924015e-03 -6.59597889e-02
  9.44947824e-03  1.44015048e-02 -2.82679908e-02 -3.65833715e-02
 -7.28323683e-02 -2.03856751e-02 -2.07989570e-02 -2.66217049e-02
  2.24878378e-02  1.56835245e-03  7.80926412e-03 -4.08620983e-02
 -5.71786077e-04 -2.77100895e-02 -9.51818973e-02  3.31826136e-02
  3.43213342e-02 -4.13212627e-02  6.23585097e-02 -7.04201311e-03
  5.57501242e-03  2.09768713e-02  1.00286774e-01 -5.29808141e-02
  6.77765906e-02 -1.49162048e-02 -4.56359237e-02 -5.39381355e-02
  2.82360446e-02  2.69777086e-02 -6.21650368e-02  4.69668359e-02
 -3.81324403e-02 -5.22545055e-02  3.80313881e-02 -7.87734836e-02
  2.67111473e-02 -7.70142302e-02 -6.27161190e-02  6.09957390e-02
 -6.01557083e-02  3.23980972e-02 -8.21713358e-03  2.12740386e-03
 -1.74709409e-03 -4.75661717e-02 -2.01360565e-02  5.47158495e-02
 -5.80151826e-02 -5.67745827e-02 -7.77729303e-02 -1.56943593e-02
  2.55423188e-02  2.37190188e-03 -2.54276488e-02 -4.28722149e-33
  2.48251520e-02 -1.46081736e-02 -3.71199064e-02  2.94657461e-02
 -1.66601837e-02  2.09476613e-02 -2.55698953e-02  1.85293071e-02
  3.49634103e-02 -5.24811186e-02  4.46953401e-02 -6.31628484e-02
  4.87962142e-02 -2.80180909e-02  3.04473191e-02 -5.69718666e-02
 -3.86457853e-02 -1.06318355e-01  1.80929527e-02  3.80740799e-02
 -6.26084358e-02  6.38543218e-02 -6.11991137e-02 -4.30298634e-02
  3.86944674e-02 -5.02564684e-02 -5.16715385e-02 -3.50175565e-03
  3.46533358e-02 -7.00000394e-03 -3.52172600e-03  5.11674173e-02
  2.43917629e-02  7.74081144e-03 -4.14177738e-02  1.14424936e-02
  4.90467995e-02 -4.51870896e-02  8.27445462e-02  3.92839909e-02
  9.10209864e-02  9.36721489e-02 -9.86505225e-02  3.99056002e-02
  5.54740429e-03  3.97987403e-02 -6.60914630e-02  2.80536339e-02
  5.14342748e-02 -1.09067529e-01 -1.47056784e-02 -5.71952499e-02
 -8.96452647e-03 -4.29111756e-02 -3.15480195e-02  4.75085787e-02
  6.13238737e-02  1.33084357e-02  1.06431022e-02  2.35786326e-02
 -4.88940030e-02 -3.71252522e-02  1.17417667e-02 -2.77868249e-02
 -6.74742460e-02  4.18330282e-02 -6.74960539e-02 -2.47663092e-02
 -2.30240487e-02 -4.71986569e-02 -1.33777615e-02  3.77951935e-02
  3.20862159e-02  1.22920424e-01 -6.54929206e-02 -3.26579921e-02
 -6.93908602e-04  8.95370822e-03  2.14147363e-02  2.82486826e-02
  1.88815538e-02  2.97173299e-02  3.45763862e-02  8.93730745e-02
  5.42508885e-02 -3.23214661e-03  5.84170967e-02  4.07566391e-02
 -4.94143646e-03 -7.72787780e-02 -6.24307320e-02  6.92530572e-02
  6.42995834e-02  1.30127013e-01 -9.56221670e-03 -2.83757728e-08
 -4.56273220e-02  7.58470595e-02  9.30919945e-02  1.00886682e-02
 -2.57946532e-02  1.18565023e-01  3.03248540e-02  5.83038814e-02
  1.38845397e-02  3.26889455e-02  2.48004706e-03 -1.29586169e-02
 -1.30491942e-01 -1.08033577e-02 -2.46473327e-02  1.23764381e-01
  2.33123768e-02  6.28054840e-03  1.40718017e-02  2.52522305e-02
  1.16102323e-01  2.66038254e-02  1.27393920e-02 -4.15488742e-02
  5.55507578e-02 -3.51745002e-02  3.24518830e-02  4.60747369e-02
 -5.36844619e-02 -1.33052673e-02 -4.92160097e-02  6.75965799e-04
  3.08446977e-02 -2.42974926e-02  6.85027242e-02  1.04988366e-01
  5.11964336e-02 -3.89464609e-02 -7.62745412e-03  7.68120289e-02
  1.38784759e-02  5.28888442e-02 -3.40565033e-02 -2.52062995e-02
  3.58217843e-02  5.02016433e-02  2.07112241e-03 -2.08417047e-02
 -3.23643982e-02  9.46193188e-02 -5.97186983e-02  3.79989743e-02
 -5.40746413e-02  3.38151902e-02 -8.43277127e-02  8.83770809e-02
  6.79316558e-03 -8.42680931e-02  2.06652582e-02 -3.99213247e-02
  5.79472864e-03  4.12001200e-02  5.54925166e-02  2.53447834e-02]",2,2
mnist,2,the mnist database is available at http yann lecun com exdb mnist the mnist database is a dataset of handwritten digit it ha training sample and test sample each image is represented by x pixel each containing a value with it grayscale value it is a subset of a larger set available from nist the digit have been size normalized and centered in a fixed size image it is a good database for people who want to try learning technique and pattern recognition method on real world data while spending minimal effort on preprocessing and formatting there are four file available which contain separately train and test and image and label thanks to yann lecun corinna cortes christopher j c burges mnist make it easier to download and parse mnist file,"[('mnist database', 0.6722), ('mnist file', 0.5984), ('mnist', 0.4811), ('pattern recognition method', 0.4789), ('dataset', 0.471), ('training sample', 0.4522), ('handwritten digit', 0.4485), ('good database', 0.4367), ('real world data', 0.4114), ('file', 0.3508)]","[-4.17840108e-02 -9.27032158e-02 -3.62313725e-02 -5.91188073e-02
 -4.40726355e-02 -2.83102728e-02 -1.73574984e-02  5.32456487e-02
 -1.57585531e-01  2.18448173e-02  2.73867305e-02  2.46525407e-02
  1.05282590e-01  1.21290551e-03 -5.13728075e-02  5.46314865e-02
 -6.39486387e-02  3.91605757e-02 -1.38810836e-02 -2.53890380e-02
 -2.20174529e-02  1.32778063e-02  1.37600191e-02 -3.74452174e-02
  4.42691892e-02  3.04573551e-02  6.78982660e-02 -6.93831667e-02
 -1.48642501e-02 -3.49930339e-02 -4.39774767e-02  6.42092004e-02
  9.07500386e-02  2.10016891e-02  8.79299175e-03  2.74124257e-02
  6.78765252e-02 -3.15320864e-03 -2.90464573e-02 -1.87845267e-02
 -4.93860357e-02 -1.61808729e-02  1.92348938e-02 -1.17624197e-02
  1.02556787e-01  9.37518850e-02  4.78468165e-02 -4.89093140e-02
  3.55961248e-02  7.82846436e-02 -1.36362731e-01 -1.69745628e-02
 -1.88790988e-02  4.64583300e-02 -2.77303066e-03 -2.92229261e-02
  1.93138123e-02  1.98426917e-02 -5.13865687e-02 -1.22449957e-02
  5.90581335e-02 -3.04027423e-02 -7.98737705e-02 -2.29257643e-02
  2.81101316e-02 -1.35702146e-02 -5.40764481e-02  3.54511291e-02
  1.60323322e-01 -1.53762519e-01 -8.97310165e-05  2.80830599e-02
 -1.09945357e-01  7.45719895e-02 -4.97891894e-03  2.02771481e-02
  6.87563196e-02 -7.60359690e-03  1.16081219e-02 -5.17849512e-02
 -3.76950800e-02 -1.36083793e-02  5.34432307e-02  4.54466790e-03
  3.63486521e-02  1.66509405e-03 -3.08390940e-03  6.48847520e-02
 -3.49999778e-02 -2.33761147e-02  7.04411510e-03 -3.53016630e-02
 -2.82933731e-02 -2.98402254e-02 -5.67538328e-02  7.56824901e-03
 -4.25888738e-03  3.18673849e-02  2.64598075e-02  3.39908153e-02
 -8.17572847e-02 -2.53303368e-02 -3.66224013e-02  6.11280389e-02
 -3.53468284e-02 -1.44462222e-02  7.60720670e-02 -1.14328079e-02
  6.09636530e-02 -3.41178514e-02 -1.76362339e-02  9.57361050e-03
 -1.34254128e-01 -7.52359852e-02  7.29440227e-02 -3.91751677e-02
 -5.68209961e-02  5.63082024e-02 -2.27836538e-02  5.49040399e-02
 -9.46258754e-02  4.93512116e-02 -9.15513560e-03 -9.74166114e-03
  5.27353073e-03  3.42935808e-02 -7.56003335e-02  1.62609403e-33
  5.04684821e-03  2.10521705e-02  1.58880036e-02 -8.69673193e-02
  5.68489321e-02 -8.12980458e-02 -1.41398245e-02  3.17234471e-02
  8.70643836e-03  1.09815178e-02  2.61217472e-03  8.71972591e-02
 -1.88922286e-02  1.30893484e-01  3.11310813e-02 -2.44353358e-02
  1.22702997e-02  4.73175384e-02  4.39688340e-02  2.82259416e-02
  2.44310386e-02  1.52840083e-02  4.28098515e-02  3.99647057e-02
  2.35818438e-02  6.50399411e-03  3.67420428e-02 -1.84908812e-03
  3.93955261e-02 -2.17974912e-02 -4.18437831e-02 -3.66880037e-02
  9.38964635e-03  5.38882287e-03  6.23497646e-04  2.35140286e-02
 -3.58705707e-02 -7.99551159e-02 -5.05181309e-03 -8.39872938e-03
 -2.50676796e-02  5.09167761e-02  2.73139384e-02 -1.13534424e-02
  1.39343925e-02 -6.90168981e-03  2.48778127e-02  3.25728878e-02
  4.87160757e-02 -1.83490720e-02 -3.96082997e-02  3.56020071e-02
 -2.67440099e-02  1.73518565e-02 -2.18692292e-02  5.75654730e-02
  4.41990700e-03  2.09357999e-02  2.90300120e-02  6.77473992e-02
 -7.13736787e-02  4.97215101e-03  4.36928794e-02  3.64640802e-02
 -1.65759365e-03 -5.23355789e-02 -4.75180708e-02 -1.08210728e-01
  7.63510540e-02  2.18764823e-02 -3.35500315e-02 -1.04192588e-02
  2.50936262e-02 -3.81112285e-02 -2.85933502e-02  3.35275903e-02
 -4.25225385e-02 -8.93497616e-02 -7.68064111e-02  5.97440563e-02
 -3.12827826e-02 -5.78495348e-03 -4.93352897e-02 -6.26180768e-02
 -4.11829390e-02  8.54064152e-02  2.58865990e-02 -1.03116676e-01
  9.14099347e-03 -2.84760576e-02 -4.79811318e-02  6.70601651e-02
 -4.12966684e-02 -2.19290182e-02 -4.72003445e-02 -2.08402255e-33
 -2.54486334e-02  4.02962863e-02 -4.47295792e-02  3.48412953e-02
  3.17458785e-03  2.87867039e-02 -3.87208536e-03 -2.16858741e-02
 -3.41783427e-02  4.64273989e-03  3.59678641e-02 -4.85912822e-02
  3.73294251e-03 -1.94864627e-02 -5.17709032e-02 -2.64914613e-02
 -1.61817688e-02 -3.03483102e-02  5.31106703e-02  3.07130236e-02
 -5.64757385e-04  1.23207390e-01 -7.04644248e-02  1.43769267e-03
  2.41507739e-02 -3.81957144e-02 -1.28446192e-01 -4.18042205e-03
  6.23275489e-02  3.39515768e-02 -4.58680801e-02 -4.75511216e-02
  4.73578572e-02 -3.43755372e-02  1.32759074e-02 -4.55808155e-02
  8.18990823e-03 -2.50460226e-02  4.35413048e-02  4.28503454e-02
  5.60711809e-02  9.97782797e-02 -1.78710818e-02 -2.86610033e-02
 -6.03994653e-02 -9.75369141e-02 -1.18912250e-01  8.25582966e-02
  1.31198075e-02 -1.48973055e-02  2.79406514e-02 -2.54323445e-02
 -1.15942501e-03 -1.05699822e-01  7.02307969e-02 -6.08766312e-03
  7.15561956e-03 -1.18722459e-02  1.74758788e-02  6.17743246e-02
 -4.12577130e-02 -6.23146817e-02 -8.82443320e-03  5.01129441e-02
 -1.51791070e-02  1.50222927e-02 -2.82008555e-02  1.42148566e-02
 -5.84103949e-02 -3.98834283e-03  1.68463066e-02  4.71471585e-02
  1.11466371e-01  6.98066726e-02 -2.31917184e-02 -6.59167990e-02
 -9.22974497e-02  5.16971014e-02  2.77031828e-02  1.01130102e-02
  5.67069091e-02 -2.44103521e-02  1.81007367e-02  1.02934383e-01
  4.29664217e-02  9.55945402e-02  5.69800586e-02 -5.65419272e-02
  1.36316177e-02 -3.00663300e-02 -5.18290214e-02  4.68271524e-02
 -5.84317558e-03  1.21421747e-01 -3.86461951e-02 -2.09616378e-08
  2.61967834e-02 -7.45052472e-02  1.14196315e-01 -1.89392809e-02
  7.12166028e-03  1.28581654e-02 -5.78834116e-02  7.99365714e-02
 -5.56165464e-02  2.03934615e-03  5.10449708e-02  2.07376052e-02
 -1.22943431e-01 -7.65301241e-03  4.56514582e-02 -4.02682126e-02
  1.08848624e-01 -1.08016981e-02 -1.92039032e-02  5.26823625e-02
  2.65444349e-02  6.92153126e-02 -1.62903257e-02  2.70573255e-02
  3.16472799e-02  4.08533737e-02 -4.43278626e-02  3.73938903e-02
 -5.82184568e-02  5.27943745e-02  2.13339850e-02 -2.50960924e-02
  1.48961321e-01 -7.39310980e-02  2.84126811e-02  3.61681059e-02
  5.36827147e-02  2.80287713e-02 -7.29258582e-02 -4.86273840e-02
 -1.12816365e-02  4.53566387e-02 -1.19451126e-02 -1.26642080e-05
  4.63688634e-02 -1.23536043e-01  1.34836640e-02 -7.39317015e-02
 -3.72005962e-02 -3.68574485e-02 -2.12390665e-02  4.39496078e-02
 -2.02078689e-02  9.35604610e-03  7.95729458e-02  1.09317444e-01
 -3.61022912e-03 -3.79131287e-02 -6.68953639e-03  6.77334592e-02
 -7.78218778e-03  5.76003343e-02 -4.71749865e-02 -3.82897095e-03]",2,0
dm-sonnet,2,documentation example warning sonnet is currently beta we would love to have you use it a an early adopter and please let u know if thing aren t working a you would expect sonnet is a library built on top of tensorflow designed to provide simple composable abstraction for machine learning research sonnet ha been designed and built by researcher at deepmind it can be used to construct neural network for many different purpose un supervised learning reinforcement learning we find it is a successful abstraction for our organization you might too more specifically sonnet provides a simple but powerful programming model centered around a single concept snt module module can hold reference to parameter other module and method that apply some function on the user input sonnet ship with many predefined module e g snt linear snt conv d snt batchnorm and some predefined network of module e g snt net mlp but user are also encouraged to build their own module unlike many framework sonnet is extremely unopinionated about how you will use your module module are designed to be self contained and entirely decoupled from one another sonnet doe not ship with a training framework and user are encouraged to build their own or adopt those built by others sonnet is also designed to be simple to understand our code is hopefully clear and focussed where we have picked default e g default for initial parameter value we try to point out why the easiest way to try sonnet is to use google colab which offer a free python notebook attached to a gpu or tpu to get started install tensorflow and sonnet you can run the following to verify thing installed correctly sonnet ship with a number of built in module that you can trivially use for example to define an mlp we can use the snt sequential module to call a sequence of module passing the output of a given module a the input for the next module we can use snt linear and tf nn relu to actually define our computation to use our module we need to call it the sequential module and most module define a call method that mean you can call them by name it is also very common to request all the parameter for your module most module in sonnet create their parameter the first time they are called with some input since in most case the shape of the parameter is a function of the input sonnet module provide two property for accessing parameter the variable property return all tf variable that are referenced by the given module it is worth noting that tf variable are not just used for parameter of your model for example they are used to hold state in metric used in snt batchnorm in most case user retrieve the module variable to pas them to an optimizer to be updated in this case non trainable variable should typically not be in that list a they are updated via a different mechanism tensorflow ha a built in mechanism to mark variable a trainable parameter of your model v non trainable other variable sonnet provides a mechanism to gather all trainable variable from your module which is probably what you want to pas to an optimizer sonnet strongly encourages user to subclass snt module to define their own module let s start by creating a simple linear layer called mylinear using this module is trivial by subclassing snt module you get many nice property for free for example a default implementation of repr which show constructor argument very useful for debugging and introspection you also get the variable and trainable variable property you may notice the my linear prefix on the variable above this is because sonnet module also enter the module name scope whenever method are called by entering the module name scope we provide a much more useful graph for tool like tensorboard to consume e g all operation that occur inside my linear will be in a group called my linear additionally your module will now support tensorflow checkpointing and saved model which are advanced feature covered later sonnet support multiple serialization format the simplest format we support is python s pickle and all built in module are tested to make sure they can be saved loaded via pickle in the same python process in general we discourage the use of pickle it is not well supported by many part of tensorflow and in our experience can be quite brittle reference http www tensorflow org alpha guide checkpointstensorflow checkpointing can be used to save the value of parameter periodically during training this can be useful to save the progress of training in case your program crash or is stopped sonnet is designed to work cleanly with tensorflow checkpointing reference http www tensorflow org alpha guide saved modeltensorflow saved model can be used to save a copy of your network that is decoupled from the python source for it this is enabled by saving a tensorflow graph describing the computation and a checkpoint containing the value of weight the first thing to do in order to create a saved model is to create a snt module that you want to save next we need to create another module describing the specific part of our model that we want to export we advise doing this rather than modifying the original model in place so you have fine grained control over what is actually exported this is typically important to avoid creating very large saved model and such that you only share the part of your model you want to e g you only want to share the generator for a gan but keep the discriminator private we now have a saved model in the tmp example saved model folder loading this model is simple and can be done on a different machine without any of the python code that built the saved model note that the loaded object is not a sonnet module it is a container object that ha the specific method e g inference and property e g all variable that we added in the previous block reference http www tensorflow org alpha guide distribute strategywe are still working on making sonnet compatible with distribution strategy currently module that compute forward function but don t update internal state e g conv d work well with tf distribute mirroredstrategy and tf distribute experimental tpustrategy in general our philosophy with sonnet is not to add special casing inside module to support library in some case this is unavoidable since component that update state must do so in a distribution aware manner for example optimizers metric or batch normalization for these module we plan on creating new version in the snt distribute namespace to indicate that these module are distribution aware,"[('many framework sonnet', 0.6528), ('module e g snt net mlp', 0.6357), ('sonnet module', 0.6258), ('input sonnet module', 0.5742), ('machine learning research sonnet', 0.5527), ('single concept snt module module', 0.5129), ('training framework', 0.4721), ('sonnet', 0.4699), ('trainable other variable sonnet', 0.4698), ('tensorflow', 0.4574)]","[ 2.65975279e-04 -1.11721888e-01 -5.96715398e-02 -7.16498047e-02
  1.08957544e-01  7.25241452e-02  3.05172764e-02 -9.87840164e-03
 -6.11076355e-02 -4.69554849e-02 -3.63213196e-02  4.17013168e-02
 -5.13743702e-03  2.93133724e-02  3.96124609e-02 -6.18892163e-03
  4.43610027e-02  5.49470298e-02 -3.07638478e-03 -1.00127518e-01
 -1.31433755e-02  6.71284348e-02  3.16355154e-02 -6.60502585e-04
 -2.07384042e-02  6.53156452e-03 -2.89156586e-02 -7.90350046e-03
  4.82232608e-02 -4.20131050e-02 -2.16520540e-02  7.26308674e-02
  3.32170241e-02 -6.93163136e-03 -4.72694710e-02  5.27170375e-02
 -1.72801819e-02 -3.32912356e-02 -5.20959012e-02 -4.71429341e-02
 -7.63846487e-02  2.81181373e-02 -5.23563251e-02 -5.93105257e-02
  1.30220398e-01 -1.96691453e-02 -1.70087125e-02 -6.44923151e-02
  1.53705664e-02 -2.07752287e-02 -4.48549949e-02 -1.05968982e-01
  1.56907330e-03  2.50678994e-02  9.08360258e-03 -4.08316292e-02
 -8.88655335e-03 -9.91404057e-03  1.46948993e-02 -7.93653652e-02
 -1.18578123e-02 -2.70803571e-02 -6.17317483e-02  1.14564989e-02
  6.11964576e-02  3.30358669e-02 -9.79246572e-03  1.56138288e-02
  9.36948955e-02 -4.74069193e-02 -2.84716859e-03 -3.18002850e-02
 -8.92056525e-02  9.14954469e-02 -2.08423473e-02 -2.53948383e-02
  6.29182532e-02 -3.72289717e-02  7.45484829e-02 -3.53561454e-02
 -1.95290726e-02  1.01121224e-01  7.35677481e-02  2.18376163e-02
  1.39155583e-02  3.32693532e-02 -3.21374796e-02  8.49275291e-02
  7.14553380e-03 -3.02442275e-02 -1.84229650e-02 -5.80030866e-02
  6.67418316e-02  2.72051524e-02  1.69288125e-02 -1.84924260e-03
 -4.50066850e-02 -7.71861002e-02 -2.51838528e-02  7.47171184e-03
 -2.32605767e-02 -3.13562378e-02  1.29635902e-02  4.39185463e-02
  2.45938655e-02  4.50725742e-02  4.23371755e-02 -1.19186640e-02
  8.86267498e-02 -4.08542715e-02 -1.06992736e-01  1.74862966e-02
 -4.11631204e-02 -1.12369083e-01  4.85008098e-02 -1.00580052e-01
  1.07414378e-02  6.61061257e-02  2.36386862e-02  1.36141285e-01
 -8.64747316e-02  2.25390345e-02 -8.88714567e-03 -1.53896511e-02
  6.06789142e-02 -5.11619523e-02 -1.03415802e-01  5.44056952e-33
 -3.55421305e-02  2.08456758e-02  5.92971826e-03  6.19308725e-02
  7.48889744e-02 -6.36593476e-02  9.11969244e-02  2.23705918e-02
  4.76990081e-02 -4.59602214e-02 -6.44245446e-02  8.66110697e-02
  3.16332001e-03  8.37997124e-02  2.76293904e-02 -8.52488205e-02
  6.04825728e-02  1.01800486e-02  6.90530315e-02 -1.11856889e-02
 -2.19183061e-02  6.46943599e-02  6.10728934e-02  2.35878266e-02
  1.06514543e-01 -2.59194244e-02  4.99605760e-02  2.89375149e-02
  3.13151926e-02  1.39488988e-02 -3.97898667e-02  5.51752225e-02
  1.43314444e-03  6.22760318e-02 -9.50178970e-03 -1.00522684e-02
  1.66665278e-02 -4.70147096e-02  2.68827621e-02 -1.21934917e-02
 -7.03735203e-02  6.10305443e-02 -5.83199039e-03 -1.79856494e-02
 -2.17164960e-02 -4.86308858e-02 -4.22763675e-02  6.45514429e-02
 -2.65960908e-03 -8.26511681e-02 -4.13480438e-02  4.64772359e-02
 -1.12262992e-02 -6.26220107e-02 -4.37804237e-02  2.30211187e-02
 -2.49184184e-02  6.69441372e-02 -2.08797958e-03  2.93871053e-02
 -9.29483920e-02 -7.89966509e-02  7.46830786e-03  1.74734555e-02
  5.00943884e-02 -1.65519863e-02 -5.42317368e-02 -2.53011938e-02
  1.17588099e-02 -4.93887533e-03 -5.11412285e-02  5.64952530e-02
 -4.57669944e-02 -1.09084668e-02 -4.41436246e-02 -4.63129245e-02
 -1.21115707e-02 -4.82273884e-02  1.73055474e-02  5.37323840e-02
 -1.19188108e-01  4.95560989e-02  2.38560773e-02 -1.47347348e-02
 -2.79821120e-02 -4.39661331e-02 -2.50362866e-02 -3.65093462e-02
  7.36104846e-02 -4.59651761e-02 -4.46476527e-02 -2.91859340e-02
  1.18881673e-01  1.60958767e-02 -4.29402152e-03 -4.97625513e-33
 -5.52010722e-02  9.12112966e-02 -1.11850470e-01  9.16489512e-02
  4.31195349e-02  3.56208906e-02  5.25841527e-02 -1.36443786e-02
 -2.32261438e-02  4.06730063e-02  5.09953536e-02 -5.83230481e-02
  7.13313604e-03 -5.50893992e-02 -4.59096543e-02  1.30930850e-02
 -4.65084426e-02 -3.53568532e-02  5.19566499e-02  6.75927801e-03
 -7.54462108e-02  3.97352986e-02 -6.89584613e-02 -7.96219632e-02
  2.43892018e-02 -3.30268107e-02 -1.03746027e-01 -1.97259262e-02
  3.16036008e-02  8.85237977e-02  2.02713721e-03 -3.76816541e-02
 -1.20542003e-02  8.13486576e-02  2.49320772e-02  3.62745188e-02
  8.18889439e-02  1.19365975e-02  1.23700183e-02  1.27791911e-02
  1.24841750e-01  2.86670998e-02  2.76688039e-02 -1.98648754e-03
  2.14325897e-02 -5.20892553e-02 -1.01554729e-01  5.88832684e-02
 -6.34219905e-04 -7.33620748e-02 -5.21240868e-02 -5.42117618e-02
 -6.10260805e-03 -7.68392235e-02 -2.83098426e-02  3.08666881e-02
  1.32045493e-01 -2.10925601e-02  4.33462374e-02  1.62835829e-02
 -4.74363007e-03 -1.35845626e-02 -1.99889820e-02  4.40159924e-02
 -1.03647932e-02 -1.40562430e-02 -9.92792100e-02 -1.23577407e-02
 -3.14154364e-02 -9.04318243e-02 -8.71333107e-03  2.47058179e-02
  1.87592786e-02  6.34069443e-02 -1.19379662e-01  1.15365870e-02
  9.06956382e-03  3.05659790e-02  2.03287061e-02 -6.53654635e-02
  9.11157951e-02  4.49292175e-03 -3.66652198e-02  4.67020869e-02
  5.70918582e-02  4.97483984e-02  8.41233656e-02 -3.02823652e-02
  4.14700992e-02 -3.61043587e-02  4.34760861e-02  2.62732413e-02
  2.76367310e-02  1.03164308e-01  7.08570611e-03 -2.72758438e-08
  1.47111388e-02  2.53511332e-02  3.23447995e-02 -2.63070110e-02
  3.68459113e-02  1.12947291e-02  4.27392572e-02  3.74567844e-02
  3.66299525e-02  3.19651738e-02 -4.69578765e-02  3.00616641e-02
 -7.48968795e-02 -6.45941198e-02  2.78970264e-02  3.69431190e-02
  1.30704197e-03  1.61688179e-02  2.23364923e-02 -5.67971356e-02
  8.04736242e-02  4.46874537e-02 -3.46075441e-03  2.43958198e-02
  1.98914520e-02 -7.77238086e-02  2.97249947e-02  7.08017200e-02
  9.07935854e-03 -1.41263483e-02 -8.11804384e-02 -1.18118031e-02
  2.11244281e-02 -4.94190529e-02  9.41399932e-02  9.80680436e-02
  1.54887578e-02 -4.87999618e-02  1.78086832e-02  3.58515941e-02
 -3.21078822e-02  1.54621126e-02 -1.66350864e-02 -6.56000003e-02
  8.24439004e-02 -2.15667170e-02 -1.82325002e-02 -9.41180289e-02
 -8.91718362e-03 -2.66838889e-03 -5.25660440e-03  3.84983830e-02
 -2.51195580e-02 -5.94994426e-03  6.89612627e-02  9.65270847e-02
  1.33937746e-02 -6.24568947e-02 -3.71790379e-02  8.26262753e-04
 -7.65499845e-02  4.07199226e-02  1.53504405e-02 -6.35438263e-02]",2,2
efficientnet_pytorch,2,install with pip install efficientnet pytorch and load a pretrained efficientnet with the efficientnetv paper ha been released i am working on implementing it a you read this about efficientnetv efficientnetv is a new family of convolutional network that have faster training speed and better parameter efficiency than previous model to develop this family of model we use a combination of training aware neural architecture search and scaling to jointly optimize training speed and parameter efficiency the model were searched from the search space enriched with new ops such a fused mbconv here is a comparison this update add this update add comprehensive comment and documentation thanks to workingcoder this update add a new category of pre trained model based on adversarial training called advprop it is important to note that the preprocessing required for the advprop pretrained model is slightly different from normal imagenet preprocessing a a result by default advprop model are not used to load a model with advprop use there is also a new large efficientnet b pretrained model that is only available in advprop form when using these model replace imagenet preprocessing code a follows this update also address multiple other issue this update allows you to choose whether to use a memory efficient swish activation the memory efficient version is chosen by default but it cannot be used when exporting using pytorch jit for this purpose we have also included a standard export friendly swish activation function to switch to the export friendly version simply call model set swish memory efficient false after loading your desired model this update address issue and this update make the swish activation function more memory efficient it also address pull request and thanks to the author of all the pull request upgrade the pip package with pip install upgrade efficientnet pytorchthe b and b model are now available additionally all pretrained model have been updated to use autoaugment preprocessing which translates to better performance across the board usage is the same a before this update add easy model exporting and feature extraction it is also now incredibly simple to load a pretrained model with a new number of class for transfer learning the b and b model are now available their usage is identical to the other model this repository contains an op for op pytorch reimplementation of efficientnet along with pre trained model and example the goal of this implementation is to be simple highly extensible and easy to integrate into your own project this implementation is a work in progress new feature are currently being implemented at the moment you can easily upcoming feature in the next few day you will be able to if you re new to efficientnets here is an explanation straight from the official tensorflow implementation efficientnets are a family of image classification model which achieve state of the art accuracy yet being an order of magnitude smaller and faster than previous model we develop efficientnets based on automl and compound scaling in particular we first use automl mobile framework to develop a mobile size baseline network named a efficientnet b then we use the compound scaling method to scale up this baseline to obtain efficientnet b to b efficientnets achieve state of the art accuracy on imagenet with an order of magnitude better efficiency in high accuracy regime our efficientnet b achieves state of the art top top accuracy on imagenet with m parameter and b flop being x smaller and x faster on cpu inference than previous best gpipe in middle accuracy regime our efficientnet b is x smaller and x faster on cpu inference than resnet with similar imagenet accuracy compared with the widely used resnet our efficientnet b improves the top accuracy from of resnet to under similar flop constraint efficientnet pytorch is a pytorch re implementation of efficientnet it is consistent with the original tensorflow implementation such that it is easy to load weight from a tensorflow checkpoint at the same time we aim to make our pytorch implementation a simple flexible and extensible a possible if you have any feature request or question feel free to leave them a github issue install via pip or install from source load an efficientnet load a pretrained efficientnet detail about the model are below below is a simple complete example it may also be found a a jupyter notebook in example simple or a a colab notebook we assume that in your current directory there is a img jpg file and a label map txt file imagenet class name these are both included in example simple you can easily extract feature with model extract feature exporting to onnx for deploying to production is now simple here is a colab example see example imagenet for detail about evaluating on imagenet if you find a bug create a github issue or even better submit a pull request similarly if you have question simply post them a github issue i look forward to seeing what the community doe with these model,"[('efficientnet pytorch', 0.6316), ('efficientnetv efficientnetv', 0.5972), ('efficientnet pytorchthe b', 0.5637), ('efficientnet b', 0.5571), ('similar flop constraint efficientnet pytorch', 0.5445), ('official tensorflow implementation efficientnets', 0.5421), ('efficientnetv paper', 0.5299), ('efficientnet', 0.5284), ('new large efficientnet b', 0.5261), ('efficientnet load', 0.4981)]","[-5.86626083e-02 -2.84073371e-02 -1.98385753e-02  1.73042051e-03
  6.24959208e-02  2.71498431e-02  8.88539106e-03 -6.66536242e-02
 -3.61550711e-02 -3.68364453e-02 -2.53902301e-02  1.01371937e-01
 -2.88850255e-02  2.15942618e-02 -3.74819478e-03  8.37422684e-02
  1.07958904e-02  7.49011338e-02 -6.71937838e-02 -6.71393573e-02
 -2.11722171e-03 -4.45379838e-02  2.39990279e-02 -3.28870565e-02
  3.59416865e-02 -1.05041731e-02 -3.94569188e-02 -6.06917068e-02
  3.09546851e-02 -5.62724145e-03  1.62388273e-02  3.93735617e-02
  4.60155569e-02  4.15833108e-02  4.79654438e-04 -3.82227935e-02
 -8.31257626e-02  2.48715337e-02 -3.77697423e-02  1.33903685e-03
  3.32170725e-02 -8.08589011e-02 -3.80187482e-02  8.19511991e-03
  2.97645740e-02  3.72333378e-02 -1.78621281e-02 -3.90753709e-03
 -6.83148112e-03 -3.97990458e-02 -9.33098607e-03 -1.10307643e-02
 -6.09477498e-02  4.16056179e-02  4.27850075e-02 -1.08730793e-02
 -1.81627516e-02 -3.11731193e-02 -3.39039974e-02 -2.22578235e-02
  1.65653769e-02  1.33214174e-02 -3.82176787e-02  2.02026907e-02
 -2.00355370e-02  8.21677744e-02  6.60963133e-02  7.44182244e-03
  5.90208620e-02 -4.84563448e-02 -1.32643562e-02  2.17346996e-02
 -8.72771591e-02  4.59976718e-02 -1.25708589e-02 -2.95548216e-02
  1.59297854e-01 -8.27287231e-03 -5.51375076e-02 -9.67711732e-02
 -1.06074875e-02 -2.31873021e-02  7.65360473e-03 -4.42556404e-02
  1.07145011e-02 -3.03747989e-02 -1.18298367e-01  8.08433630e-03
  4.13918048e-02 -6.07570745e-02 -2.59475596e-02  3.72765190e-03
  4.57582176e-02  4.92427424e-02  1.01489313e-01  6.38248306e-03
 -1.41274007e-02 -1.04167931e-01 -1.07697539e-01  8.13272893e-02
 -2.22217548e-03 -4.93407883e-02 -4.26324606e-02 -3.37529257e-02
  2.21949015e-02  5.33626936e-02  2.65483856e-02  3.10198423e-02
  4.07889783e-02 -1.46232937e-02  5.73607013e-02  4.01888192e-02
  1.34291416e-02 -5.10741025e-02  5.32344393e-02 -2.56390665e-02
 -6.58872128e-02  9.63521451e-02  7.05407783e-02  7.40754977e-02
 -8.75684172e-02  1.90617852e-02 -9.98852216e-03  9.99455899e-03
 -1.01684034e-01 -3.75728160e-02 -1.02947749e-01  8.58290066e-33
 -5.09266406e-02  3.33504863e-02  4.36116103e-03 -5.13165221e-02
  5.96175902e-02  8.15310888e-03  9.08235013e-02 -1.29266921e-02
 -2.88967136e-02 -2.02005375e-02 -1.24594748e-01 -2.22995747e-02
  2.31053593e-04  1.02378048e-01  4.81245928e-02 -9.56047177e-02
  1.81997418e-02  8.70145038e-02  7.37341046e-02  7.97390658e-03
  5.80676496e-02  3.68473828e-02  5.48978243e-03  7.56181926e-02
  8.89995415e-03 -9.40858871e-02  1.56962015e-02 -1.68456547e-02
  4.21458483e-02  3.26837078e-02 -4.65071835e-02 -4.03200872e-02
  5.17361611e-02 -4.92846407e-02 -1.04272189e-02 -6.23001866e-02
 -6.46067560e-02 -4.45058011e-02 -2.36965716e-03 -7.53410161e-02
 -6.65101483e-02  2.08378341e-02 -2.95730829e-02 -5.58872744e-02
 -8.62690434e-02 -1.38548724e-02 -7.80894235e-02  1.00702502e-01
  2.23317631e-02 -4.20328528e-02 -2.55893287e-03  1.74994823e-02
  4.30750195e-03 -3.15298960e-02  2.55514421e-02 -8.28615054e-02
  7.31597915e-02  7.22732469e-02  8.57588351e-02  1.41507149e-01
 -2.02765111e-02  2.05703918e-02 -5.34330308e-02 -1.09267496e-02
 -1.61535554e-02  1.92429554e-02  3.65015818e-03 -1.15745217e-02
  3.73967778e-04  2.68346146e-02 -3.94782685e-02  6.67773709e-02
 -1.48908021e-02 -8.47169198e-03  4.09781225e-02  1.05642632e-03
 -3.59321907e-02 -8.93949047e-02 -3.35274078e-02  1.67274289e-02
 -1.45058602e-01  7.75506645e-02 -7.39884283e-03 -1.45547874e-02
 -8.30132663e-02  1.87429152e-02 -3.62067409e-02 -2.73906160e-02
  1.17064930e-01  3.71156074e-02 -2.77515259e-02  3.65202012e-03
  3.33092064e-02 -1.76847037e-02 -2.55891611e-03 -6.83206206e-33
 -1.13450643e-02  1.26874208e-01 -1.07153639e-01  9.68057439e-02
  9.47841909e-03 -4.04335298e-02  1.89915281e-02 -7.22185373e-02
 -4.49961387e-02  1.85551625e-02 -1.87832005e-02 -4.90147918e-02
  3.30401352e-03 -4.99931462e-02  1.07422598e-01 -6.28256053e-02
 -6.75973073e-02 -4.83783074e-02  7.82126188e-03  7.89755583e-02
 -1.59493107e-02  4.08647805e-02 -4.47917543e-02 -1.10881869e-02
 -2.04955135e-02 -3.83506268e-02 -6.31234571e-02  6.48920387e-02
  2.03313045e-02  2.48602331e-02 -5.95865250e-02 -2.17418242e-02
 -4.28528935e-02  2.75863409e-02  3.20886038e-02  3.11839618e-02
  9.67390314e-02 -4.15212587e-02  2.40389840e-03  9.01085231e-03
  9.65727493e-02  3.03260498e-02 -2.28571463e-02  9.58025903e-02
 -3.93460132e-02  2.21899822e-02 -8.26013088e-02 -2.13748353e-05
 -2.23556510e-03 -1.95408817e-02 -2.75862198e-02  2.00056750e-02
 -3.86312753e-02  9.09867883e-03  2.01273113e-02  8.63584038e-03
  1.07793650e-03  6.54830262e-02  4.68879230e-02  2.97103263e-02
 -7.25274580e-03 -1.26731083e-01  6.70367153e-04  5.86245731e-02
  6.07912429e-02  3.30399424e-02 -3.27357985e-02  4.08619158e-02
 -2.74871029e-02 -5.45463637e-02 -7.19403252e-02  1.71963926e-02
 -4.50686887e-02  5.57206236e-02 -7.01053962e-02  1.02547230e-02
  8.42067897e-02 -9.28185135e-03 -1.33855343e-02  2.80544418e-03
 -3.79972011e-02  5.88382818e-02  2.30587134e-03  1.86965056e-02
 -1.40409898e-02  7.39635602e-02  5.82956821e-02 -9.88075975e-03
  4.57966514e-02  4.37763929e-02  8.99577048e-03  2.14329679e-02
  2.21462436e-02  7.76191354e-02  6.08481616e-02 -3.49176155e-08
 -8.12982395e-02  7.06454143e-02  3.06042228e-02  1.42139504e-02
  7.15884566e-02 -2.70488355e-02  5.54065928e-02  1.05682492e-01
  3.18308659e-02 -4.22507636e-02 -4.50694747e-02 -2.25884449e-02
 -3.14717218e-02 -6.42742887e-02  6.63790256e-02  8.84077400e-02
 -5.93327023e-02 -4.02772427e-02  4.03330177e-02 -7.10645244e-02
  4.75172922e-02  1.11370003e-02  1.35796249e-03  5.35228364e-02
 -6.76109642e-02 -3.44231017e-02 -5.25796693e-03  2.18230765e-02
  2.78326701e-02 -1.42476205e-02 -3.64967547e-02 -3.05707958e-02
  2.95049790e-02 -9.20029357e-02  1.06102556e-01  7.73541629e-02
 -3.60352919e-02  8.12283531e-03  1.13054812e-02  5.13685942e-02
 -6.75182715e-02 -2.12527555e-03 -1.24226836e-03 -1.04378341e-02
  5.73555231e-02 -6.52918518e-02 -8.15214962e-02 -5.85745424e-02
 -2.32604295e-02 -1.44198006e-02  2.84195244e-02  2.60562599e-02
  5.11705084e-03  5.22850342e-02 -5.00225555e-03  4.35340330e-02
 -4.97523881e-02 -6.74206764e-02  3.18096764e-02  5.85487671e-02
  7.83407986e-02  6.34490550e-02  3.03738452e-02 -4.36520353e-02]",2,2
pytorch-nlp,2,pytorch nlp or torchnlp for short is a library of basic utility for pytorch natural language processing nlp torchnlp extends pytorch to provide you with basic text data processing function logo by chloe yeo corporate sponsorship by wellsaid labsmake sure you have python and pytorch you can then install pytorch nlp using pip or to install the latest code via the complete documentation for pytorch nlp is available via our readthedocs website within an nlp data pipeline you ll want to implement these basic step load the imdb dataset for example load a custom dataset for example don t worry we ll handle caching for you tokenize and encode your text a a tensor for example a whitespaceencoder break text into term whenever it encounter a whitespace character with your loaded and encoded data in hand you ll want to batch your dataset pytorch nlp build on top of pytorch s existing torch utils data sampler torch stack and default collate to support sequential input of varying length with your batch in hand you can use pytorch to develop and train your model using gradient descent pytorch nlp ha a couple more nlp focused utility package to support you now you ve setup your pipeline you may want to ensure that some function run deterministically wrap any code that s random with fork rng and you ll be good to go like so this will always print now that you ve computed your vocabulary you may want to make use of pre trained word vector like so for example from the neural network package apply the state of the art lockeddropout compute common nlp metric such a the bleu score maybe looking at longer example may help you at example need more help we are happy to answer your question via gitter chatwe ve released pytorch nlp because we found a lack of basic toolkits for nlp in pytorch we hope that other organization can benefit from the project we are thankful for any contribution from the community read our contributing guide to learn about our development process how to propose bugfixes and improvement and how to build and test your change to pytorch nlp torchtext and pytorch nlp differ in the architecture and feature set otherwise they are similar torchtext and pytorch nlp provide pre trained word vector datasets iterators and text encoders pytorch nlp also provides neural network module and metric from an architecture standpoint torchtext is object orientated with external coupling while pytorch nlp is object orientated with low coupling allennlp is designed to be a platform for research pytorch nlp is designed to be a lightweight toolkit if you find pytorch nlp useful for an academic publication then please use the following bibtex to cite it,"[('research pytorch nlp', 0.6734), ('pytorch nlp torchtext', 0.6697), ('dataset pytorch nlp', 0.6635), ('text encoders pytorch nlp', 0.6602), ('natural language processing nlp torchnlp', 0.6428), ('pytorch nlp', 0.6188), ('nlp data pipeline', 0.5734), ('gradient descent pytorch', 0.4991), ('architecture standpoint torchtext', 0.4729), ('similar torchtext', 0.4643)]","[-7.32880309e-02 -7.08204508e-02  1.09090032e-02 -1.57362018e-02
  3.80146839e-02  5.22838347e-03 -2.49429960e-02  4.59281988e-02
 -1.67784654e-02 -5.42630553e-02 -2.79564895e-02  2.90292706e-02
 -1.11095227e-01  6.60948604e-02  6.34362474e-02  8.32644030e-02
 -5.33497054e-03  1.22520134e-01 -7.22563788e-02 -1.01601243e-01
  6.41415492e-02  4.94616777e-02  3.40853329e-03  3.16747390e-02
  2.69176513e-02  7.64814839e-02 -9.64983273e-03 -2.11565513e-02
  8.77070948e-02  6.31958023e-02  1.03315730e-02 -4.27250825e-02
 -4.21367064e-02  9.17036384e-02 -4.72611301e-02  3.29365805e-02
  6.82340120e-04  8.97563994e-03 -7.21453968e-03 -6.33775815e-03
 -2.20339149e-02  2.11407270e-04 -4.20798436e-02  6.19664323e-04
  7.02730864e-02 -2.15694830e-02  1.35053070e-02 -3.73597741e-02
 -7.62244314e-02 -2.87742876e-02 -3.39418091e-02 -3.87229919e-02
 -1.98501069e-02  9.95150059e-02 -1.67513397e-02  4.19821963e-02
  5.08752652e-02 -2.16356218e-02  3.66887525e-02 -8.51444006e-02
 -5.49121061e-03 -4.87873890e-02  3.19465734e-02 -4.54034703e-03
 -7.11275116e-02  6.19599745e-02  2.58799978e-02  2.11622026e-02
  8.99517909e-02 -9.54161584e-02 -3.46083306e-02 -6.21273415e-04
 -1.61602069e-02  1.05326362e-01 -2.33692434e-02  1.04325721e-02
  1.00600101e-01 -1.12696802e-02  3.70498523e-02 -1.48118734e-01
  4.76309098e-02  3.88873480e-02  6.98052496e-02  7.77082518e-02
  5.06263264e-02 -4.97417189e-02 -1.93733983e-02  7.70207047e-02
 -1.83410812e-02  5.66188432e-03 -4.09829021e-02 -1.04900725e-01
  2.16862820e-02  1.22786760e-02 -3.11355311e-02 -1.08541232e-02
 -2.83618774e-02 -5.90355806e-02 -5.02615646e-02 -6.71382016e-03
 -8.98043346e-03  1.53597267e-02 -1.94871332e-02 -2.32075658e-02
 -5.91076538e-02  2.12017484e-02  6.22756779e-04  1.52599076e-02
  1.36796460e-02 -2.89467145e-02  2.55469084e-02  2.61516459e-02
 -3.95068452e-02 -5.75165302e-02  1.04469419e-01 -9.31960568e-02
 -5.57762980e-02 -2.13306979e-03  2.73892991e-02  1.35278091e-01
 -1.33721277e-01  4.24315855e-02 -5.35034202e-02  5.86783960e-02
 -5.40628657e-02 -4.34465446e-02  6.02036342e-03  8.82348908e-33
  1.08305616e-02  3.71530466e-02 -4.04400863e-02 -6.27854913e-02
  4.56706956e-02 -4.29440178e-02  6.98376000e-02 -1.96386012e-03
 -5.54582588e-02 -3.41087319e-02  8.02825019e-03 -5.02456836e-02
 -4.67515700e-02  5.16391061e-02 -4.17623408e-02 -3.91302519e-02
 -4.06013094e-02  3.13833505e-02  4.85995524e-02  1.07256509e-02
 -6.30867667e-03  5.93294762e-02  1.61904898e-02  6.08263500e-02
 -2.17911825e-02  2.12836266e-02 -3.63189913e-02 -5.05542867e-02
 -3.98291796e-02 -3.11083300e-03 -4.98415083e-02  3.93686444e-02
  5.90907447e-02  1.18519831e-02 -1.73337711e-03 -2.93602832e-02
  7.56802317e-03 -4.11429107e-02  1.34911556e-02 -5.90712726e-02
 -7.19598308e-02  8.53502229e-02 -6.94562728e-03 -4.18528207e-02
 -8.37947149e-03  1.70318104e-04 -8.26970786e-02  4.47339006e-02
  2.07479782e-02 -9.17924717e-02 -1.54891415e-02  6.32868931e-02
 -1.76959187e-02  5.53096868e-02  9.12782922e-02 -2.72511318e-02
 -3.19702295e-03  2.04071030e-02  1.10141061e-01  5.42102903e-02
  3.79429013e-02  7.24057555e-02  1.18058957e-02 -6.06220169e-03
  1.12604396e-02 -6.88201468e-03 -6.34744763e-03 -3.73599045e-02
 -5.11822365e-02 -2.45820154e-02 -3.90623249e-02  4.75511327e-02
 -1.72074214e-02  4.97616641e-03  2.31671836e-02  8.00721534e-03
  1.97692979e-02 -8.51198062e-02 -6.14690743e-02  7.90428370e-02
 -1.03493184e-01 -6.88685998e-02  5.17875962e-02 -3.66093451e-03
 -5.89699969e-02 -2.09913626e-02 -2.67027896e-02 -6.09358177e-02
  5.91406934e-02  1.92100331e-02 -8.80218521e-02 -1.11812288e-02
  1.84068587e-02  3.03597450e-02  4.34630141e-02 -6.30027797e-33
 -3.52759883e-02  6.81859255e-02 -1.02805883e-01  8.73864964e-02
  5.39445505e-03  7.90115166e-03 -2.07186509e-02 -1.07685879e-01
  1.55549636e-02  2.32290626e-02  4.10662079e-03 -1.02664642e-02
  4.47233021e-03 -5.14296629e-02  4.23380435e-02 -4.13391273e-03
  1.58796366e-02  5.24136703e-03  5.91587499e-02  5.97558580e-02
  8.44462123e-03  6.15673587e-02 -1.67634219e-01 -8.43388401e-03
 -1.27524678e-02  1.79957524e-02 -4.99430895e-02  3.89197655e-03
 -1.63226575e-02 -2.37561837e-02  5.71894981e-02  4.24268544e-02
  2.21965332e-02 -5.75766452e-02 -8.12158659e-02  1.48244938e-02
  4.14597951e-02 -6.69622868e-02  3.80084291e-03  1.00393612e-02
  1.38275906e-01  5.90547398e-02 -2.41878931e-03 -3.21428217e-02
 -7.91979507e-02  1.39444731e-02 -1.25956178e-01  7.54630798e-03
  1.56665556e-02  2.52661593e-02 -1.39321126e-02 -3.05134878e-02
 -8.45488980e-02  1.27984369e-02 -1.46285919e-02 -6.48854151e-02
  6.30778968e-02 -9.36474837e-03 -3.21450122e-02  8.62846337e-03
 -8.78886953e-02 -1.81071181e-02  5.47376238e-02  3.64723913e-02
  8.13038182e-03 -4.16563600e-02 -2.24381667e-02  1.88493095e-02
 -9.92216635e-03 -3.51707898e-02  6.13712110e-02  5.81789985e-02
  7.58570731e-02  1.49811298e-01  1.24050463e-02  7.31050875e-03
  2.31728740e-02 -2.07364541e-02 -3.87214571e-02 -5.09108379e-02
  1.21552795e-01  1.09464945e-02 -1.05745662e-02  7.34539330e-02
  9.70137939e-02  1.13553695e-01 -1.83095876e-02  2.11317930e-02
  9.60575882e-03 -3.20239924e-02 -5.27015189e-03 -1.16287870e-02
 -2.05107569e-03  1.77747067e-02  2.18066983e-02 -3.13438022e-08
 -6.53632358e-02 -1.63864736e-02 -2.17691697e-02  2.45574191e-02
 -3.23216878e-02 -3.73615734e-02  3.99009064e-02  1.23956516e-01
 -1.50042539e-02 -5.12025617e-02  3.36708464e-02 -8.75860639e-03
  8.56797583e-03 -3.27846929e-02  2.55617090e-02  7.48687088e-02
  7.41661564e-02 -7.86438659e-02  7.37019405e-02 -5.63893840e-02
  4.19652425e-02  4.44785431e-02 -3.57975513e-02  2.03401595e-02
  1.14789885e-02 -2.03890316e-02  1.56423338e-02  3.39546613e-02
  4.52485261e-03 -3.18059884e-02  9.14692041e-03 -1.57122046e-03
  6.92608505e-02 -4.63942662e-02  1.47134900e-01  9.15706903e-02
  2.37329565e-02 -5.94683141e-02 -7.65031204e-02  2.08560447e-03
  2.57300176e-02  3.55209261e-02 -4.40162197e-02 -4.33930978e-02
  6.77754879e-02  1.99460220e-02 -2.70122904e-02 -8.17987770e-02
 -2.92217974e-02 -3.11292876e-02 -1.32297426e-02  2.01021675e-02
 -8.83114897e-03  4.73187678e-02  6.29730076e-02  3.42199765e-02
 -3.95820215e-02 -3.06738596e-02 -1.76213514e-02 -6.79378305e-03
 -5.33987954e-03  5.61498217e-02 -6.69177761e-03 -5.45005128e-02]",2,2
sagemaker-inference,2,serve machine learning model within a docker container using amazon sagemaker amazon sagemaker is a fully managed service for data science and machine learning ml workflow you can use amazon sagemaker to simplify the process of building training and deploying ml model once you have a trained model you can include it in a docker container that run your inference code a container provides an effectively isolated environment ensuring a consistent runtime regardless of where the container is deployed containerizing your model and code enables fast and reliable deployment of your model the sagemaker inference toolkit implement a model serving stack and can be easily added to any docker container making it deployable to sagemaker this library s serving stack is built on multi model server and it can serve your own model or those you trained on sagemaker using machine learning framework with native sagemaker support if you use a prebuilt sagemaker docker image for inference this library may already be included for more information see the amazon sagemaker developer guide section on building your own container with multi model server and using your own model to install this library in your docker image add the following line to your dockerfile here is an example of a dockerfile that installs sagemaker inference toolkit to use the sagemaker inference toolkit you need to do the following implement an inference handler which is responsible for loading the model and providing input predict and output function here is an example of an inference handler note passing context a an argument to the handler function is optional customer can choose to omit context from the function declaration if it s not needed in the runtime for example the following handler function declaration will also work implement a handler service that is executed by the model server here is an example of a handler service for more information on how to define your handler service file see the mm custom service documentation implement a serving entrypoint which start the model server here is an example of a serving entrypoint define the location of the entrypoint in your dockerfile here is a complete example demonstrating usage of the sagemaker inference toolkit in your own container for deployment to a multi model endpoint this library is licensed under the apache license for more detail please take a look at the license file contribution are welcome please read our contributing guideline if you d like to open an issue or submit a pull request,"[('prebuilt sagemaker docker image', 0.6142), ('sagemaker inference toolkit', 0.5852), ('native sagemaker support', 0.5437), ('amazon sagemaker amazon sagemaker', 0.539), ('amazon sagemaker', 0.53), ('amazon sagemaker developer guide section', 0.5238), ('sagemaker', 0.5178), ('model server', 0.4674), ('multi model server', 0.4553), ('docker container', 0.4425)]","[-4.07911427e-02 -9.00499895e-02  3.58176641e-02 -1.05187921e-02
  1.13235474e-01 -1.97608285e-02 -4.59178388e-02  2.43483577e-02
 -8.79866928e-02  5.64547852e-02  6.69943681e-03 -2.77319946e-03
 -1.16455611e-02  1.04299281e-02  8.34558681e-02  2.58100703e-02
 -9.66112781e-03 -6.17443584e-02  1.50932716e-02 -7.46551901e-03
 -1.12223692e-01  2.41002459e-02 -3.97064164e-02  4.29839939e-02
  2.85292082e-02 -2.13309079e-02 -1.06613031e-02  1.31284026e-02
  3.52757261e-03 -6.49953783e-02  1.31217185e-02  3.05323256e-03
  9.86317843e-02 -4.59224312e-03 -2.26294678e-02  4.62459736e-02
  1.63660813e-02 -3.81817110e-02  3.05578504e-02 -1.17087001e-02
 -6.06880640e-04 -6.14325814e-02 -6.49309084e-02 -1.05085382e-02
  7.24103600e-02 -3.59814381e-03  2.95069143e-02 -6.60127997e-02
  3.22670750e-02  3.67376232e-03 -7.90904462e-02 -1.27622560e-01
 -2.98748612e-02  5.70019260e-02 -1.66266412e-02  8.68019741e-03
  7.98802450e-03 -2.63179075e-02  3.40008996e-02  2.58476008e-02
  2.02151365e-03 -2.47604139e-02 -4.19135801e-02  2.61536557e-02
  1.84256937e-02  7.47581720e-02 -3.42674889e-02  4.31368034e-03
  9.57732871e-02 -7.59973824e-02 -5.39803691e-02 -6.72702342e-02
 -5.68811893e-02  3.60876322e-02 -9.53457057e-02  3.10923392e-03
  3.03309783e-02 -4.15140688e-02  2.04997696e-02 -6.11187816e-02
 -4.50054221e-02  4.13689613e-02 -6.48040250e-02 -6.12369878e-03
 -6.62479624e-02  6.31626099e-02  1.73396152e-03  3.52058448e-02
  5.97915687e-02 -1.01966873e-01  5.29100597e-02 -5.65200336e-02
 -3.48804668e-02 -5.32640005e-03 -5.85908368e-02  2.95650447e-03
  1.95938274e-02 -4.67818789e-02  1.81288663e-02  6.19954392e-02
  2.21051406e-02 -2.76244879e-02  7.28001893e-02 -4.84619327e-02
 -1.57265663e-02  3.06527950e-02  9.42509715e-03  2.37555951e-02
  2.89929379e-02 -1.21907080e-02  4.93702590e-02 -3.12156491e-02
 -9.74284634e-02 -2.27060057e-02  1.78032424e-02 -2.24183127e-02
  3.78025789e-03 -1.55700240e-02  4.97189909e-03 -2.13296246e-03
  1.44854300e-02  1.62123796e-02  1.42741546e-01 -8.37250333e-03
 -4.01737727e-02 -2.24158261e-02 -1.11853227e-01  5.73139087e-33
  2.77463882e-03 -9.09021199e-02  3.01295635e-03  1.08869758e-03
  1.23848706e-01 -1.04312822e-02  1.69896018e-02  2.32653953e-02
 -2.20321510e-02 -4.67090011e-02  6.89771250e-02 -4.04217839e-03
 -2.84501407e-02  8.12827572e-02 -5.93075790e-02 -2.68743243e-02
  2.50646379e-03  4.33271416e-02  3.74370860e-03 -1.35581885e-02
 -1.54657597e-02 -2.72794757e-02 -5.58271147e-02  7.62205869e-02
  2.26835702e-02  1.79396737e-02  7.21696839e-02 -1.96429621e-02
  2.98985839e-02  1.20449578e-02 -3.27378348e-03  2.05668714e-02
 -6.34655496e-03 -4.92629409e-03 -2.16307528e-02 -2.12849230e-02
 -4.42303009e-02 -6.73952177e-02 -1.29623758e-03 -4.45257276e-02
 -7.62457624e-02  4.44058664e-02  8.93369131e-03  2.92998534e-02
  2.02242229e-02 -4.35072519e-02 -5.39114736e-02  3.71935125e-03
  8.49866644e-02  3.13165896e-02 -9.11357626e-02 -2.02699546e-02
  4.51927520e-02  3.97045761e-02 -2.38972083e-02 -6.69654533e-02
  1.24354530e-02 -1.00732679e-02  5.62982373e-02  3.54887135e-02
 -9.84392688e-02 -1.28209908e-02 -7.58471787e-02 -4.89965193e-02
 -4.61517647e-02  1.33004757e-02  1.59414038e-02  6.06344715e-02
  3.29040512e-02  1.20762832e-01 -9.61290598e-02  7.34716654e-02
  8.60470906e-03  3.25533114e-02  3.05668102e-03 -1.20890941e-02
 -6.49441779e-02 -1.27883004e-02 -7.87381362e-03 -1.82796177e-02
 -1.53401405e-01  7.80755281e-02 -2.19845288e-02  9.77135822e-02
 -2.77670026e-02 -5.67124560e-02  5.41375428e-02  4.78235334e-02
 -4.24423181e-02 -7.24835843e-02 -7.13393325e-03  1.28933564e-01
 -4.24717832e-03 -6.36391435e-03 -6.60022050e-02 -5.79659701e-33
  2.94540897e-02  2.17231214e-02 -5.91603816e-02  8.67431238e-02
  4.14687432e-02  2.29248684e-02  6.26270771e-02 -1.38497883e-02
 -2.32946258e-02 -8.63199532e-02  4.81508598e-02  3.25604938e-02
 -2.99501065e-02  1.17592122e-02  8.23091343e-03  5.89515315e-03
 -9.77032706e-02 -8.82676020e-02  3.29778120e-02  1.35751944e-02
  1.05361296e-02 -1.55944033e-02 -1.70118585e-02 -9.66372900e-03
 -1.06479358e-02 -9.09385737e-03  3.65433991e-02 -4.71961610e-02
  6.91573396e-02  3.03494800e-02  4.16770354e-02  3.35930400e-02
 -2.38097794e-02  3.17511186e-02 -2.81770667e-03  1.10921049e-02
  4.21880968e-02  4.79666926e-02 -7.45733753e-02 -4.10008989e-02
  1.74003556e-01 -1.76731441e-02  3.72831686e-03 -2.79142056e-03
 -1.29641652e-01  7.95240141e-03 -4.50776592e-02  1.32731553e-02
 -3.69615690e-03 -4.69229110e-02 -4.19376157e-02 -1.02286831e-01
 -4.41839366e-04 -2.66870931e-02 -3.90749685e-02 -3.30063403e-02
  6.64796382e-02  5.22139966e-02  5.05182706e-02 -8.50886479e-03
  2.67654337e-04 -1.18538039e-02  3.77251729e-02  6.83583459e-03
 -7.03382269e-02 -3.93206514e-02 -6.38572453e-03  3.74679901e-02
 -8.36225599e-02 -6.74282620e-03 -9.42507945e-03  4.18882351e-03
  2.77656242e-02  2.64208317e-02 -2.89693871e-03  3.67434360e-02
 -1.37259522e-02 -2.58384924e-02  5.87946251e-02 -4.95230108e-02
  9.64057595e-02 -1.33303297e-03  1.29285036e-02  3.47711742e-02
  1.17696133e-02  3.59555632e-02 -1.02133967e-03  2.19718087e-02
  2.14868430e-02 -7.95301236e-03 -5.18783741e-02  4.30672355e-02
 -3.93138221e-03  1.03296317e-01  4.90711890e-02 -3.24434701e-08
 -6.28159055e-03 -2.87881563e-03  2.90801637e-02  4.33452316e-02
  4.24427576e-02  3.49077396e-02  5.46298735e-02  1.02044724e-01
 -2.01732311e-02  8.49724934e-02  1.81443561e-02 -8.26400220e-02
 -7.91052356e-02  1.60040483e-02  2.94798128e-02  7.05482811e-02
  1.58597548e-02  1.08290426e-01 -8.40742607e-03 -7.70008191e-02
 -7.01294988e-02  2.05926243e-02  6.27158722e-03 -7.68964514e-02
  8.58738902e-04 -1.66573543e-02 -2.42449082e-02  1.09978013e-01
 -1.93515513e-02 -3.40220034e-02  5.86648919e-02  5.55020897e-03
  2.82665081e-02 -2.74417847e-02  1.42166227e-01  5.01842052e-02
 -1.46775022e-01 -2.16754116e-02  2.24502925e-02  2.74120849e-02
 -1.10717207e-01 -1.70345679e-02 -3.85035910e-02 -7.70444199e-02
  4.12349030e-02 -4.41488326e-02  5.71616320e-03 -1.74977388e-02
 -9.80005860e-02  5.79665825e-02  1.66873429e-02 -5.16471937e-02
  7.57404342e-02  3.09591498e-02 -3.84162017e-03  1.00219622e-01
  7.38629187e-03 -8.59615281e-02  1.14272429e-04 -8.52735713e-03
  1.19978920e-01  7.39897937e-02  7.29717538e-02  5.93978539e-02]",2,1
tensorflow_datasets,2,tensorflow datasets is a library of public datasets ready to use with tensorflow each dataset definition contains the logic necessary to download and prepare the dataset a well a to read it into a model using the tf data dataset api usage outside of tensorflow is also supported see the readme on github for further documentation,"[('tensorflow datasets', 0.8029), ('tensorflow', 0.5789), ('dataset api usage', 0.5634), ('dataset definition', 0.4996), ('tf data', 0.4907), ('dataset', 0.4851), ('public datasets', 0.4248), ('library', 0.2544), ('github', 0.1881), ('further documentation', 0.179)]","[-2.57585291e-02 -1.09775059e-01 -1.38138421e-02 -2.63253711e-02
  6.69185817e-02 -7.23646767e-03 -9.36925411e-03  4.01127376e-02
 -3.23693939e-02 -5.40466141e-03 -3.29588167e-02 -2.04834179e-03
 -4.16953154e-02 -7.45069236e-02  1.27088521e-02 -1.01054143e-02
 -1.95648223e-02  1.53766219e-02 -2.41777897e-02 -9.38833356e-02
 -4.01839912e-02 -3.32710310e-03  1.40437076e-03  3.74991149e-02
 -2.36210413e-02  3.86409322e-03  1.11730425e-02 -7.99769834e-02
  9.24680009e-03  3.73583063e-02 -5.25508523e-02  7.38972202e-02
  4.46711592e-02  3.58044319e-02 -7.18896985e-02  2.49947160e-02
 -1.12932481e-04 -4.25403938e-02 -5.07230349e-02  8.04855488e-03
 -3.36065888e-02 -3.09227314e-02 -5.42359948e-02 -1.16353687e-02
  6.15623258e-02  2.21296046e-02  3.93649153e-02 -1.05506577e-01
 -2.26895157e-02  3.09444405e-02 -6.20048232e-02 -4.89187613e-02
 -2.63488200e-02  4.14119288e-02  4.49887430e-03 -8.11932236e-02
  4.08330187e-02  2.89959069e-02 -3.88741791e-02 -3.35767753e-02
  2.43700221e-02 -7.02209175e-02 -6.08573966e-02  6.07855394e-02
  2.09381934e-02  4.09810841e-02 -2.98584010e-02  2.76344735e-02
  1.32937446e-01 -1.30788505e-01 -8.75394195e-02  3.80604006e-02
 -8.97996500e-03  1.10369995e-02 -2.13845391e-02 -2.82814074e-02
  5.89917675e-02  5.72999520e-03  7.45761022e-02 -9.56717357e-02
  2.38200347e-03  1.93470065e-02  5.91910072e-02  4.87440899e-02
 -2.65139900e-02 -1.45469559e-03  1.57222543e-02  7.62333870e-02
  5.18611027e-03 -1.08592473e-02  4.81493399e-02 -1.46675827e-02
  4.01900969e-02  1.22404592e-02  3.90966684e-02  6.25906810e-02
 -6.40380010e-02 -2.22077742e-02  6.83298893e-03 -4.19113338e-02
 -1.31579950e-01  1.15656084e-03  6.43643215e-02  8.41379091e-02
 -4.98124883e-02  6.74363822e-02 -1.39306476e-02 -2.53305864e-02
  6.90235496e-02  4.10033874e-02  4.81905136e-03  5.19081801e-02
 -1.30069377e-02 -9.60276052e-02  8.09922814e-02  1.61018297e-02
 -1.03389442e-01  3.67930345e-02  1.14395507e-02  7.16027394e-02
 -9.84940752e-02  4.12285179e-02  4.25196290e-02 -4.62839492e-02
 -3.82740051e-02 -2.06143372e-02 -7.76117072e-02  1.44153159e-33
  8.41649696e-02  3.32635315e-03 -3.91098950e-03 -1.21974675e-02
  3.24090831e-02 -8.80408436e-02  5.23514152e-02 -2.05891915e-02
 -1.70092806e-02 -5.46746841e-03  5.56468405e-03  1.32942051e-01
 -5.75297885e-03  8.51110369e-02 -4.27161604e-02 -5.04501052e-02
  2.15055607e-02  3.73669975e-02  2.37095784e-02  5.81711642e-02
  7.29542896e-02  3.11544407e-02  4.46199253e-02  9.43663046e-02
  8.09911117e-02 -2.61659175e-03 -9.92360711e-03  8.80107749e-03
 -1.25214383e-02  2.46802345e-02 -5.46832457e-02 -2.11496521e-02
 -4.53833630e-03 -4.75836499e-03  3.98620032e-02 -2.99611837e-02
 -5.01463711e-02  2.00392362e-02  1.24548925e-02 -2.46795602e-02
  5.04111685e-02  3.08170319e-02 -3.09303477e-02 -1.64380409e-02
 -3.96880172e-02 -4.86893617e-02  3.36587504e-02  2.83532273e-02
  1.19358972e-01 -1.14113558e-02 -2.11365381e-03 -2.53046956e-02
 -1.01454342e-02 -9.06174164e-03  1.75102837e-02 -1.76323354e-02
 -4.19039652e-03 -1.89769659e-02  5.07073812e-02  5.26852943e-02
 -8.47446844e-02  2.36632898e-02  8.90363380e-02  3.08782905e-02
 -3.61458920e-02  4.80401190e-03 -1.08340405e-01  7.93375820e-03
  3.27283926e-02 -2.29691435e-02 -6.21913299e-02  1.00100346e-01
 -5.17846085e-02  2.57732272e-02  9.16519726e-04 -4.72028777e-02
  2.47026905e-02 -4.30829078e-02 -2.87929699e-02  3.56023200e-02
 -8.11708346e-02 -1.30728967e-02  2.19167825e-02  2.78442372e-02
 -4.01584357e-02  2.84125917e-02 -2.54850630e-02  4.58784960e-02
 -6.25022035e-03 -5.68408892e-02 -7.75684938e-02  3.61064859e-02
 -2.41690371e-02 -2.89063039e-03 -5.65228574e-02 -2.90888236e-33
 -6.46551652e-03  3.28824259e-02 -7.05239847e-02  6.59584627e-02
  1.22912582e-02  8.29985365e-02  5.56484237e-03  2.75231861e-02
  1.21149465e-01  5.31138554e-02  2.39296164e-02 -4.75823600e-03
  5.23699727e-03 -1.06966220e-01 -2.56294273e-02 -9.59853753e-02
 -4.09974493e-02 -4.99173254e-02 -4.74503487e-02 -2.84329783e-02
 -1.01559043e-01 -1.60753559e-02 -5.79098165e-02 -3.35319191e-02
  2.27584485e-02 -1.61916427e-02 -9.43669602e-02 -4.00184914e-02
  4.51464728e-02 -1.24496929e-02  9.53065790e-03 -6.29310235e-02
 -2.42387112e-02  1.52840922e-02 -3.35439555e-02 -9.66641819e-04
  8.46013874e-02  8.30260199e-03  2.19872296e-02  1.09172624e-03
  8.37815255e-02  7.79498816e-02  3.54981124e-02 -8.77230056e-03
 -8.05699080e-02 -1.68909028e-03 -1.08098842e-01  5.51756620e-02
 -5.78581803e-02 -8.80557075e-02  3.08640692e-02 -3.35670896e-02
  6.05848571e-03 -6.71360046e-02  7.44219357e-03 -6.58519391e-04
  1.26943260e-01  2.27344669e-02  3.39616351e-02  1.73895694e-02
  8.08952004e-03 -7.51579646e-03  2.95769866e-03  6.47498071e-02
 -6.94536269e-02 -1.66772548e-02 -1.19645774e-01 -5.87173691e-03
 -3.72312963e-02 -1.54166156e-02 -2.97450945e-02 -6.61823992e-03
  3.92486516e-04  3.79727595e-02 -4.34480980e-02  2.66617909e-02
 -8.31057318e-03  3.17913597e-03  9.16014463e-02  2.86880806e-02
  8.25742111e-02  2.48213112e-02  3.87278348e-02  5.19313291e-02
  1.04339793e-01  7.84464106e-02  4.42325994e-02 -7.28602409e-02
  2.51105875e-02 -4.37113736e-03 -6.92995787e-02 -6.06427640e-02
 -2.20650174e-02  1.46592915e-01  4.23372351e-02 -2.51372381e-08
 -6.37819469e-02  5.75681999e-02  7.77056292e-02  2.57495828e-02
 -1.46205397e-02  7.46237338e-02  5.23218624e-02  1.46060616e-01
  6.76792162e-03 -5.05213486e-03  6.56017102e-03  3.21969949e-02
 -8.30956921e-02 -8.47284030e-03  3.12416125e-02  2.62236893e-02
 -7.38104852e-03 -3.70380953e-02  1.54636437e-02 -1.82052478e-02
  4.32910360e-02 -1.25492951e-02 -1.06462715e-02 -5.63368984e-02
  6.60836920e-02 -7.89670460e-03  4.06374075e-02  5.56243360e-02
 -8.12641010e-02 -6.65308386e-02 -7.89148286e-02 -5.27188070e-02
  1.84181500e-02 -9.35884044e-02  7.06409737e-02  3.07741277e-02
 -3.00566573e-02 -7.62540847e-02 -1.48031153e-02  6.65944964e-02
 -5.17777167e-02  5.03445119e-02  3.07017379e-02 -2.45938394e-02
  1.55330440e-02  8.31775088e-03  2.83760251e-03  1.64345969e-02
 -4.75898162e-02 -6.75299764e-03 -3.78385894e-02  5.92520572e-02
 -5.63943461e-02  1.21640965e-01  1.95971895e-02  7.47335628e-02
 -4.27125357e-02 -6.41786829e-02  7.54637178e-03 -7.14783520e-02
 -5.90555593e-02  9.84166116e-02 -1.00760837e-03 -1.80026144e-02]",2,2
adabound,2,an optimizer that train a fast a adam and a good a sgd for developing state of the art deep learning model on a wide variety of pupolar task in the field of cv nlp and etc based on luo et al adaptive gradient method with dynamic bound of learning rate in proc of iclr adabound requires python or later we currently provide pytorch version and adabound for tensorflow is coming soon the preferred way to install adabound is via pip with a virtual environment just runin your python environment and you are ready to go a adabound is a python class with only line an alternative way is directly downloading adabound py and copying it to your project you can use adabound just like any other pytorch optimizers a described in the paper adabound is an optimizer that behaves like adam at the beginning of training and gradually transforms to sgd at the end the final lr parameter indicates adabound would transforms to an sgd with this learning rate in common case a default final learning rate of can achieve relatively good and stable result on unseen data it is not very sensitive to it hyperparameters see appendix g of the paper for more detail despite of it robust performance we still have to state that there is no silver bullet it doe not mean that you will be free from tuning hyperparameters once using adabound the performance of a model depends on so many thing including the task the model structure the distribution of data and etc you still need to decide what hyperparameters to use based on your specific situation but you may probably use much le time than before thanks to the awesome work by the github team and the jupyter team the jupyter notebook ipynb file can render directly on github we provide several notebook like this one for better visualization we hope to illustrate the robust performance of adabound through these example for the full list of demo please refer to this page if you use adabound in your research please cite adaptive gradient method with dynamic bound of learning rate apache,"[('other pytorch optimizers', 0.5956), ('tensorflow', 0.4967), ('adabound', 0.4951), ('iclr adabound', 0.4625), ('pytorch version', 0.4524), ('optimizer', 0.429), ('learning rate', 0.4037), ('deep learning model', 0.3981), ('paper adabound', 0.3656), ('sgd', 0.3626)]","[-1.17713012e-01 -1.04341291e-01 -1.55425929e-02 -2.30200905e-02
  5.31065911e-02 -2.51324587e-02 -1.24437464e-02  2.04956215e-02
 -4.81544398e-02 -3.30605581e-02 -5.82408048e-02  5.84571995e-02
 -9.70173255e-02  5.40097654e-02 -1.07716564e-02  1.27173942e-02
  1.85996871e-02  1.30380452e-01 -9.28133726e-02 -8.66563842e-02
 -9.59839392e-03 -1.98991150e-02  6.33207336e-03  8.97368137e-03
  4.38330026e-05 -1.32670263e-02  7.44556356e-03 -8.39397237e-02
  4.15156819e-02 -8.40657670e-03 -2.41964287e-03 -3.38032544e-02
  5.50755039e-02  1.50604658e-02 -2.53464542e-02  1.27607882e-02
 -3.66772860e-02  7.76003907e-03  3.98086533e-02 -3.52704637e-02
 -1.60989165e-02 -1.01779224e-02 -7.46872555e-03  5.76817757e-03
  1.15558699e-01 -1.23150572e-02  1.92829706e-02 -7.62694329e-02
 -2.72038244e-02 -1.37461592e-02 -4.51826230e-02 -2.48008203e-02
 -7.09411502e-02  3.60647403e-02  1.71613065e-04 -4.31564674e-02
  3.42795737e-02 -5.70745803e-02  2.26858389e-02 -5.13584726e-02
 -3.49998707e-03 -7.11256564e-02 -4.84663881e-02  1.16438195e-02
 -7.14738667e-02  3.27877291e-02  2.44570058e-02  9.56872106e-03
  9.63906422e-02 -5.63769713e-02 -3.37352194e-02  5.81909623e-03
 -4.73939106e-02  3.71651910e-02  4.85972166e-02 -2.97285374e-02
  1.37511492e-01 -2.25505922e-02  6.32716641e-02 -1.25107899e-01
  3.28917056e-02 -1.78003777e-02  1.30019234e-02  4.66609672e-02
  7.16311857e-02 -2.30565425e-02 -1.46452310e-02  4.54933718e-02
  1.96621045e-02 -3.49868387e-02  6.25113621e-02 -3.86609398e-02
 -6.94326088e-02 -1.10676391e-02 -4.39267308e-02 -1.29702967e-02
 -3.07142492e-02 -1.08453102e-01 -6.54530376e-02  5.24270050e-02
 -2.90812496e-02 -2.64131613e-02  2.66285567e-03  1.36758685e-02
 -3.16298343e-02  1.36579694e-02  4.98094223e-02  4.42479365e-02
  1.15004033e-01  4.47977409e-02  1.49438651e-02  6.31891340e-02
  1.38329789e-02 -5.61991297e-02  9.08677354e-02 -1.16495546e-02
 -2.37545874e-02  2.93322261e-02  8.57419148e-02  1.09328032e-01
 -1.59732938e-01 -5.13580674e-03 -5.28119393e-02  3.87965254e-02
 -7.39169344e-02  5.16486447e-03 -9.35144424e-02  5.09076628e-33
 -7.34210685e-02 -1.75413527e-02  2.21544113e-02 -9.08674896e-02
  3.87206636e-02 -4.51709181e-02  4.70131598e-02  3.67000094e-03
 -7.97430333e-03 -2.74047256e-02 -1.79308970e-02 -3.69268656e-03
 -5.31483293e-02  1.06592156e-01  2.85110530e-02 -7.39077628e-02
 -7.24115297e-02  1.92835797e-02  3.80479656e-02  2.59214491e-02
  5.62133975e-02 -3.88554600e-03 -1.53873721e-03  8.89333114e-02
  4.07417864e-02  2.21903827e-02  7.06461724e-03  2.24235989e-02
  3.71790379e-02  2.47689541e-02 -5.90542704e-02  1.84434652e-02
 -4.14179005e-02  1.59905329e-02 -1.51022691e-02  2.17133705e-02
 -5.97937331e-02 -4.54535075e-02 -9.15807486e-03 -6.31783381e-02
 -6.38983771e-02  3.06819472e-02 -2.36829109e-02 -4.06755805e-02
 -2.32453831e-02 -4.07997556e-02 -2.47713272e-02  1.02784820e-01
 -1.17376214e-02 -7.21190497e-02 -8.04982856e-02  4.40513864e-02
 -5.09256236e-02  8.66289996e-03  2.79824957e-02 -4.44388278e-02
  1.66161507e-02  5.43856435e-02  8.44775289e-02  8.25294927e-02
  1.30539425e-02  1.56419333e-02  1.99859273e-02 -3.06383576e-02
  9.81018785e-03  3.69638801e-02 -8.16299859e-03  1.31620234e-02
 -1.02678023e-03  2.79149469e-02 -8.43972489e-02  3.58212665e-02
  1.50060551e-02 -2.57708710e-02  8.01644381e-03 -1.49484072e-02
  7.51859322e-02 -5.56108318e-02  2.12707948e-02  4.36050221e-02
 -1.43736854e-01  6.56167492e-02  3.01161148e-02 -1.16765890e-02
 -7.08165839e-02 -4.78383973e-02 -4.02713381e-02 -2.98172273e-02
 -3.89870107e-02 -1.81625262e-02 -9.95261446e-02 -8.50726676e-04
  7.41041964e-03  7.76336491e-02 -4.31340896e-02 -4.04296258e-33
 -1.13199987e-02  1.29598267e-02 -7.31755495e-02  8.33251774e-02
  1.13609284e-02 -1.36759672e-02 -3.12134027e-02 -5.60417399e-02
  1.95420776e-02 -1.55756436e-02  3.32204811e-02 -1.18202176e-02
  4.48421054e-02  2.21905615e-02  2.75147054e-02 -2.85568484e-03
 -3.55359167e-02 -3.68954092e-02  3.54006775e-02  3.86436582e-02
 -2.90815551e-02  1.28140986e-01 -5.96366152e-02  2.60193758e-02
 -6.31037634e-03 -1.65839512e-02 -5.08621931e-02  2.50589978e-02
 -2.83661708e-02  1.12588110e-03  2.29040328e-02  3.41107957e-02
 -8.34938698e-03  3.54462154e-02 -3.26666832e-02  1.44180087e-02
  7.79076740e-02 -2.96051074e-02 -1.88802811e-03  6.13462105e-02
  1.30092710e-01  4.73192856e-02  5.23779616e-02  3.75412479e-02
  1.19694532e-03  5.01763821e-02 -6.16806597e-02  2.44614221e-02
 -1.02938609e-02 -3.45176496e-02 -2.17842311e-03  2.70000868e-03
 -1.22007933e-02 -2.64622513e-02 -4.42425981e-02 -7.28461193e-03
  4.47553955e-02 -9.11812391e-03  8.24132636e-02  4.76067737e-02
 -6.99876547e-02 -6.57770364e-03  9.85458901e-05 -3.47077884e-02
 -5.09153344e-02  1.49795767e-02  7.35650712e-04  3.48982476e-02
 -2.25021522e-02  1.43511361e-02  1.08205816e-02  3.83734964e-02
  5.10067008e-02  1.09256543e-01 -5.96514419e-02  4.28085960e-02
  6.37760013e-02  2.05208063e-02 -2.76271421e-02 -5.25719486e-02
  5.98537549e-02 -1.30200014e-02  3.66228074e-02  9.73156244e-02
 -1.38149224e-02  9.19587910e-02  4.44902815e-02 -3.01518664e-03
  5.99188842e-02 -3.77878994e-02 -5.59576750e-02 -1.13824001e-02
  5.21831512e-02  5.86160310e-02 -2.77002007e-02 -2.76562524e-08
 -2.18608901e-02  3.38478498e-02  1.09918602e-01 -5.48705906e-02
  2.70688776e-02  2.79361606e-02  9.11095925e-03  1.29501909e-01
 -4.88024242e-02 -1.44589099e-03 -2.40019266e-03  5.51429810e-03
 -1.10863276e-01 -1.25790071e-02  6.47203773e-02  6.67346269e-02
  3.44793163e-02 -5.96968643e-03  3.49852331e-02 -9.74603817e-02
  2.51001362e-02  3.36121134e-02  1.45903947e-02 -5.21924794e-02
  5.38267894e-03 -7.66502097e-02  3.22709084e-02 -5.30418456e-02
  3.88264493e-03 -2.97202934e-02 -2.39455383e-02  3.29813696e-02
  1.24240398e-01 -8.43305439e-02  1.44863456e-01  1.31416902e-01
  2.66780034e-02 -5.39318100e-02 -3.17815579e-02  9.26122963e-02
 -2.22138185e-02  5.34513891e-02 -9.05334856e-03 -5.24867214e-02
  3.19177248e-02 -1.37167517e-03 -2.89348438e-02 -9.67945307e-02
  5.02518564e-03  3.27326506e-02  3.94774526e-02  3.15933600e-02
 -2.67454851e-02  1.85786448e-02  1.53292259e-02  7.81173632e-02
 -3.23323160e-02 -1.46507457e-01 -2.03966647e-02  4.78529148e-02
  2.37270668e-02  1.71776824e-02 -7.01760361e-03  5.26047014e-02]",2,2
torchinfo,2,formerly torch summary torchinfo provides information complementary to what is provided by print your model in pytorch similar to tensorflow s model summary api to view the visualization of the model which is helpful while debugging your network in this project we implement a similar functionality in pytorch and create a clean simple interface to use in your project this is a completely rewritten version of the original torchsummary and torchsummaryx project by sksq and nmhkahn this project address all of the issue and pull request left on the original project by introducing a completely new api support pytorch version alternatively via conda note if you are using a jupyter notebook or google colab summary model must be the returned value of the cell if it is not you should wrap the summary in a print e g print summary model see test jupyter test ipynb for example this version now support other new feature community contribution alternatively you can also pas in the input data itself and torchinfo will automatically infer the data type all issue and pull request are much appreciated if you are wondering how to build the project,"[('torch summary torchinfo', 0.5325), ('new api support pytorch version', 0.4942), ('pytorch', 0.4569), ('model summary api', 0.4479), ('google colab summary model', 0.4347), ('print e g print summary model', 0.4058), ('jupyter notebook', 0.4046), ('torchsummaryx project', 0.3685), ('test jupyter test ipynb', 0.3613), ('conda note', 0.3213)]","[-9.05941427e-02 -9.17430371e-02 -2.82938480e-02  2.41519362e-02
  4.68507297e-02  2.00139992e-02 -8.96973312e-02  6.86304867e-02
 -2.95957737e-02 -5.78645431e-02  2.10172590e-02  2.26354450e-02
 -1.13140672e-01  2.01833695e-02  3.63844074e-02 -5.38620353e-03
  1.42869791e-02  1.53573062e-02 -4.64166999e-02 -1.49373174e-01
  4.52073105e-02 -4.31314632e-02  8.22662115e-02  2.77818311e-02
 -1.42439012e-03 -4.77536842e-02 -2.72002332e-02  1.48292268e-02
  2.59083211e-02  1.73477978e-02 -2.64443792e-02  5.32254092e-02
 -3.72132845e-02  5.42014912e-02  3.52047756e-02 -5.78551516e-02
 -1.72643531e-02 -2.12481637e-02  1.68372206e-02 -3.36058177e-02
  6.32628798e-02  4.75798687e-03 -9.44898501e-02  9.91096534e-03
 -9.12862178e-03 -5.44928834e-02  1.33466953e-02 -8.06059316e-02
 -3.12935072e-03 -2.13691462e-02 -4.63257432e-02  1.30902212e-02
 -1.76001880e-02 -2.66174297e-03  7.37770423e-02 -5.57844564e-02
  1.14279687e-02 -5.79963923e-02  4.10013199e-02 -5.28494604e-02
 -1.47116566e-02  4.47476469e-03 -2.88170185e-02 -3.41065265e-02
 -1.98445413e-02  2.50502769e-02 -6.62872940e-03  3.90180685e-02
  1.35551497e-01 -1.15379125e-01 -7.01125339e-02 -1.77423041e-02
  4.20305831e-03  4.04936783e-02 -6.27009720e-02 -6.51628303e-04
  7.41292536e-02  3.27111892e-02  7.89170526e-03 -8.00144374e-02
 -6.81219175e-02 -1.59366857e-02  9.57881883e-02  1.04762621e-01
  8.24168383e-04  4.57530208e-02 -3.17085464e-03  9.39169452e-02
  7.13607762e-03 -6.78204671e-02  2.30255555e-02 -6.92488924e-02
 -2.24929471e-02  5.81376851e-02 -8.35305676e-02  1.67381726e-02
  8.45040232e-02 -7.53460079e-02 -5.64225093e-02  4.91106734e-02
 -3.22124586e-02 -1.48403775e-02  3.28662843e-02 -3.78910936e-02
 -1.27638865e-03  3.23283300e-02  4.07096706e-02  4.63232473e-02
 -1.85451051e-03  2.37301551e-02  8.45147744e-02 -1.09012323e-02
  1.29084731e-03 -5.70844673e-02  8.24953541e-02 -9.21202898e-02
 -5.05593233e-02  2.49033812e-02  9.90198255e-02  5.38950525e-02
  4.78817970e-02  6.56078085e-02 -2.08004452e-02  5.48776127e-02
 -6.84219524e-02 -1.18749430e-02 -6.38936535e-02  7.74321418e-33
  5.01966337e-03 -2.86489390e-02  2.51515061e-02  2.31017359e-02
  4.69108820e-02 -5.94824227e-03  7.75853693e-02 -5.70767035e-04
 -4.29096408e-02 -5.50112911e-02 -3.80843543e-02  8.90507922e-03
 -4.87332307e-02  2.02908348e-02 -1.06844090e-01 -2.07081102e-02
 -1.91288330e-02  6.70431703e-02 -4.21613548e-03  7.06564635e-02
  5.00939190e-02  2.05084607e-02  1.53826354e-02  3.80798317e-02
 -3.44602787e-03  3.24662589e-02 -4.22999859e-02  1.44985048e-02
 -5.38721345e-02  2.48071607e-02 -1.21993793e-03  3.30148600e-02
  5.13079651e-02 -2.13713646e-02 -7.95409903e-02 -3.52842622e-02
 -6.58525974e-02 -7.13923275e-02 -3.33962962e-02 -6.49746731e-02
 -1.00699477e-01  7.79288635e-02 -6.60585761e-02 -9.21244472e-02
 -5.53212836e-02  3.32593955e-02 -5.89761995e-02  5.84286191e-02
  6.23698793e-02 -2.79035643e-02 -8.28155056e-02  5.12888171e-02
 -3.04090045e-02  2.94513833e-02  2.43620612e-02 -3.50371525e-02
  6.41786084e-02 -2.22916761e-03  9.76698920e-02 -7.74335582e-03
  4.55115326e-02  8.02897811e-02 -4.03777361e-02  3.28881331e-02
  4.52351430e-03  5.59070408e-02 -9.29496884e-02  2.33416092e-02
 -3.71535048e-02  5.87394908e-02 -1.32281333e-01  1.03463806e-01
 -8.70820228e-03  2.98735853e-02  5.04614338e-02 -7.04760998e-02
  3.04033384e-02 -5.82961440e-02 -6.31724596e-02  5.64534962e-03
 -5.53857461e-02  3.84754222e-03  3.37776057e-02 -2.19653528e-02
 -6.01176806e-02  2.58702859e-02  1.76135376e-02  4.16235700e-02
  1.79984123e-02 -5.24449982e-02 -5.76866753e-02 -3.51784639e-02
 -1.06472304e-04  2.74797063e-02 -3.93099263e-02 -6.59329345e-33
 -1.49896247e-02  7.37750754e-02 -4.57751453e-02  1.15420474e-02
 -8.58600438e-03  2.31317990e-02  2.69564074e-02 -3.18650901e-02
 -1.35793053e-02 -1.97802056e-02  1.70383919e-02 -5.59095480e-02
 -5.67123434e-03  1.76710337e-02  5.28100133e-02  1.99115369e-03
 -4.92974743e-02 -7.44487122e-02  4.62778881e-02  3.41064967e-02
 -9.44039077e-02  9.49123353e-02 -4.52551804e-02 -4.42242511e-02
 -4.77873646e-02 -4.21914943e-02  6.79696724e-03 -8.04356765e-03
 -9.61755810e-04 -4.94169518e-02  1.28077507e-01  9.77060758e-03
 -4.95449416e-02 -6.45166310e-03 -6.39023930e-02 -5.00421487e-02
  1.12548664e-01 -9.24457982e-02  1.97023116e-02  3.99902575e-02
  1.28878281e-01  2.33098119e-03  1.53154591e-02  2.52673700e-02
 -3.85926031e-02  1.78729836e-02 -4.83905412e-02  8.77448882e-04
  1.65699609e-02 -4.36587632e-02 -3.08411359e-03 -6.33498132e-02
 -2.99868882e-02 -4.17914167e-02 -8.36510095e-04  2.16214350e-04
  7.68520758e-02  2.23248508e-02 -6.81540072e-02 -1.35281943e-02
 -8.25423002e-02 -7.61005208e-02  5.45110437e-04  6.13904651e-03
 -1.57882739e-02 -3.32177356e-02 -5.20053543e-02  4.54924591e-02
  1.97005216e-02 -2.39091888e-02  1.14839459e-02  5.27749732e-02
  9.36565697e-02  7.58798495e-02  3.51561010e-02  5.71608394e-02
 -1.48537373e-02  5.15521690e-03 -2.42748614e-02  1.29687029e-03
 -2.22140905e-02  1.64056495e-02  5.60520440e-02  1.43152224e-02
  7.08021671e-02  4.41657268e-02  2.36842446e-02  9.56653580e-02
 -8.51330906e-03 -4.46424037e-02 -2.18870863e-02 -1.15930969e-02
  6.85361251e-02  8.95488933e-02  8.17670450e-02 -3.39682806e-08
 -4.46956605e-02  3.38034742e-02 -4.11880910e-02  3.23808528e-02
 -3.21720131e-02  2.82473024e-02  4.79659587e-02  4.72243465e-02
  9.42356978e-03  8.23757201e-02  6.44389493e-03 -3.48737463e-02
 -4.02236432e-02  1.46048656e-02 -2.13069357e-02  1.50746003e-01
  4.64045852e-02  2.51904223e-02  5.78832515e-02 -5.77315614e-02
 -1.65992435e-02 -1.06517654e-02  7.00665987e-04 -3.77025753e-02
 -4.47721854e-02 -7.52916560e-03 -2.52080262e-02  6.72017857e-02
 -6.27011284e-02 -7.34879673e-02  1.99104268e-02 -2.12416891e-02
  1.19175404e-01  1.06201274e-03  1.11022249e-01  3.41208242e-02
 -2.92829275e-02  2.09091641e-02  2.63455901e-02  1.01680540e-01
 -1.74861066e-02  1.04700765e-02 -3.17568704e-02  7.15741282e-03
  9.08505246e-02  2.22940054e-02 -6.05379529e-02 -7.07087144e-02
 -4.66175824e-02  3.49317119e-02  4.08000387e-02  1.69627718e-03
  2.50124969e-02  5.01486659e-02  2.42698286e-02  9.52462852e-02
 -5.20670451e-02 -3.98043953e-02  3.22881229e-02 -3.07020731e-02
  8.31494927e-02  2.92339195e-02  2.60816514e-02 -9.08150151e-03]",2,2
poutyne,2,poutyne is a simplified framework for pytorch and handle much of the boilerplating code needed to train neural network use poutyne to read the documentation at poutyne org poutyne is compatible with the latest version of pytorch and python the core data structure of poutyne is a model a way to train your own pytorch neural network how poutyne work is that you create your pytorch module neural network a usual but when come the time to train it you feed it into the poutyne model which handle all the step stats and callback similar to what kera doe here is a simple example select a pytorch device so that it run on gpu if you have one create yourself a pytorch network you can now use poutyne s model to train your network easily since poutyne is inspired by kera one might have notice that this is really similar to some of it function you can evaluate the performance of your network using the evaluate method of poutyne s model or only predict on new data see the complete code here also see this for an example for regression one of the strength poutyne are callback they allow you to save checkpoint log training statistic and more see this notebook for an introduction to callback in that vein poutyne also offer an modelbundle class that offer automatic checkpointing logging and more using callback under the hood here is an example of usage see the complete code here also see this for an example for regression before installing poutyne you must have the latest version of pytorch in your environment look at notebook file with full working example or in google colab we welcome user input whether it is regarding bug found in the library or feature proposition make sure to have a look at our contributing guideline for more detail on this matter this project supported by fr d rik paradis and david beauchemin join the sponsor show your and support and appear on the list poutyne is lgplv licensed a found in the license file poutyne s name come from poutine the well known dish from quebec it is usually composed of french fry squeaky cheese curd and brown gravy however in quebec poutine also ha the meaning of something that is an ordinary or common subject or activity thus poutyne will get rid of the ordinary boilerplate code that plain pytorch training usually entail yuri long from arlington va usa cc by,"[('pytorch network', 0.6024), ('plain pytorch training', 0.5758), ('pytorch device', 0.5589), ('poutyne model', 0.5496), ('poutyne work', 0.5339), ('pytorch module', 0.5129), ('poutyne', 0.5098), ('pytorch', 0.4851), ('own pytorch', 0.4759), ('poutyne org poutyne', 0.4753)]","[-1.07018083e-01 -5.32587655e-02 -8.00446421e-02 -1.34531138e-02
 -2.97794584e-02  5.19134384e-03 -8.50873254e-03 -2.65972596e-03
 -2.36517861e-02 -8.41004550e-02  3.51007795e-03  5.38253374e-02
 -4.75048721e-02  9.40328091e-03 -1.92989130e-02  6.17608652e-02
 -1.99365839e-02  1.23666912e-01  1.51303196e-02 -8.44230950e-02
 -2.23525707e-02 -4.66358922e-02  4.95707095e-02 -8.95983540e-03
 -9.67071764e-03 -4.79081310e-02  1.37716737e-02 -1.98661350e-02
  5.81031330e-02 -3.46278660e-02  2.03663036e-02  1.44502521e-02
  5.85168600e-02  4.73728850e-02 -9.59546119e-03 -1.20843248e-02
  6.07383251e-03 -1.97294485e-02 -2.05359031e-02  1.94395222e-02
  4.22241576e-02 -1.86706167e-02 -7.45915249e-02  5.63131869e-02
  3.05608176e-02  4.23272187e-03  3.94086316e-02 -3.14668566e-02
 -5.38257249e-02 -9.49450582e-02 -7.04173977e-03 -1.88669108e-03
 -2.58714687e-02 -4.51767072e-03 -5.12946164e-03 -1.28642423e-02
 -3.91479069e-03 -3.21114366e-03  1.40132811e-02 -4.70898747e-02
  4.30833288e-02 -1.13003002e-02 -4.86642458e-02 -1.50341019e-02
 -3.25381607e-02  2.00035218e-02 -5.11646038e-03  1.97647102e-02
  8.47477615e-02 -1.05060920e-01  1.10054770e-02 -4.38297242e-02
 -1.15837820e-01  7.67271668e-02 -1.16713205e-02  3.06312386e-02
  8.62005204e-02  6.13860302e-02  1.84586588e-02 -4.45674695e-02
 -6.39897212e-02  4.85770451e-03  2.98722964e-02  2.61861105e-02
  4.84320037e-02  3.23685780e-02 -8.28064531e-02  1.18441269e-01
 -3.23263393e-03 -6.25972897e-02  3.16382535e-02 -6.66876286e-02
 -5.36270626e-02  6.72049373e-02 -5.60415443e-04  2.05191821e-02
  4.53356979e-03 -7.14100897e-02 -7.57715926e-02  5.23106754e-02
 -5.03259227e-02 -4.25134823e-02 -3.15453075e-02  4.43988182e-02
 -3.92209105e-02  3.61240059e-02  3.71378125e-03  3.78799923e-02
  4.57228161e-02 -3.04809418e-02 -2.59074569e-02  1.34953158e-02
 -5.15868180e-02 -3.57799158e-02  6.73035830e-02 -2.09735632e-02
 -8.83043483e-02  9.55982730e-02  1.17577620e-01 -4.61198995e-03
 -9.06810388e-02  3.78165580e-02 -6.61357120e-02  7.04961792e-02
 -3.96309718e-02 -4.00625505e-02 -6.57786578e-02  7.39455374e-33
  2.46296991e-02  4.52040918e-02  7.14398101e-02  7.40430783e-03
  1.12439152e-02 -3.66504714e-02  4.18603122e-02 -7.85520580e-03
  9.65643972e-02 -3.55198160e-02 -3.54548618e-02 -6.26607463e-02
 -2.23920867e-02  3.74648571e-02 -2.80561168e-02 -6.68822154e-02
 -1.34829246e-03  4.49691787e-02  9.52476263e-02  4.60607596e-02
  5.40009551e-02  7.89167359e-02 -7.27095008e-02  1.03130981e-01
 -2.27805786e-02  3.98365222e-02 -2.04069521e-02 -6.10001534e-02
 -2.06130128e-02  2.44961251e-02 -5.23620807e-02 -5.15913265e-03
  3.24324332e-02 -1.72597989e-02 -7.30684623e-02 -3.48352417e-02
 -4.15035076e-02 -4.47563194e-02 -1.33961476e-02 -1.01481788e-01
 -1.25219434e-01 -5.53720258e-03 -1.98631343e-02 -2.23619025e-02
 -3.52082029e-02 -3.53468731e-02 -1.09257484e-02  7.62300417e-02
  3.56528908e-04 -3.64452340e-02 -8.54279548e-02  3.34372446e-02
 -4.30180654e-02  5.68165071e-02  1.39905522e-02 -1.22358575e-02
  2.56575383e-02  3.49273495e-02  1.07835427e-01  4.94986698e-02
  4.14098464e-02  8.52554291e-02  2.12777648e-02 -1.30565157e-02
 -1.40775368e-02  5.28331697e-02 -8.40730816e-02 -4.43096273e-02
 -1.14183556e-02  1.05805889e-01 -7.24640340e-02  9.38227773e-02
 -6.48660660e-02 -2.08960008e-02  7.21309930e-02  7.36055290e-03
 -1.76628679e-02 -6.43822029e-02 -1.93979498e-02 -2.61984463e-03
 -1.07527651e-01 -7.66792987e-03 -2.06270367e-02  1.03070782e-02
 -8.21323842e-02 -5.88074699e-03 -2.90350318e-02 -1.83157474e-02
  4.76732552e-02  6.08538873e-02 -1.59407109e-02 -3.29269283e-02
  2.53635161e-02  7.08344132e-02  1.69834048e-02 -6.37772615e-33
  6.15000445e-03  4.42690104e-02 -7.66185969e-02  1.71722136e-02
  3.62778641e-02  1.03539126e-02 -3.72954756e-02 -5.63890152e-02
 -2.34127454e-02  9.64678898e-02  2.31250338e-02 -4.85770144e-02
 -2.49542054e-02 -2.15541311e-02  4.60473225e-02  9.99254268e-03
 -1.71128362e-02  1.67801771e-02  8.78767949e-03  5.39309010e-02
 -1.24148041e-01  1.17955595e-01 -6.89649433e-02 -5.18901870e-02
  2.79902779e-02 -1.38942106e-03 -3.23985592e-02  3.59118427e-03
  2.94178948e-02 -9.90870874e-03  8.79451726e-03 -3.25520076e-02
  5.83961327e-03  2.52957866e-02 -2.49926858e-02  3.87728140e-02
  4.08511758e-02  1.33438725e-02 -1.54800701e-03 -1.85362045e-02
  1.04271397e-01  1.03842579e-02 -8.88566375e-02  5.09736612e-02
  1.96928103e-02  7.66250771e-03 -7.94547051e-02  9.45585500e-03
 -1.51746655e-02 -1.78218842e-03 -6.77343365e-03  6.63685873e-02
  5.23032062e-02 -6.70550764e-02  2.44026743e-02  3.39041837e-02
  6.08211569e-02  1.91034898e-02  2.75457520e-02  1.72539931e-02
 -5.97140081e-02 -1.07396454e-01 -6.07703663e-02  8.91503096e-02
 -2.70708408e-02 -6.73128590e-02 -4.22799811e-02  1.50038630e-01
  2.45766882e-02 -3.63435671e-02 -3.25688645e-02  9.42115337e-02
  7.06905723e-02  1.10534213e-01 -1.09770680e-02  4.83537652e-02
  9.29515902e-03 -6.09655492e-02  2.34726220e-02 -2.21079849e-02
 -5.20747565e-02  1.37439594e-02  2.00847760e-02  1.58761386e-02
  5.39869405e-02  6.83420748e-02  7.86337852e-02  4.33992259e-02
  3.68510447e-02 -5.22415638e-02  2.20052898e-02  1.76620949e-02
  2.72519863e-03  7.26819932e-02  9.32240486e-03 -3.27146310e-08
 -5.36444597e-02  2.64936984e-02  1.99349057e-02  1.46556031e-02
  6.59487620e-02 -7.14746341e-02  6.26078546e-02  1.45875782e-01
 -4.00458612e-02  3.86004150e-02 -2.75501795e-02 -3.83429378e-02
 -3.28028165e-02 -4.21198085e-02  5.38571626e-02  9.41370726e-02
  4.13371436e-02 -4.20599543e-02  4.81199324e-02 -3.81151191e-03
  2.16161758e-02 -1.12876808e-02 -2.69909706e-02  1.54371168e-02
 -3.92583944e-02 -3.80585641e-02  1.19754924e-02  4.29318510e-02
 -5.40593779e-03 -1.42420945e-03  4.19148952e-02 -4.28685360e-02
  4.46953364e-02  1.83222890e-02  9.73010138e-02  9.55034792e-02
 -1.08101359e-02 -2.08237171e-02  5.82858128e-03  7.98808038e-02
 -1.21604301e-01 -3.84770706e-02 -2.29128916e-02  8.28499720e-03
  6.48165271e-02  3.08958441e-02 -6.35406151e-02 -1.34651765e-01
 -4.02091555e-02  1.08760241e-02 -1.18911508e-02  5.71177080e-02
  1.24353841e-02 -8.07976164e-03 -2.72563025e-02  9.62505788e-02
 -3.03614829e-02 -8.49667042e-02 -4.48979139e-02 -2.27030870e-02
 -4.01039682e-02  5.67272641e-02  3.93700227e-02 -3.28384601e-02]",2,2
onnxmltools,2,onnxmltools enables you to convert model from different machine learning toolkits into onnx currently the following toolkits are supported pytorch ha it builtin onnx exporter check here for detail you can install latest release of onnxmltools from pypi or install from source if you choose to install onnxmltools from it source code you must set the environment variable onnx ml before installing the onnx package this package relies on onnx numpy and protobuf if you are converting a model from scikit learn core ml kera lightgbm sparkml xgboost h o catboost or libsvm you will need an environment with the respective package installed from the list below onnxmltools is tested with python if you want the converted onnx model to be compatible with a certain onnx version please specify the target opset parameter upon invoking the convert function the following kera model conversion example demonstrates this below you can identify the mapping from onnx operator set referred to a opsets to onnx release in the versioning documentation next we show an example of converting a kera model into an onnx model with target opset which corresponds to onnx release version here is a simple code snippet to convert a core ml model into an onnx model below is a code snippet to convert a h o mojo model into an onnx model the only prerequisite is to have a mojo model saved on the local file system onnxmltools convert model into the onnx format which can be then used to compute prediction with the backend of your choice you can check the operator set of your converted onnx model using netron a viewer for neural network model alternatively you could identify your converted model s opset version through the following line of code if the result from checking your onnx model s opset is smaller than the target opset number you specified in the onnxmltools convert function be assured that this is likely intended behavior the onnxmltools converter work by converting each operator to the onnx format individually and finding the corresponding opset version that it wa most recently updated in once all of the operator are converted the resultant onnx model ha the maximal opset version of all of it operator to illustrate this concretely let s consider a model with two operator ab and add a of december ab wa most recently updated in opset and add wa most recently updated in opset therefore the converted onnx model s opset will always be even if you request target opset the converter behavior wa defined this way to ensure backwards compatibility documentation for the onnx model format and more example for converting model from different framework can be found in the onnx tutorial repository all converter unit test can generate the original model and converted model to automatically be checked with onnxruntime or onnxruntime gpu the unit test case are all the normal python unit test case you can run it with pytest command line for example it requires onnxruntime numpy for most model panda for transforms related to text feature and scipy for sparse feature one test also requires kera to test a custom operator that mean sklearn or any machine learning library is requested once the converter is implemented a unit test is added to confirm that it work at the end of the unit test function dump data and model or any equivalent function must be called to dump the expected output and the converted model once these file are generated a corresponding test must be added in test backend to compute the prediction with the runtime apache license v,"[('converted onnx model', 0.6727), ('onnx model format', 0.6314), ('onnxmltools converter work', 0.6055), ('onnx package', 0.5789), ('onnx tutorial repository', 0.5757), ('onnxmltools', 0.5653), ('builtin onnx exporter check', 0.5503), ('onnxruntime numpy', 0.5416), ('onnx format', 0.5295), ('variable onnx ml', 0.5225)]","[-3.38032618e-02 -8.85568336e-02 -1.87767819e-02 -2.93812007e-02
  8.78505409e-03  4.98854974e-03 -3.97797711e-02 -9.79751162e-03
 -7.84481093e-02 -3.74057405e-02 -3.12212408e-02 -5.21690547e-02
  4.51281071e-02 -2.36408655e-02 -4.25145105e-02 -3.55073065e-03
 -8.09802860e-03 -2.04092357e-02 -1.39376223e-02 -1.69302560e-02
  3.43653262e-02  2.25657858e-02  4.28621769e-02 -4.76442315e-02
 -7.39648647e-04 -8.95090401e-02  1.90474801e-02  3.53447944e-02
  4.03428487e-02  1.10461256e-02  8.56573228e-03  4.34586182e-02
 -4.80930693e-02 -5.70405647e-03  7.30488449e-02  4.42694649e-02
  3.49062942e-02 -1.60349123e-02 -3.61336693e-02  1.30898459e-03
  1.05907440e-01 -2.47169230e-02  7.13986382e-02 -3.68357413e-02
  5.43730520e-02  2.86786724e-02 -3.07284435e-03 -6.40573204e-02
  1.15779052e-02  4.23849821e-02  1.40017802e-02 -1.44068208e-02
 -1.23204561e-02  2.40081381e-02  1.13634011e-02  3.62775698e-02
 -7.39846677e-02 -2.94453208e-03 -6.53552413e-02  1.45758148e-02
 -1.00030251e-01  4.39520404e-02 -9.23583955e-02  2.34296359e-02
  7.63632432e-02  7.12739006e-02 -2.91752145e-02  1.40589532e-02
 -4.10453901e-02  9.80801601e-03 -6.95912093e-02 -8.14736038e-02
  8.94324761e-03  1.49551569e-03 -6.26499355e-02 -6.67223781e-02
  1.13103412e-01 -2.52637547e-02  3.91848497e-02 -8.45257267e-02
 -5.70832454e-02  6.66585192e-02  4.55230772e-02  2.80489288e-02
  2.62980582e-03 -3.58797656e-03 -6.91611692e-02  8.27843621e-02
  4.45253551e-02  6.09788559e-02 -4.04852219e-02 -1.30725326e-02
  1.37790767e-02  3.50074507e-02  6.61637485e-02  3.24380770e-02
  5.98845519e-02  1.20737381e-01  3.29109132e-02 -6.69489522e-03
  2.09464952e-02 -2.48832777e-02 -4.58347984e-02 -9.40936133e-02
 -5.30911498e-02  1.11630261e-02  8.78359899e-02  5.81812812e-04
  1.81462970e-02  2.93671750e-02  9.94342566e-03  2.61498112e-02
 -5.47000999e-03 -2.71716062e-02  6.39920160e-02 -2.90895235e-02
  1.04628885e-02 -5.75562716e-02 -4.00826260e-02  2.27855202e-02
 -6.66967034e-02 -7.31655955e-03 -7.48560578e-03  8.49406123e-02
 -1.54362610e-02  2.34555658e-02 -2.67979577e-02  7.28763739e-33
 -8.88827592e-02 -4.51388545e-02  6.61393702e-02 -2.70718783e-02
  1.82842426e-02  1.09593533e-01  8.76974985e-02 -3.19578983e-02
 -7.41360039e-02  3.55486684e-02 -1.01605237e-01  4.76541743e-02
 -1.02761060e-01 -1.21762957e-02 -4.93827723e-02 -5.50939143e-02
  6.03078194e-02  3.79623994e-02 -7.46422773e-03  5.53698801e-02
  8.63320529e-02  5.70642017e-02 -3.67589593e-02  1.38118947e-02
 -2.48322152e-02 -1.19613437e-03  2.92301690e-03  5.81435226e-02
  5.94830997e-02 -2.78234035e-02 -1.06054775e-01 -7.13973269e-02
  2.08214391e-02 -6.20697588e-02 -5.76435216e-02 -3.54563743e-02
 -5.09253778e-02  2.45379489e-02 -3.16125266e-02 -3.64151187e-02
 -4.11295295e-02  3.17071006e-02 -4.39172350e-02 -7.25317523e-02
 -1.03177510e-01 -6.25324319e-04 -7.51954690e-03 -2.20100284e-02
  1.28932044e-01 -8.37999210e-02  3.34092751e-02 -2.81102350e-03
  9.32274479e-03 -5.60282134e-02 -2.88918670e-02 -6.02667732e-03
  2.63711382e-02  4.43105921e-02  1.82918180e-02 -1.94832496e-03
 -1.47197640e-03 -3.00736818e-02  1.13776751e-01 -1.71712006e-03
  8.52971971e-02  1.00852316e-02 -2.10538451e-02 -6.72951341e-02
 -3.79718058e-02 -4.70118783e-02 -7.42249191e-02 -8.67171271e-04
 -5.19266874e-02  4.89023654e-03  1.21631898e-01 -2.98465658e-02
 -3.26206572e-02 -4.62421626e-02  2.98898723e-02  9.83955943e-06
  1.11640655e-02  6.48017377e-02 -1.37513143e-03 -1.68422814e-02
  5.42531982e-02 -3.25549357e-02 -1.20326877e-02 -4.67505679e-02
 -1.22153750e-02  5.95615804e-03  3.89645994e-02 -1.81183413e-01
 -3.38815451e-02 -2.67273057e-02 -9.09507498e-02 -6.95086267e-33
 -3.60673703e-02  7.73193836e-02 -3.39975245e-02  2.07228828e-02
 -1.15162313e-01  2.59603653e-02  2.49163597e-04  3.98544781e-02
 -6.31734058e-02 -5.98467886e-03  1.05999403e-01 -5.08242473e-02
 -2.35481523e-02 -4.12749201e-02  3.86119215e-03 -5.28142340e-02
  9.26955417e-03  1.04610026e-02  6.66253492e-02 -6.43137563e-03
  4.29752022e-02  4.72092777e-02 -2.35457402e-02  1.02659157e-02
 -9.67365056e-02  3.24779376e-02  5.37925549e-02  1.54539257e-01
  2.43030731e-02  2.88369581e-02 -2.69589182e-02  1.02284160e-02
 -7.47497678e-02  3.58367972e-02  2.84366198e-02  3.02628372e-02
  5.05271964e-02 -1.19583048e-02 -5.75490342e-03 -1.54570760e-02
  8.03186223e-02 -4.34984500e-03 -5.17420657e-02  1.88104901e-02
 -8.79160501e-03  5.56075051e-02 -1.17745519e-01  1.09494161e-02
  3.27020697e-02 -4.81745601e-02  5.72823398e-02  3.19570443e-03
  4.49236762e-03  2.26389859e-02 -5.72069362e-02  2.82151927e-03
 -1.54972207e-02 -2.20743958e-02  6.56477585e-02 -2.06532292e-02
 -8.02903622e-02 -9.36857536e-02 -1.39262183e-02 -7.46774897e-02
 -2.27527246e-02  1.30895525e-02 -8.62653255e-02  1.07295841e-01
 -4.63289861e-03  3.85968983e-02  8.73525906e-03  1.08838910e-02
  1.08540699e-01  6.55024201e-02 -3.71562727e-02 -4.65178676e-02
  1.11342166e-02 -2.49963701e-02  2.84681153e-02  1.00482814e-03
 -9.91636515e-03  1.39604164e-02  5.97974807e-02  5.68850860e-02
  8.54067877e-03  5.28568402e-02  1.61121264e-01  1.18415736e-01
  4.23471108e-02  1.44567471e-02  2.07322519e-02 -1.13033848e-02
  3.52495350e-04  7.47065693e-02 -6.04713634e-02 -3.34406778e-08
 -4.54944931e-02  7.87999388e-03  6.10568710e-02  1.89129710e-02
 -1.09715559e-01  7.36358389e-02  2.50607375e-02 -7.18410462e-02
  3.79349589e-02  3.50572877e-02  2.82163769e-02 -7.95191899e-02
 -2.29232758e-03 -4.63756472e-02 -1.40215466e-02  7.39231631e-02
  2.47149915e-02  3.00769936e-02  3.76223624e-02 -3.04956604e-02
 -9.76778660e-03 -2.66342927e-02 -1.65132266e-02  6.25238121e-02
  2.10944861e-02 -1.01874843e-02 -5.06216288e-02  9.19075683e-02
  2.45566349e-02 -5.11077456e-02 -3.29196826e-02  1.15007497e-02
  8.16078484e-02  6.76378235e-02 -4.18541357e-02  1.79772568e-03
  9.84799396e-03  3.91461290e-02  7.05011711e-02  9.02853440e-03
 -1.05898507e-01  2.58670077e-02 -9.82481092e-02 -3.32209617e-02
 -3.41970921e-02  3.00813690e-02 -2.48668250e-02  3.48310149e-03
  4.36614864e-02  5.67680150e-02  3.09480149e-02  1.92627311e-03
 -1.43400964e-03 -5.24581820e-02  4.31646081e-03 -8.04005414e-02
 -5.25406227e-02  3.88437659e-02  5.43663651e-02  3.25781405e-02
 -5.08625917e-02  2.77383383e-02 -1.61987003e-02 -4.39192401e-03]",2,2
monai,2,medical open network for ai monai is a pytorch based open source framework for deep learning in healthcare imaging part of pytorch ecosystem it ambition are please see the technical highlight and what s new of the milestone release to install the current release you can simply run please refer to the installation guide for other installation option mednist demo and monai for pytorch user are available on colab example and notebook tutorial are located at project monai tutorial technical documentation is available at doc monai io the monai model zoo is a place for researcher and data scientist to share the latest and great model from the community utilizing the monai bundle format make it easy to get started building workflow with monai for guidance on making a contribution to monai see the contributing guideline join the conversation on twitter projectmonai or join our slack channel ask and answer question over on monai s github discussion tab,"[('project monai tutorial', 0.636), ('monai model zoo', 0.576), ('monai', 0.5199), ('doc monai', 0.5155), ('monai bundle format', 0.4776), ('medical open network', 0.4707), ('ai monai', 0.4412), ('pytorch user', 0.3857), ('pytorch', 0.385), ('pytorch ecosystem', 0.3482)]","[-4.18796763e-02 -8.77935514e-02  4.87464573e-03 -1.72330663e-02
 -5.36481291e-02 -1.66652147e-02 -5.13032861e-02  9.61098447e-02
 -8.09402987e-02 -7.60239512e-02 -4.00145091e-02 -5.33038005e-02
 -7.06169978e-02  8.71578902e-02 -3.32150003e-03  3.95579822e-02
 -2.95828059e-02  5.82599230e-02 -3.53606828e-02 -7.71344900e-02
  1.67829134e-02 -3.70933232e-03  3.44957672e-02  2.43091416e-02
 -5.39389886e-02 -4.87300418e-02 -1.58693101e-02 -5.28412238e-02
  3.04905120e-02  5.72597757e-02 -2.13836487e-02  4.68701907e-02
  5.28921410e-02 -1.90920774e-02 -7.75514171e-03  6.00656345e-02
 -3.24296736e-04 -6.73897117e-02 -6.87949406e-03 -1.22072883e-02
  4.20181155e-02 -1.54374884e-02 -1.73761845e-02 -1.52798900e-02
  1.09514341e-01 -5.27419522e-02  3.90780419e-02 -7.79406801e-02
  7.88094029e-02 -6.94309548e-02 -1.62450060e-01 -3.41720954e-02
 -7.35669732e-02  5.37017267e-03  2.13411581e-02  2.04757378e-02
  3.47026400e-02 -1.05092056e-01 -4.62309569e-02 -9.80496928e-02
  1.15169131e-03  8.60943832e-03  3.05242762e-02  5.94178252e-02
  5.34065403e-02  9.85293984e-02 -2.29080264e-02  7.14432746e-02
  1.06851719e-01 -8.49639177e-02 -5.17987795e-02 -4.71377708e-02
 -8.66778716e-02  5.97958528e-02  9.31336172e-03  2.85047814e-02
  1.14559069e-01  1.24983564e-02  5.03200069e-02 -6.76279142e-02
 -4.35561091e-02 -4.16256115e-02  5.80361299e-02  7.87100494e-02
  8.48396029e-03  7.08143599e-03 -1.26550635e-02  3.39181572e-02
  2.19241567e-02 -1.94238387e-02  4.90574837e-02 -5.35532907e-02
  3.83787649e-03  1.44598335e-02  5.86282462e-02  1.99235789e-03
 -8.59143492e-03 -6.21448532e-02 -3.81405205e-02  6.88800290e-02
 -9.47530791e-02 -3.91540378e-02  7.97184650e-03  1.60405424e-03
 -2.58415677e-02  5.07966876e-02 -3.90143320e-03  7.46522564e-03
  2.09060982e-02  2.27623712e-02  1.04055377e-02 -5.99722825e-02
 -3.19340546e-03 -6.46118447e-02  2.41775364e-02  3.69376764e-02
  8.16287007e-03  2.36228369e-02  6.93592355e-02  9.34471264e-02
 -5.68167977e-02  8.03823676e-03 -1.45426989e-02  9.78168938e-03
 -6.81335758e-03 -5.62425666e-02 -5.61928488e-02  4.92826594e-33
 -1.48391631e-02 -5.35747148e-02  8.42589214e-02  3.00763156e-02
  1.34391472e-01 -3.28366943e-02  4.35767025e-02 -3.44059020e-02
 -6.10379875e-02 -5.05710058e-02  1.11597190e-02  3.59527469e-02
 -1.02543645e-01  5.03843650e-02 -1.63433328e-02 -6.20044135e-02
 -9.98569001e-03  1.38767855e-02  1.20202973e-02  2.99293268e-02
  3.83303203e-02  1.51278554e-02 -7.57877529e-02  2.94734705e-02
 -1.67440530e-02  5.98630533e-02  1.79542378e-02 -9.76865552e-03
  6.60893694e-03  3.87011208e-02 -4.48258109e-02 -4.53868099e-02
 -5.60158268e-02 -2.15358362e-02 -6.39050528e-02 -3.75042073e-02
 -3.75973284e-02 -5.31878620e-02 -3.24588567e-02  6.16532471e-03
 -9.11136866e-02  5.83732389e-02  2.70314235e-02  2.39968151e-02
 -1.55788129e-02 -1.96901290e-03  1.27189457e-02  8.79301652e-02
  8.49667564e-02 -3.09442189e-02 -1.38115855e-02  7.26311561e-03
 -6.31952658e-02 -3.13584842e-02  2.28318889e-02 -2.69675013e-02
 -1.41476588e-02  5.21548018e-02  4.38495316e-02  1.08691178e-01
  7.38896281e-02  2.88269091e-02  7.81379174e-03  2.35795844e-02
  2.72492580e-02  3.47247832e-02 -1.01847976e-01 -5.69608323e-02
  6.00658357e-02  1.10752702e-01 -1.01041712e-01  3.66527662e-02
 -3.16233523e-02 -2.89818347e-02  1.43073776e-04 -8.99091829e-03
 -1.01486128e-02 -1.25323251e-01 -4.49041501e-02  2.67693237e-03
 -8.22449327e-02  6.86401222e-03 -1.98672097e-02  7.83908963e-02
 -6.31783530e-02 -1.08817108e-02 -8.19219556e-03  4.60513011e-02
 -3.87738310e-02 -6.91302819e-03  1.57951247e-02  2.17173696e-02
  3.57663445e-02  3.25806551e-02  3.52755189e-02 -5.32641396e-33
 -4.74675978e-03  6.25811238e-03 -8.96788910e-02 -3.69488187e-02
 -5.67420647e-02 -3.59405726e-02 -2.36824434e-03  3.02198771e-02
  7.50394762e-02  5.01202866e-02  2.10894980e-02 -2.30480041e-02
  2.83327848e-02  9.18694027e-03  2.50278134e-03  5.83715690e-03
  4.26836195e-04 -7.98138455e-02 -2.00913846e-02  8.63159895e-02
 -1.48869874e-02  1.81858078e-01 -7.17299581e-02 -5.46254106e-02
  2.86700688e-02  2.15390213e-02 -7.13116080e-02  6.35470077e-02
  2.95612533e-02 -7.93914869e-02 -3.38193141e-02 -5.25727943e-02
 -4.74210903e-02 -6.79457188e-02 -5.75088523e-02  4.34762239e-02
  5.02668843e-02 -4.16621342e-02  1.68157797e-02  4.65358375e-03
  1.37219921e-01 -2.36804411e-02 -8.71722102e-02 -1.04329614e-02
 -2.00568605e-02  4.83569652e-02 -1.03371136e-01  5.88980019e-02
  1.91846602e-02 -2.01154388e-02 -2.58650053e-02 -2.31424756e-02
  7.15227006e-03 -7.64229968e-02 -8.63550045e-03  9.91715677e-03
  1.08444229e-01 -4.81882878e-02  1.14111817e-02  1.37439966e-02
 -1.80265401e-02 -1.02207363e-01  3.39543670e-02  2.88566910e-02
 -8.45320299e-02 -6.82973713e-02 -6.46778345e-02 -2.60729622e-02
 -3.08025647e-02 -3.20152752e-02 -2.05006991e-02  8.79904404e-02
  3.30751278e-02  8.35914761e-02  4.30266298e-02  4.18015607e-02
 -3.22891437e-02  3.18238549e-02  6.46900237e-02 -2.84950212e-02
 -3.68338008e-03  1.16772763e-02  4.66695651e-02  6.86785132e-02
  6.54054284e-02 -4.07524370e-02  9.67401788e-02  6.34520501e-02
  3.13291587e-02  1.63572840e-02 -7.29862154e-02  6.79056114e-03
  6.74997037e-03  6.33633137e-02 -3.96206379e-02 -3.25658682e-08
 -1.79934800e-02  1.50268674e-02  2.76922109e-03  5.47306798e-02
 -1.04360757e-02  2.16190796e-02 -4.19543535e-02  1.76805276e-02
  2.11544777e-03  7.18081743e-02 -1.39571214e-02  8.32857490e-02
 -8.11454654e-02  1.74177438e-02  3.00162472e-02  9.84898135e-02
  4.28688116e-02 -1.73440017e-02  8.85522272e-03 -6.56010509e-02
  3.91000882e-02  1.08705624e-03 -3.66763733e-02 -1.28053436e-02
  1.50361946e-02 -2.56855153e-02  1.51009876e-02  5.65784648e-02
 -6.02159612e-02  1.46069359e-02  4.80592716e-03 -2.69446056e-02
  5.11453301e-02 -3.36838840e-03  4.53568622e-02  9.21180546e-02
 -6.52888492e-02 -2.22933553e-02  6.03903458e-03  1.85402203e-02
 -9.17970575e-03 -7.78933540e-02 -2.17193216e-02 -1.78184956e-02
  1.12350084e-01  4.89164479e-02  2.50133984e-02 -5.15107550e-02
 -1.98606495e-03  3.31421979e-02  2.09828671e-02 -2.22876929e-02
  2.89216377e-02  8.71384807e-04 -3.64098288e-02  8.95350203e-02
  1.91077162e-02 -8.41196626e-02 -2.03673244e-02  1.09577458e-02
  1.52569339e-02  5.69337122e-02  4.24496597e-03 -6.77930117e-02]",2,2
torch_optimizer,2,torch optimizer collection of optimizers for pytorch compatible with optim module installation process is simple just http pytorch optimizer rtfd ioplease cite original author of optimization algorithm if you like this package or use github feature cite this repository button a gradexphttps arxiv org ab a gradinchttps arxiv org ab a gradunihttps arxiv org ab accsgdhttps arxiv org ab adabeliefhttps arxiv org ab adaboundhttps arxiv org ab adamodhttps arxiv org ab adafactorhttps arxiv org ab adahessianhttps arxiv org ab adamphttps arxiv org ab aggmohttps arxiv org ab apollohttps arxiv org ab diffgradhttps arxiv org ab lambhttps arxiv org ab lookaheadhttps arxiv org ab madgradhttps arxiv org ab novogradhttps arxiv org ab pidhttps www comp polyu edu hk cslzhang paper cvpr pid pdfqhadamhttps arxiv org ab qhmhttps arxiv org ab radamhttps arxiv org ab rangerhttps medium com lessw new deep learning optimizer ranger synergistic combination of radam lookahead for the best of dc f a drangerqhhttps arxiv org ab rangervahttps arxiv org ab v sgdphttps arxiv org ab sgdwhttps arxiv org ab swatshttps arxiv org ab shampoohttps arxiv org ab yogihttps paper nip cc paper adaptive method for nonconvex optimizationvisualizations help u to see how different algorithm deal with simple situation like saddle point local minimum valley etc and may provide interesting insight into inner working of algorithm rosenbrock and rastrigin benchmark function wa selected because rosenbrock also known a banana function is non convex function that ha one global minimum the global minimum is inside a long narrow parabolic shaped flat valley to find the valley is trivial to converge to the global minimum however is difficult optimization algorithm might pay a lot of attention to one coordinate and have problem to follow valley which is relatively flat rastrigin function is a non convex and ha one global minimum in finding the minimum of this function is a fairly difficult problem due to it large search space and it large number of local minimum each optimizer performs optimization step learning rate is best one found by hyper parameter search algorithm rest of tuning parameter are default it is very easy to extend script and tune other optimizer parameter do not pick optimizer based on visualization optimization approach have unique property and may be tailored for different purpose or may require explicit learning rate schedule etc best way to find out is to try one on your particular problem and see if it improves score if you do not know which optimizer to use start with built in sgd adam once training logic is ready and baseline score are established swap optimizer and see if there is any improvement paper optimal adaptive and accelerated stochastic gradient descent http arxiv org ab reference code http github com severilov a grad optimizerpaper optimal adaptive and accelerated stochastic gradient descent http arxiv org ab reference code http github com severilov a grad optimizerpaper optimal adaptive and accelerated stochastic gradient descent http arxiv org ab reference code http github com severilov a grad optimizerpaper on the insufficiency of existing momentum scheme for stochastic optimization http arxiv org ab reference code http github com rahulkidambi accsgdpaper adabelief optimizer adapting stepsizes by the belief in observed gradient http arxiv org ab reference code http github com juntang zhuang adabelief optimizerpaper adaptive gradient method with dynamic bound of learning rate http arxiv org ab reference code http github com luolc adaboundadamod method restricts the adaptive learning rate with adaptive and momental upper bound the dynamic learning rate bound are based on the exponential moving average of the adaptive learning rate themselves which smooth out unexpected large learning rate and stabilize the training of deep neural network paper an adaptive and momental bound method for stochastic learning http arxiv org ab reference code http github com lancopku adamodpaper adafactor adaptive learning rate with sublinear memory cost http arxiv org ab reference code http github com pytorch fairseq blob master fairseq optim adafactor pypaper adahessian an adaptive second order optimizer for machine learning http arxiv org ab reference code http github com amirgholami adahessianadamp propose a simple and effective solution at each iteration of adam optimizer applied on scale invariant weight e g conv weight preceding a bn layer adamp remove the radial component i e parallel to the weight vector from the update vector intuitively this operation prevents the unnecessary update along the radial direction that only increase the weight norm without contributing to the loss minimization paper slowing down the weight norm increase in momentum based optimizers http arxiv org ab reference code http github com clovaai adamppaper aggregated momentum stability through passive damping http arxiv org ab reference code http github com athemathmo aggmopaper apollo an adaptive parameter wise diagonal quasi newton method for nonconvex stochastic optimization http arxiv org ab reference code http github com xuezhemax apollooptimizer based on the difference between the present and the immediate past gradient the step size is adjusted for each parameter in such a way that it should have a larger step size for faster gradient changing parameter and a lower step size for lower gradient changing parameter paper diffgrad an optimization method for convolutional neural network http arxiv org ab reference code http github com shivram diffgradpaper large batch optimization for deep learning training bert in minute http arxiv org ab reference code http github com cybertronai pytorch lambpaper lookahead optimizer k step forward step back http arxiv org ab reference code http github com alphadl lookahead pytorchpaper adaptivity without compromise a momentumized adaptive dual averaged gradient method for stochastic optimization http arxiv org ab reference code http github com facebookresearch madgradpaper stochastic gradient method with layer wise adaptive moment for training of deep network http arxiv org ab reference code http github com nvidia deeplearningexamples paper a pid controller approach for stochastic optimization of deep network http www comp polyu edu hk cslzhang paper cvpr pid pdf reference code http github com tensorboy pidoptimizerpaper quasi hyperbolic momentum and adam for deep learning http arxiv org ab reference code http github com facebookresearch qhoptimpaper quasi hyperbolic momentum and adam for deep learning http arxiv org ab reference code http github com facebookresearch qhoptimpaper on the variance of the adaptive learning rate and beyond http arxiv org ab reference code http github com liyuanlucasliu radampaper new deep learning optimizer ranger synergistic combination of radam lookahead for the best of both http medium com lessw new deep learning optimizer ranger synergistic combination of radam lookahead for the best of dc f a d reference code http github com lessw ranger deep learning optimizerpaper quasi hyperbolic momentum and adam for deep learning http arxiv org ab reference code http github com lessw ranger deep learning optimizerpaper calibrating the adaptive learning rate to improve convergence of adam http arxiv org ab v reference code http github com lessw ranger deep learning optimizerpaper slowing down the weight norm increase in momentum based optimizers http arxiv org ab reference code http github com clovaai adamppaper sgdr stochastic gradient descent with warm restarts http arxiv org ab reference code http github com pytorch pytorch pull paper improving generalization performance by switching from adam to sgd http arxiv org ab reference code http github com mrpatekful swatspaper shampoo preconditioned stochastic tensor optimization http arxiv org ab reference code http github com moskomule shampoo pytorchyogi is optimization algorithm based on adam with more fine grained effective learning rate control and ha similar theoretical guarantee on convergence a adam paper adaptive method for nonconvex optimization http paper nip cc paper adaptive method for nonconvex optimization reference code http github com rtemi yogi optimizer kerasrevert for drop radam drop radam optimizer since it is included in pytorch do not include test a installable package preserver memory layout where possible add madgrad optimizer initial release added support for a gradexp a gradinc a graduni accsgd adabelief adabound adamod adafactor adahessian adamp aggmo apollo diffgrad lamb lookahead novograd pid qhadam qhm radam ranger rangerqh rangerva sgdp sgdw swat shampoo yogi,"[('torch optimizer collection', 0.6822), ('optimizers', 0.5861), ('cybertronai pytorch lambpaper lookahead optimizer k step', 0.5514), ('http pytorch optimizer rtfd ioplease', 0.5226), ('optim adafactor pypaper', 0.5215), ('optimizer', 0.4937), ('madgrad optimizer', 0.4858), ('other optimizer parameter', 0.4795), ('pytorch', 0.4659), ('radam optimizer', 0.437)]","[-1.03347592e-01 -2.27301251e-02 -8.55634212e-02 -3.47785763e-02
  3.02043632e-02 -2.32868474e-02 -8.89212824e-03  3.70500349e-02
 -1.00246862e-01 -3.00139859e-02  2.04492081e-02  6.66640513e-03
 -8.86767879e-02  2.82206479e-03  9.42704827e-03  7.10198134e-02
 -2.36731097e-02  8.74943510e-02 -2.88491808e-02 -1.64818481e-01
 -4.31974279e-03 -7.22023025e-02  1.85409151e-02  2.16426142e-03
 -3.03854514e-02  4.53413129e-02 -3.72490585e-02  2.35106354e-03
  8.66314620e-02 -1.77235138e-02  2.24775430e-02 -1.03935208e-02
  2.58255955e-02 -2.65909731e-03  1.61180757e-02  2.62969416e-02
 -7.55960867e-02  5.04776184e-03 -2.43778024e-02  1.14358617e-02
 -2.45196931e-02  2.37544719e-03 -7.00047463e-02 -6.66656420e-02
  2.48111114e-02  3.62986289e-02 -1.20606821e-03 -2.33418830e-02
 -4.10753712e-02 -8.28473717e-02 -1.12277485e-01 -1.52936857e-02
 -9.78245288e-02  4.70586754e-02  5.21769077e-02 -2.43696831e-02
  5.69481030e-03 -7.02994913e-02  5.60978316e-02 -2.39476673e-02
 -8.26667715e-03 -2.08299141e-02 -2.61450261e-02 -4.09157667e-03
 -3.47060151e-02  1.00331148e-02  2.23668236e-02 -4.70886938e-03
  1.06668025e-01 -3.22305085e-03 -2.98309103e-02 -2.32524369e-02
 -4.75957021e-02  6.46933839e-02  9.37569514e-03  1.13074947e-02
  9.61152613e-02 -2.44241469e-02  8.18539690e-03 -1.28518835e-01
  4.71217837e-03 -1.84809007e-02 -2.35957243e-02  3.43775600e-02
  1.76627934e-02  6.06887741e-03 -5.04029579e-02 -1.94187239e-02
  6.29338920e-02  1.08886808e-02  2.16991939e-02 -7.22968727e-02
 -3.79725024e-02 -1.07736159e-02 -2.12152377e-02 -3.63491513e-02
  1.98195726e-02 -5.24276793e-02 -5.08648641e-02  2.62638628e-02
 -2.92491913e-02 -8.66430327e-02  1.08331128e-03 -3.25211585e-02
 -1.15686856e-01 -2.49454267e-02  1.65721960e-03  1.29590511e-01
  1.03240170e-01  2.02040542e-02 -1.93136744e-02  3.49057280e-02
 -1.93445086e-02 -7.62531161e-02  4.05471176e-02 -1.91449262e-02
 -5.32967374e-02  1.94168855e-02  7.57578686e-02  7.87494928e-02
 -5.54974712e-02  2.75814487e-03 -4.93349694e-03  6.34795800e-02
 -2.07807608e-02 -5.16162370e-04 -1.05252042e-01  1.20346774e-32
  3.63834086e-03  3.74327227e-02 -5.79438023e-02 -1.11642234e-01
 -6.66993335e-02 -1.20194703e-02  4.37467918e-02  1.88768730e-02
 -1.34926811e-02 -5.27116731e-02 -6.46453211e-03  1.59991381e-03
 -7.65625089e-02  7.77860135e-02  4.06090822e-03 -9.64355990e-02
  3.21829356e-02  7.56329000e-02  8.55833013e-03 -2.67984520e-04
  3.85698788e-02  7.75022525e-03 -3.75014776e-03  7.94423148e-02
  2.23344266e-02  2.95483917e-02  1.32659795e-02  9.87548963e-04
 -7.37006664e-02  1.96260661e-02  2.24898048e-02  1.08446255e-02
 -3.12083792e-02  2.80715004e-02 -6.21917984e-03  1.59058273e-02
 -7.91455507e-02 -6.15428127e-02 -6.48507522e-03 -2.39241496e-02
 -6.72309101e-02  6.90570101e-02  5.00594825e-03 -5.98285021e-03
  4.60777410e-05  6.96726795e-03 -9.10422876e-02  1.61337540e-01
  4.74956185e-02  1.92169752e-02 -1.28770486e-01  7.74571374e-02
  4.30742949e-02  4.91672941e-02  2.15862542e-02 -1.95444692e-02
  3.73272970e-02  3.49681787e-02  1.10738181e-01  6.72906712e-02
 -7.35598728e-02  3.53870019e-02 -2.64367200e-02 -5.19741252e-02
  2.00998504e-02 -3.93777937e-02  1.44817820e-02 -5.76266088e-02
 -4.31194268e-02  4.90421802e-02 -1.27279133e-01  4.05067839e-02
  6.33470118e-02  1.17645394e-02  4.55449335e-02 -5.67911267e-02
  3.23641859e-02 -1.48308799e-02 -6.78908005e-02 -6.56177998e-02
 -1.22523829e-01  7.53834471e-02  2.20808331e-02 -6.27822578e-02
 -3.39393392e-02 -4.83485684e-02 -2.39734389e-02 -3.94633152e-02
 -2.09652111e-02 -3.82588208e-02 -8.57854933e-02  7.16592302e-04
  2.72836778e-02  7.78354108e-02 -5.03279790e-02 -1.01632646e-32
  2.03121472e-02 -1.66252274e-02 -1.22242020e-02  1.27038747e-01
  3.74960490e-02  4.92119081e-02 -9.50201042e-03 -7.24513531e-02
 -6.20929711e-03  3.24065611e-02  4.86300848e-02 -7.43681286e-03
 -1.86009873e-02 -4.33806553e-02  3.34917605e-02  2.50917673e-02
  3.34710963e-02 -7.20543936e-02  9.52362865e-02  2.82663899e-03
 -6.07447140e-02  1.04381584e-01 -5.81008606e-02 -1.71252135e-02
 -5.25104627e-02  4.00558189e-02 -1.29361944e-02 -1.28952861e-02
  1.53717203e-02  2.37212284e-03  2.16963273e-02  2.13053096e-02
 -5.10016046e-02  6.69518709e-02 -3.89303826e-02 -2.22985400e-03
  8.92111510e-02 -1.20377950e-02 -1.91579238e-02  6.95517883e-02
  1.55413538e-01  6.95917904e-02  5.89311011e-02  5.17424345e-02
 -1.05618030e-01 -2.73695518e-03 -9.91481915e-02  7.59098772e-03
  4.75669140e-03 -2.13001599e-03  6.62462146e-04 -9.81768593e-03
 -3.46842147e-02  1.80289075e-02  1.34397140e-02 -1.71741899e-02
  3.75616513e-02  1.13001373e-02  7.85019994e-03  3.61587480e-02
 -2.16447725e-03 -6.46581277e-02  4.19906862e-02  1.25551485e-02
 -2.95578185e-02 -9.51238500e-04  6.26459569e-02  5.25180586e-02
 -1.15494849e-02 -6.11376890e-05  3.26394439e-02  6.74930513e-02
  8.49563926e-02  6.80755004e-02 -6.72205910e-02 -2.62110345e-02
  4.96581718e-02  9.21242163e-02  1.20743029e-02  1.17793621e-03
  5.86213320e-02  1.71321891e-02 -1.22426795e-02  2.75180247e-02
  1.30645989e-03  4.74200249e-02  1.05793336e-02  7.13867322e-02
  3.37413624e-02 -4.78140339e-02 -1.33112175e-02 -1.28476229e-02
  1.08595699e-01  7.79315159e-02  3.66071127e-02 -4.04836307e-08
 -4.06278521e-02 -2.56177112e-02  8.55390728e-03  1.44782700e-02
  6.21005297e-02 -2.41787601e-02  2.56390981e-02  1.38741925e-01
 -3.08998786e-02 -4.29861993e-02  4.92081679e-02  3.98007501e-03
 -1.05180396e-02  3.23255174e-03  6.42887279e-02 -2.18999647e-02
 -3.75812501e-02  3.34222354e-02  7.03116693e-03 -5.09099253e-02
  1.85517278e-02 -1.87267754e-02 -5.62818274e-02 -1.05669256e-02
  1.82041246e-02 -1.34213781e-02  4.96277139e-02 -1.34753026e-02
  6.65171677e-03  4.31977585e-02 -5.62711097e-02  2.24897265e-02
  9.53290910e-02 -5.57136722e-02  1.03445746e-01  8.50646719e-02
 -7.43621066e-02 -1.83979198e-02 -7.03200847e-02  4.97267246e-02
 -3.86855975e-02  1.23033999e-02 -3.21214050e-02 -1.35378782e-02
  6.38264790e-02 -3.15303393e-02 -3.24694323e-03 -7.99142867e-02
 -3.41946520e-02 -1.88622568e-02  4.28051082e-03  4.40544002e-02
 -1.42803148e-03 -5.23497630e-03 -2.80632847e-03  5.60432673e-02
 -5.53077844e-04 -1.20322309e-01  3.32810059e-02 -2.39248425e-02
  5.48806041e-02 -1.82794109e-02 -2.67418288e-02  4.18577120e-02]",2,2
torch-cluster,2,this package consists of a small extension library of highly optimized graph cluster algorithm for the use in pytorch the package consists of the following clustering algorithm all included operation work on varying data type and are implemented both for cpu and gpu update you can now install pytorch cluster via anaconda for all major o pytorch cuda combination given that you have pytorch installed simply runwe alternatively provide pip wheel for all major o pytorch cuda combination see here to install the binary for pytorch simply runwhere cuda should be replaced by either cpu cu cu or cu depending on your pytorch installation to install the binary for pytorch pytorch and pytorch simply runwhere cuda should be replaced by either cpu cu cu or cu depending on your pytorch installation note binary of older version are also provided for pytorch pytorch pytorch pytorch pytorch and pytorch following the same procedure for older version you might need to explicitly specify the latest supported version number in order to prevent a manual installation from source you can look up the latest supported version number here ensure that at least pytorch is installed and verify that cuda bin and cuda include are in your path and cpath respectively e g then run when running in a docker container without nvidia driver pytorch need to evaluate the compute capability and may fail in this case ensure that the compute capability are set via torch cuda arch list e g a greedy clustering algorithm of picking an unmarked vertex and matching it with one it unmarked neighbor that maximizes it edge weight the gpu algorithm is adapted from fagginger auer and bisseling a gpu algorithm for greedy graph matching lncs a clustering algorithm which overlay a regular grid of user defined size over a point cloud and cluster all point within a voxel a sampling algorithm which iteratively sample the most distant point with regard to the rest point computes graph edge to the nearest k point args computes graph edge to all point within a given distance args cluster point in x together which are nearest to a given query point in y batch x y vector need to be sorted sample random walk of length walk length from all node index in start in the graph given by row col torch cluster also offer a c api that contains c equivalent of python model,"[('pytorch installation', 0.6712), ('pytorch cluster', 0.6705), ('major o pytorch cuda combination', 0.5991), ('pytorch pytorch', 0.5505), ('pytorch pytorch pytorch pytorch pytorch', 0.5408), ('nvidia driver pytorch', 0.5279), ('pytorch', 0.5173), ('least pytorch', 0.438), ('graph cluster algorithm', 0.3873), ('cuda', 0.3751)]","[-5.87383136e-02 -2.88651101e-02 -7.35648349e-02  1.12446281e-03
  5.75588606e-02 -4.30725864e-04 -6.28295913e-02  1.77553075e-03
 -9.35462043e-02 -7.00900033e-02 -4.67335340e-03 -3.88903506e-02
 -4.51069176e-02  1.13516571e-02 -3.19435187e-02  4.77550998e-02
  3.14851291e-02  7.71415532e-02 -1.73637941e-02 -1.30190402e-01
 -8.38285610e-02 -5.51399402e-02  1.84931960e-02  1.57120991e-02
  2.26961076e-02  6.26741117e-03  1.69190764e-02 -5.05509526e-02
  6.46710396e-02 -4.63425741e-03  3.38862538e-02  2.86914147e-02
  1.06775999e-01  4.91387658e-02 -7.41269265e-04 -1.86330248e-02
  3.05852923e-03 -1.30701074e-02 -6.53731897e-02  8.01677164e-03
  1.49347838e-02  1.25824027e-02 -3.58325616e-02  1.59273967e-02
  1.10810471e-03 -3.50874290e-02 -3.34038138e-02  2.52856128e-03
  3.84883559e-03 -1.28414661e-01 -4.30766568e-02 -2.23063063e-02
 -1.19187720e-01 -7.42940418e-03 -1.83719080e-02  3.02473567e-02
  3.50025669e-02 -8.68044645e-02  8.45480263e-02 -2.81077549e-02
  9.98288020e-02 -6.23315126e-02 -3.57604474e-02  6.34784773e-02
  1.06854998e-02  3.11668403e-02  5.84694929e-02 -3.93608175e-02
  9.85389203e-02 -9.75067765e-02  2.58811265e-02  1.02586281e-02
 -7.48417377e-02  4.07024585e-02 -3.13779004e-02  2.44526062e-02
  7.50354230e-02  2.67128479e-02 -1.57750119e-02 -5.29664047e-02
 -6.21024221e-02  2.61824634e-02  3.02955862e-02  1.88107565e-02
  6.36443943e-02  4.53935517e-03 -9.93070155e-02  1.58211589e-02
  3.03864200e-02 -5.35692088e-02  3.59220654e-02 -1.62614789e-02
 -5.59387431e-02  6.88209832e-02 -1.68048441e-02  9.54694767e-03
  5.67634664e-02 -9.45521072e-02 -1.69293098e-02  2.99178623e-02
 -4.44704071e-02 -7.51709118e-02  1.80549771e-02  4.99372296e-02
 -5.26766777e-02  4.98200059e-02 -1.67232621e-02  5.90166748e-02
  3.49823385e-02  4.52292524e-02  1.86010897e-02  5.50502241e-02
 -1.28233125e-02 -6.79848017e-03  1.46550089e-02  2.67602876e-02
 -4.00030352e-02  2.06421129e-02  3.97602133e-02  2.25903355e-02
 -5.76925911e-02 -2.46036705e-02 -3.08763050e-02 -2.18177438e-02
 -1.18479412e-02  1.37236752e-02 -5.53278252e-02  1.07076008e-32
  1.13638556e-02 -1.38356620e-02  1.42299561e-02 -5.56860641e-02
  7.61609524e-02  9.35643387e-04 -6.23542443e-03  1.27261654e-02
  7.14669079e-02 -7.40865618e-02 -3.05909067e-02 -1.99794043e-02
 -3.02123521e-02  4.14812490e-02  2.14804243e-02 -6.64443225e-02
 -5.66758541e-03  7.77117833e-02  1.64027829e-02 -4.96551534e-03
  6.12888150e-02  6.64822608e-02 -4.98469621e-02  8.33568275e-02
 -6.97858110e-02  8.89788866e-02 -1.14622777e-02 -5.50399348e-02
  4.46474627e-02  2.38485448e-02 -3.38751599e-02  2.09259661e-03
 -4.83970996e-03  2.09557284e-02 -2.87157875e-02  2.24411748e-02
 -2.06740610e-02 -4.08435054e-02  2.03818064e-02 -5.44035919e-02
 -7.58393258e-02  6.86966702e-02 -1.86197851e-02 -1.21544540e-01
  2.21480858e-02  2.51686499e-02 -1.59119498e-02  5.95879257e-02
  3.11924573e-02 -2.22800076e-02 -1.16765819e-01  4.13637683e-02
 -1.17007606e-02  1.27932623e-01  2.89681163e-02 -3.18503678e-02
  1.90141611e-02 -5.31673711e-03  1.18071653e-01  4.94070537e-02
 -2.32916302e-03  3.78038362e-02  1.55993765e-02 -2.04197522e-02
 -3.25251669e-02 -2.04535779e-02 -3.97661403e-02  1.18989674e-02
 -2.30068713e-02  1.19840786e-01 -8.07738379e-02  8.16138163e-02
 -5.41940928e-02 -1.42369093e-03  6.68812543e-02 -2.34566964e-02
 -4.40937579e-02 -7.83201074e-04 -6.87161237e-02 -1.37077048e-02
 -1.21262319e-01  3.01780496e-02 -2.33391430e-02  2.39113905e-03
 -4.06057239e-02 -9.05946456e-03 -1.31115858e-02  2.81829163e-02
 -4.55602584e-03 -1.62771717e-02 -8.75525847e-02  1.62684522e-03
  7.24042356e-02  5.21816127e-02  2.56855898e-02 -9.67044966e-33
  1.28174862e-02 -2.22127885e-04  6.06275871e-02  7.18739927e-02
  4.47539501e-02 -9.63629317e-03 -1.16252914e-01 -9.28054973e-02
  3.57493311e-02  3.03782951e-02  1.25287175e-02 -1.28079671e-02
  2.36189496e-02 -1.90517809e-02 -2.65807728e-03  2.78048478e-02
  2.17371248e-02  7.21856728e-02 -8.21627676e-02  1.25445537e-02
 -9.30455253e-02  6.22136295e-02 -7.96254426e-02 -6.42865077e-02
  4.47616987e-02 -1.77074410e-02 -6.03636168e-03 -3.89875397e-02
 -8.35191645e-03  2.86089666e-02 -1.04749529e-02  4.91304286e-02
 -6.89945593e-02  4.63981777e-02 -2.88681798e-02  3.14791389e-02
  3.02949958e-02 -2.99216192e-02 -4.38651517e-02  4.97627445e-02
  1.56930447e-01  8.96881148e-02 -9.62050408e-02 -1.51923709e-02
 -2.02929825e-02  3.05908732e-02 -8.91377628e-02  2.36061215e-02
 -6.28267378e-02  2.45133601e-03  1.87019526e-04  1.57896671e-02
  4.30672988e-02  1.81801897e-02  7.22944460e-05  4.34346758e-02
  1.43421426e-01  9.77256224e-02 -3.57417986e-02 -1.78800616e-02
 -5.01692593e-02 -9.59675163e-02 -4.37923707e-02 -4.26719822e-02
 -2.30052192e-02 -5.81143722e-02 -8.63848533e-03  1.36663079e-01
  2.16720160e-03  1.12253344e-02 -3.97317149e-02  8.29366967e-02
  5.93961291e-02  8.05994496e-02 -6.89030215e-02  5.65121472e-02
  1.63307842e-02  1.56478062e-02  2.84825452e-02 -1.48767857e-02
  1.25807291e-03 -5.91418147e-02  6.30902052e-02  2.95235254e-02
  3.34668644e-02  6.49190769e-02  6.01245761e-02 -1.45010939e-02
  7.78054893e-02  1.20028341e-02 -4.52641360e-02 -2.03924701e-02
  1.70112401e-02  7.94714317e-02  3.82623933e-02 -3.71274318e-08
 -6.64676577e-02 -3.72833312e-02 -3.02014630e-02  6.70551276e-03
  6.28243014e-02  2.17264649e-02  3.10876779e-02  1.75715074e-01
 -5.24007864e-02  4.34498712e-02  5.71347661e-02 -4.14049476e-02
 -5.24942353e-02 -9.75266099e-03 -1.19592308e-03  7.99761638e-02
  1.99101623e-02 -3.36405449e-02  4.78135049e-02 -1.63757671e-02
 -2.02566311e-02 -3.28178778e-02  3.36730562e-04  4.37483341e-02
 -5.86063080e-02 -9.14799869e-02  3.49546112e-02  1.10792117e-02
 -9.98732448e-03 -3.64460126e-02 -8.02125335e-02 -4.69086915e-02
  3.97388935e-02 -2.30205134e-02  1.13086365e-01  7.12760091e-02
  1.86869316e-02 -8.18344578e-03  1.01585072e-02  4.12940197e-02
 -4.34768051e-02 -9.58463643e-03 -1.70124229e-02 -5.17211407e-02
  3.97840925e-02  4.90332022e-02  2.32897187e-03 -3.36253792e-02
 -3.72148268e-02 -1.25871494e-03 -3.43967453e-02  2.68026758e-02
 -1.47123458e-02  2.65249051e-03 -5.64645976e-03  8.17359239e-02
 -1.01345360e-01 -5.49537577e-02 -2.51148101e-02 -6.99332580e-02
  3.05389948e-02  2.53114589e-02  3.61296861e-03 -6.44762442e-02]",2,2
keract,2,tested with tensorflow rc may you have just found a way to get the activation output and gradient for each layer of your tensorflow kera model lstm conv net important note the nested model are not well supported the recent version of tensorflow made it extremely tricky to extract the layer output reliably please refer to the example section to see what is possible fetch activation node layer output a numpy array for a kera model and an input x by default all the activation for all the layer are returned return dict layer name specified by output format activation of the layer output node numpy array exampleplot the activation for each layer using matplotlibinputs are the ordering of the dimension in the input channel last corresponds to input with shape batch step channel default format for temporal data in kera while channel first corresponds to input with shape batch channel step plot heatmaps of activation for all filter overlayed on the input image for each layerinputs are the output is a dictionary mapping each trainable weight to the value of it gradient regarding x and y return dict layer name specified by output format grad activation of the layer output node numpy array the output is a dictionary mapping each layer to the value of it gradient regarding x and y it return the activation example are provided for in the case of mnist with lenet we are able to fetch the activation for a batch of size we can visualise the activation here s another example using vgg a cat output of the first convolutional layer of vgg also we can visualise the heatmaps of the activation,"[('tensorflow kera model lstm conv', 0.604), ('tensorflow', 0.4498), ('layer output node numpy array', 0.442), ('shape batch channel step plot heatmaps', 0.4308), ('possible fetch activation node layer output', 0.4152), ('layer output', 0.394), ('activation output', 0.3938), ('tensorflow rc', 0.3923), ('first convolutional layer', 0.3451), ('gradient', 0.2986)]","[-3.23360339e-02 -1.01016730e-01  2.30908208e-02  8.69999360e-03
  6.08208925e-02  7.51814395e-02 -7.09232390e-02 -3.47828642e-02
 -1.06580704e-01 -4.75141145e-02 -5.90424333e-03 -7.62057826e-02
 -3.66924629e-02 -5.45547307e-02  1.00605481e-03  6.72878406e-04
  2.46725976e-02  5.12350872e-02 -2.88798586e-02 -9.25852880e-02
  1.10883461e-02  2.83283368e-02 -4.74717095e-02  1.75821781e-02
 -2.46669166e-02  2.64018364e-02  1.75841600e-02  8.79837945e-03
  1.08883753e-02 -7.48879611e-02  8.12141895e-02  2.09762305e-02
  3.03051956e-02  8.56241882e-02 -3.23610865e-02 -3.10583953e-02
 -8.04250613e-02 -1.01258278e-01 -2.33673956e-02 -1.84847284e-02
  2.79764347e-02 -3.98962349e-02 -3.72942686e-02 -2.12743711e-02
  8.61134827e-02  1.45923328e-02  1.90654136e-02 -4.18287963e-02
 -2.34547649e-02 -1.29505032e-04 -3.92331295e-02 -5.49088754e-02
 -2.38766242e-02  9.38752517e-02 -3.13146152e-02 -3.47270854e-02
  8.84287801e-05 -1.64626334e-02 -1.05900913e-02 -2.72932108e-02
 -5.50812706e-02 -2.53628492e-02 -5.63923419e-02 -1.61511805e-02
  4.73368960e-03 -1.03236521e-02 -1.85382050e-02  5.64773232e-02
  1.46132484e-01 -3.79462838e-02  7.67906103e-03  4.49125096e-03
 -2.49986351e-02 -4.92555015e-02 -1.13229547e-02 -1.47313420e-02
  1.48836821e-01  5.03008952e-04 -3.18507478e-02 -8.22837874e-02
 -3.98216955e-02  1.08358040e-02  4.37090136e-02 -6.97948635e-02
 -7.91148096e-02 -1.13579398e-03 -3.17087509e-02  2.77185626e-02
 -7.94244837e-03 -1.41365230e-02 -2.52950173e-02  1.70274209e-02
 -8.24138299e-02 -3.59052084e-02  8.16203803e-02  4.99980301e-02
 -3.22041251e-02 -4.72579189e-02 -1.70195382e-02 -5.06140757e-03
 -1.95332547e-03 -8.57304484e-02  6.12696807e-04  4.94176708e-02
 -5.81353195e-02  2.63409056e-02  3.78760733e-02  2.43725348e-02
 -1.33774290e-02  2.12742966e-02 -2.41658073e-02  4.64843288e-02
  3.19555998e-02 -4.21842523e-02  1.61038160e-01 -2.84058806e-02
  2.28798613e-02 -3.30653973e-02  3.47051360e-02  6.73957020e-02
 -4.71528769e-02 -2.23848876e-02  4.22446616e-02  3.08894031e-02
 -8.64697546e-02  1.16929468e-02 -5.34506999e-02  8.35897265e-33
  1.62298866e-02 -1.50147937e-02 -7.76992664e-02 -2.56614340e-03
  8.67273360e-02 -2.11186167e-02  4.28993106e-02 -4.98268045e-02
  6.15259167e-03 -3.78805138e-02 -1.28398627e-01  9.00644287e-02
 -3.02540343e-02  1.09556794e-01 -5.79097234e-02 -3.50507088e-02
  2.42291186e-02  1.31884515e-02  1.09807812e-02  6.10220693e-02
 -9.73736111e-04 -1.65011745e-03  4.07217219e-02  8.41247663e-02
  3.57855372e-02  2.36575622e-02  2.00819671e-02  2.31545568e-02
 -8.40355679e-02 -5.60928136e-03 -3.22964899e-02  1.62427616e-03
  1.60023272e-02 -5.32724299e-02 -3.84347439e-02  2.04173420e-02
 -3.44246887e-02  1.11744376e-02 -4.40738238e-02 -1.06405370e-01
 -2.38100961e-02  4.89913635e-02 -2.12362548e-03  5.69153752e-04
 -9.32180360e-02  8.90091062e-03 -7.07990676e-02  1.33287814e-02
  8.46887007e-02 -8.32324009e-03  1.66606382e-02 -4.28022072e-02
 -3.49967182e-02 -1.11876749e-01  2.97929328e-02  4.50740941e-02
  3.88255455e-02  6.36391938e-02  9.05630067e-02  1.50522927e-03
 -1.66447181e-02  6.10621348e-02  2.43859142e-02 -2.69406987e-03
 -2.35070437e-02 -2.29792893e-02 -7.81584159e-02  1.56166907e-02
  5.51955365e-02  2.65127569e-02 -1.03589803e-01  5.75469509e-02
  2.34686770e-02 -3.66873965e-02  6.92424625e-02  3.95041890e-02
 -3.83361708e-04 -1.04671471e-01 -7.44174048e-02  9.14669335e-02
  1.08812544e-02  2.14213622e-03 -6.18010527e-03  5.52148744e-03
  2.61345319e-02 -2.92438697e-02  4.70563304e-03  8.11547041e-03
  7.10520297e-02 -1.93418991e-02 -6.89355731e-02  1.56603307e-02
  5.56269251e-02  5.89763327e-03 -6.01299703e-02 -6.52903431e-33
  2.50365902e-02  1.14606589e-01 -3.04821618e-02  8.16450045e-02
  4.50305343e-02  1.67422965e-02  1.04760081e-02 -5.97148854e-03
  2.58259457e-02  9.26673412e-02  2.41571609e-02 -7.06261694e-02
  2.66662575e-02 -5.66116199e-02  8.20252672e-02 -7.74331763e-02
 -4.12375405e-02 -6.23345971e-02  5.52298948e-02  5.63974725e-03
 -5.39021939e-02  6.83881417e-02 -9.84786302e-02  7.39829708e-03
 -8.81455466e-02 -1.66715831e-02 -5.41393645e-02  5.32747358e-02
  1.89707230e-03 -5.36991656e-02 -8.80178064e-03 -2.60366369e-02
  1.39358928e-02  7.39938319e-02  1.63816549e-02  7.09859282e-02
  8.36423784e-02  2.58467230e-03  4.18851227e-02 -2.76551135e-02
  7.67126158e-02  5.64648472e-02  3.48076224e-03  3.24919121e-03
 -6.69470206e-02 -8.23299866e-03  2.63220910e-02  2.84601972e-02
 -7.43113160e-02 -3.28990147e-02  1.67783108e-02 -2.55687889e-02
 -4.87937070e-02 -3.20855081e-02  1.22530467e-03 -4.39171121e-03
  5.04011512e-02  4.26241606e-02  8.45866129e-02 -6.33031502e-03
 -2.05398202e-02 -1.09279938e-01  1.16701517e-02 -7.07811713e-02
 -6.42516511e-03 -3.59253027e-03 -5.58251552e-02  1.69528686e-02
 -2.09725350e-02  5.74832112e-02  3.23432200e-02  1.65405907e-02
  4.24328111e-02  6.21449798e-02  1.89765394e-02 -5.69581650e-02
 -1.10548101e-01  8.59606732e-03  1.05150901e-02  8.05103406e-03
  1.98450238e-02  2.43403111e-02  1.06856758e-02  1.03240654e-01
  1.74320325e-01  1.00367904e-01 -7.40251224e-03  5.64301424e-02
  9.06722546e-02 -6.46143826e-03  2.59490944e-02  1.20625040e-02
 -3.94602912e-03  6.89316466e-02  5.41313998e-02 -3.56749723e-08
 -2.74987221e-02 -5.06171118e-03  7.10157827e-02  4.85476777e-02
  3.96161247e-03 -2.80951262e-02  8.87208059e-02  5.70194013e-02
  6.34597987e-02  8.37973319e-03 -1.83074828e-02  2.38749906e-02
 -8.21505114e-02 -3.20306942e-02  1.82122830e-02  7.17193782e-02
 -1.29197666e-03  6.69412017e-02  5.85577376e-02 -1.28435232e-02
  4.95267473e-02  8.19882676e-02  4.01066290e-03  5.18296547e-02
  3.24254259e-02 -2.74528600e-02  3.28242476e-03  7.71624073e-02
 -2.31968462e-02  4.40827198e-03 -4.24606986e-02 -1.80085152e-02
  6.14767186e-02 -5.42754643e-02  5.25747091e-02  8.18062515e-04
  5.28749637e-03  1.58158205e-02 -5.31961955e-02  6.64262325e-02
 -6.21863119e-02  4.18810211e-02 -8.07404891e-03 -2.44584512e-02
 -5.93955405e-02 -2.30334792e-03 -6.98990673e-02 -1.20503150e-01
 -1.63632166e-02 -3.73248048e-02 -1.39933834e-02  2.72719632e-03
 -2.29031518e-02  1.27493083e-01 -1.68975927e-02 -2.39931755e-02
 -6.46848977e-02 -7.45435357e-02  4.24459726e-02 -1.95858032e-02
 -7.74894701e-03  9.36466977e-02 -5.38760796e-03 -6.20254911e-02]",2,2
tfx,2,tensorflow extended tfx is a google production scale machine learning platform based on tensorflow it provides a configuration framework to express ml pipeline consisting of tfx component tfx pipeline can be orchestrated using apache airflow and kubeflow pipeline both the component themselves a well a the integration with orchestration system can be extended tfx component interact with a ml metadata backend that keep a record of component run input and output artifact and runtime configuration this metadata backend enables advanced functionality like experiment tracking or warmstarting resuming ml model from previous run please see the tfx user guide the tfx roadmap which is updated quarterly for detailed previous and upcoming change please check heretfx is an open source project and we strongly encourage active participation by the ml community in helping to shape tfx to meet or exceed their need an important component of that effort is the rfc process please see the listing of current and past tfx rfcs please see the tensorflow request for comment tf rfc process page for information on how community member can contribute the following table describes how the tfx package version are compatible with it major dependency pypi package this is determined by our testing framework but other untested combination may also work,"[('tfx roadmap', 0.6667), ('tfx component tfx pipeline', 0.6515), ('tfx', 0.5739), ('tfx package version', 0.5523), ('tensorflow request', 0.5496), ('tensorflow', 0.5338), ('tfx user', 0.4899), ('past tfx', 0.4892), ('tfx component interact', 0.4496), ('ml pipeline', 0.3791)]","[ 2.08618422e-03 -8.31200480e-02  4.32632640e-02 -1.14660628e-01
  6.59802323e-03 -8.74310918e-03 -6.42452436e-03  4.05299850e-02
 -3.64386104e-02 -7.03807324e-02 -8.02865028e-02 -4.55006473e-02
 -7.24451467e-02  4.06415984e-02 -8.81151762e-03 -7.27842469e-03
 -2.85410875e-04 -5.99224027e-03  1.36781102e-02 -8.92363638e-02
 -7.71572515e-02  6.08975254e-03  4.05783020e-03  2.16495134e-02
 -3.87537293e-02  1.50499102e-02 -2.38474850e-02 -3.99017595e-02
  2.38556471e-02  3.05082500e-02 -2.23926939e-02  1.06580570e-01
 -2.44720336e-02 -5.18160127e-02 -6.66025132e-02  4.11115736e-02
 -3.26076820e-02 -3.11224684e-02 -4.92105596e-02 -3.64058539e-02
 -3.55234444e-02 -8.63928720e-02 -4.54095900e-02 -1.80765670e-02
  6.29705340e-02 -8.61306340e-02  7.59466365e-02 -1.07593589e-01
 -3.87062952e-02  7.11187199e-02 -1.03093334e-01 -5.55585511e-02
 -6.05960414e-02  3.86689231e-02 -5.45631647e-02  1.89852063e-02
  7.76217356e-02  4.18865867e-03 -2.50513013e-02 -7.42461998e-03
 -8.44223276e-02 -6.45157695e-02  4.56882967e-03  8.12440515e-02
  4.39387672e-02  9.08545256e-02 -8.57633725e-02 -9.74525791e-03
  9.06214714e-02 -8.99670422e-02 -9.19255391e-02  2.13849712e-02
 -7.73053318e-02 -3.23910788e-02 -3.62762883e-02  5.12003414e-02
  9.61036682e-02 -6.77203611e-02  4.25166003e-02 -6.55716360e-02
  2.74757692e-03  2.83318814e-02  4.12220061e-02 -6.74606883e-04
  1.11444509e-02  4.29468267e-02  3.85397710e-02  1.81447137e-02
  2.76336968e-02 -6.32662978e-03  4.16870676e-02  9.48745105e-03
  3.54959555e-02  8.59388709e-02  5.35657220e-02  6.99646119e-03
 -6.49357215e-02 -2.43809801e-02 -8.19147285e-03  1.97614580e-02
 -1.18347593e-01 -4.60982509e-02  7.07688183e-02  5.46666756e-02
 -4.71209176e-02  6.39173985e-02 -2.03337218e-03 -4.31031957e-02
  2.47001275e-02 -2.21021231e-02 -1.57601535e-02  3.36327148e-03
  2.96228081e-02 -6.66495711e-02  5.88668734e-02 -2.48071942e-02
 -2.80168466e-02  3.83766106e-04  3.14983390e-02  7.91664869e-02
 -8.05046335e-02 -1.32118752e-02 -1.88344661e-02  1.39557524e-02
  2.61301920e-02  2.26538610e-02 -3.05769406e-02  7.26860540e-33
 -4.41216975e-02  2.57756840e-02  2.98899729e-02  7.87419677e-02
  6.52719438e-02 -5.54958954e-02  9.73121002e-02 -2.69026179e-02
 -4.51145209e-02 -1.62532534e-02 -2.03062538e-02  8.88038874e-02
 -3.87018137e-02  8.54348764e-02 -5.04416712e-02 -1.53376460e-01
  2.13153120e-02  4.34989668e-02 -3.15971579e-03  7.37398192e-02
  8.66376013e-02  7.41797537e-02 -3.65470257e-03  2.35401765e-02
  1.05559245e-01 -1.86073186e-03  1.92914847e-02  4.49852347e-02
 -5.44795021e-02  9.89395194e-03 -7.95584619e-02 -4.10693847e-02
  8.96374695e-03 -1.07683977e-02  3.48565094e-02  9.76828206e-03
 -3.88288051e-02 -1.54449139e-02 -4.31764238e-02 -3.92050482e-02
  1.13878781e-02  4.25330922e-02 -6.27993746e-03 -7.27181956e-02
 -3.47040035e-02 -3.56445350e-02 -6.23087073e-03 -4.29531001e-02
  3.89248468e-02 -3.29680815e-02  2.33217813e-02  5.95336314e-03
  1.23311738e-02 -3.38266529e-02  4.07961681e-02 -5.42085916e-02
  1.28467502e-02 -1.72108470e-03  1.04353800e-01  8.86632651e-02
 -5.43421619e-02  8.64924584e-03  1.44805862e-02  2.00205352e-02
  2.56187469e-02  1.54437367e-02 -6.17016340e-03 -3.25112864e-02
  2.31057927e-02  7.22310320e-03 -7.05968291e-02  8.48678350e-02
 -2.33802851e-03 -4.56376970e-02  6.68597668e-02 -4.19960208e-02
 -3.54675464e-02 -4.25400548e-02  2.37301756e-02 -3.26801389e-02
 -1.22940853e-01 -2.34015863e-02  3.85540798e-02  6.77976757e-02
  5.00945859e-02 -2.49421820e-02 -6.69236556e-02  6.99285641e-02
  2.59133782e-02 -3.18117253e-02 -7.60992244e-02 -1.06886392e-02
  2.68334001e-02  4.94020358e-02  1.30193830e-02 -6.82144074e-33
 -3.34252534e-03 -1.52678508e-02 -7.71641731e-02  8.08375478e-02
  2.07102839e-02  3.52537408e-02  3.93360108e-02 -3.39320675e-02
  8.70167091e-02  7.12465569e-02  5.69284447e-02 -1.67398562e-03
  1.77663937e-02 -9.34617966e-02 -3.91330570e-02 -9.20073092e-02
  2.92060226e-02 -8.21515918e-02 -1.91970766e-02 -2.51687560e-02
 -4.99554276e-02  1.26040122e-02 -6.07905388e-02 -7.65126338e-03
 -2.46442370e-02  5.70586771e-02  1.16761932e-02 -2.00301353e-02
 -5.02453558e-02 -5.26074804e-02 -6.97695985e-02 -3.35578360e-02
  3.43338773e-02  2.03126576e-02  2.80888062e-02  5.73644601e-02
  2.87894942e-02  7.79191107e-02  5.76100647e-02  7.85148889e-03
  4.27219905e-02 -1.71779506e-02  5.92766404e-02  1.23680746e-02
 -1.15186594e-01  1.99648719e-02 -5.31924479e-02 -6.41204044e-03
  5.96872484e-03 -2.44708005e-02 -1.31227085e-02  5.10664023e-02
 -1.80737523e-03 -8.55982974e-02 -5.71984574e-02  4.90289852e-02
  1.18588895e-01 -2.48062108e-02  2.64605489e-02  5.15595675e-02
  5.68781570e-02  4.85966355e-02  7.40765035e-02 -2.29463466e-02
  2.79023517e-02 -1.39582949e-02 -1.37264162e-01  2.61514820e-02
 -1.25446785e-02  3.47586139e-03 -3.11040562e-02  2.79822350e-02
 -4.29671705e-02  5.50760254e-02  5.11996038e-02 -3.08754276e-02
  2.98304074e-02  9.69782821e-04  8.20950940e-02  2.65747192e-03
  4.75553945e-02  2.55370773e-02  2.69056503e-02  1.06674664e-01
  6.66271225e-02  5.99872209e-02  2.62816753e-02 -8.80567431e-02
  4.37478460e-02 -1.49601419e-02  2.96287946e-02 -9.88541264e-03
 -3.56362686e-02  4.61340137e-02  4.22031358e-02 -3.67268704e-08
 -9.02511477e-02  4.14356589e-02 -1.57976593e-03  3.34905684e-02
 -1.27499383e-02  4.27084006e-02  5.31689972e-02  1.16539210e-01
  4.93586957e-02  6.84180763e-03  3.96454930e-02 -1.04409698e-02
 -4.72928435e-02  4.06632535e-02  5.33986539e-02 -3.05722412e-02
 -8.94945562e-02 -1.60176028e-02 -2.87367050e-02 -9.18210819e-02
  1.94308590e-02  9.98499617e-03  3.22301164e-02 -1.21372640e-02
  2.26529501e-02 -6.11902997e-02  4.02558036e-02  4.05737162e-02
  3.56856100e-02 -7.92398956e-03 -1.04140520e-01 -3.83433960e-02
 -6.17330298e-02  1.56313404e-02  7.56167918e-02  6.18378446e-02
 -2.77525038e-02 -1.84619501e-02 -5.81654012e-02  5.13020866e-02
 -5.08404411e-02  5.78965172e-02  2.93171890e-02 -8.07506144e-02
  5.97479870e-04  5.16942963e-02 -5.85813336e-02 -4.09226678e-02
  6.31096819e-03  8.07134621e-03 -4.00321893e-02  7.68810436e-02
 -6.72617257e-02  1.21073201e-01  3.16060297e-02  5.11302762e-02
 -1.66726038e-02 -6.90992102e-02  3.54945362e-02 -2.92530414e-02
 -4.21847636e-03 -1.86563674e-02  3.14748324e-02 -6.96910918e-02]",2,2
pytoda,2,pytoda paccmann pytorch dataset classesa python package that eas handling biochemical data for deep learning application with pytorch pytoda ship via pypi please find the full documentation here for development setup we recommend to work in a dedicated conda environment activate the environment install in editable mode note the conda env ship with the official rdkit implementation but the pip installation overwrites the rdkit package with the community contributed pypi package called rdkit pypi this is intentional because pytoda is distributed via pypi too and most user will thus depend on rdkit pypi keep in mind that rdkit pypi might contain bug or be outdated wrt rdkit if developer experience issue with rdkit pypi they can temporarily uninstall rdkit pypi and will then fall back on using the proper rdkit package for some example on how to use pytoda see hereif you use pytoda in your project please cite the following,"[('rdkit pypi', 0.6436), ('pytoda paccmann pytorch dataset classesa python package', 0.629), ('rdkit pypi keep', 0.5728), ('rdkit package', 0.559), ('proper rdkit package', 0.5541), ('official rdkit implementation', 0.527), ('pypi package', 0.5116), ('pytorch pytoda ship', 0.4982), ('pytoda', 0.4962), ('dedicated conda environment', 0.4648)]","[-4.04270403e-02 -2.57268231e-02 -5.59686981e-02 -2.60999594e-02
  3.68852541e-02 -4.97038253e-02 -4.00608815e-02  1.78395454e-02
 -9.79328305e-02  3.24643194e-03  4.82812151e-03 -2.24816110e-02
 -8.62409770e-02  1.78070534e-02  1.80023741e-02  3.68183032e-02
  8.05400126e-03  5.42931780e-02 -1.41085777e-02 -5.75465970e-02
 -3.90264243e-02  5.39591573e-02  3.04708686e-02  6.26890957e-02
 -5.20251915e-02 -4.30351794e-02  3.66587155e-02 -2.99162809e-02
  2.83499844e-02 -5.20140305e-02 -4.90114950e-02  6.37920573e-02
  1.21033275e-02 -4.27799718e-03 -3.73486616e-03  8.24427083e-02
 -1.50238781e-03 -4.32537906e-02  3.49636935e-02 -1.56903714e-02
  7.74674118e-03 -3.37212980e-02 -4.62585650e-02  6.67037368e-02
  2.73965718e-03 -1.19799422e-03 -5.18734707e-03 -4.25247476e-02
  5.70900030e-02 -9.25851464e-02 -5.32482378e-02 -8.48682690e-03
 -8.44165385e-02  4.96693775e-02 -1.94577258e-02 -5.17253578e-03
  1.73931511e-03 -1.36429246e-03  5.15835658e-02 -3.97134572e-02
 -1.11680932e-03  4.24738191e-02 -2.60286741e-02  2.71863472e-02
 -4.12055477e-02  3.73187736e-02 -8.09397101e-02  4.27241921e-02
  1.53861836e-01 -6.21235259e-02 -3.46420966e-02 -2.07564142e-02
 -3.78594808e-02  6.60987422e-02 -7.60033131e-02 -6.83455821e-03
  6.56273663e-02  1.06008127e-02 -2.63689496e-02 -4.78029065e-02
 -3.44528779e-02  1.65374037e-02  1.42281465e-02  7.31045473e-03
  6.42261282e-02  6.21235324e-03 -6.18530698e-02  5.13765588e-02
 -4.95747775e-02 -3.06838807e-02  8.65083411e-02 -6.23299144e-02
  6.46953285e-02  3.56955430e-03 -7.55847394e-02  1.00550568e-02
  2.00787410e-02 -2.08711345e-02 -6.36623474e-03  8.80515203e-03
 -3.74566913e-02 -5.65484129e-02 -6.40330091e-02  6.79619014e-02
 -3.77974324e-02 -1.28672812e-02  1.16065051e-02 -5.54691702e-02
  7.17895627e-02  5.76317944e-02 -4.31769639e-02 -3.64262871e-02
 -8.57228488e-02 -9.32787731e-02  2.18160972e-02  1.06171938e-02
 -1.16126046e-01  6.43066540e-02  7.39465207e-02  1.54252341e-02
 -4.95158471e-02  3.45358215e-02  5.10710180e-02 -4.02337313e-02
 -6.44777576e-03 -1.72847100e-02 -3.54968496e-02  9.56840500e-33
  2.25472637e-02 -4.13767993e-02 -1.73244961e-02 -1.10771749e-02
  1.00322329e-01 -4.17242534e-02  1.10829687e-02 -5.82096763e-02
 -9.08160582e-03 -4.19663861e-02 -7.00447857e-02  2.09068637e-02
 -6.86227754e-02 -1.60707105e-02  1.26810884e-02  2.32265238e-02
 -7.48025626e-02  7.98089877e-02 -8.27647222e-04  3.58674563e-02
  5.63042164e-02  9.48348269e-02  2.68748794e-02  4.97794971e-02
  1.62901506e-02 -9.33738519e-03  2.23894734e-02  1.58397830e-03
  7.97773432e-03  3.40582505e-02 -3.57020646e-02 -1.08933239e-03
 -1.84242763e-02  3.21408585e-02 -7.78180063e-02 -7.40649626e-02
 -7.39410445e-02 -5.87985106e-02 -2.90689990e-02 -7.81296864e-02
 -5.34357578e-02  1.04366176e-01  6.22454658e-03 -6.46905368e-03
  3.59513648e-02 -1.12229949e-02  1.80066768e-02  1.04201369e-01
  2.26064920e-02  3.54429558e-02 -8.60801190e-02 -5.10590943e-03
 -6.45604953e-02  5.54564819e-02 -2.27868017e-02 -1.15957811e-01
  1.24140836e-01  3.69221829e-02  9.35992822e-02  6.29598871e-02
  1.01300012e-02  2.85026617e-02  3.91946025e-02  6.21967344e-03
 -1.89313013e-02  5.03675975e-02 -6.65429160e-02 -2.10014693e-02
  6.42314926e-03  5.19220792e-02 -8.04063529e-02  4.02006395e-02
 -4.97875959e-02 -1.75859872e-02  5.72947748e-02 -1.05430573e-01
 -9.11049172e-03 -2.69876663e-02 -9.55633074e-02  5.08528687e-02
 -3.79964747e-02  2.60658353e-03 -5.07418951e-03  2.60932930e-02
 -7.67390653e-02  6.69302046e-03 -1.35117874e-03  1.00434236e-02
  2.08769012e-02 -7.06734555e-03 -6.46296591e-02 -3.26783732e-02
  1.81470141e-02  1.56384371e-02 -2.87408289e-02 -8.82336713e-33
  4.73403782e-02  3.82052287e-02 -1.69073846e-02  6.73569143e-02
 -1.91157758e-02  6.82590529e-02 -5.75615093e-02 -1.70349739e-02
  2.73147151e-02 -1.88116543e-02  4.99033555e-03 -9.34734941e-03
  5.77563643e-02  2.68301647e-03  6.18112972e-03  2.02793516e-02
 -3.77293900e-02 -1.52060539e-02 -6.70236126e-02 -3.20868380e-02
 -5.74915037e-02  3.97850461e-02 -6.34940863e-02 -8.79720878e-03
  7.02555776e-02 -3.48316617e-02 -4.13602404e-02 -1.96834886e-03
  3.16695161e-02 -7.05158757e-03  7.63352681e-03  1.40195610e-02
 -1.27271608e-01  2.34352928e-02 -9.43565294e-02 -3.44454534e-02
  4.14691754e-02 -5.33726327e-02 -7.08812922e-02  5.00457548e-02
  1.53689280e-01  6.31307662e-02 -1.24158971e-01  4.92278785e-02
 -8.38075504e-02  2.79401848e-03 -9.87318829e-02  8.92227367e-02
  1.90790929e-02 -2.67392639e-02 -9.38095897e-03 -3.39827836e-02
  1.48085775e-02 -1.03409253e-02  4.48606499e-02  3.85792926e-02
  9.95547622e-02  1.17112391e-01 -2.25077085e-02  2.16975715e-02
 -1.39158992e-02 -6.62428588e-02 -2.48119351e-03 -5.38247637e-03
 -6.28837049e-02  2.30825022e-02  8.09150375e-03  1.02525450e-01
 -9.05306265e-03  4.32174169e-02  3.32517028e-02  2.87781153e-02
  4.01059315e-02  2.55179591e-02 -7.14774728e-02  6.57098740e-02
  1.67671014e-02 -1.15498723e-02  4.38653827e-02  8.72253925e-02
  3.36992405e-02 -2.52967086e-02  6.14469983e-02  1.31084740e-01
  7.94134662e-02  1.94071755e-02  3.32869664e-02 -1.98529325e-02
  3.66039462e-02  7.59868771e-02  1.38481623e-02  1.51842530e-03
 -6.11114455e-03  1.29661232e-01  8.40796083e-02 -3.62245807e-08
 -2.31645722e-02 -1.08056245e-02 -4.38699685e-02  7.12756142e-02
  1.38837555e-02  7.22365268e-03  7.80155510e-03  1.07757643e-01
 -4.06761430e-02  5.66929765e-02  3.07198744e-02  2.99898000e-03
 -8.96383300e-02 -9.91696119e-03  4.84597590e-03  1.18704349e-01
  5.12354672e-02  5.27305976e-02  2.74223313e-02 -9.32915660e-04
  9.90332197e-03 -1.30620981e-02 -2.22082622e-03 -1.24929836e-02
 -4.82252203e-02  3.97073701e-02  6.59041554e-02  4.78482023e-02
  3.78367305e-02  4.00490686e-03 -3.38996015e-02 -7.28644133e-02
  6.92752153e-02 -6.83384985e-02  8.58900920e-02  4.57096919e-02
 -8.42762887e-02  3.09515260e-02 -1.84549559e-02  2.76233498e-02
 -6.68878928e-02 -7.23726749e-02 -6.17054217e-02 -1.51417349e-02
  3.65493223e-02  6.47925809e-02 -6.36720732e-02 -2.86638252e-02
 -4.88161147e-02  2.95857769e-02 -2.29926221e-03  9.34812333e-03
  2.94918567e-02  3.23625393e-02  7.09817372e-03  1.11951321e-01
 -6.95443973e-02 -9.90227386e-02 -1.28897307e-02 -9.23567191e-02
  2.51069739e-02  7.19571777e-04  3.68214585e-02  2.79505318e-03]",2,2
spconv,2,spconv is a project that provide heavily optimized sparse convolution implementation with tensor core support check benchmark to see how fast spconv x run spconv x code we won t provide any support for spconv x since it s deprecated use spconv x if possible check spconv x algorithm introduction to understand sparse convolution algorithm in spconv x use spconv cu if possible cuda can compile greatly faster kernel in some situation update spconv you must uninstall all spconv cumm spconv cuxxx cumm cuxxx first use pip list grep spconv and pip list grep cumm to check all installed package then use pip to install new spconv firstly you need to use import spconv pytorch a spconv in spconv x then see this don t forget to check performance guide see common problem you need to install python first to use spconv x you need to install cuda toolkit first before using prebuilt binary or build from source you need at least cuda to build and run spconv x we won t offer any support for cuda we offer python and cuda prebuilt binary for linux manylinux we offer python and cuda prebuilt binary for window for linux user you need to install pip first to install prebuilt warning spconv cu may require cuda driver pip install spconv for cpu only linux only you should only use this for debug usage the performance isn t optimized due to manylinux limit no omp support pip install spconv cu for cuda pip install spconv cu for cuda linux only pip install spconv cu for cuda pip install spconv cu for cuda pip install spconv cu for cuda note it s safe to have different minor cuda version between system and conda pytorch in cuda because of cuda minor version compatibility for example you can use spconv cu with anaconda version of pytorch cuda in a o with cuda installed note in linux you can install spconv cuxxx without install cuda to system only suitable nvidia driver is required for cuda we need driver see this page to check supported gpu name by arch if you use a gpu architecture that isn t compiled in prebuilt spconv will use nvrtc to compile a slightly slower kernel the c code will be built automatically when you change c code in project for nvidia embedded platform you need to specify cuda arch before build export cumm cuda arch list for xavier export cumm cuda arch list for tx export cumm cuda arch list for orin you need to remove cumm in requires section in pyproject toml after install editable cumm and before install spconv due to pyproject limit can t find editable installed cumm you need to ensure pip list grep spconv and pip list grep cumm show nothing before install editable spconv cumm you need to rebuild cumm first if you are build along a cuda version that not provided in prebuilts the work is done when the author is an employee at tusimple apache,"[('fast spconv', 0.6545), ('import spconv pytorch', 0.6342), ('run spconv', 0.6186), ('install spconv', 0.6123), ('use spconv x', 0.6109), ('prebuilt spconv', 0.5893), ('spconv', 0.5813), ('new spconv', 0.5778), ('spconv cu', 0.5758), ('spconv cuxxx', 0.5588)]","[-6.96675405e-02 -8.51638615e-02 -9.66822132e-02  2.85265855e-02
  4.30380590e-02 -3.72314379e-02 -2.16969866e-02 -7.41421618e-03
 -4.38164063e-02 -1.00571640e-01  4.55089845e-02 -3.35663906e-03
 -7.70246163e-02  6.92226319e-03 -2.29569133e-02 -5.87823652e-02
 -3.14768543e-03  6.34995848e-02 -1.69651657e-02 -5.66671863e-02
 -8.02183226e-02 -6.71938285e-02 -2.50023045e-03 -4.39087208e-03
  2.54499856e-02 -3.16673741e-02 -1.21006044e-02 -3.58481072e-02
  1.47624426e-02  1.22834079e-03  1.52496062e-02  7.54085630e-02
  4.84370925e-02  5.17280959e-02  1.43679129e-02  3.26198451e-02
 -6.63995817e-02  4.16143425e-02 -3.40585969e-02 -4.66522574e-02
  3.10015697e-02  1.06980566e-04 -1.68411322e-02  2.55838060e-03
  1.29713900e-02 -3.46392742e-03  4.42217737e-02  1.38703613e-02
  9.38804820e-02 -1.02645479e-01 -6.30484819e-02  5.23828752e-02
 -4.23844084e-02 -8.74524638e-02 -5.37477620e-03  1.50024183e-02
  2.80137025e-02 -1.74582675e-02  3.90722156e-02 -5.33916801e-02
  3.24248755e-03  3.50848725e-03 -2.50427816e-02  2.68094167e-02
  2.45226491e-02 -8.52515164e-04  6.43955171e-02  4.87519652e-02
  8.57522339e-02 -4.00723852e-02 -6.58724979e-02  8.19672570e-02
 -7.39576072e-02  5.30234426e-02 -2.80809496e-02 -6.43374340e-04
  7.55328760e-02 -4.84438166e-02 -1.46746263e-02 -8.90205353e-02
 -2.42932420e-02  2.35019010e-02 -1.29354428e-02  1.35029908e-02
  2.31135301e-02  2.39242371e-02  3.58396955e-03  9.50754061e-02
 -3.44017670e-02 -2.85520572e-02  6.35235086e-02  2.89024636e-02
  2.25155149e-03 -7.90292025e-03 -3.52787040e-02  1.08693019e-01
  5.18131256e-02 -3.84408683e-02 -4.11303267e-02  3.54484506e-02
  1.04020361e-03 -8.33795220e-02  1.47564784e-02  4.69640493e-02
  1.08096274e-02 -1.60345845e-02  8.84134620e-02  6.92813545e-02
 -8.17827880e-03  6.98164180e-02  5.32568693e-02  2.70230118e-02
  7.83218909e-03 -8.88030827e-02  6.45453557e-02  3.43955727e-03
 -4.62997071e-02  2.20833477e-02  3.39701995e-02  1.74206067e-02
 -7.27170631e-02 -2.37169070e-03 -5.15782125e-02  1.54344393e-02
 -4.86515276e-02 -1.61422119e-02  1.30577041e-02  1.09825526e-32
 -6.37140125e-02  5.58910146e-03  4.83964421e-02  4.90058661e-02
 -1.85968503e-02 -4.15496947e-03  7.40332231e-02 -4.31113392e-02
 -4.25910316e-02  2.25729570e-02 -5.94614521e-02 -1.24899317e-02
 -5.90174645e-02  7.84950405e-02 -7.44053274e-02 -1.05229244e-01
  6.17570011e-04 -8.40678345e-03  4.51216549e-02  2.83674430e-02
  3.25686596e-02 -2.62459386e-02  6.27098233e-03  3.07960454e-02
  2.43790857e-02 -1.61418971e-02 -1.96312983e-02 -3.21426913e-02
  2.58831400e-02  1.52612093e-03  1.76229756e-02  5.02164215e-02
 -2.54179668e-02 -1.14946971e-02 -1.76520683e-02 -6.71683103e-02
 -3.33366953e-02 -1.19678415e-02 -2.64110081e-02  1.17440075e-02
 -5.80322295e-02  3.97488512e-02 -3.31023075e-02 -3.99886928e-02
 -5.29486574e-02  2.45804898e-02 -2.05766521e-02  1.83631212e-01
  2.34340336e-02  3.68769793e-03  1.56525541e-02  5.30779995e-02
 -4.09612544e-02  2.90533341e-02 -3.00480053e-02 -5.80974808e-03
  1.00023083e-01 -2.61362772e-02  1.08726040e-01  8.58604908e-03
  1.65792592e-02  5.28882146e-02 -1.00884680e-02  6.15769774e-02
  8.99760984e-03 -9.01507735e-02  1.09785646e-02  5.82517982e-02
 -5.57970367e-02  7.99071416e-03 -1.12203874e-01  2.19913404e-02
 -6.69091940e-02 -9.42121372e-02  8.29295143e-02  6.00224966e-03
 -2.46036761e-02 -4.34484072e-02 -6.64852932e-02  1.46649627e-03
 -8.56452882e-02  5.35889305e-02  2.08873842e-02 -7.45903701e-02
  3.22519355e-02 -3.64250988e-02 -7.38309622e-02  4.07822989e-02
  6.03511855e-02 -7.07727298e-02 -7.53566250e-02 -4.64843959e-02
  6.00254387e-02 -4.74097505e-02 -1.92757584e-02 -8.96074201e-33
  4.04511504e-02 -3.28765996e-02  7.06981588e-03  9.06622782e-02
 -1.41045509e-03  5.66863781e-03 -2.64153909e-02 -5.11809364e-02
  1.00364443e-03  4.56029223e-03 -1.51552288e-02 -8.08036700e-02
  1.80267897e-02 -6.58099949e-02  8.75057131e-02  7.68036917e-02
  4.75369245e-02  5.59726320e-02 -3.62802520e-02  2.64224000e-02
 -9.51267034e-02  4.14064229e-02 -3.11542558e-03 -2.37460379e-02
 -9.13342237e-02 -5.59127294e-02  1.26620464e-04  5.48063107e-02
 -5.65775391e-03  2.80215442e-02 -5.65008372e-02  4.26646285e-02
 -6.57378733e-02  4.98764291e-02 -5.46364076e-02  1.14430638e-03
  1.37043357e-01 -4.21472266e-02 -3.39584798e-02 -2.64246091e-02
  1.55136809e-01  9.05682743e-02 -9.09730121e-02  1.82993058e-02
 -9.39754117e-03  4.35492098e-02 -5.33048511e-02  1.14493016e-02
 -2.46833544e-02  1.08662404e-01  2.15242393e-02 -1.20187756e-02
 -2.81953681e-02  3.11300438e-03  5.62992580e-02 -3.01207602e-02
  1.92415360e-02  2.08318159e-02 -5.41817956e-03 -2.77516469e-02
 -5.18037081e-02 -1.32775426e-01 -3.22656371e-02 -2.75951419e-02
  6.84491768e-02  2.90876608e-02 -5.05005680e-02  9.15172622e-02
 -2.60873530e-02 -5.72541840e-02  2.72655338e-02  1.29396692e-02
  1.42386258e-02  8.20091926e-03 -8.07966292e-02 -1.74875220e-03
  2.26667020e-02  4.91916649e-02  3.25333849e-02  7.68674910e-02
  4.49659675e-02 -2.19245791e-03  4.45866436e-02  2.90994160e-02
  2.34787632e-02  8.64166245e-02 -1.78772882e-02  6.03801478e-03
  1.14646748e-01  3.29915658e-02 -5.29980101e-02  4.30109762e-02
  1.55022293e-01  5.42533807e-02  3.14394385e-02 -4.03321820e-08
  2.38731294e-03  1.80829912e-02  1.73720028e-02  2.37522237e-02
  3.17694480e-03  5.40125184e-02 -1.16408998e-02  3.95694450e-02
  1.77698943e-03 -3.63748074e-02 -1.49380593e-02 -7.52706751e-02
 -3.18808085e-03 -2.07259655e-02  2.31720731e-02  1.08577736e-01
  3.47398454e-03  1.02767451e-02  1.18495822e-02 -4.93284762e-02
  1.18875336e-02  3.44125032e-02  2.45358832e-02  1.02545775e-01
 -4.08971012e-02 -5.67079149e-02  2.84019094e-02 -1.71277840e-02
  5.09949140e-02 -1.12438880e-01 -9.10945460e-02 -1.02757871e-01
 -8.33968073e-03 -4.02598940e-02  1.34091780e-01  5.60095198e-02
 -2.89022177e-02  4.35470827e-02  6.62758052e-02  3.39660868e-02
 -9.92261618e-02  2.39907112e-03 -4.51656803e-02 -7.10561872e-02
 -2.33679097e-02 -1.23076374e-02 -9.72392559e-02 -4.23943810e-02
  6.46509696e-03  4.57160436e-02 -9.92248021e-03  6.40309379e-02
 -5.43064438e-03  5.51548563e-02  1.91745143e-02  3.37584056e-02
 -1.44322272e-02 -1.97779555e-02  6.94159744e-03 -4.43885885e-02
 -4.83080968e-02  2.79328320e-02  1.43112568e-02 -6.02875948e-02]",2,2
torchcrepe,2,pytorch implementation of the crepe pitch tracker the original tensorflow implementation can be found here the provided model weight were obtained by converting the tiny and full model using mmdnn an open source model management framework perform the system dependent pytorch install using the instruction found here pip install torchcrepea periodicity metric similar to the crepe confidence score can also be extracted by passing return periodicity true to torchcrepe predict by default torchcrepe us viterbi decoding on the softmax of the network output this is different than the original implementation which us a weighted average near the argmax of binary cross entropy probability the argmax operation can cause double half frequency error these can be removed by penalizing large pitch jump via viterbi decoding the decode submodule provides some option for decoding when periodicity is low the pitch is le reliable for some problem it make sense to mask these le reliable pitch value however the periodicity can be noisy and the pitch ha quantization artifact torchcrepe provides submodules filter and threshold for this purpose the filter and threshold parameter should be tuned to your data for clean speech a millisecond window with a threshold of ha worked for more fine grained control over pitch thresholding see torchcrepe threshold hysteresis this is especially useful for removing spurious voiced region caused by noise in the periodicity value but ha more parameter and may require more manual tuning to your data crepe wa not trained on silent audio therefore it sometimes assigns high confidence to pitch bin in silent region you can use torchcrepe threshold silence to manually set the periodicity in silent region to zero a in differentiable digital signal processing this us the output of the fifth max pooling layer a a pretrained pitch embeddingtorchcrepe defines the following function convenient for predicting directly from audio file on disk each of these function also take a device argument that can be used for device placement e g device cuda the module test can be run a follows j w kim j salamon p li and j p bello crepe a convolutional representation for pitch estimation in ieee international conference on acoustic speech and signal processing icassp j h engel l hantrakul c gu and a robert ddsp differentiable digital signal processing in international conference on learning representation iclr,"[('pitch thresholding', 0.5325), ('pitch estimation', 0.5221), ('reliable pitch value', 0.4909), ('pitch embeddingtorchcrepe', 0.4888), ('crepe pitch', 0.4669), ('pytorch implementation', 0.458), ('crepe confidence score', 0.3915), ('quantization artifact torchcrepe', 0.3808), ('original tensorflow implementation', 0.3641), ('torchcrepe threshold silence', 0.3631)]","[-7.90067613e-02 -4.87298407e-02  2.11548456e-03 -6.80947453e-02
  5.00027137e-03  8.29594284e-02 -4.17749994e-02  5.25836647e-02
 -3.51647101e-02 -4.26395386e-02 -2.33817343e-02 -7.53808394e-02
 -6.11077137e-02 -3.80618162e-02 -3.18968445e-02 -2.35025398e-02
  1.80279184e-02  7.88403302e-02 -4.21074666e-02 -1.40288427e-01
  3.43641900e-02  2.62664743e-02  3.86408232e-02  4.05008309e-02
  6.48235753e-02  5.11838384e-02 -1.23218084e-02  1.10787731e-02
  5.16436361e-02 -6.24385662e-03 -2.22066157e-02  4.41915877e-02
  1.44600123e-01 -4.34032269e-03 -7.52682090e-02  2.26271874e-03
  4.54218872e-02  1.08597297e-02 -2.33923346e-02  5.31135015e-02
 -1.91977099e-02  4.73388359e-02 -2.25828891e-03 -1.44917360e-02
 -2.51147076e-02 -6.14173943e-03 -5.16680442e-03 -4.59473915e-02
 -1.41328171e-01 -3.35779749e-02 -9.34098195e-03 -1.65008511e-02
 -2.84304121e-03  8.64848644e-02  2.88548302e-02 -1.08998735e-02
  5.25645688e-02 -4.44007106e-03  4.82676886e-02 -5.33208996e-02
  5.01495611e-04 -1.91157795e-02 -2.96948273e-02 -3.37061845e-02
  1.90224815e-02 -2.66928617e-02 -2.05218643e-02 -2.80362996e-03
  1.42892301e-01 -1.48540651e-02 -5.03860377e-02 -2.27122027e-02
 -8.15127566e-02  1.08647078e-01 -1.19710006e-02  2.28436105e-02
  2.74260789e-02 -4.96847257e-02 -3.31701562e-02 -5.64351343e-02
 -3.05533484e-02 -8.50056335e-02  1.25436727e-02 -4.42678072e-02
  2.71264464e-02 -5.35440929e-02  6.05207076e-03  4.74330746e-02
 -6.85991207e-03 -7.64930155e-03  2.71168835e-02 -6.34516636e-03
 -4.70556170e-02 -1.61832925e-02  2.44132988e-02  3.90979722e-02
 -6.29068464e-02 -5.70373051e-02  1.43108927e-02  3.07720955e-02
  6.33435650e-03 -7.10393116e-02  2.27931719e-02  2.65344419e-03
 -5.08687980e-02  3.89363468e-02  7.30935261e-02  4.36022989e-02
  1.85683742e-02  3.33775883e-03  1.71327256e-02  6.95530176e-02
 -1.89250894e-02 -2.68604476e-02  1.24734759e-01  1.14371302e-02
 -4.67008613e-02  5.27638048e-02  7.29649290e-02  8.68925229e-02
 -4.19124067e-02  1.93953179e-02 -2.50979178e-02  5.45050669e-03
 -2.14356612e-02  4.80574183e-02 -1.20968781e-01  7.41438507e-33
 -9.35046468e-03  6.28713518e-02 -7.15856776e-02 -4.87415045e-02
 -3.84219885e-02 -7.13336617e-02 -4.12458442e-02  5.70335574e-02
  7.10226521e-02 -9.83684510e-03 -6.52254885e-03  5.95005192e-02
  1.36451144e-02  3.14615183e-02  3.77674587e-03 -3.53121348e-02
 -3.60752791e-02  3.72986048e-02 -1.61740705e-02  5.90852834e-02
  3.91223244e-02 -2.20130477e-02  7.70368278e-02  3.43038514e-02
  3.34635861e-02  8.93525630e-02  1.40532050e-02 -6.90592378e-02
 -2.51530930e-02  1.27073089e-02 -5.21360114e-02  4.16225828e-02
  2.39217654e-02 -6.16278350e-02 -4.07079980e-02 -8.29762546e-04
 -1.17371557e-02 -4.49412577e-02 -5.59951067e-02 -9.54388306e-02
 -6.96187373e-03  3.18124741e-02 -5.76723507e-03 -9.70057175e-02
  1.14479018e-02 -3.48519534e-02 -8.20387825e-02  8.77385885e-02
  1.04491815e-01  6.36906475e-02  1.75642688e-02 -5.53967315e-04
 -1.65581349e-02  2.66957674e-02  5.75382784e-02 -3.73877063e-02
 -1.80087201e-02 -1.46665489e-02  7.01928586e-02  2.42854934e-02
  3.77982557e-02  5.98050607e-03  1.32632358e-02 -9.68582928e-02
  9.76754818e-03  4.46236506e-02 -8.12972337e-02 -1.00374326e-01
 -3.89544852e-02 -6.16624132e-02 -1.06396794e-01  2.05438100e-02
 -6.50414079e-02  2.97194235e-02 -2.90269591e-02 -3.59976068e-02
  1.52846687e-02  2.69313287e-02  1.14449151e-02  9.19977017e-03
 -9.33179036e-02  4.98019047e-02  6.30235020e-03 -7.34649822e-02
 -7.35846236e-02 -4.09778170e-02  6.76202215e-03  1.11941397e-02
 -9.93898138e-03  4.63409722e-02 -9.43062305e-02  2.64059957e-02
 -2.20528431e-02  4.33922783e-02 -2.90441327e-02 -4.79181860e-33
 -5.18479161e-02  1.06753141e-01 -2.70017236e-02  1.45945385e-01
  5.54038212e-03  1.00610703e-01  2.62044556e-02 -3.18045029e-03
  5.45320734e-02 -4.14479300e-02  1.47785507e-02 -3.36950161e-02
 -5.74424071e-03 -6.52820319e-02  3.33496183e-02 -5.61947711e-02
 -1.72517691e-02  5.05130785e-03  5.49223162e-02  3.92125025e-02
  3.60073261e-02  7.15618879e-02 -8.29837918e-02  2.80635767e-02
 -3.82398888e-02  9.31357685e-03 -4.56885584e-02 -9.68816411e-03
 -2.89444323e-03 -9.29514021e-02  4.88747358e-02  7.00800028e-03
 -5.54147959e-02  3.89574058e-02 -5.86187020e-02  4.10689507e-03
  5.35260513e-02  1.61458906e-02 -8.37413967e-03  4.24050540e-02
  1.58528313e-01  1.07135788e-01  9.03392211e-03  2.13961303e-02
 -1.05020590e-01 -9.43640713e-04 -4.45628054e-02  7.57433996e-02
 -3.13727674e-03 -3.66518162e-02  8.32535699e-03 -3.23052481e-02
  2.26314310e-02 -4.49813567e-02 -2.71623563e-02 -6.48607221e-03
  1.61473099e-02 -1.85013358e-02  4.92265634e-02  5.01069874e-02
 -4.08957563e-02  2.36250740e-02 -2.87846141e-02 -8.56215227e-03
  7.62225594e-03  3.27924006e-02 -1.70920230e-02  5.72493933e-02
  1.11623285e-02  3.70794795e-02 -2.19861958e-02 -8.38371553e-03
  7.52187520e-02  7.22592324e-02 -3.29615064e-02  5.52528463e-02
 -5.25483899e-02 -1.45025039e-02 -6.46140352e-02 -3.64823639e-02
  5.06786592e-02 -6.55521406e-04 -6.68816036e-04  5.38746752e-02
  1.09080426e-01  1.19316094e-01 -2.80857515e-02 -3.29218730e-02
  4.40845564e-02 -2.31223577e-03  1.44754932e-03  2.76214276e-02
  1.60473082e-02  6.40227720e-02  9.10303593e-02 -3.01033616e-08
 -1.73101891e-02  3.01649161e-02  3.13063078e-02  4.81391605e-03
  3.75065114e-03 -2.14864388e-02  5.20046130e-02  9.28645488e-03
 -3.43169901e-03 -3.38412225e-02  3.05712633e-02 -7.38451630e-02
 -5.31047173e-02 -6.17044233e-03  2.23480281e-03  5.23672700e-02
 -3.98609638e-02  1.73601657e-01 -4.85462416e-03 -2.50530429e-02
  7.57567538e-03  8.14133734e-02  1.49550596e-02 -3.19389477e-02
  2.68812049e-02 -2.52882726e-02  5.70726283e-02  6.44720346e-02
 -2.40476355e-02 -1.96422972e-02  2.81794313e-02  6.59702942e-02
  1.46540686e-01 -3.51218656e-02  4.79626358e-02  5.45495972e-02
  8.09519459e-03 -1.78087875e-02 -5.98948002e-02  4.52827811e-02
 -5.31847030e-02  8.14070553e-02 -9.75826904e-02 -6.72609061e-02
  4.49266918e-02 -1.79988593e-02  1.93766654e-02 -7.17523545e-02
 -6.39554039e-02  4.22054380e-02 -4.52049039e-02  9.45894867e-02
 -7.64372945e-02  2.48704329e-02  2.99726166e-02  9.70219523e-02
 -1.39631229e-02 -6.85934797e-02 -6.73660077e-03 -6.39759824e-02
  8.16643052e-03 -1.90863654e-03 -5.01587987e-03 -8.57262406e-03]",2,2
optimum,2,optimum is an extension of transformer providing a set of performance optimization tool enabling maximum efficiency to train and run model on targeted hardware the ai ecosystem evolves quickly and more and more specialized hardware along with their own optimization are emerging every day a such optimum enables user to efficiently use any of these platform with the same ease inherent to transformer optimum aim at providing more diversity towards the kind of hardware user can target to train and finetune their model to achieve this we are collaborating with the following hardware manufacturer in order to provide the best transformer integration along with supporting dedicated ai hardware for training optimum also provides inference optimization towards various framework and platform optimum enables the usage of popular compression technique such a quantization and pruning by supporting onnx runtime along with intel neural compressor optimum can be installed using pip a follows if you d like to use the accelerator specific feature of optimum you can install the required dependency according to the table below if you d like to play with the example or need the bleeding edge of the code and can t wait for a new release you can install the base library from source a follows for the accelerator specific feature you can install them by appending egg optimum accelerator type to the pip command e g check out the example below to see how optimum can be used to train and run inference on various hardware accelerator to train transformer on graphcore s ipus optimum provides a iputrainer that is very similar to the transformer trainer here is a simple example to train transformer on habana s gaudi processor optimum provides a gauditrainer that is very similar to the transformer trainer here is a simple example to train transformer with onnx runtime s acceleration feature optimum provides a orttrainer that is very similar to the transformer trainer here is a simple example to accelerate inference with onnx runtime optimum us configuration object to define parameter for optimization these object are then used to instantiate dedicated optimizers and quantizers before applying quantization or optimization first export our model to the onnx format let s see now how we can apply dynamic quantization with onnx runtime in this example we ve quantized a model from the hugging face hub but it could also be a path to a local model directory the result from applying the quantize method is a model quantized onnx file that can be used to run inference here s an example of how to load an onnx runtime model and generate prediction with it here is an example on how to perform inference with the openvino runtime,"[('dedicated optimizers', 0.5778), ('optimum accelerator type', 0.5558), ('performance optimization tool', 0.5376), ('optimum', 0.5357), ('platform optimum', 0.5324), ('intel neural compressor', 0.5019), ('training optimum', 0.497), ('ipus optimum', 0.4636), ('maximum efficiency', 0.455), ('best transformer integration', 0.446)]","[-6.46218732e-02  3.89365926e-02 -2.85869110e-02 -5.63754030e-02
 -5.95019944e-02 -5.54376990e-02 -4.73036133e-02  7.92166963e-02
 -8.18495303e-02 -2.72989273e-02 -1.36137987e-02  2.72640120e-02
 -4.32995940e-03  6.67966995e-03  4.56945598e-03  2.33441722e-02
  3.99080776e-02  8.01865831e-02 -3.01004592e-02 -1.71376497e-01
  6.95170555e-03 -4.25768644e-02  1.08121159e-02 -4.82444502e-02
 -1.99682657e-02  3.90689112e-02  7.55392527e-03 -1.71880238e-02
  1.52498009e-02 -3.66933793e-02  3.19252117e-03 -7.22063631e-02
  4.21173684e-02  2.78778635e-02 -5.55052757e-02  2.09801812e-02
 -6.97441911e-03 -3.70079167e-02  5.88702820e-02 -1.13910101e-02
 -2.60439869e-02 -8.11887672e-04  2.53339205e-02 -6.02745637e-02
  6.88930973e-02  2.74519417e-02  2.15790421e-03 -8.94271061e-02
 -4.74514998e-02 -4.45650406e-02 -2.89953984e-02 -8.83967876e-02
 -8.30150843e-02  4.90826741e-02 -1.03236400e-02 -1.09326160e-02
  4.16995864e-03 -2.35850923e-02  3.09874695e-02 -2.40957178e-02
 -3.91298719e-02 -2.52686255e-02 -1.83403306e-02  1.07449647e-02
 -6.37264252e-02 -2.60954723e-02  9.39180925e-02 -6.02706335e-03
  4.67057414e-02  1.45823089e-02  6.55360566e-03 -1.51104704e-02
 -4.91866171e-02  3.61124575e-02 -2.97734328e-02 -1.67050660e-02
  1.18737772e-01 -2.31839530e-02  3.84693407e-02 -5.44453748e-02
  2.33096406e-02  6.17116271e-03 -2.58393474e-02  9.31353122e-03
  5.28466031e-02  3.53121608e-02 -1.17122658e-01 -1.05170198e-02
 -8.77613726e-04 -3.13027725e-02  4.21056617e-03 -2.36078389e-02
 -9.73118469e-03 -5.44550568e-02  6.19142223e-03  7.25692662e-04
 -4.70527522e-02 -5.85041521e-03 -4.89006974e-02  4.89053316e-02
 -1.10022712e-03 -5.53573370e-02  6.60181046e-02 -7.22225336e-03
 -1.21294439e-01 -3.25047076e-02 -2.74578482e-02  8.57153311e-02
  5.18100373e-02 -4.32813726e-03 -8.70394334e-02 -6.75547263e-03
 -3.20498161e-02 -1.22319646e-01  5.78302070e-02  5.72570711e-02
 -5.97414114e-02  4.36643399e-02  7.17881769e-02  4.46074829e-02
 -6.37575984e-02 -1.12691950e-02 -5.29271848e-02  4.89212982e-02
 -3.34652909e-03  3.38587202e-02 -9.58235413e-02  4.13895051e-33
 -5.71259409e-02  2.89954860e-02  1.15881274e-02 -8.15239176e-02
 -4.11657877e-02  2.08709612e-02  3.61202024e-02  5.61677180e-02
  6.29659295e-02  2.31043063e-02 -8.02763104e-02  6.23718612e-02
 -5.60706481e-02  1.07460037e-01  5.26874065e-02 -9.97353494e-02
  4.54570316e-02  1.25578344e-01 -1.22170206e-02 -1.12650106e-02
  5.74173853e-02  9.04215425e-02  8.29387978e-02  1.13239000e-02
  7.57739916e-02  3.00125685e-02  7.44870380e-02  1.19717009e-02
 -6.82434216e-02 -5.08469017e-03 -2.45708730e-02  1.43325357e-02
 -2.82019228e-02 -6.73896400e-03  1.81175012e-03  3.26176570e-03
 -3.94681133e-02 -4.19849046e-02 -2.67668180e-02  4.57910746e-02
  2.14797584e-03  4.72279191e-02 -7.56930336e-02 -2.60684732e-02
  4.19467092e-02  3.35793273e-04 -3.45566170e-03  8.57296959e-02
  5.89633659e-02  6.71139639e-03 -8.55530277e-02  7.27028996e-02
  1.67861842e-02  2.09847726e-02  1.05927855e-01  1.07847452e-01
  5.92769124e-02  4.21431474e-02  7.14819655e-02  1.05847083e-01
 -1.21941715e-01 -5.44471256e-02 -4.84354086e-02 -1.95106287e-02
  7.41261542e-02 -3.49048115e-02  5.18904179e-02 -1.81125593e-03
 -1.92267839e-02 -4.76684840e-03 -5.47936335e-02 -1.55899487e-02
  3.11140902e-02 -4.53103632e-02  6.20413162e-02 -2.53369529e-02
 -1.05583370e-02  2.08139662e-02 -6.02269359e-02 -3.05029508e-02
 -1.12985052e-01  9.81380716e-02  8.39861576e-03 -6.32723272e-02
  6.00402504e-02  1.19361468e-02 -7.81668629e-03  3.08404025e-02
 -1.39021901e-02 -3.82727683e-02 -5.63066155e-02  6.35388494e-02
 -1.11511098e-02  4.24673446e-02 -2.55871192e-02 -3.53025235e-33
  1.35025028e-02  4.95090662e-03 -9.21095833e-02  1.14844948e-01
 -1.72782857e-02  3.41182463e-02  1.32594593e-02 -9.88024101e-02
 -2.08226312e-02  3.57531607e-02  1.28562041e-02 -8.32891837e-03
  7.47530907e-02 -2.66055670e-02 -3.78654972e-02  1.62494630e-02
 -7.84295499e-02 -1.09133564e-01  3.71436886e-02  3.25314775e-02
  7.09657744e-03  8.88427496e-02  6.88628806e-03 -7.13707739e-03
  2.01730197e-03 -2.86621451e-02 -7.91544914e-02 -2.22819541e-02
  9.31941122e-02  3.65010388e-02 -1.05071533e-02 -1.64469387e-02
  3.25088133e-03  6.00162446e-02  1.51798660e-02 -1.29271615e-02
  4.90519479e-02  5.77385444e-03 -8.27248488e-03  5.95345348e-02
  3.37191410e-02  3.11200731e-02  6.76247850e-02  2.05707476e-02
 -7.68493414e-02 -6.85133925e-03 -6.41733184e-02 -5.89748062e-02
 -1.38088623e-02  1.30578503e-02 -3.74593437e-02 -8.34518000e-02
 -8.15285519e-02 -1.02941869e-02  1.16136521e-02 -6.30643591e-02
 -1.72700509e-02 -1.93443932e-02  7.86863938e-02 -5.96016124e-02
  2.76020803e-02 -8.23279321e-02  6.18841313e-02  1.88060163e-03
 -2.92562004e-02 -1.39655620e-02  1.10208970e-02  2.49461588e-02
 -3.51221524e-02  1.18339797e-02  1.02900090e-02  2.53705084e-02
  9.66954380e-02  7.32564600e-03 -9.40420181e-02 -2.38271547e-03
  2.97659449e-02  1.58088431e-02  1.74475610e-02 -1.84994191e-02
  1.83298141e-02  3.49312238e-02 -2.44340636e-02  3.64382379e-02
 -5.95405959e-02  2.69966070e-02  2.24329755e-02 -2.75270604e-02
  6.97256252e-02 -6.76185712e-02 -3.54487784e-02 -7.48699438e-03
  6.72313198e-02  8.47937316e-02 -7.55788386e-02 -3.02535916e-08
 -2.72127949e-02 -4.75499146e-02 -1.30424136e-02  2.78692320e-02
  4.68416139e-02 -5.24099134e-02  6.99873641e-03  1.05455756e-01
 -5.15393587e-03 -5.89948744e-02  4.35019098e-02 -2.54916679e-02
 -6.37208112e-03  3.22204009e-02  7.19387978e-02 -4.60768566e-02
 -1.99077409e-02  6.72825128e-02  3.54446918e-02 -5.14168106e-02
 -5.63385636e-02  6.15822487e-02 -9.03623935e-04 -3.03822607e-02
  4.99320799e-04  1.32152271e-02 -4.54458632e-02 -8.28353986e-02
  2.81505939e-02  1.23409905e-01 -7.44454712e-02 -1.72583014e-02
  7.82854110e-02 -9.22157392e-02  6.14053831e-02  1.26106560e-01
  4.74606678e-02  1.03685269e-02 -1.31894406e-02 -1.06835384e-02
  4.55164090e-02 -7.88591278e-04 -2.19564233e-02 -2.84326226e-02
  4.98770960e-02 -6.15792796e-02  1.74102169e-02 -5.63824549e-02
 -3.07402052e-02  6.03665784e-03  1.23711070e-02  6.96384981e-02
 -5.54513410e-02 -1.51889082e-02  4.84798364e-02  5.00928387e-02
  5.98912239e-02 -1.02376834e-01  2.88310796e-02  7.41326287e-02
  5.06730005e-02  7.11560622e-02  5.70292771e-02  1.73782371e-02]",2,0
pytorch-warmup,2,this library contains pytorch implementation of the warmup schedule described in on the adequacy of untuned warmup for adaptive optimization make sure you have python and pytorch then run the following command orthe scheduled learning rate is dampened by the multiplication of the warmup factor when the learning rate schedule us the global iteration number the untuned linear warmup can be used a follows if you want to use the learning rate schedule chaining which is supported for pytorch or above you may simply give a code of learning rate scheduler a a suite of the with statement when the learning rate schedule us the epoch number the warmup schedule can be used a follows the warmup factor w t depends on the warmup period which must manually be specified for linearwarmup and exponentialwarmup w t min t warmup period w t exp t warmup period the warmup period is given by a function of adam s beta parameter for untunedlinearwarmup and untunedexponentialwarmup warmup period beta warmup period beta the warmup factor depends on adam s beta parameter for radamwarmup please see the original paper for the detail the apex library provides an adam optimizer tuned for cuda device fusedadam the fusedadam optimizer can be used with the warmup scheduler for example mit licensecopyright c takenori yamamoto,"[('warmup scheduler', 0.6409), ('warmup schedule', 0.6171), ('learning rate schedule', 0.5748), ('untunedexponentialwarmup warmup period beta warmup period', 0.5482), ('warmup period', 0.5163), ('untuned linear warmup', 0.4961), ('warmup factor w t', 0.4856), ('warmup factor', 0.4619), ('pytorch implementation', 0.4277), ('learning rate', 0.4033)]","[-6.63239285e-02 -4.59969640e-02 -3.02844383e-02  6.59485459e-02
  2.19143890e-02  5.27093410e-02 -7.79658034e-02  1.13537246e-02
 -7.21492618e-02  3.13863088e-03  2.62894705e-02 -4.52293921e-03
 -6.76750951e-03  3.18532635e-04  3.73346172e-02  2.90376954e-02
 -3.53663042e-02  6.76991269e-02 -1.60776526e-02 -6.58277348e-02
 -1.19933486e-02 -5.41056320e-02  5.34131378e-02  4.22308929e-02
  6.71805590e-02  3.20604369e-02 -5.07814635e-04  3.97926010e-02
  6.66090101e-02  6.70373961e-02 -2.56294608e-02  1.01859798e-03
  7.00336024e-02 -2.65217535e-02 -7.30185956e-02  1.45527087e-02
 -6.81161582e-02  7.32568000e-03 -9.93487611e-02  6.88565448e-02
  1.76298115e-02 -7.44298548e-02 -3.37202474e-02 -2.42611561e-02
  8.93886015e-02  4.76957373e-02 -4.43727337e-02 -3.19481641e-02
 -7.91532695e-02 -2.39729788e-02 -4.97890972e-02 -3.74231450e-02
 -4.21162099e-02  8.12662691e-02 -3.49793537e-03  7.52414018e-02
  3.68319862e-02 -4.09757607e-02  2.89656539e-02 -8.34867805e-02
 -9.47145075e-02 -7.56629510e-03 -1.25872016e-01  2.87264735e-02
  1.18597886e-02  4.82570902e-02 -4.50795330e-02  3.88817862e-02
  7.50907883e-02 -3.44410129e-02 -8.82802159e-02  2.50649806e-02
  3.13514508e-02 -2.81708059e-03  4.59492579e-03 -4.16947007e-02
  6.89583272e-02  2.53353771e-02  5.36889657e-02 -8.06949213e-02
  1.96037465e-03  1.17470324e-02  3.25869257e-03 -1.82106793e-02
  3.35737653e-02  5.51365409e-03  6.90440610e-02  5.02742678e-02
  2.73731574e-02  2.09561754e-02  6.79017603e-02 -2.00921539e-02
  2.27980156e-04 -5.31462778e-04 -6.17883261e-03 -9.32524039e-04
 -1.58827696e-02  2.25259997e-02 -3.55676450e-02 -3.53438896e-03
 -9.73601826e-03 -9.69114602e-02 -5.88476211e-02  4.19360735e-02
 -8.02886710e-02  9.40853171e-03 -3.23852599e-02  3.35454829e-02
  1.06621452e-01 -3.43910530e-02 -1.75400861e-02  3.18895243e-02
 -3.70906368e-02 -6.97174594e-02  8.90880600e-02 -3.02204378e-02
 -2.28209049e-02  1.86713599e-02  1.50874540e-01  1.09968260e-01
  2.21497957e-02 -5.30745648e-02  1.56313665e-02 -4.37465981e-02
 -5.48137538e-02 -5.09866662e-02 -3.82466689e-02  8.20668957e-33
  4.50807400e-02  7.55085144e-03  3.89977568e-03 -5.33256680e-02
  3.62132154e-02 -4.74131294e-02 -5.37885651e-02  3.40723209e-02
  9.14931595e-02 -2.37256847e-03 -5.08122891e-03  8.46741274e-02
 -2.12252475e-02  9.03803855e-02 -1.17446976e-02 -1.12936534e-01
 -1.15543520e-02  1.32218942e-01  8.54053125e-02 -3.74491210e-03
 -6.74984709e-04  2.13386174e-02  1.99629348e-02  4.46510920e-03
 -1.35315862e-02  3.73471156e-02  1.52193923e-02  1.83777623e-02
 -7.48551637e-02  2.02396233e-02  4.11425196e-02 -1.37936203e-02
 -1.19992316e-01  6.01686072e-03  1.73349604e-02 -1.37892654e-02
  1.45937968e-03  6.49932818e-03  3.51156071e-02 -1.03694856e-01
 -2.70621479e-02 -4.70897853e-02  2.34483015e-02  8.50466546e-03
  3.97770181e-02 -2.61345170e-02  5.97531116e-03  4.21434874e-03
  3.91425751e-02  9.72052943e-03 -1.58546325e-02 -1.74642168e-02
 -7.99663540e-04 -5.01275882e-02  1.42651414e-02  1.14642516e-01
  2.62585953e-02  7.16400146e-02 -1.40995793e-02  8.41275230e-02
 -5.36401607e-02  5.19635566e-02  6.89420700e-02 -1.01597346e-01
 -5.15558682e-02  1.76071599e-02 -6.39832169e-02 -7.42411762e-02
 -5.83878998e-03 -2.45836023e-02 -3.38855572e-02  6.75266087e-02
 -3.38119753e-02 -6.13677956e-04  3.24689411e-02 -4.98656277e-03
  9.48889405e-02  4.38551186e-03 -5.51842824e-02 -2.11541411e-02
 -5.56331836e-02  4.54648435e-02  2.00006533e-02 -1.74667202e-02
 -4.91770096e-02 -7.05031827e-02  1.32912137e-02  1.31748896e-02
 -1.43213961e-02  1.85424823e-03 -1.31871728e-02 -2.93446742e-02
  3.23036164e-02  5.64895272e-02  2.00136863e-02 -6.89838933e-33
  2.26072595e-02  9.70101822e-03 -5.63696772e-02  1.06703952e-01
  2.18785461e-02 -8.26646481e-03 -3.89502496e-02 -6.13375902e-02
  1.01871260e-01  2.12259572e-02  1.29760206e-02 -1.44571206e-03
 -5.63768372e-02 -1.26818240e-01  2.35969182e-02  4.06326167e-02
 -2.21296027e-02  5.04219197e-02 -1.83979385e-02  4.69401628e-02
 -4.24993709e-02  5.04521467e-02 -1.31380513e-01 -2.81319171e-02
 -4.14848439e-02 -1.98113285e-02 -1.34998197e-02  5.92113510e-02
  6.82865530e-02  2.65242513e-02 -4.16399240e-02 -8.00148621e-02
 -2.74938270e-02  2.13025007e-02 -5.92712946e-02  3.22769098e-02
  2.15042625e-02 -1.99894179e-02 -7.15060486e-03  5.55841066e-02
  2.10385263e-01  4.63284515e-02 -2.85231676e-02 -8.25619698e-03
  1.13338046e-02  3.43635082e-02 -5.54367602e-02 -2.77916738e-03
 -5.68541624e-02  4.95267771e-02 -1.49605172e-02 -7.92585760e-02
 -5.54093905e-02  1.72017999e-02 -4.06241091e-03  2.87033548e-03
  5.27613387e-02 -4.83213589e-02 -1.42893111e-02  9.69518721e-03
  9.39464848e-03 -2.09422838e-02 -6.14093384e-03  3.88472565e-02
 -3.68757620e-02 -4.84008938e-02  2.44738664e-02 -2.55692136e-02
  2.67258403e-03 -2.66759563e-02 -4.72318614e-03  5.90467714e-02
  3.84784490e-02  6.64316490e-03 -4.56053950e-03 -2.28856206e-02
 -3.08946054e-03  3.21827196e-02 -7.15046376e-02 -1.97487976e-02
 -4.79551367e-02  2.21557207e-02 -5.01548424e-02  5.51949181e-02
 -6.54416680e-02  4.16518711e-02  1.06571138e-01  4.45854031e-02
  7.66061768e-02 -8.87930840e-02  3.13702375e-02  4.74925451e-02
  5.24759516e-02  1.70938373e-01  1.79528818e-02 -3.34192904e-08
 -1.35995587e-02 -4.42417227e-02  5.76063395e-02 -1.77578647e-02
  9.88292694e-02 -8.47735330e-02 -3.67765944e-03 -1.46459285e-02
 -4.86289188e-02  2.08897376e-03  9.06765461e-02  2.21415889e-02
 -6.55987952e-03 -6.04645275e-02  1.73917245e-02  3.43822837e-02
  5.82338981e-02  5.44694625e-02 -3.25722247e-02 -6.55689389e-02
  3.39115858e-02  1.35055473e-02  2.20488533e-02  1.65369250e-02
  4.36227396e-02 -3.45017388e-02 -1.08215911e-02  6.28914163e-02
  3.99330668e-02  1.15080667e-03 -1.12176761e-02  4.99420688e-02
  2.76728235e-02 -9.37667340e-02  6.42680675e-02  1.01093724e-01
 -5.61755197e-03 -3.98828536e-02  4.80553359e-02  9.10127908e-02
 -2.73508988e-02 -4.85423766e-02 -3.76178809e-02 -2.49354616e-02
  9.06474981e-03 -1.13660737e-03 -9.64038149e-02 -8.94092098e-02
 -7.01247230e-02 -6.62283897e-02 -7.68076070e-03  6.44021481e-02
 -4.59041037e-02  8.58499110e-03  6.03059074e-03  4.12385464e-02
  1.17606577e-02 -9.53060910e-02 -1.35560390e-02 -3.29330638e-02
 -4.77843843e-02  2.71909442e-02 -9.50826555e-02  8.42555147e-03]",2,0
lasagne,1,lasagne is a lightweight library to build and train neural network in theano it main feature are support feed forward network such a convolutional neural network cnns recurrent network including long short term memory lstm and any combination thereofallows architecture of multiple input and multiple output including auxiliary classifiersmany optimization method including nesterov momentum rmsprop and adamfreely definable cost function and no need to derive gradient due to theano s symbolic differentiationtransparent support of cpu and gpus due to theano s expression compilerits design is governed by six principle simplicity be easy to use easy to understand and easy to extend to facilitate use in researchtransparency do not hide theano behind abstraction directly process and return theano expression or python numpy data typesmodularity allow all part layer regularizers optimizers to be used independently of lasagnepragmatism make common use case easy do not overrate uncommon casesrestraint do not obstruct user with feature they decide not to usefocus do one thing and do it well in short you can install a known compatible version of theano and the latest lasagne development version via for more detail and alternative please see the installation instruction documentation is available online http lasagne readthedocs org for support please refer to the lasagne user mailing list for a fully functional example see example mnist py and check the tutorial for in depth explanation of the same more example code snippet and reproduction of recent research paper are maintained in the separate lasagne recipe repository lasagne is a work in progress input is welcome please see the contribution instruction for detail on how you can contribute first release core contributor in alphabetical order eric battenberg ebattenberg sander dieleman benanne daniel nouri dnouri eben olson ebenolson a ron van den oord avdnoord colin raffel craffel jan schl ter f k s ren kaae s nderby skaae extra contributor in chronological order daniel maturana dimatura documentation cudnn layer lrnjonas degrave get all param value fixjack kelly jackkelly help with recurrent layersg bor tak c takacsg support broadcastable parameter in lasagne updatesdiogo moitinho de almeida diogo mnist example fixesbrian mcfee bmcfee maxpool dlayer fixmartin thoma martinthoma documentationjeffrey de fauw jeffreydf documentation adam fixmichael heilman mheilman nonlinearitylayer lasagne randomgregory sander instagibbs documentation fixjon crall erotemic check for non positive input shapeshendrik weideman hjweide set all param value test maxpool dcclayer fixkashif rasul kashif adam simplificationpeter de rivaz peterderivaz documentation fix,"[('theano s expression compilerits design', 0.5266), ('latest lasagne development version', 0.5133), ('lasagne', 0.4439), ('theano', 0.4355), ('neural network', 0.4222), ('separate lasagne recipe', 0.4206), ('theano expression', 0.4205), ('available online http lasagne readthedocs org', 0.404), ('repository lasagne', 0.4033), ('recurrent network', 0.3912)]","[-1.19973719e-01 -8.26997682e-02 -4.21941765e-02  2.08367556e-02
 -3.64498410e-04  9.20974091e-02 -8.78330171e-02  2.08655223e-02
 -9.80159044e-02 -3.48923206e-02 -2.98110526e-02 -3.39507721e-02
 -8.83999318e-02 -1.33973667e-02 -6.16916455e-02  7.53479674e-02
 -3.57124470e-02  4.55585010e-02  7.81282187e-02 -1.92720629e-02
  4.70830547e-03 -4.51187463e-03  5.27650816e-03  6.73663765e-02
  3.20858881e-02  5.68349250e-02  3.68863419e-02 -5.62469941e-03
  2.68073510e-02 -1.75048318e-02  5.75076379e-02  3.49609666e-02
  8.30540806e-02  4.53320378e-03 -2.10573990e-02 -1.69786066e-02
 -7.35891536e-02 -8.61396641e-02 -3.13811116e-02 -1.51933730e-02
 -3.49890180e-02  4.68954258e-03  7.47677460e-02  2.02478301e-02
  7.95024708e-02 -2.42047608e-02  1.18924500e-02 -1.15746930e-02
 -3.47139612e-02 -2.46781502e-02 -4.59714010e-02 -4.24422286e-02
 -6.67666495e-02  2.97483876e-02  4.42234054e-02 -4.17607240e-02
 -3.22353765e-02 -2.76802486e-04 -4.49135676e-02 -4.44202535e-02
  3.89926583e-02  1.68436114e-02 -9.28094909e-02  3.69873978e-02
  9.48876664e-02  2.22495873e-03 -6.33396301e-03  7.24495724e-02
 -6.73772907e-03  1.80284716e-02 -2.87587512e-02 -3.79717611e-02
 -1.26217995e-02  7.56091923e-02  1.34932334e-02  4.81612720e-02
  1.45741686e-01 -6.06953651e-02  6.36211187e-02 -3.80032510e-02
 -4.46746685e-02  2.41841152e-02  1.12482300e-02 -3.66832539e-02
 -2.76792962e-02  4.06720154e-02 -8.51441175e-02  4.74184640e-02
  4.29207236e-02 -1.95420650e-03  5.01839854e-02  1.96929816e-02
 -3.30696329e-02 -1.03979237e-01 -4.47455831e-02  1.28230900e-02
 -3.72279622e-03  1.94693040e-02  1.90076698e-02  1.28937084e-02
 -8.48390535e-03 -1.17885703e-02  2.11241078e-02 -5.13023138e-02
 -1.02504060e-01  1.99737810e-02  1.26476839e-01  3.27434763e-02
 -5.05994121e-03  1.50433434e-02 -3.98021638e-02  3.50410491e-02
 -7.43983909e-02 -1.50863752e-01  8.52993503e-03 -4.38684523e-02
  1.64487250e-02 -2.48474348e-02  6.08678721e-02  8.24867040e-02
 -6.06046394e-02  1.05046798e-02 -1.51956975e-02  3.45971473e-02
 -4.78178076e-02 -3.46268341e-02 -6.27302006e-02  3.70238917e-33
  4.65605110e-02 -1.37890643e-02 -6.01807237e-02  2.08569560e-02
  6.20851405e-02 -2.11665425e-02 -8.24407768e-03 -5.42352460e-02
 -6.19966909e-02  2.01021601e-03 -4.77041602e-02  2.11908314e-02
 -6.60034642e-02  4.64638136e-02 -8.15047920e-02 -1.37211740e-01
  4.59352471e-02 -2.02202369e-02 -2.84613166e-02 -1.86179709e-02
 -3.57398279e-02  7.27732778e-02 -1.00993505e-02  1.07386090e-01
 -1.22083602e-02  5.87904826e-02  6.11086711e-02 -2.21584328e-02
 -6.54112250e-02  1.65224839e-02 -7.34221190e-02  6.43655136e-02
 -4.91542332e-02 -2.38494389e-02  6.30127788e-02 -1.17391115e-02
 -2.13543586e-02 -1.17368829e-02  9.04809684e-02 -5.55763021e-03
  3.97685282e-02 -2.51077637e-02 -2.08933875e-02  1.23806968e-02
 -3.78296152e-02 -5.15534319e-02 -1.89621616e-02  6.02919012e-02
  1.55981869e-01 -1.15977146e-01  2.40649022e-02 -5.41430339e-02
 -7.10608587e-02 -3.59598338e-03  4.99944575e-02  4.70827147e-02
 -4.07660287e-03  5.71176745e-02  7.97238871e-02  2.10756250e-02
  1.54148147e-03  2.85156276e-02  5.56443520e-02 -4.39827368e-02
  4.19853702e-02  2.04255320e-02 -4.37369607e-02 -2.15009674e-02
  8.50254223e-02  2.39133667e-02 -4.81870510e-02 -5.01338318e-02
  1.06728256e-01 -2.54662056e-02  9.54911783e-02 -1.77737121e-02
  2.27926206e-02 -2.60103922e-02 -8.54850747e-03  4.68152836e-02
 -1.06476225e-01 -3.19352970e-02  2.21999086e-04  5.27345352e-02
 -1.76445884e-03  1.03500579e-02 -1.32920900e-02 -2.06512045e-02
  2.12503858e-02  6.46648407e-02 -2.31402162e-02 -5.07004373e-02
  6.02147132e-02 -2.70754173e-02  2.98511907e-02 -4.22644476e-33
 -4.51595560e-02 -4.29255143e-02 -4.89327461e-02  1.18898246e-02
 -5.51918149e-02  2.58869827e-02 -5.57584576e-02 -8.55450705e-02
 -1.03115505e-02  4.66046222e-02  5.99212535e-02 -6.61308244e-02
 -2.20516939e-02 -1.25055671e-01 -6.87912758e-03  6.79877102e-02
  1.49241295e-02 -3.21893357e-02  5.40732546e-03 -1.76267582e-04
 -5.04200459e-02  6.36893436e-02 -7.22313523e-02 -2.39223638e-03
 -6.20025471e-02 -3.93649638e-02  3.34280618e-02  5.83295338e-02
  5.51303290e-02  1.08362734e-02 -8.41764137e-02 -7.61236250e-02
 -3.24155726e-02  2.80857887e-02 -4.11851332e-02  4.56780642e-02
  5.68553060e-02  4.07977737e-02 -4.56798188e-02 -3.41665931e-02
  1.15413189e-01  1.33851292e-02 -2.97943577e-02  3.64519581e-02
  6.22539371e-02  2.68386472e-02 -9.14220884e-02 -6.45781010e-02
 -2.31782626e-02  4.28497419e-02  6.36906326e-02 -6.90639690e-02
 -6.64646178e-02 -8.10594857e-03 -6.48403913e-02 -4.34172079e-02
  2.54601650e-02 -2.46741250e-02  8.96581933e-02  6.26849458e-02
 -3.40182371e-02 -5.10450564e-02  1.10822916e-02 -5.13278097e-02
  7.50806034e-02  1.14643164e-02 -6.48386851e-02  2.17639133e-02
  9.08550546e-02 -8.09184927e-03  7.89753646e-02  9.28578712e-03
  1.68832447e-02  3.20772491e-02 -6.54023439e-02 -2.48782579e-02
  8.44473392e-03 -3.23883519e-02 -6.15439191e-02 -6.95061758e-02
  6.26013726e-02  2.01278962e-02  3.90527919e-02  6.74864054e-02
  3.96686047e-02  4.67253774e-02  6.15181215e-02  3.73158753e-02
 -7.31853694e-02  5.00836410e-03 -6.20258860e-02  5.50273024e-02
  1.25705975e-03  5.76641336e-02 -2.38214619e-02 -3.31970256e-08
 -7.18625709e-02  1.44606717e-02  7.26665705e-02  4.33348268e-02
  9.59033668e-02  5.34947962e-02  5.09729609e-02 -1.91771090e-02
  5.70786931e-02 -1.92502663e-02  4.12981492e-03 -2.14493535e-02
  1.49268173e-02 -3.79261412e-02  5.58742583e-02  1.28751574e-02
 -1.41475340e-02  5.32287732e-02 -4.56638113e-02  5.82697336e-04
  7.92346373e-02  7.09421709e-02 -1.31831644e-02 -8.88847113e-02
 -4.87484410e-02 -6.64051995e-03 -7.72126615e-02  8.32348838e-02
  2.42083753e-03 -1.95433572e-02  2.39368211e-02  4.84035611e-02
  7.57002607e-02 -5.21592312e-02  2.83931885e-02  3.22764106e-02
  3.91925499e-02 -6.14670627e-02  5.84108988e-03  5.98780476e-02
 -2.94720847e-02  6.05807230e-02 -5.14235673e-03  3.54903266e-02
 -5.27237877e-02 -6.89743971e-03 -2.45844852e-02 -1.05200164e-01
  1.73177924e-02  7.18536377e-02  5.72347753e-02  1.57990456e-02
 -5.06123714e-02  4.43859547e-02  5.14724106e-02 -8.63024313e-03
  3.67192402e-02 -8.21963325e-03  3.06470878e-02 -9.30066500e-03
 -2.14261245e-02  2.69461274e-02  2.62309480e-02 -1.12521807e-02]",2,2
dopamine-rl,1,getting started doc baseline result changelistdopamine is a research framework for fast prototyping of reinforcement learning algorithm it aim to fill the need for a small easily grokked codebase in which user can freely experiment with wild idea speculative research our design principle are dopamine support the following agent implemented with jax for more information on the available agent see the doc many of these agent also have a tensorflow legacy implementation though newly added agent are likely to be jax only this is not an official google product we provide docker container for using dopamine instruction can be found here alternatively dopamine can be installed from source preferred or installed with pip for either of these method continue reading at prerequisite dopamine support atari environment and mujoco environment install the environment you intend to use before you install dopamine atarimujocothe most common way to use dopamine is to install it from source and modify the source code directly after cloning install dependency dopamine support tensorflow legacy and jax actively maintained agent view the tensorflow documentation for more information on installing tensorflow note we recommend using a virtual environment when working with dopamine note we strongly recommend installing from source for most user installing with pip is simple but dopamine is designed to be modified directly we recommend installing from source for writing your own experiment you can test whether the installation wa successful by running the following from the dopamine root directory view the doc for more information on training agent we supply baseline for each dopamine agent we also provide a set of colaboratory notebook which demonstrate how to use dopamine bellemare et al the arcade learning environment an evaluation platform for general agent journal of artificial intelligence research machado et al revisiting the arcade learning environment evaluation protocol and open problem for general agent journal of artificial intelligence research hessel et al rainbow combining improvement in deep reinforcement learning proceeding of the aaai conference on artificial intelligence mnih et al human level control through deep reinforcement learning nature schaul et al prioritized experience replay proceeding of the international conference on learning representation haarnoja et al soft actor critic algorithm and application arxiv preprint arxiv if you use dopamine in your work we ask that you cite our white paper here is an example bibtex entry,"[('install dependency dopamine support tensorflow legacy', 0.662), ('dopamine instruction', 0.5972), ('prerequisite dopamine support', 0.588), ('dopamine agent', 0.5617), ('dopamine support', 0.4968), ('dopamine', 0.4576), ('dopamine note', 0.4414), ('dopamine bellemare et al', 0.4402), ('tensorflow documentation', 0.4142), ('dopamine root directory view', 0.4141)]","[-1.00291502e-02 -1.19958945e-01  2.50133444e-02 -3.39385718e-02
  2.78670937e-02  3.20674945e-03 -1.47905834e-02  1.81775000e-02
 -5.54277822e-02 -4.83688824e-02 -5.84164634e-02 -3.95631678e-02
 -1.00217529e-01  4.97593209e-02  4.47778553e-02  9.81646776e-02
  2.82531269e-02 -1.01637552e-02  3.19112241e-02 -2.81540733e-02
  2.96569504e-02  4.07199189e-02  9.18139890e-02  1.41800242e-02
 -3.59168202e-02  5.34355417e-02 -2.22108271e-02 -6.89179748e-02
  3.28505524e-02  2.57389452e-02  3.80316526e-02  6.83171600e-02
  8.39028209e-02  4.70441347e-03 -6.53398186e-02  4.14534062e-02
 -7.51965195e-02 -2.45246105e-02 -7.43196066e-03  5.27545065e-02
  8.81430972e-03  7.22945035e-02 -5.94708091e-03 -1.69842411e-02
 -1.92369744e-02 -3.04183085e-02  4.29584309e-02 -7.27539212e-02
  3.26201320e-02  8.06466211e-03 -3.31849828e-02 -4.95381169e-02
  3.47696133e-02  6.20962232e-02 -1.75413154e-02  3.93441468e-02
  6.82879686e-02  9.92778465e-02 -5.90056591e-02  4.64229062e-02
 -1.00305239e-02  4.40157689e-02 -1.54596595e-02 -7.08437562e-02
 -2.74286680e-02  9.42204148e-02 -2.55406871e-02 -2.17863135e-02
  4.74006198e-02 -1.01228751e-01 -4.31926399e-02 -3.13922167e-02
 -1.62722133e-02  2.57930830e-02 -3.49111483e-02  2.37720478e-02
  8.30463096e-02  9.54318605e-03  7.95415416e-02 -9.28291604e-02
 -9.85193402e-02  3.43571454e-02  9.65257958e-02  4.11930168e-03
  4.86516543e-02  1.28320679e-02 -2.73159910e-02  7.53837302e-02
  4.91897799e-02 -4.96325195e-02 -1.27063449e-02 -8.89040902e-02
 -6.08164109e-02 -6.95043383e-03  6.61769062e-02 -3.81852081e-03
 -5.37424050e-02  1.59491431e-02 -2.22605076e-02  2.90652867e-02
 -3.91517915e-02 -1.22968107e-01 -1.65586397e-02  8.33052322e-02
 -9.11266580e-02  1.31282173e-02  6.83193207e-02  6.66634133e-03
  4.01732288e-02 -2.69873999e-02  1.61692705e-02  8.54441617e-03
  9.31832716e-02  1.57350767e-02  2.01970935e-02 -4.86826850e-03
 -6.56731501e-02  1.39987972e-02  3.66353802e-02  5.95010854e-02
 -2.39898302e-02  2.76824068e-02 -2.44158763e-03  4.35598530e-02
  2.28843540e-02 -7.69774616e-02 -7.26489052e-02  7.97984120e-33
  1.32159293e-02  9.33765993e-03  3.06126364e-02  2.69685984e-02
 -7.13021681e-03 -5.99052478e-03  6.47810921e-02 -3.30076665e-02
 -2.64368802e-02  1.81791037e-02 -4.03877981e-02  3.19473073e-02
 -1.65912230e-02  9.54639316e-02 -9.01580900e-02 -1.20208845e-01
  1.17316442e-02  6.71684835e-03  8.45457241e-02 -3.15096639e-02
  7.20210746e-02  4.13946137e-02 -1.60735473e-02  7.62692913e-02
 -1.03038913e-02  5.78793623e-02 -6.87669916e-03 -2.23613647e-03
  3.06666382e-02  1.21448869e-02 -5.30473664e-02 -5.99948876e-02
 -2.80432850e-02 -2.98497267e-02 -1.18461154e-01  1.63836330e-02
 -6.16442673e-02  5.46814241e-02 -5.28273284e-02 -5.46488650e-02
  4.56555039e-02  8.44731554e-02 -5.34415022e-02 -3.99818346e-02
 -2.36938897e-04  2.94320080e-02  2.32224464e-02 -1.36772403e-02
  1.13105342e-01  3.11215036e-02 -8.82426277e-02 -7.91912973e-02
  1.44973549e-03  7.02335592e-03 -2.34992616e-02 -7.93584511e-02
 -3.75144258e-02 -2.25575604e-02  8.21034312e-02 -5.91011858e-03
  3.67446914e-02  9.20770457e-04 -1.27927270e-02  9.01750010e-03
  3.85081992e-02 -4.16938104e-02 -4.54716496e-02  1.64830335e-03
  1.61406153e-03  3.43587920e-02 -8.13726261e-02  6.84590116e-02
 -3.51375639e-02 -1.32377548e-02 -1.09505206e-01 -9.18858424e-02
  3.35225835e-03 -3.39028053e-02 -2.29526926e-02 -3.36305909e-02
 -6.56143948e-02 -7.92942718e-02  1.47340484e-02  4.53125574e-02
  7.05248676e-03  3.69667336e-02  2.75717452e-02 -7.00358069e-03
 -2.02121232e-02 -2.40236446e-02 -3.15148421e-02 -6.97472692e-02
  3.13600227e-02  2.54605822e-02 -3.62900533e-02 -6.72006978e-33
 -3.18191126e-02 -5.17848283e-02 -5.07242279e-03 -5.15891705e-03
  9.99227818e-03  8.35826918e-02  3.78010496e-02 -7.10718110e-02
  3.73645164e-02  1.36775523e-02  2.84027662e-02 -7.51335174e-03
  6.24024495e-03 -8.69709591e-04  1.68811958e-02 -1.10960053e-02
 -7.03730285e-02  4.78217565e-02  3.42407487e-02 -1.18996913e-03
 -6.50672019e-02  5.00308536e-02 -1.07893057e-01 -1.80392917e-02
 -1.05730370e-02  2.74953954e-02  3.86479571e-02  2.76180729e-02
  5.41097149e-02  2.18965933e-02 -3.96811031e-02  3.76768187e-02
 -8.35632309e-02 -1.00519434e-02  3.05939242e-02  9.07869451e-03
  1.57831088e-02 -5.48486300e-02 -1.06280230e-01 -2.33734027e-02
  7.06080720e-02  1.14249624e-01  1.14305010e-04  1.16587477e-02
 -1.53946569e-02  3.72210629e-02 -4.79461513e-02 -9.56008583e-03
 -2.87877358e-02 -2.54188329e-02  2.33959332e-02 -8.90375078e-02
  2.29443004e-03 -1.16605252e-01 -8.62384811e-02  7.97677878e-03
  9.89037603e-02  2.41595600e-02 -1.23887509e-02 -5.49031198e-02
 -4.28676978e-02 -9.47565120e-03 -4.95118983e-02 -5.63477464e-02
 -2.93975249e-02  3.38395573e-02 -8.25885832e-02  9.67402458e-02
  3.76575664e-02  1.83541700e-02 -6.96432516e-02  2.11520270e-02
  6.65485561e-02  5.10004088e-02  1.84547156e-02  3.14820185e-02
 -2.72922553e-02  4.91108634e-02  7.95644745e-02 -1.12097152e-01
  3.08587272e-02 -5.31499386e-02  2.21522823e-02  8.83122757e-02
  7.28995577e-02  5.15814871e-02  5.17567573e-03  4.12412994e-02
  4.80461977e-02  1.78228784e-02 -2.09657606e-02  3.41349468e-02
  3.77026796e-02  4.99336235e-02  3.47187147e-02 -3.56673020e-08
 -2.05961298e-02 -2.01004278e-02  1.28554669e-03  1.79988388e-02
  6.52710795e-02  8.71150643e-02  1.27220675e-01  5.24432771e-02
 -4.72510569e-02  1.11801540e-02  5.01705334e-03 -3.41102704e-02
 -8.44627153e-03 -4.14576381e-02  4.05539572e-02  4.42094840e-02
 -8.39974061e-02  1.08205982e-01  1.21787982e-03 -2.93574948e-02
  4.14733626e-02 -4.29990962e-02  3.10363546e-02  1.45629309e-02
  9.62479226e-03 -9.53057781e-02  2.57659946e-02  5.03853746e-02
 -2.74239611e-02 -5.01459949e-02 -1.84502508e-02  2.02769358e-02
 -2.04510279e-02 -8.18676278e-02  1.17298849e-01  4.42170873e-02
  1.16811730e-02 -1.18912654e-02  1.28764017e-02  2.16007233e-02
 -1.16653405e-01  3.62711139e-02  3.99538912e-02 -5.62841147e-02
  4.45727706e-02 -4.03517820e-02 -3.98971252e-02 -4.12751399e-02
  4.44832481e-02  3.77160101e-03 -3.95576842e-02  1.02496244e-01
 -4.18805815e-02  7.17242509e-02 -4.41241032e-03  1.12687247e-02
 -5.34708835e-02 -4.58737202e-02 -6.41899928e-02 -7.67123327e-02
  5.14224619e-02  6.96804076e-02  2.33681351e-02 -1.17601864e-01]",2,2
gradient-sdk,1,this is an sdk for performing machine learning with gradient it can be installed in addition to gradient cli this sdk requires python to install it run set the tf config environment variable for multi worker training you need to set the tf config environment variable for each binary running in your cluster set the value of tf config to a json string that specifies each task within the cluster including each task s address and role within the cluster we ve provided a kubernetes template in the tensorflow ecosystem repo which set tf config for your training task get tf config function to set value of tf config when run on machine within paperspace infrastructure it can raise a configerror exception with message if there s a problem with it configuration in a particular machine usage example currently gradient only support hyperopt for hyperparameter tuning hyper tune function to run hyperparameter tuning it accepts the following argument it return a dict with information about the tuning process it can raise a configerror exception with message if there s no connection to mongodb note you do not need to worry about setting your mongodb version it will be set within paperspace infrastructure for hyperparameter tuning usage example get mongo conn str function to check and construct mongodb connection string it return a connection string to mongodb it can raise a configerror exception with message if there s a problem with any value used to prepare the mongodb connection string usage example data dir function to retrieve path to job space usage example model dir function to retrieve path to model space usage example export dir function to retrieve path for model export usage example worker host function to retrieve information about worker host usage example p host function to retrieve information about paperspace host usage example task index function to retrieve information about task index usage example job name function to retrieve information about job name usage example,"[('tensorflow ecosystem repo', 0.4532), ('gradient cli', 0.3983), ('kubernetes template', 0.3825), ('multi worker training', 0.3718), ('gradient only support hyperopt', 0.3715), ('tf config environment', 0.3608), ('cluster', 0.3149), ('paperspace host usage example task index function', 0.3138), ('task index usage example job name function', 0.3101), ('training task', 0.3)]","[-8.01378582e-03 -8.79045799e-02 -8.42025969e-03 -4.38629910e-02
  7.32729435e-02  2.14870088e-02 -2.25886069e-02 -8.02595615e-02
 -3.25797498e-02 -3.45708393e-02 -9.22142714e-02 -4.62155081e-02
 -7.86955506e-02  4.40802760e-02  2.18335204e-02  1.51288584e-02
  2.25760005e-02  1.51759814e-02  4.58801771e-03 -1.05915397e-01
 -2.45999079e-02 -1.73306540e-02 -2.87375152e-02 -9.98314408e-06
 -8.93847421e-02 -1.99003797e-02 -8.40526968e-02 -1.96942352e-02
  6.89737545e-03  8.20196047e-03 -1.94666572e-02  2.62215808e-02
  6.85968250e-03  6.95884749e-02 -3.72043923e-02  1.38705671e-01
 -5.83642088e-02 -4.90365699e-02 -4.47782092e-02  3.05290464e-02
 -3.91370244e-03 -4.48416136e-02 -9.78188813e-02 -8.21215659e-02
  7.92231336e-02 -1.85440406e-02  1.04948077e-02 -1.26053795e-01
 -6.66508172e-03 -2.75093634e-02 -3.66306454e-02 -1.13189496e-01
  3.27033065e-02  4.07000743e-02  4.75670174e-02  3.12781855e-02
  4.65770811e-02  2.36487761e-02  2.59800628e-02 -6.93124384e-02
  2.01121308e-02 -5.57010509e-02  3.10807652e-03 -3.04252710e-02
 -1.36785358e-02  2.05872394e-02 -4.74404804e-02  3.07895802e-02
  5.49976900e-02 -1.33385509e-01 -3.92837599e-02  4.77378406e-02
 -7.94224888e-02 -4.81768511e-02 -3.00551951e-02  6.83755428e-02
  4.62149233e-02  1.53064663e-02  6.78548068e-02 -1.17534034e-01
  3.77999805e-02  6.69680908e-02  7.58284330e-02 -1.16911214e-02
 -6.74448954e-03 -4.42294739e-02  1.68283097e-03  1.99068058e-02
  9.04119909e-02 -2.78014271e-03  3.77091132e-02 -2.33525056e-02
  7.67638395e-03 -5.17879501e-02  2.27328800e-02 -1.27461863e-05
 -5.97250760e-02  1.88167822e-02 -2.70650424e-02 -5.46753313e-03
 -6.48781732e-02 -7.73920715e-02  6.33755699e-02  5.58542162e-02
  1.11131575e-02  6.84285834e-02 -2.23244634e-02  4.44283411e-02
  6.26031533e-02  4.05827463e-02 -2.83292439e-02  9.84964967e-02
  3.21985297e-02 -3.11305840e-02  2.97010746e-02  2.60374583e-02
  1.03610810e-02  5.06434543e-03  9.74957552e-03  1.81565702e-01
 -5.44670559e-02 -5.33586666e-02  2.84689642e-03 -9.85848438e-03
 -3.02899871e-02  2.47114934e-02 -6.56624660e-02  8.56154413e-33
  3.70692201e-02 -5.11523075e-02  3.26593332e-02 -5.17170094e-02
  1.21197119e-01 -9.55863521e-02  3.14845741e-02 -3.13230939e-02
 -1.13344612e-02 -7.42569417e-02 -8.92235041e-02  8.94484445e-02
  4.77639958e-02  1.24396823e-01 -2.03766283e-02 -5.92649467e-02
 -3.18305343e-02  8.86803567e-02  6.71830103e-02  5.04476391e-02
  7.06441626e-02 -2.74925884e-02 -5.10625690e-02  2.27482077e-02
  1.69968291e-04 -5.87013131e-03  1.03786349e-01 -2.99158823e-02
  7.10555445e-03  3.99230160e-02 -4.80387509e-02 -3.20471935e-02
 -1.87339950e-02  3.54763195e-02 -8.26276019e-02  7.40308091e-02
 -4.28863689e-02  2.41506975e-02 -7.67406309e-03 -1.00481594e-02
 -1.76691953e-02  4.27390672e-02  2.35134433e-03 -1.08378746e-01
  2.93694320e-03 -4.18528132e-02  7.61066005e-02 -6.53524045e-03
  9.12856162e-02  1.24774277e-02 -7.82268867e-02 -4.28283513e-02
  2.55364068e-02 -6.10928647e-02  6.87553063e-02  5.45636788e-02
  1.18874729e-01  6.27822056e-02  3.28362621e-02  2.03386210e-02
 -3.72028053e-02 -5.60772419e-03 -3.39151546e-02  3.79831530e-02
  4.19335030e-02 -4.77367528e-02 -4.31896932e-02  1.06455097e-02
  2.00579949e-02  6.53203055e-02 -2.11819634e-02  5.33381850e-02
 -2.48891814e-03 -1.49454072e-03  6.34098286e-03 -6.20649643e-02
  4.27834988e-02 -1.72930174e-02 -8.27599689e-02  3.35307643e-02
 -7.55059645e-02  7.40266545e-03  6.34199232e-02 -5.81581192e-03
 -6.44160842e-04 -2.94887628e-02 -1.25279706e-02  6.53139651e-02
  2.70065125e-02 -5.64443693e-02 -5.35351820e-02 -7.16621568e-03
  1.20942695e-02  4.39530686e-02 -1.13309599e-01 -6.66651352e-33
 -4.35867719e-02  4.11430560e-03 -7.32049942e-02  3.92400138e-02
  6.72246814e-02  8.95290747e-02  2.66030040e-02 -3.10946698e-03
 -6.29160628e-02  4.20475267e-02  3.18680219e-02 -6.57451227e-02
  6.09367853e-03 -8.62366147e-03 -1.94034521e-02 -3.45454514e-02
 -8.42997953e-02 -3.96381542e-02 -9.35754087e-03  2.47952598e-03
 -5.40587939e-02  1.25185512e-02 -3.61645371e-02  7.05293790e-02
  4.09842283e-02 -8.20615049e-03 -4.01360951e-02 -7.66896550e-03
 -3.05711478e-02  1.34420050e-02 -3.60215455e-02 -4.07781415e-02
 -3.32928002e-02  6.60704821e-02  5.78151233e-02 -1.67703535e-02
 -3.66753265e-02  2.53349431e-02 -4.92976140e-03  7.27530420e-02
  3.88600305e-02 -2.32516695e-03  5.29210679e-02  5.19853644e-03
 -1.03408262e-01  3.40957418e-02 -7.63631761e-02 -6.96156025e-02
 -2.48519164e-02 -6.01974390e-02 -4.94947806e-02 -3.25607359e-02
 -8.52936357e-02 -3.19063328e-02 -9.42756534e-02  1.48656042e-02
  4.21027504e-02  6.46417886e-02 -7.31043145e-02 -2.65425001e-03
  8.25962499e-02 -1.47608053e-02  4.85844761e-02  1.94584262e-02
  3.13746184e-02  8.27705115e-03 -5.91487698e-02  1.24715902e-02
 -7.18181208e-02  2.27118507e-02 -1.86556969e-02  4.13743779e-02
  4.64739725e-02  4.42116745e-02 -2.60054357e-02  2.01542722e-03
  2.04487592e-02  7.32951891e-03  2.96617839e-02 -7.80048221e-03
  4.85806838e-02  3.09169292e-02  4.92489375e-02  4.83164117e-02
  6.91288933e-02  1.07524157e-01  2.74769198e-02  9.86756161e-02
  2.36382335e-03 -1.48489596e-02  7.85784051e-03 -2.63578575e-02
  1.00035768e-03  7.78194070e-02  1.95959979e-03 -3.91632611e-08
  1.39559666e-02  2.58649923e-02 -1.31440829e-04 -6.61841780e-03
 -5.67848235e-02 -1.16522210e-02  7.25531876e-02  6.49993047e-02
 -1.07025297e-03  5.53888641e-02 -4.88590971e-02 -2.36359406e-02
 -6.41950741e-02  3.68528515e-02  1.07470363e-01  2.62688082e-02
 -4.38796766e-02  8.49522501e-02 -1.09558245e-02 -6.15711138e-02
  4.15014699e-02  4.99730632e-02 -2.27348916e-02  7.81182479e-03
  2.99238991e-02  1.16525381e-03  4.44647372e-02  4.05661240e-02
  4.57634144e-02 -4.58346345e-02 -4.48392481e-02  1.63257401e-02
 -6.66277483e-02 -1.13023948e-02  6.46238998e-02  7.83369094e-02
 -2.25086082e-02  1.70855522e-02 -4.58826795e-02  7.38610923e-02
 -5.90751693e-02 -1.30868070e-02  1.34294191e-02 -7.26003796e-02
  6.24995045e-02  6.65142089e-02 -8.23121965e-02  7.18055572e-03
 -1.44563876e-02 -2.32752115e-02 -9.40606557e-03 -2.41354271e-03
 -1.36927646e-02  1.04943901e-01  3.31626981e-02  6.76710159e-03
 -2.46501435e-02 -1.09067999e-01  5.27897477e-02 -4.11484577e-02
  3.24048549e-02 -9.46087064e-04  3.47073153e-02 -3.43286097e-02]",2,2
auto_ml,1,automated machine learning for production and analytics pip install auto mlauto ml is designed for production here s an example that includes serializing and loading the trained model then getting prediction on single dictionary roughly the process you d likely follow to deploy the trained model auto ml ha all of these awesome library integrated generally just pas one of them in for model name ml predictor train data model name deeplearningclassifier available option are deeplearningclassifier and deeplearningregressor xgbclassifier and xgbregressor lgbmclassifer and lgbmregressor catboostclassifier and catboostregressorall of these project are ready for production these project all have prediction time in the millisecond range for a single prediction and are able to be serialized to disk and loaded into a new environment after training depending on your machine they can occasionally be difficult to install so they are not included in auto ml s default installation you are responsible for installing them yourself auto ml will run fine without them installed we check what s isntalled before choosing which algorithm to use get linear model esque interpretation from non linear model see the doc for more information and caveat binary and multiclass classification are both supported note that for now label must be integer and for binary classification auto ml will automatically detect if it is a binary or multiclass classification problem you just have to pas in ml predictor predictor type of estimator classifier column description column description also known a finally found a way to make this deep learning stuff useful for my business deep learning is great at learning important feature from your data but the way it turn these learned feature into a final prediction is relatively basic gradient boosting is great at turning feature into accurate prediction but it doesn t do any feature learning in auto ml you can now automatically use both type of model for what they re great at if you pas feature learning true fl data some dataframe to train we will do exactly that train a deep learning model on your fl data we won t ask it for prediction standard stacking approach instead we ll use it s penultimate layer to get it s most useful feature then we ll train a gradient boosted model or any other model of your choice on those feature plus all the original feature across some problem we ve witnessed this lead to a gain in accuracy while still making prediction in millisecond depending on model complexity ml predictor train df train feature learning true fl data df fl data this feature only support regression and binary classification currently the rest of auto ml support multiclass classification ever wanted to train one market for every store customer but didn t want to maintain hundred of thousand of independent model with ml predictor train categorical ensemble we will handle that for you you ll still have just one consistent api ml predictor predict data but behind this single api will be one model for each category you included in your training data just tell u which column hold the category you want to split on and we ll handle the rest a always saving the model loading it in a different environment and getting speedy prediction live in production is baked right in ml predictor train categorical ensemble df train categorical column store name http auto ml readthedocs io en latest before you go any further try running the code load up some data either a dataframe or a list of dictionary where each dictionary is a row of data make a column description dictionary that tell u which attribute name in each row represents the value we re trying to predict pas all that into auto ml and see what happens everything else in these doc assumes you have done at least the above start there and everything else will build on top but this part get you the output you re probably interested in without unnecessary complexity the full doc are available at http auto ml readthedocs io again though i d strongly recommend running this on an actual dataset before referencing the doc any futher automates the whole machine learning process making it super easy to use for both analytics and getting real time prediction in production a quick overview of buzzword this project automates analytics pas in data and auto ml will tell you the relationship of each variable to what it is you re trying to predict feature engineering particularly around date and nlp robust scaling turning all value into their scaled version between the range of and in a way that is robust to outlier and work with sparse data feature selection picking only the feature that actually prove useful data formatting turning a dataframe or a list of dictionary into a sparse matrix one hot encoding categorical variable taking the natural log of y for regression problem etc model selection which model work best for your problem we try roughly a dozen apiece for classification and regression problem including favorite like xgboost if it s installed on your machine hyperparameter optimization what hyperparameters work best for that model big data feed it lot of data it s fairly efficient with resource unicorn you could conceivably train it to predict what is a unicorn and what is not ice cream mmm tasty hug this make it much easier to do your job hopefully leaving you more time to hug those those you care about if you ve cloned the source code and are making any change highly encouraged or just want to make sure everything work in your environment run nosetests v test ci is also set up so if you re developing on this you can just open a pr and the test will run automatically on travis ci the test are relatively comprehensive though a with everything with auto ml i happily welcome your contribution here,"[('model auto ml', 0.554), ('binary classification auto ml', 0.5279), ('auto ml', 0.4357), ('ml predictor train', 0.4312), ('whole machine learning process', 0.4179), ('real time prediction', 0.4036), ('deeplearningregressor xgbclassifier', 0.3945), ('training data', 0.3939), ('machine learning', 0.3851), ('analytics pip', 0.3838)]","[-4.23850901e-02 -8.03301036e-02 -2.01991107e-02  2.67589781e-02
  5.52586690e-02 -1.83346160e-02  1.51747232e-02  3.81102413e-03
 -1.29575834e-01 -3.64664495e-02 -5.30683994e-02 -8.73954371e-02
 -8.80370755e-03 -2.63864007e-02 -6.43082708e-02 -6.35620160e-03
 -1.38174687e-02 -1.01521676e-02 -9.16011110e-02 -9.89508703e-02
 -2.53607277e-02 -5.78122539e-03 -4.39964570e-02  7.00504780e-02
 -3.87881845e-02 -7.56340753e-03  3.24368365e-02 -4.98764552e-02
 -1.91202704e-02 -4.41339426e-02 -7.20288530e-02 -7.88130015e-02
  1.21545263e-01  2.03958824e-02 -2.98711695e-02 -4.81874235e-02
  4.25682440e-02  2.01999173e-02  2.54000742e-02 -7.00200126e-02
 -1.89303234e-02 -1.41692877e-01  2.48012077e-02 -2.85447836e-02
  1.29037023e-01  6.53843880e-02  1.44409882e-02 -9.69355926e-02
 -3.55310366e-03  2.20902730e-02 -1.19267479e-01 -5.52515648e-02
 -3.64344567e-02  2.92036496e-02  1.82605279e-03 -1.64918788e-02
  4.41369638e-02 -7.21017306e-04  6.11285530e-02  2.67062783e-02
 -4.82411385e-02 -5.40570468e-02 -5.60472049e-02  6.26246557e-02
 -9.70128458e-03 -1.40268705e-03 -4.63102609e-02  4.50097919e-02
  1.42041072e-01  1.53180538e-03 -3.19891497e-02  1.46919554e-02
 -8.06257278e-02 -2.73483004e-02  3.79997119e-02 -5.57551626e-03
  9.61465761e-03 -1.15952417e-02  6.84316754e-02 -3.69352140e-02
 -9.51712802e-02  7.55810132e-03  4.39916812e-02  6.43468052e-02
 -2.08175909e-02 -7.33574759e-03 -6.09656535e-02  5.86511344e-02
 -3.90393287e-02 -3.81026790e-03  4.98579070e-02 -8.77364501e-02
 -1.19490596e-02  4.06772904e-02  2.03735824e-03  6.04932085e-02
 -6.20672340e-03 -9.81263146e-02  9.32383314e-02 -7.00696791e-03
 -3.20945755e-02  2.34061275e-02  3.89742106e-03  6.75127134e-02
 -5.09239845e-02  1.35584618e-03  4.61124294e-02  2.26744954e-02
  7.74352774e-02  2.12319884e-02 -1.97865944e-02 -2.45139357e-02
 -4.99089621e-02 -9.56084207e-02  5.26892766e-02  2.03751251e-02
 -7.18748942e-02  2.40017436e-02 -2.88515538e-02  1.25773355e-01
 -1.25085443e-01  3.94040942e-02  3.62123661e-02 -1.38437850e-02
  2.95701362e-02 -3.09470426e-02 -1.07734874e-01  5.86522788e-33
 -3.38008478e-02 -2.18399800e-02 -3.13086174e-02 -1.80616435e-02
 -6.64516864e-03 -5.55982850e-02 -6.02473971e-03  3.09141297e-02
  6.09007515e-02  6.27486967e-04 -2.94949338e-02  1.32658422e-01
 -7.06727654e-02  8.45618844e-02  3.65340672e-02  6.48419783e-02
  1.67777557e-02  6.62737936e-02 -2.59620473e-02  3.22161838e-02
  1.20073669e-01  1.06910234e-02  1.69626977e-02  1.69509687e-02
  2.62250472e-02  8.17573965e-02  1.23707820e-02  1.61199551e-03
 -2.19154134e-02  5.18613048e-02 -2.25601308e-02 -9.81613994e-03
  1.27459466e-02  3.08822542e-02 -1.10834753e-02  4.60928977e-02
 -4.68708761e-02  9.87428613e-03 -3.85670476e-02  3.85595448e-02
 -8.85512680e-03  3.68537270e-02 -1.41723370e-02 -3.87732647e-02
 -3.86157408e-02 -5.31234220e-02  1.68692991e-02  3.10034361e-02
 -1.14434743e-02  1.28108822e-02 -2.48524714e-02 -3.92674096e-02
 -3.46815004e-03 -5.17518893e-02 -1.73136480e-02  7.65376016e-02
  1.21953236e-02  1.01320110e-02  1.45389764e-02  5.57608865e-02
 -4.88783792e-02  1.52667444e-02  4.19741534e-02 -3.58380601e-02
 -6.25585718e-03 -4.77420911e-02  6.34921268e-02 -8.72985460e-03
 -7.22242100e-03  2.65741199e-02 -9.37801898e-02 -2.12506056e-02
 -1.28296018e-02 -3.97678502e-02  3.68161388e-02 -8.39244295e-03
 -2.39736568e-02 -6.39290884e-02 -3.53659764e-02  1.17633842e-01
 -4.06876318e-02  1.99945755e-02  4.19249795e-02 -2.93717105e-02
 -1.31233130e-02 -1.89248901e-02  2.15313975e-02 -1.08292233e-02
 -5.13630956e-02 -1.85798146e-02 -1.33115500e-01 -4.76688473e-03
 -4.24435921e-02  4.44689691e-02 -3.05931401e-02 -6.54972228e-33
 -6.62631541e-03  8.01779032e-02 -3.31229754e-02  1.13080621e-01
  1.52737731e-02  2.13530324e-02 -5.63740097e-02 -3.08081079e-02
  5.88253811e-02  7.87364505e-03  2.25886665e-02  7.02341204e-04
  2.33445838e-02  4.26618084e-02 -4.65031527e-02 -2.20836364e-02
 -8.31092224e-02 -6.80942014e-02 -1.08645838e-02  4.93243486e-02
 -9.69377160e-02  7.65577704e-02 -8.24303180e-02 -2.64844131e-02
  2.52514035e-02 -2.12810878e-02 -3.97187769e-02  1.89950913e-02
  8.35159793e-02  4.46906462e-02  4.09768559e-02  1.95999220e-02
 -6.63718733e-04  1.21335546e-02 -2.01015491e-02 -4.59260866e-02
  6.38011768e-02 -1.83444321e-02  5.11310175e-02  4.54424173e-02
  1.14247061e-01  3.23134921e-02 -6.31753495e-03 -5.10722809e-02
 -1.87349673e-02  2.55808625e-02 -6.10764809e-02  8.46894383e-02
  1.69009790e-02 -4.99159060e-02 -1.57720875e-02  2.01730356e-02
 -4.69412617e-02 -7.06640771e-03 -1.67313367e-02  1.39314188e-02
  5.13938554e-02  2.51294356e-02  4.62968275e-03  2.96336655e-02
 -5.83160669e-02 -5.17090375e-04  7.64037892e-02  2.33981200e-03
 -3.59703712e-02  2.63967421e-02 -1.88626684e-02 -1.49290552e-02
 -2.74588075e-02 -6.09051920e-02  4.68214191e-02  2.02432014e-02
  1.61901265e-02  7.40926862e-02 -5.34633398e-02 -4.45482805e-02
 -6.59198463e-02 -3.13322768e-02  2.28694752e-02 -4.94711585e-02
  4.78904881e-02  8.55573826e-03  2.21056696e-02  1.11997262e-01
  1.30445296e-02  4.88887802e-02  4.71456982e-02  1.21484033e-03
  3.46659571e-02 -5.42467162e-02 -5.84921353e-02  4.24081311e-02
 -2.86542382e-02  7.41351098e-02 -6.97425306e-02 -2.78151440e-08
  1.15028312e-02  2.50863135e-02  1.27813265e-01  7.06047565e-02
  4.19673622e-02  3.88197415e-02 -8.77205655e-02  1.94283187e-01
 -1.27213420e-02  9.16297510e-02 -1.53776992e-03 -3.63097079e-02
 -1.22425109e-01 -2.20651031e-02  7.64185656e-03  2.44026147e-02
  6.23699054e-02  5.42034581e-03  2.07161438e-02 -4.81961109e-02
  5.29263988e-02 -2.23767869e-02  3.84851247e-02 -3.23699638e-02
  5.59387393e-02 -9.14407670e-02  3.46154161e-02 -1.11119673e-02
 -2.72348821e-02  2.91142184e-02 -4.75729592e-02 -2.26377621e-02
  4.11977246e-02 -3.91047709e-02  3.58792357e-02  7.11910725e-02
  8.01965147e-02 -8.16032961e-02 -4.80566323e-02 -2.48508272e-03
  1.22483652e-02  5.62236756e-02 -2.90412996e-02 -2.77239569e-02
  1.04780393e-02 -7.17948191e-03 -1.46412430e-02 -6.53708950e-02
 -1.77540295e-02  1.17898956e-02 -3.93591039e-02  1.39276143e-02
  3.68545614e-02  3.41432840e-02  3.57003435e-02  1.17947340e-01
  3.86090651e-02 -1.22120611e-01  1.07035581e-02  2.25265920e-02
 -1.62690468e-02  3.80645283e-02  3.47740538e-02 -2.06094570e-02]",2,0
tf-big,1,tf big add big number support to tensorflow allowing computation to be performed on arbitrary precision integer internally these are represented a variant tensor of gmp value and exposed in python through the tf big tensor wrapper for convenience for importing and exporting number are typically expressed a string python package are available from pypi see below for further instruction for setting up a development environment we recommend using miniconda or anaconda to set up and use a python or environment for all instruction below the only requirement for ubuntu is to have docker installed this is the recommended way to build custom operation for tensorflow we provide a custom development container for tf big with all dependency already installed setting up a development environment on macos is a little more involved since we cannot use a docker container we need four thing using homebrew we first make sure that both bazel and gmp are installed we recommend using a bazel version earlier than e g the remaining pypi package can then be installed using run the test on ubuntu by running the make test command inside of a docker container right now the docker container doesn t exist on docker hub yet so we must first build it then we can run make test once the development environment is set up we can simply run this will install tensorflow if not previously installed and build and run the test just run for linux doing it inside the tensorflow tensorflow custom op container is recommended note that circleci is currently used to build the official pip package we use circle ci for integration testing and deployment of tf big,"[('tensorflow tensorflow custom op container', 0.5678), ('big tensor wrapper', 0.4587), ('tensorflow', 0.4572), ('custom development container', 0.3607), ('docker container', 0.3585), ('docker', 0.3358), ('docker hub', 0.3196), ('official pip package', 0.3194), ('pypi package', 0.3192), ('arbitrary precision integer', 0.3123)]","[-2.97473837e-02 -1.24470592e-02  2.34571043e-02 -1.12985158e-02
  4.57061604e-02 -1.95504315e-02 -1.30621353e-02  3.08596995e-02
 -5.43172881e-02 -3.35952714e-02 -5.23843728e-02 -8.51684157e-03
 -1.04608402e-01  4.93267104e-02 -1.49524733e-02 -5.06468974e-02
  8.17193091e-03  9.60654169e-02 -2.73871683e-02 -3.83500122e-02
 -1.25747487e-01  3.94544005e-02  7.87553377e-03  2.95403805e-02
 -5.19978590e-02 -2.31541526e-02 -1.34476647e-02 -4.71769311e-02
  6.76904619e-02 -4.07816283e-02  2.13271659e-02  1.11570604e-01
  9.31088328e-02 -5.44880405e-02  5.28770611e-02  5.05694114e-02
  3.10689793e-03 -7.21944422e-02 -2.90890015e-03 -3.24376114e-02
  6.13776222e-02 -5.85022494e-02 -2.51461212e-02  3.81336883e-02
  3.23788146e-03 -3.83922607e-02  5.38319349e-02 -9.05483887e-02
  7.63299735e-03  2.24307235e-02 -2.22134758e-02 -3.94992605e-02
 -6.55528083e-02  3.73238735e-02 -8.08543935e-02 -4.00378183e-02
  1.16813183e-02 -1.38489651e-02 -3.77372466e-02 -1.86769143e-02
 -4.59916564e-03  2.51261201e-02 -2.02899184e-02 -1.79610047e-02
  1.15520041e-02  6.81612045e-02 -3.88345346e-02 -1.77411251e-02
  1.27383485e-01 -3.10509223e-02  1.80550627e-02  1.01312192e-03
 -2.61254478e-02  3.28500718e-02 -2.72759907e-02 -2.37807240e-02
  3.09267323e-02 -2.47762259e-02 -1.18769882e-02  4.16010432e-02
 -3.50598469e-02 -1.65813752e-02  5.10538816e-02 -7.77440742e-02
 -1.28660686e-02  1.88488117e-03  5.47949076e-02  4.88200597e-02
  5.29805049e-02 -1.05970159e-01  4.77863327e-02 -2.36325264e-02
 -3.10518022e-04  1.32212993e-02  8.29255879e-02  5.21438792e-02
 -6.57702759e-02 -6.92308089e-03 -5.40435240e-02  5.00832982e-02
 -4.12326902e-02 -3.66533250e-02 -3.53260315e-03  6.42461926e-02
  1.76715031e-02  5.96539266e-02  1.53526608e-02 -4.44029048e-02
 -5.80923120e-03  1.35453856e-02  2.06453949e-02 -3.76461335e-02
 -2.21596807e-02 -6.36275411e-02  6.07274733e-02  7.27960549e-04
 -9.11290348e-02  3.53377871e-02  5.23349196e-02  5.19861765e-02
 -2.38988586e-02  3.20152193e-02  8.25135708e-02  1.25378380e-02
 -6.79104030e-02  7.63988048e-02 -1.00991137e-01  4.00569316e-33
 -3.20298746e-02 -4.40615937e-02  2.98289862e-02  1.57567319e-02
  1.11109607e-01 -2.77598016e-02  6.98760897e-02 -2.35905834e-02
  1.40515454e-02 -1.20220603e-02 -8.65106806e-02  6.54753968e-02
  4.24051695e-02  7.22585842e-02 -1.36012584e-03 -1.15845390e-01
 -7.35072121e-02  4.31832559e-02  2.90416628e-02  3.84059362e-02
  6.92242086e-02  5.71750896e-03 -2.25344356e-02  3.70231308e-02
  8.77330601e-02  1.01382490e-02  3.01636737e-02 -9.39543080e-03
 -9.80931427e-03  1.34611102e-02 -5.88541254e-02 -1.00405209e-01
  1.95421595e-02  1.69101134e-02 -9.72429477e-03 -2.47970447e-02
 -5.54799587e-02 -1.66696142e-02 -4.06345129e-02 -3.56335454e-02
 -3.37146334e-02  4.59355153e-02  1.19900210e-02 -4.37074853e-03
  2.34532524e-02 -9.20199417e-03  8.54934007e-03  5.19968383e-02
  3.16421464e-02 -1.17809670e-02 -2.05549970e-02 -1.38500445e-02
 -9.07927305e-02 -2.19015218e-02 -8.54114443e-03 -1.21397741e-01
 -1.04352308e-03  2.28693262e-02  1.02987170e-01  2.81134043e-02
 -9.26130340e-02  2.39239484e-02  2.33227294e-02  1.06307175e-02
  3.38559821e-02  4.58750986e-02 -3.51646952e-02  3.53676900e-02
  1.94649845e-02  7.86897093e-02 -1.33167908e-01  8.59572738e-02
 -1.04971137e-02  3.53309251e-02  6.31898362e-03 -2.09059268e-02
  6.00258075e-02 -3.80953737e-02  1.93432849e-02 -6.59145322e-03
 -7.10852817e-02  7.21554607e-02  3.38190310e-02  3.15049700e-02
 -1.03367455e-01 -7.14366660e-02 -1.56669989e-02  5.01877181e-02
 -6.35947008e-03 -4.93256040e-02 -1.04539618e-02  7.69113079e-02
  6.11370243e-02 -4.65462776e-03 -1.14118971e-01 -3.86165176e-33
 -2.54029203e-02  7.82288611e-04 -7.20009357e-02  7.19605982e-02
 -1.32632703e-02  6.32355586e-02  8.53454620e-02 -9.59625561e-03
  8.16156119e-02 -1.05405683e-02 -7.97178876e-03  2.53611300e-02
  7.58279637e-02  1.84839871e-02 -7.03012059e-03 -1.34102593e-03
 -1.01604760e-01 -6.98341802e-02  5.37453964e-02 -2.44500823e-02
 -2.06666291e-02  5.38546257e-02 -1.89699139e-02  5.41093722e-02
 -1.12981750e-02 -7.82793667e-03 -3.44880149e-02 -9.82641876e-02
  2.45070085e-04  5.28231310e-03 -3.05994339e-02 -1.93921383e-02
 -2.31935102e-02  4.93439175e-02  2.94063264e-03  1.57019999e-02
  7.75408978e-03  4.14906889e-02 -3.44558321e-02  2.23836526e-02
  1.63749345e-02 -1.90436356e-02 -1.97525714e-02  6.68548197e-02
 -8.47445652e-02 -3.64012085e-02 -3.18752304e-02  1.38807734e-02
 -2.74144001e-02 -9.84687805e-02 -8.49312395e-02 -2.02177409e-02
 -6.50187209e-03 -8.67083445e-02 -6.27266616e-03  2.34503709e-02
  6.69945404e-02  5.59246503e-02  3.06648593e-02  6.11497611e-02
  8.70826691e-02 -1.48648396e-02  6.98623732e-02 -3.04593332e-02
 -3.77520965e-03  2.70557292e-02 -9.37103555e-02 -3.38637233e-02
 -8.30274597e-02  4.60302345e-02  1.78896599e-02 -3.80627513e-02
  1.04161818e-02  7.24682137e-02 -5.82261123e-02 -2.96772867e-02
  7.70887434e-02  5.05623482e-02  1.01756006e-01  6.00390770e-02
  2.24409048e-02  3.50782648e-02  3.65851261e-02  4.38456722e-02
  4.81346548e-02 -2.26001199e-02  5.10360524e-02 -2.53666681e-03
  7.57856369e-02  3.62243429e-02 -7.36466725e-04  8.98500998e-03
 -6.10286891e-02  8.58994797e-02  4.72736247e-02 -2.43180267e-08
 -9.89810005e-03  4.83511947e-02 -7.97928963e-03  5.08519374e-02
 -3.15846950e-02  9.65481903e-03  6.55687526e-02  6.64346442e-02
 -1.08310552e-02 -1.62215009e-02 -1.48838796e-02 -4.57098112e-02
 -1.32579833e-01  6.21643569e-03  5.65201975e-02  7.21653104e-02
 -7.99015835e-02  6.79126456e-02 -3.75645272e-02 -1.02819420e-01
  1.00880321e-02  8.25662836e-02  6.14218712e-02 -6.41329437e-02
 -6.56514764e-02 -7.03398883e-03  9.41944346e-02  9.08004567e-02
  1.18549226e-03  2.66880030e-03  1.15646757e-02 -6.30586818e-02
 -2.11823005e-02 -1.21948160e-02  4.52412479e-02  5.43045253e-02
 -5.78607321e-02 -1.71729419e-02  1.67319796e-03  4.77830805e-02
 -1.23572655e-01 -3.32001820e-02  1.75218098e-04 -6.66124448e-02
  3.04036997e-02 -6.79786783e-03 -6.11966476e-02 -5.26691303e-02
 -5.13558798e-02  4.96372916e-02  6.30304515e-02  7.52112716e-02
 -4.02268507e-02  1.10898621e-01  2.74609942e-02  7.16523677e-02
 -1.03601962e-01 -9.96478721e-02 -6.13549501e-02 -6.37546256e-02
 -1.45887798e-02  3.44698206e-02  3.42142619e-02 -8.60225875e-03]",2,2
dynet,1,the dynamic neural network toolkitdynet is a neural network library developed by carnegie mellon university and many others it is written in c with binding in python and is designed to be efficient when run on either cpu or gpu and to work well with network that have dynamic structure that change for every training instance for example these kind of network are particularly important in natural language processing task and dynet ha been used to build state of the art system for syntactic parsing machine translation morphological inflection and many other application area read the documentation to get started and feel free to contact the dynet user group group with any question if you want to receive email make sure to select all email when you sign up we greatly appreciate any bug report and contribution which can be made by filing an issue or making a pull request through the github page you can also read more technical detail in our technical report you can find tutorial about using dynet here c and here python and here emnlp tutorial one aspect that set dynet apart from other tookits is the auto batching feature see the documentation about batching the example folder contains a variety of example in c and python dynet relies on a number of external program library including cmake and eigen cmake can be installed from standard repository for example on ubuntu linux or on macos first make sure the apple command line tool are installed then get cmake and mercurial with either homebrew or macports on window see documentation to compile dynet you also need a specific version of the eigen library if you use any of the released version you may get assertion failure or compile error you can get it easily using the following command you can install dynet for c with the following commandsfor more detail refer to the documentationyou can install dynet for python by using the following commandfor more detail refer to the documentationif you use dynet for research please cite this report a follows we welcome any contribution to dynet you can find the contributing guideline here,"[('dynamic neural network toolkitdynet', 0.6955), ('neural network library', 0.5215), ('python dynet', 0.451), ('auto batching feature', 0.4014), ('training instance', 0.35), ('dynet', 0.3409), ('machine translation', 0.3197), ('python', 0.3151), ('email', 0.3134), ('natural language processing task', 0.2827)]","[-1.14363827e-01 -7.46114925e-02  8.69275443e-03 -7.02937087e-03
 -6.22011814e-03 -4.51827869e-02  7.79601187e-03  4.42830399e-02
 -6.21712394e-02 -6.67512491e-02 -6.93207160e-02 -9.58440360e-03
 -4.46549468e-02  2.62701884e-02  3.90993292e-03  3.17519978e-02
  8.13714787e-03  6.59341440e-02 -3.93800698e-02 -1.09947316e-01
  3.46269202e-03  4.89993840e-02  5.13070188e-02  3.01532205e-02
 -5.63903050e-05  1.94248941e-03  2.06320663e-03 -6.88941032e-02
  1.49810957e-02 -7.17954244e-03 -2.22318117e-02 -3.36281694e-02
 -6.56339899e-03  7.15309605e-02 -3.69299129e-02  3.39182839e-02
 -3.28174569e-02 -2.98850965e-02  3.00112460e-02 -6.48259446e-02
 -4.32823040e-02  1.81105044e-02 -2.26910114e-02 -1.21843917e-02
  1.40667409e-01  1.02450326e-02 -4.15974744e-02  9.63424053e-03
 -1.69731062e-02  2.85075717e-02 -1.10226825e-01 -6.98340610e-02
 -1.72853488e-02  6.95106462e-02  1.26110353e-02  3.35351899e-02
  4.80600931e-02  3.24645936e-02  2.39763893e-02 -3.90935875e-02
  2.40663416e-03 -2.55976263e-02 -2.47673634e-02  9.66329221e-03
 -2.10591964e-02  2.61368603e-02  4.73281369e-03  5.36489636e-02
  1.35966018e-01 -8.42888653e-02 -5.52101098e-02  2.91972160e-02
 -6.19997829e-02  7.64507279e-02  2.17791577e-03 -3.54980840e-03
  6.36443570e-02 -5.37178852e-02  7.04772100e-02 -8.81398469e-02
 -4.09122109e-02  2.79977415e-02  8.04960877e-02  1.29701486e-02
  1.22613162e-02  3.16466354e-02 -5.30972704e-02  5.89798465e-02
 -4.43946896e-03 -1.31456961e-03  2.17029769e-02 -1.41036391e-01
  3.68253440e-02 -3.86253782e-02 -1.71275940e-02 -1.14717325e-02
  4.33765259e-03 -7.66432509e-02 -2.01107990e-02  4.52133305e-02
 -2.55678426e-02 -2.75170282e-02 -2.98201405e-02 -1.14673597e-03
 -6.96021914e-02  4.45024669e-02  9.16693956e-02 -9.19846445e-02
  7.54671097e-02 -2.36239582e-02 -3.83555964e-02  8.61668028e-03
 -3.04660648e-02 -8.80934224e-02  5.30757494e-02 -4.55663353e-02
 -2.54471153e-02 -4.93708206e-03  6.45952448e-02  7.18896613e-02
 -1.25055417e-01  2.09167656e-02 -8.67782999e-03  5.46683976e-03
  3.37050445e-02 -1.32087013e-02 -4.83795926e-02  3.51616625e-33
  3.73278395e-03 -3.18572633e-02  2.11229231e-02 -5.78449061e-03
  6.29993528e-02 -1.07101306e-01  5.58424108e-02  5.13761044e-02
  8.66802409e-03 -9.00823846e-02 -5.85047752e-02  1.01406477e-01
 -4.24210131e-02  7.17210621e-02  1.17078787e-02 -9.55149233e-02
  5.21762967e-02  6.91072196e-02  5.99208958e-02  2.68697999e-02
  3.82767692e-02  3.70242521e-02  3.32026072e-02  7.08889291e-02
  4.36927378e-02  1.66838220e-03  6.57023937e-02 -2.03839932e-02
 -1.49111096e-02  1.64529420e-02 -1.43802971e-01 -6.06099106e-02
  3.08579151e-02  5.07299304e-02 -6.00235676e-03 -2.74045728e-02
 -5.63185327e-02 -5.36576658e-02  4.58988659e-02  1.68152004e-02
 -4.42652926e-02 -1.67173222e-02  4.64874692e-03 -3.38062830e-02
 -5.10750860e-02  7.65166385e-03  1.45389605e-02  6.81435317e-02
  1.71679109e-02  3.37326108e-03 -3.29685770e-02 -8.96849390e-03
  2.98248278e-03 -7.09178373e-02 -4.13124412e-02  5.95711730e-02
  2.53786445e-02 -1.04832454e-02  7.22509772e-02  7.31142014e-02
  3.00718863e-02  7.73207657e-03  1.28522152e-02 -1.17625250e-02
  7.09371269e-02 -3.77893113e-02  1.93085633e-02 -4.60945023e-03
  3.11485212e-02 -2.46938393e-02 -3.80297340e-02  2.02443823e-03
  6.74331561e-03 -3.45956087e-02  2.55756322e-02 -7.48163741e-03
  2.53744815e-02 -7.59952590e-02 -7.46813044e-02  7.43220150e-02
 -4.05249558e-02 -1.90379359e-02  2.47873906e-02 -4.36510108e-02
  6.27191216e-02  1.63708627e-03  3.13187465e-02 -3.38732339e-02
  1.30864950e-02  2.59943008e-02 -5.11972643e-02 -2.88803335e-02
 -5.49565349e-03  8.20656866e-02  1.21167675e-02 -2.53350881e-33
 -4.43985388e-02  4.90573831e-02 -1.49246663e-01  1.40844241e-01
 -2.39451780e-04  5.47132753e-02 -4.90493663e-02 -6.79541193e-03
  2.19553113e-02 -1.35287666e-03 -4.21452010e-03 -5.55035286e-02
  2.26639141e-03 -4.77063991e-02 -1.34896897e-02 -2.61969417e-02
 -1.36501208e-01 -1.72553770e-02  3.34364772e-02  1.26623273e-01
 -5.75391427e-02  1.04473613e-01 -8.58786553e-02 -3.59430187e-03
 -5.93494577e-03 -4.08946276e-02 -3.14182602e-02  3.07279360e-02
  3.37002613e-02  2.13572644e-02  6.42014248e-03  2.89719868e-02
 -2.98656859e-02  4.17179391e-02  3.21063623e-02  3.84566095e-03
  6.63579628e-02 -2.44704541e-02  1.12543963e-02  3.54444645e-02
  1.29988581e-01  4.07281816e-02 -1.04514686e-02  1.56511758e-02
 -2.44619027e-02  1.55575965e-02 -1.76879615e-01  7.42967129e-02
 -3.07341851e-02  2.87540052e-02  2.84142513e-02 -6.82768459e-03
 -4.91089895e-02  5.70176542e-03 -1.69490743e-02  1.65300872e-02
  9.49943811e-02  1.12277325e-02  1.91817142e-03  4.74997945e-02
 -9.77226794e-02 -6.67824224e-02  1.08558983e-01 -9.33524035e-03
 -1.28547894e-02 -1.34014714e-04 -9.27692875e-02  3.19396220e-02
 -4.12375666e-02 -6.06013536e-02  6.30855411e-02  3.78696881e-02
  5.68478256e-02  8.07565153e-02 -1.16058208e-01 -3.41596566e-02
 -6.30136877e-02 -7.77299851e-02  6.02264936e-06 -7.75921682e-04
 -1.97212142e-03  2.28678305e-02  3.88728804e-03  9.10441726e-02
 -4.18008864e-02  9.54694301e-02  3.85803431e-02  4.67410013e-02
  6.04221895e-02  2.38904078e-02  2.44442653e-02  1.50634199e-02
  1.12720765e-03  1.09876946e-01 -1.09656937e-02 -2.77389134e-08
 -1.84255615e-02  1.16164908e-02  8.27367082e-02  3.27551514e-02
  2.54238881e-02 -5.46182841e-02  3.54243144e-02  1.22917041e-01
 -1.67183764e-02  2.45299414e-02  7.80238025e-03 -5.32840006e-02
 -6.70035481e-02 -3.72442529e-02 -1.17125800e-02  4.82861698e-02
  2.29352731e-02  6.78801071e-03  2.49926597e-02 -4.77466024e-02
  8.13384354e-02  7.34269712e-03  2.84930039e-03  3.36379223e-02
  2.89276168e-02 -3.48425880e-02 -1.97203103e-02  3.74188796e-02
  2.00190730e-02 -2.32086587e-03  8.26317817e-03 -1.55122271e-02
  3.09672318e-02 -8.17276388e-02  4.07864340e-02  6.89410195e-02
  1.67417917e-02 -7.18446076e-02 -1.95034109e-02  5.65719977e-02
 -1.86170936e-02  2.34309528e-02 -6.55238517e-03 -5.42446412e-03
  2.36073546e-02 -2.67048441e-02 -6.79568499e-02 -4.96737771e-02
  4.52737808e-02 -6.18069209e-02  2.73331255e-03  4.15130965e-02
 -4.46495935e-02  6.86408626e-03  2.89401840e-02  6.80159256e-02
  4.53175325e-03 -7.69247264e-02 -1.44020030e-02  9.21821520e-02
 -4.86609861e-02  7.53991380e-02  4.84552868e-02 -3.73741947e-02]",2,2
torch-struct,1,a library of tested gpu implementation of core structured prediction algorithm for deep learning application designed to be used a efficient batched layer in other pytorch code tutorial paper describing methodology full doc http nlp sea harvard edu pytorch struct current distribution implemented each distribution includes extension everything implemented through semiring dynamic programming,"[('deep learning application', 0.5035), ('other pytorch code tutorial paper', 0.4936), ('structured prediction algorithm', 0.4433), ('gpu implementation', 0.409), ('core', 0.3178), ('layer', 0.2729), ('dynamic programming', 0.2119), ('library', 0.1761), ('current distribution', 0.1331), ('extension everything', 0.0924)]","[-1.55055210e-01 -7.73697048e-02 -2.12753308e-03 -1.31966816e-02
  2.51273415e-03 -4.48783189e-02 -5.52396439e-02  8.68038759e-02
 -1.38573870e-01 -6.26083165e-02 -7.22844526e-02  8.89086281e-04
 -5.93240522e-02 -2.70061474e-02 -4.72261831e-02  5.37834968e-03
  8.80411919e-03  8.09445307e-02 -7.19569474e-02 -1.17429599e-01
  2.47048866e-02 -3.31025049e-02 -2.02549342e-02 -5.26581053e-03
  2.74231546e-02  3.28398645e-02  3.05919610e-02 -5.23238629e-02
  8.45074132e-02  1.77745037e-02  1.92348696e-02 -4.12221067e-02
  7.46325105e-02  7.29211569e-02 -3.53120677e-02  1.22519042e-02
 -6.43542632e-02 -3.35185491e-02 -2.75537614e-02  2.69303122e-03
 -3.41169015e-02 -1.14890700e-02  8.54048692e-03  4.31374349e-02
  7.35153034e-02 -3.06113157e-03 -1.91205516e-02 -9.99833420e-02
 -6.07194286e-03 -2.98542511e-02 -6.12250306e-02 -1.76827461e-02
 -3.50143649e-02  7.48530356e-03 -2.45332047e-02 -3.74673232e-02
  2.04481427e-02 -5.18840645e-03  7.94477819e-04 -5.16528450e-02
  7.56068826e-02 -5.19047342e-02 -6.65265769e-02  2.40841545e-02
 -1.83102544e-02  5.68251796e-02 -1.94766875e-02  7.93361068e-02
  1.15652263e-01 -1.56892926e-01 -1.58541761e-02  3.73768955e-02
 -3.37868221e-02 -6.70387736e-03 -1.38903852e-03 -5.00432365e-02
  4.86587547e-02  3.66429071e-04  3.92480791e-02 -1.05262309e-01
  3.34428996e-02  7.09211593e-03  2.49944124e-02  7.18342662e-02
  5.78800365e-02 -2.71247774e-02 -1.74863562e-02  4.39867936e-02
 -3.83722186e-02 -1.14102094e-02  1.06952690e-01 -9.56074893e-02
 -3.74779031e-02  1.27831204e-02 -9.16890422e-05  1.71665773e-02
 -2.86867246e-02 -1.18174784e-01 -4.05598395e-02  3.60038951e-02
 -4.01701257e-02 -4.86124530e-02  4.11662981e-02 -1.56802256e-02
 -8.41800030e-03  2.62023546e-02  1.88619997e-02  4.49510179e-02
  6.27328008e-02 -2.36342754e-02 -1.75296590e-02  5.95221445e-02
 -6.07592147e-03 -6.24320246e-02  1.01191193e-01  5.32044191e-03
  3.89039703e-02  2.26916140e-03 -2.84084678e-02  7.29080215e-02
 -1.02113269e-01  2.76098661e-02 -3.01170293e-02 -3.45388353e-02
 -6.28337413e-02 -3.71578410e-02 -8.14413577e-02  1.94788763e-33
 -3.06545608e-02 -3.75771448e-02 -3.53282653e-02 -3.13048624e-02
  3.92207466e-02 -8.27050731e-02  2.80485097e-02 -2.93979179e-02
 -1.78518035e-02 -3.73458154e-02  2.76673865e-02  4.51792590e-02
 -4.22419868e-02  1.58149570e-01 -1.52676264e-02 -8.78765155e-03
 -2.44915448e-02  4.69403714e-02  8.49749744e-02  3.65277790e-02
  5.36657870e-02  4.00252603e-02  1.00537827e-02  3.93362455e-02
 -1.63900238e-02  1.15317561e-01 -3.35598886e-02 -7.45616795e-04
  2.01676483e-03  1.83676910e-02 -5.28718121e-02  3.72600891e-02
 -8.74569491e-02 -1.58811361e-02 -3.28672528e-02  2.93290168e-02
 -7.65738310e-03 -5.31179644e-02 -3.69809121e-02 -7.12182606e-04
 -4.58775349e-02  1.75330229e-02 -2.16432679e-02 -5.22684865e-02
 -1.06931450e-02 -6.02020174e-02  2.47015711e-02  6.30438998e-02
  4.07758951e-02 -5.59058525e-02 -4.39942218e-02 -1.41555294e-02
 -5.29097989e-02  2.42281910e-02 -6.39048917e-03 -6.60754973e-04
  5.67045063e-02  1.88887715e-02  4.39287946e-02  8.50373730e-02
  2.34499294e-02  2.90353713e-03 -2.53499728e-02 -5.03936410e-02
 -6.39664829e-02 -2.74048615e-02 -3.78165171e-02  1.15087964e-02
 -2.06117779e-02  7.91168138e-02 -3.66943628e-02  4.35807668e-02
  5.99680580e-02 -3.93159725e-02 -1.05311451e-02  1.29087949e-02
  3.32257524e-02 -6.82670325e-02 -3.56349200e-02  6.06909320e-02
 -1.04890622e-01  8.77048671e-02  4.29488756e-02 -2.46677455e-03
 -5.32488115e-02  2.75838934e-02  2.23670062e-02 -1.39097683e-02
 -4.30130064e-02  2.69622984e-03 -7.07885250e-02 -7.42138457e-03
  7.23751336e-02  4.71285991e-02  1.96129717e-02 -1.44912776e-33
 -3.70136509e-03  8.79108161e-02 -5.89709431e-02  7.73565993e-02
  9.12298039e-02  1.13203544e-02 -7.14294314e-02  1.72736996e-03
  2.69819889e-02 -9.30248015e-03  1.25954747e-02 -3.80555950e-02
  4.13356572e-02  8.29342753e-02  1.98546462e-02 -5.75380214e-03
 -7.76671916e-02  5.67817455e-03 -2.03374662e-02 -7.30939675e-03
 -6.57185987e-02  1.29840568e-01 -5.70929311e-02 -3.69495302e-02
  3.08586806e-02 -7.84003809e-02 -4.38862033e-02  2.54122466e-02
  6.15689903e-02 -4.53638211e-02 -2.82588601e-03 -5.97942527e-03
 -4.10220809e-02  3.46829891e-02  3.01001011e-03  3.66906039e-02
  6.42061904e-02 -2.42640022e-02  2.60007419e-02 -5.01957908e-02
  1.70247361e-01  1.95968952e-02  5.86971268e-02 -3.14770192e-02
 -3.32930088e-02  6.50181249e-02 -4.56383303e-02  2.69805118e-02
  4.38115094e-03 -8.24354310e-03  4.78280708e-02 -2.56293882e-02
  4.36583608e-02 -1.89335402e-02 -1.23717152e-02  3.34967822e-02
  6.79081827e-02  5.18497676e-02  6.17637187e-02  3.40710767e-02
 -1.03995807e-01 -5.23600355e-02  3.48089859e-02 -5.36271045e-03
 -4.88222353e-02 -2.20463220e-02 -7.82878622e-02  4.24926355e-02
 -4.01126333e-02 -3.92185003e-02 -6.54613087e-03  5.72339222e-02
  8.33799914e-02  1.03930868e-01 -6.67023808e-02  3.11722443e-03
 -5.06116403e-03  3.38406898e-02  1.49238734e-02 -4.09618318e-02
  2.47535910e-02  3.25869434e-02 -5.33832295e-04  4.75467071e-02
  3.45060453e-02  1.01469472e-01  4.80758809e-02 -1.84105746e-02
  5.02808951e-02 -6.14251085e-02 -8.87215063e-02  8.24642777e-02
  4.66339663e-02  7.60700852e-02 -4.56724502e-02 -2.58517616e-08
 -2.48732790e-02  1.81179475e-02  2.37167515e-02 -1.65629610e-02
  1.73007231e-02  2.80103646e-02  9.48808193e-02  9.38297734e-02
 -5.56203574e-02  6.52573109e-02  9.97484941e-03 -5.63360564e-02
 -1.03054360e-01 -1.76377576e-02  1.55175952e-02  8.76270831e-02
  3.16204652e-02 -1.46761648e-02  5.72750606e-02 -6.06915094e-02
  4.30793688e-02  4.13015820e-02  4.12147865e-03  1.56313535e-02
 -1.98873803e-02 -3.38734798e-02  6.41034544e-02 -2.10210849e-02
 -1.45267770e-02 -2.70566973e-03 -4.25790902e-03  2.68867947e-02
  9.37939063e-02 -4.25208509e-02  8.68446827e-02  8.93517509e-02
  7.07550347e-02 -1.17791230e-02  5.92398457e-03  4.72019054e-02
 -2.09452417e-02  1.95225682e-02  1.17200809e-02 -2.95697786e-02
  2.83868704e-02 -6.04333449e-03 -1.33218886e-02 -8.16774815e-02
  9.74394195e-03  5.53956740e-02 -1.67234372e-02  4.30570655e-02
 -1.45606427e-02 -7.50650419e-03  6.30215704e-02  9.82407033e-02
 -1.65240169e-02 -7.97181204e-02  3.52896936e-02  3.34098674e-02
 -4.17400338e-02  3.89247574e-02 -1.77519396e-02  7.71886334e-02]",2,2
spektral,1,spektral is a python library for graph deep learning based on the kera api and tensorflow the main goal of this project is to provide a simple but flexible framework for creating graph neural network gnns you can use spektral for classifying the user of a social network predicting molecular property generating new graph with gans clustering node predicting link and any other task where data is described by graph spektral implement some of the most popular layer for graph deep learning including and many others see convolutional layer you can also find pooling layer including spektral also includes lot of utility for representing manipulating and transforming graph in your graph deep learning project see how to get started with spektral and have a look at the example for some template the source code of the project is available on github read the documentation here if you want to cite spektral in your work refer to our paper graph neural network in tensorflow and kera with spektral daniele grattarola and cesare alippispektral is compatible with python and above and is tested on the latest version of ubuntu macos and window other linux distros should work a well the simplest way to install spektral is from pypi to install spektral from source run this in a terminal to install spektral on google colab the release of spektral is an important milestone for the library and brings many new feature and improvement if you have already used spektral in your project the only major change that you need to be aware of is the new datasets api this is a summary of the new feature and change spektral is an open source project available on github and contribution of all type are welcome feel free to open a pull request if you have something interesting that you want to add to the framework the contribution guideline are available here and a list of feature request is available here,"[('graph neural network gnns', 0.5566), ('graph spektral', 0.5508), ('paper graph neural network', 0.5082), ('graph', 0.4186), ('tensorflow', 0.4096), ('deep learning project', 0.3961), ('new graph', 0.3798), ('social network', 0.3734), ('deep learning', 0.3489), ('kera api', 0.3318)]","[-1.34803280e-01 -6.44589812e-02  9.44566354e-03 -4.02643643e-02
 -1.30931307e-02 -2.35341024e-02 -7.03042075e-02 -2.28786818e-03
 -1.02531098e-01 -4.37080627e-03  9.05238930e-03 -7.10656494e-03
 -2.06882283e-02  1.81225240e-02 -2.44336370e-02 -9.93440766e-03
  2.83229519e-02  4.88818698e-02 -8.94169975e-03 -6.49232939e-02
  1.56142171e-02 -1.47541454e-02 -9.61632002e-03  1.95527580e-02
  1.47210639e-02  4.51185443e-02 -6.10117428e-03 -1.10465989e-01
  5.13356701e-02 -1.44226747e-02 -1.85410995e-02  1.89328250e-02
 -1.16767501e-02  7.29028434e-02 -7.86531568e-02  1.39455320e-02
 -5.57085127e-02 -3.84647921e-02  7.78267011e-02 -7.00635137e-03
 -7.03011826e-02 -2.42282767e-02 -3.01638525e-02 -1.66406948e-02
  1.09283276e-01  1.37391798e-02 -3.20022665e-02 -1.22565841e-02
 -4.08357475e-03 -9.95415612e-04 -5.21068536e-02 -1.18152291e-01
 -4.63886298e-02 -4.42812406e-02  3.56372446e-03 -1.04645632e-01
 -3.45026180e-02 -1.31466696e-02  7.90835824e-03  1.75594278e-02
  6.27828315e-02 -2.47708727e-02 -2.27850415e-02  6.22227183e-03
  3.90913757e-03  4.33704443e-02 -6.09076256e-03  5.60992956e-02
  6.01414479e-02 -3.40942815e-02  7.01796785e-02 -1.91786618e-03
 -5.93012087e-02  3.48190544e-03  5.31372894e-03  3.09689008e-02
  4.34217937e-02  1.94453839e-02  1.38318511e-02 -8.42495486e-02
  6.32772520e-02 -3.61082703e-02  1.74211562e-02  2.68232487e-02
 -1.55867450e-02 -2.20599622e-02 -1.26300126e-01 -1.78375989e-02
 -5.78456707e-02 -3.23890783e-02 -1.78566184e-02 -1.57937780e-02
 -2.02372912e-02 -1.48379114e-02  2.20037978e-02  3.42497528e-02
 -3.35536562e-02 -2.00896803e-02 -1.30820163e-02  5.13380282e-02
 -9.67352316e-02 -3.76517475e-02  5.02007492e-02  2.26914883e-02
 -1.41716646e-02  1.16824657e-01  5.83505072e-03 -3.51253003e-02
  8.55343416e-02  5.41164279e-02 -6.17583878e-02  9.98368934e-02
 -3.37264389e-02 -5.13458326e-02  3.23204771e-02 -4.58124280e-03
 -1.39910746e-02 -5.10250777e-03  5.64994924e-02  1.29191786e-01
 -1.12833157e-01  3.90986614e-02 -3.87762599e-02 -4.60497439e-02
 -7.46880546e-02  6.53818529e-03 -9.97154936e-02  2.48034653e-33
  2.51064338e-02  6.07400620e-03  1.96463484e-02 -5.32353148e-02
  1.04351699e-01 -4.41510677e-02 -1.57703888e-02 -2.06511784e-02
  7.54875038e-03 -6.96781799e-02 -9.24069211e-02  7.01067373e-02
  1.88393351e-02  8.69004950e-02  1.82496030e-02 -1.86771490e-02
  5.51837422e-02 -4.75729406e-02  7.36653656e-02 -3.77107710e-02
  4.19957861e-02 -4.76525016e-02  4.05184478e-02  1.20841607e-01
  1.74142011e-02  7.62426332e-02 -1.52289458e-02 -2.72170585e-02
  2.41083782e-02  1.39078926e-02 -9.15215909e-03  4.37675565e-02
 -1.03047695e-02 -4.18325625e-02 -4.62066680e-02  3.88363400e-03
  1.57382693e-02 -4.31036353e-02  8.54458660e-04 -1.75274815e-02
 -1.29163489e-02  4.22641113e-02 -3.53936069e-02 -4.50423323e-02
 -4.77862032e-03 -2.38460843e-02  2.45438539e-03  2.92534735e-02
  8.42159688e-02 -1.01472251e-02 -5.14698178e-02 -3.21268961e-02
 -7.24350289e-02  5.84914759e-02  8.21358711e-02 -1.72514319e-02
  7.90931880e-02  3.21829766e-02  2.25490443e-02  4.22606394e-02
  2.69596186e-02 -1.49095412e-02  3.92156020e-02 -5.16436584e-02
 -3.02452818e-02  1.89867374e-02 -3.88909876e-02  2.71946155e-02
  2.23952997e-02 -2.78094504e-02 -4.30010036e-02  4.52392846e-02
  4.67695203e-03 -1.85928121e-02 -2.14700550e-02  2.35133525e-02
 -2.24699229e-02 -9.87215191e-02 -6.97066784e-02  7.67829642e-02
 -5.62340543e-02 -2.32518986e-02 -7.69328559e-03  2.39356561e-03
 -1.17159374e-02 -2.77471263e-02  1.13529537e-03 -1.11414865e-02
  5.51165603e-02  1.20243977e-03 -2.23318543e-02  1.29876111e-03
  3.48437913e-02  7.24018738e-02 -2.70398171e-03 -1.50280818e-33
 -5.02462797e-02  1.62692860e-01 -5.36900274e-02  9.94688570e-02
  1.15841307e-01  2.45385766e-02  1.35006597e-02 -6.87600225e-02
  8.66344373e-04  1.51052222e-01  1.44183915e-02 -7.65679553e-02
  4.68797274e-02  4.24126908e-02  3.04873269e-02 -2.84660682e-02
 -8.49128366e-02 -6.59693629e-02 -5.74716255e-02  1.86568964e-02
 -2.88037360e-02  7.97480419e-02 -9.96031612e-02  2.18745805e-02
  1.29369512e-01 -6.93324860e-03 -8.31820369e-02 -6.28049299e-03
  2.67840102e-02  3.26350480e-02 -1.71913486e-02 -3.22037376e-02
 -4.68629412e-02  4.62838784e-02  1.03989847e-01  3.29358913e-02
  6.46918565e-02 -1.39677618e-02  4.70455401e-02 -1.10591069e-01
  1.09785236e-01 -1.88362843e-03  3.02385986e-02 -4.06960025e-03
 -2.29985863e-02  2.23883279e-02 -3.97013016e-02  6.08747303e-02
 -1.08845323e-01 -3.93860079e-02  3.90019342e-02  6.83497591e-03
  6.18971214e-02 -4.99501973e-02 -8.72013718e-03 -1.76786762e-02
  5.93168139e-02  7.33979940e-02  1.47796152e-02  2.26902962e-02
 -2.87186969e-02 -8.18347409e-02  2.58737756e-03  4.44694757e-02
 -5.20645306e-02 -1.92442853e-02 -3.58805209e-02  5.48184179e-02
 -1.35868127e-02 -2.68477313e-02  1.09629305e-02  7.97018185e-02
 -1.65548902e-02  4.77278307e-02 -4.19541672e-02  4.73893387e-03
 -4.78113703e-02  5.23539037e-02  2.64527369e-03 -3.14192623e-02
  5.43814749e-02  3.84917594e-02 -2.63215713e-02  8.40181634e-02
  8.51077214e-02  1.02004811e-01  1.41127966e-02  7.85470940e-03
  3.32926288e-02  3.72980759e-02 -3.74299251e-02  4.48350608e-02
 -5.54995798e-02  8.63516703e-02 -2.10781582e-03 -2.09536477e-08
 -1.08202524e-01  2.76229326e-02  4.99953292e-02 -2.04124232e-03
  8.37997720e-02  8.11654143e-03  8.22859779e-02  1.06165521e-01
 -1.96764078e-02  1.02155603e-01 -1.03828246e-02 -5.82118193e-03
 -6.08569570e-02  7.08161533e-05 -2.48985901e-03 -4.07793410e-02
  2.93130148e-02 -5.37995845e-02  8.62279162e-02 -3.31998840e-02
  4.24635559e-02  1.48741324e-02 -2.72655021e-02  6.34839907e-02
 -1.55275175e-02 -8.54919851e-02 -7.92202482e-04 -1.46665135e-02
 -4.01084013e-02 -3.09738633e-03 -4.33214381e-02  1.03613921e-02
  1.12667799e-01 -8.54822472e-02  6.14311099e-02  9.31916982e-02
  4.79116626e-02 -8.09409656e-03 -1.02338893e-02  6.81260899e-02
 -2.52899807e-02  4.87752110e-02  5.79126831e-03 -4.42721993e-02
 -5.34986034e-02 -2.00803913e-02  1.71959084e-02 -1.87384039e-02
  4.06165756e-02 -2.84489673e-02 -9.38538611e-02  3.30033079e-02
 -5.96291833e-02  2.29271851e-03  5.80266770e-03  2.21060608e-02
 -3.72152068e-02 -8.78579393e-02  1.40354093e-02  4.32533734e-02
  1.27559658e-02  1.99981593e-02 -5.12806475e-02  2.06682775e-02]",2,2
mtcnn,1,implementation of the mtcnn face detector for kera in python it is written from scratch using a a reference the implementation of mtcnn from david sandberg facenet s mtcnn in facenet it is based on the paper zhang k et al zhang currently it is only supported python onwards it can be installed through pip this implementation requires opencv and kera any tensorflow supported by kera will be supported by this mtcnn package if this is the first time you use tensorflow you will probably need to install it in your system or with condanote that tensorflow gpu version can be used instead if a gpu device is available on the system which will speedup the result the following example illustrates the ease of use of this package the detector return a list of json object each json object contains three main key box confidence and keypoints the bounding box is formatted a x y width height under the key box the confidence is the probability for a bounding box to be matching a face the keypoints are formatted into a json object with the key left eye right eye nose mouth left mouth right each keypoint is identified by a pixel position x y another good example of usage can be found in the file example py located in the root of this repository also you can run the jupyter notebook example ipynb for another example of usage the following table show the benchmark of this mtcnn implementation running on an intel i qm cpu ghz with a cpu based tensorflow picture containing a single frontal face image sizetotal pixelsprocess timefps x second x second x second x second x second picture containing frontal face image sizetotal pixelsprocess timefps x second x second x second by default the mtcnn bundle a face detection weight model the model is adapted from the facenet s mtcnn implementation merged in a single file located inside the folder data relative to the module s path it can be overriden by injecting it into the mtcnn constructor during instantiation the model must be numpy based containing the main key pnet rnet and onet having each of them the weight of each of the layer of the network for more reference about the network definition take a close look at the paper from zhang et al zhang mit license zhang k zhang z li z and qiao y joint face detection and alignment using multitask cascaded convolutional network ieee signal processing letter,"[('mtcnn face detector', 0.7528), ('mtcnn implementation', 0.6019), ('mtcnn package', 0.5816), ('face detection weight model', 0.4938), ('facenet', 0.4842), ('tensorflow picture', 0.47), ('tensorflow', 0.4685), ('mtcnn', 0.4636), ('mtcnn constructor', 0.4449), ('y joint face detection', 0.4039)]","[-7.86556080e-02 -5.25408946e-02  7.45096058e-02 -1.09609254e-02
  8.56605992e-02  3.31479055e-03 -3.53450216e-02 -1.15719754e-02
 -1.21598944e-01 -3.55025008e-02  1.07133817e-02 -4.83309701e-02
  3.14563252e-02  5.37289456e-02 -1.90346017e-02 -6.11417405e-02
  2.86119115e-02  1.08140521e-01  1.17889633e-02  4.05381713e-03
 -2.37466730e-02 -8.75166524e-03  2.89500523e-02 -1.53172901e-02
 -1.34189473e-02 -2.23038401e-02  6.69889292e-03 -6.91023618e-02
  2.64778826e-02 -3.18278931e-02  2.40112543e-02 -1.52902503e-03
  4.10015918e-02  1.29334033e-02 -8.12053233e-02 -3.71046620e-03
  7.23992363e-02 -2.73092277e-02 -5.31524643e-02 -3.07306089e-02
 -1.08850874e-01 -4.32995111e-02  1.67773776e-02 -3.10561936e-02
  1.37071818e-01 -3.06339934e-02 -5.76025620e-03 -2.23566163e-02
  8.38048384e-03 -7.51814172e-02  2.57839449e-02 -1.09924980e-01
 -7.18915910e-02  6.57231063e-02 -1.52520556e-02 -5.54349087e-02
 -4.05830629e-02 -2.69732233e-02 -2.02682931e-02  4.14926223e-02
  1.05437869e-02 -9.01431404e-03 -3.38999331e-02  3.91142815e-02
 -1.97098460e-02  8.60894751e-03  4.84438799e-02 -1.15931341e-02
  1.38869017e-01 -7.86717907e-02  1.36813102e-03  2.27184016e-02
 -4.54170033e-02  2.85558216e-02 -4.58598584e-02  1.89666599e-02
  1.16159402e-01 -1.81719102e-02  3.20579708e-02 -4.99266759e-02
 -3.38574238e-02  3.16699483e-02  7.88409933e-02 -5.44215515e-02
  6.48397654e-02 -5.24594635e-03 -1.28334567e-01  5.32893017e-02
 -1.03863098e-01 -1.89677011e-02  5.57071045e-02 -3.34659629e-02
 -7.42425919e-02 -3.73920612e-02  1.02902558e-02 -2.51312573e-02
 -2.62427833e-02  1.83600187e-02 -8.99441689e-02  5.76667823e-02
 -5.69482148e-02 -1.11149982e-01  9.01205167e-02  2.69104242e-02
  2.41388809e-02  3.51107121e-02  1.36387367e-02 -2.38821581e-02
  7.06881881e-02 -4.00432646e-02  3.54584714e-04  2.88564209e-02
 -4.18694541e-02 -1.25761583e-01  4.56570368e-03  1.66300107e-02
 -7.75595680e-02  2.38668974e-02  6.68788478e-02  2.99744941e-02
 -9.43288729e-02  1.33195613e-02 -3.54951322e-02 -4.75364141e-02
  3.33682634e-02 -8.17919523e-03 -6.17927797e-02  3.97543300e-33
 -2.00715400e-02  5.91308111e-04 -7.84435309e-03  1.32766925e-02
  4.44447584e-02 -3.54668908e-02  5.50307445e-02  5.92270717e-02
 -1.82759855e-02 -1.24134757e-02 -4.33345214e-02  6.50966763e-02
 -1.52717242e-02  1.21695019e-01 -1.19798407e-02 -7.97309950e-02
  2.23750062e-02 -2.19094139e-02  2.40082275e-02  2.82225236e-02
  1.63276941e-02 -3.51271294e-02  5.81185855e-02  1.29257068e-01
  9.30872746e-03  2.99055558e-02  2.88716536e-02  1.09202508e-02
  5.71478298e-03  2.92469244e-02 -5.67323752e-02  8.22568014e-02
  1.30940601e-02 -1.33657428e-02  7.71865919e-02 -7.88812712e-02
 -3.59094590e-02 -2.85903476e-02  1.41770514e-02 -7.04303160e-02
 -3.16457860e-02  1.88484900e-02 -6.19998835e-02 -5.34112863e-02
 -6.82406710e-04  1.45890117e-02 -1.71791129e-02  8.94053951e-02
  1.38283893e-02  1.74187180e-02  4.00948301e-02 -2.15877220e-03
 -9.01545659e-02 -1.62073374e-02 -1.25908237e-02  2.56552231e-02
 -6.72855787e-03  1.03943283e-02  6.10737391e-02  4.13983390e-02
 -6.25421992e-03  1.08331591e-02  5.93978465e-02  2.56119035e-02
 -4.50976118e-02 -6.29819781e-02 -2.32866183e-02 -6.72550648e-02
  7.04424782e-03  5.24498820e-02 -1.45647814e-02  4.20116410e-02
 -7.77461194e-03 -1.02397124e-03  3.71362530e-02  7.46966340e-04
  2.67791841e-02 -2.06067283e-02  4.58137511e-04  5.42500764e-02
 -2.20048893e-02  5.83514683e-02  5.22204302e-02 -5.21759428e-02
 -4.88984361e-02  2.00331192e-02  1.71464570e-02 -3.97592969e-02
  3.27029042e-02  1.45119587e-02 -6.14172034e-03  3.69818718e-03
  6.94452599e-02  3.94888036e-02 -2.45436430e-02 -2.68771641e-33
  2.67771967e-02  9.31369066e-02 -7.08749592e-02 -1.77448560e-02
 -4.53443117e-02  8.62862542e-03  4.92584221e-02 -4.69367057e-02
  3.81459370e-02  3.57402265e-02  6.12561069e-02 -3.79819274e-02
  6.51954580e-03 -4.52206247e-02  3.88034806e-02 -3.29084955e-02
 -1.17560150e-02 -1.00147314e-01 -4.81650382e-02  2.02988610e-02
 -2.44389661e-02  8.67168456e-02 -9.87915099e-02 -6.18260577e-02
 -2.99643744e-02 -1.12073617e-02 -3.52829956e-02 -2.99289753e-03
  8.41620117e-02 -3.05715855e-02 -3.45872082e-02 -1.25121698e-02
 -9.12896823e-03  7.84123167e-02  3.64626781e-03  1.01164216e-02
  7.02233240e-02 -2.29412187e-02  3.67350243e-02  1.30629921e-02
  1.34215161e-01  4.07292508e-02 -1.74560361e-02  6.00109510e-02
  2.97508724e-02 -5.95185161e-02 -5.49420454e-02  5.26208393e-02
 -3.27277258e-02 -3.26138549e-02 -8.41592550e-02 -5.57542555e-02
 -6.77917674e-02 -4.66949260e-03 -4.46431823e-02  2.90464498e-02
  1.02738418e-01  6.27735481e-02  9.08300802e-02  2.83346958e-02
 -2.82044783e-02 -9.12611187e-02 -1.12776309e-02  2.08465271e-02
 -4.06074002e-02  2.96056066e-02 -5.16344421e-02  8.58798902e-03
  1.95956863e-02  1.46087594e-02 -3.86536457e-02  1.90776307e-02
  3.70820463e-02 -6.63692597e-03 -6.44890666e-02 -1.55174611e-02
 -2.73299254e-02  4.41665463e-02  8.46427772e-03 -2.38066167e-02
  4.44847457e-02 -6.29016161e-02  3.06445714e-02  1.08697698e-01
  5.90359792e-02  5.33523746e-02  2.27027871e-02 -6.75355867e-02
  4.31467928e-02 -2.50322763e-02 -5.12244366e-02 -1.24917068e-02
 -1.26997232e-02  7.93035701e-02  4.74078879e-02 -2.60560604e-08
 -6.64051324e-02 -1.77108441e-02  6.38465770e-03 -4.27449867e-02
  3.69037613e-02  5.52409403e-02  7.88399652e-02  7.93957189e-02
  8.20046477e-03  8.08893740e-02 -3.56338322e-02  3.40692066e-02
 -6.07984625e-02 -5.49192205e-02  1.85909364e-02  4.73521650e-02
 -2.85298005e-02  3.18880677e-02  1.36297038e-02 -4.42664959e-02
  4.59270887e-02  8.72336235e-03  5.30611686e-02  6.55029491e-02
 -2.56485422e-04 -6.86827675e-02 -5.51855043e-02  1.36354804e-01
 -5.19409627e-02 -4.28881049e-02 -1.59339253e-02  1.19272163e-02
  5.87492324e-02 -1.25880018e-01  1.66611746e-01  9.32659954e-02
 -7.82749336e-03 -7.17252586e-03  7.03408569e-02  4.89824116e-02
 -4.76449691e-02  4.95350249e-02 -6.66713668e-03 -1.83825288e-02
  1.85419191e-02 -2.44563315e-02  2.05176324e-03 -5.69669306e-02
 -6.04563579e-02  1.89163480e-02  6.42589703e-02  4.70079761e-03
 -9.10155997e-02  6.45723939e-02 -5.37801022e-03  4.38950323e-02
  4.81870696e-02 -1.05053693e-01 -2.51315385e-02  4.43317592e-02
 -5.07275537e-02  1.06578484e-01  1.75523981e-02 -4.88582663e-02]",2,2
neptune-client,1,neptune is a lightweight solution designed for step sign up for a free accountstep install the neptune client librarystep connect neptune to your codelearn more in the documentation or check our video tutorial to find your specific use case log and displayneptune support log and display for many different type of metadata generated during the ml model lifecycle compareyou can compare model building run you log to neptune using various comparison view filter and organizefilter sort and group model training run using highly configurable dashboard collaborateimprove team management and collaboration by grouping all experiment into project and workspace and quickly sharing any result or visualization within the team neptune come with integration with python library popular in machine learning deep learning and reinforcement learning available integration example example example example example read how various customer use neptune to improve their workflow if you get stuck or simply want to talk to u about something here are your option created with heart by the neptune ai team piotr jakub paulina kamil magdalena ma gorzata piotr aleksandra marcin hubert adam jakub pawe patrycja marcin jakub prince rafa dominika karolina parth rafa stephen sabine martyna artur franciszek aleksiej kshiteej tomasz tymoteusz piotr chaz micha siddhant karolina valentina bartosz alexandra patryk aleksander and you,"[('team neptune', 0.5134), ('neptune client librarystep', 0.4956), ('neptune', 0.4043), ('configurable dashboard collaborateimprove team management', 0.3913), ('workflow', 0.3463), ('collaboration', 0.3376), ('model building run', 0.3357), ('lightweight solution', 0.3323), ('codelearn', 0.3254), ('deep learning', 0.3239)]","[-9.53223482e-02 -4.16720547e-02 -2.24725511e-02 -3.38339694e-02
  4.96435612e-02 -3.25840339e-02 -6.19289912e-02  3.39966156e-02
 -1.82349682e-02  3.88564989e-02 -4.81904671e-02 -3.34800668e-02
  4.03583758e-02  2.49094814e-02 -2.51345281e-02 -3.76561694e-02
  8.35129432e-03 -5.59267066e-02  2.80688740e-02 -1.05154641e-01
 -1.15646534e-01 -5.46431467e-02 -4.11494225e-02  2.20558867e-02
 -1.32227056e-02  4.85578105e-02 -4.68071550e-02 -5.67646623e-02
 -1.74631122e-02 -1.48844318e-02 -7.24490434e-02  1.69783235e-02
 -5.89062199e-02  2.31379047e-02 -4.83154543e-02  1.11019090e-01
  1.66528001e-02 -3.49350832e-03 -5.94122075e-02 -5.24205603e-02
 -6.98370039e-02  2.66610440e-02 -1.43669164e-02  1.60746500e-02
  1.37972636e-02 -1.67738050e-02 -3.59104499e-02 -8.34247097e-02
 -6.17626822e-03  1.04572321e-03 -7.07630953e-03 -1.33889303e-01
  4.47588088e-03  4.82762828e-02  1.61052383e-02  3.10721602e-02
 -8.63456205e-02 -1.94501374e-02  8.65686685e-02 -6.03577942e-02
  1.94873419e-02 -2.55981702e-02 -2.17401013e-02  4.34083901e-02
  1.15775978e-02  8.53872113e-03 -1.15036154e-02  1.24878347e-01
  1.90372355e-02 -4.24997173e-02 -4.94239433e-03 -2.04892624e-02
  1.51295657e-03  4.81485855e-03 -1.74224935e-02  5.51132038e-02
  3.78856398e-02 -4.40224074e-02  5.86603507e-02 -1.09032482e-01
 -1.38674246e-03  6.48394227e-02 -8.19077864e-02  1.07031845e-01
  3.71215940e-02 -6.71166601e-03 -2.01830100e-02  2.13308744e-02
 -3.63687351e-02 -9.59508494e-03  3.73955853e-02  4.63456381e-03
  3.23499404e-02 -8.61866325e-02 -9.90809873e-02  1.03075616e-02
  2.38725282e-02 -1.49640422e-02 -9.18197483e-02  9.73309800e-02
 -6.51284531e-02  2.26804223e-02  8.74554440e-02 -4.09790576e-02
 -5.19833528e-02  4.74547297e-02  5.40804043e-02  3.45460996e-02
  7.14380592e-02  6.11752234e-02 -5.31887002e-02 -1.09095825e-03
 -9.31589603e-02 -8.41867551e-02  2.72693485e-02  2.13950407e-02
 -5.22547960e-02 -3.04204877e-03 -2.46011894e-02  7.85918757e-02
 -3.58970612e-02 -3.16394889e-03  4.35775705e-03  2.80897431e-02
  3.63529995e-02  1.78514030e-02 -4.13485169e-02  3.79023092e-33
  2.18476802e-02  2.53945962e-02 -6.20477786e-03  2.62722000e-03
  1.27590597e-01 -5.59764840e-02  5.81684001e-02  6.09504245e-02
 -1.22915171e-01 -5.07126860e-02 -2.03923751e-02  9.83553901e-02
  2.56939530e-02  5.29138744e-02  4.40290160e-02 -7.68887252e-02
 -6.45736326e-03  5.34695350e-02 -6.24777786e-02 -4.10799533e-02
 -4.69518680e-04  5.45421755e-03  1.79062672e-02  3.51744108e-02
  6.90414011e-02 -5.05984463e-02 -9.78934206e-03  6.62980415e-03
  7.81287551e-02  2.29002610e-02 -1.87916607e-02  1.93017498e-02
 -1.50998505e-02  4.05644774e-02 -3.11981663e-02  1.90069731e-02
 -6.74961135e-02 -1.14483006e-01  6.81018382e-02 -2.75702085e-02
 -1.08770765e-02  1.33516286e-02 -3.86309288e-02 -4.12003994e-02
  3.51397619e-02  6.00692583e-03  4.30236869e-02  1.96022373e-02
  5.48215397e-02 -7.89530054e-02 -2.02702098e-02  1.03997495e-02
 -9.34182946e-03  1.94237959e-02  2.24640556e-02  1.96460336e-02
  1.03076331e-01 -1.75099503e-02 -1.41642652e-02  8.44427943e-02
  1.01824412e-02  5.52004948e-02 -6.32065237e-02 -5.72641566e-02
  4.98602726e-02 -6.13721926e-03 -4.50225398e-02  1.57714859e-02
  6.83844984e-02 -7.32219517e-02 -6.19463325e-02 -1.63789117e-03
  7.57560730e-02  2.33860184e-02  4.78957556e-02 -2.39849859e-03
 -2.43801475e-02 -4.22877520e-02 -2.98548583e-02  7.01886881e-03
 -1.36900127e-01 -1.72794610e-02  1.61988419e-02  6.73343614e-02
 -2.64190836e-03  2.24160142e-02  4.86120731e-02  3.60723361e-02
 -6.06077462e-02  8.16241130e-02 -3.96507978e-03 -2.29389723e-02
  9.90658402e-02  3.56400162e-02 -1.00656390e-01 -2.53116848e-33
 -6.31963685e-02 -3.66740767e-03 -1.50688514e-02  5.67556657e-02
  1.46827444e-01  3.53173129e-02 -4.85614054e-02 -9.34955478e-02
 -3.11600138e-02  2.16048919e-02  7.13325217e-02 -8.39941576e-02
  6.99253054e-03 -3.14338803e-02  2.85738874e-02  2.83943918e-02
 -8.43523070e-03 -5.81344888e-02  6.16884138e-03 -1.45018781e-02
  1.43930269e-02  3.94537486e-02  3.92975397e-02  1.54655846e-02
 -8.94159637e-03  5.10886591e-03  3.47912163e-02 -4.15275767e-02
  6.52183816e-02  4.06541079e-02 -7.18548372e-02  9.93707683e-04
 -3.89547162e-02 -3.84147055e-02  3.16866338e-02  1.99150704e-02
 -3.92741784e-02 -5.35951369e-03 -8.24389607e-02 -3.12410351e-02
  6.22923225e-02 -5.10527007e-02 -5.78532629e-02  2.12645736e-02
  1.11297844e-02  1.59825329e-02 -8.24032649e-02  4.86152880e-02
 -5.10323010e-02 -4.26209159e-02  3.03195026e-02 -1.60781387e-02
 -2.89882105e-02 -2.27540266e-02 -4.89720330e-03  7.76810050e-02
  6.48379773e-02  5.13796099e-02  8.70859846e-02  3.35680321e-02
  3.36463116e-02 -3.31543013e-02 -1.44758383e-02  8.74638557e-02
 -7.05571147e-03 -1.14233615e-02 -3.63660306e-02  1.44511029e-01
 -1.00866705e-01  3.39785069e-02 -9.44563933e-03  1.19119240e-02
  3.26605439e-02 -5.73345684e-02 -3.09922639e-02 -2.68630814e-02
  1.83655974e-02 -9.43403095e-02  3.74385319e-03 -1.87118202e-02
 -7.29853427e-03  8.70924965e-02  6.59285113e-03  9.08732489e-02
 -3.73904258e-02  7.80550167e-02  4.80976924e-02  6.24023899e-02
 -3.14925506e-04  3.74256670e-02 -6.09194413e-02 -2.45615244e-02
 -4.20358544e-03  4.26009521e-02 -3.71656613e-03 -2.70546838e-08
  5.62714506e-03  5.20148166e-02 -7.19190622e-03  1.05497902e-02
  6.25540223e-03 -1.04562277e-02  3.33944447e-02  8.21781307e-02
  4.44172882e-02  1.45845383e-01  1.95407290e-02 -3.77146490e-02
 -8.07337612e-02 -1.82831902e-02 -4.39527631e-03  6.76814392e-02
  3.57570522e-03  8.25103074e-02 -1.58507898e-02 -7.59546608e-02
  2.31667003e-03  6.21160939e-02 -2.41752379e-02 -1.42887030e-02
 -4.47035916e-02 -4.10831980e-02 -7.68116955e-03  7.92461261e-02
 -2.36023739e-02 -1.25617646e-02 -4.78067361e-02 -2.15581967e-03
 -9.02222935e-03 -1.48513932e-02  1.32141262e-01  2.15038210e-02
 -9.33798924e-02  4.59383288e-03  2.58036032e-02  7.88696259e-02
 -7.12863076e-03  1.30910560e-01 -2.70780493e-02  2.43708901e-02
 -3.44918482e-03 -2.32157134e-03 -1.79354753e-02 -5.73924407e-02
  1.12567069e-02 -2.68066842e-02 -4.43845280e-02 -1.41128739e-02
 -3.98375131e-02  1.71891227e-02  2.33009253e-02  7.94707313e-02
 -2.27858070e-02 -1.18439153e-01  1.55224912e-02  5.93933128e-02
  6.09579263e-03  3.42725962e-02  1.59974005e-02  8.39187354e-02]",2,0
tf_agents,1,tf agent make implementing deploying and testing new bandit and rl algorithm easier it provides well tested and modular component that can be modified and extended it enables fast code iteration with good test integration and benchmarking to get started we recommend checking out one of our colab tutorial if you need an intro to rl or a quick recap start here otherwise check out our dqn tutorial to get an agent up and running in the cartpole environment api documentation for the current stable release is on tensorflow org tf agent is under active development and interface may change at any time feedback and comment are welcome agent tutorial multi armed bandit example installation contributing release principle contributor citation disclaimerin tf agent the core element of rl algorithm are implemented a agent an agent encompasses two main responsibility defining a policy to interact with the environment and how to learn train that policy from collected experience currently the following algorithm are available under tf agent see doc tutorial for tutorial on the major component provided the tf agent library contains a comprehensive multi armed bandit suite including bandit environment and agent rl agent can also be used on bandit environment there is a tutorial in bandit tutorial ipynb and ready to run example in tf agent bandit agent example v end to end example training agent can be found under each agent directory e g tf agent publishes nightly and stable build for a list of release read the release section the command below cover installing tf agent stable and nightly from pypi org a well a from a github clone run the command below to install the most recent stable release api documentation for the release is on tensorflow org if you want to install tf agent with version of tensorflow or reverb that are flagged a not compatible by the pip dependency check use the following pattern below at your own risk if you want to use tf agent with tensorflow or install version nightly build include newer feature but may be le stable than the versioned release the nightly build is pushed a tf agent nightly we suggest installing nightly version of tensorflow tf nightly and tensorflow probability tfp nightly a those are the version tf agent nightly are tested against to install the nightly build version run the following after cloning the repository the dependency can be installed by running pip install e test tensorflow need to be installed independently pip install user tf nightly we re eager to collaborate with you see contributing md for a guide on how to contribute this project adheres to tensorflow s code of conduct by participating you are expected to uphold this code tf agent ha stable and nightly release the nightly release are often fine but can have issue due to upstream library being in flux the table below list the version s of tensorflow tested with each tf agent release to help user that may be locked into a specific version of tensorflow wa the last release compatible with python wa the last release compatible with python this project adheres to google s ai principle by participating using or contributing to this project you are expected to adhere to these principle we would like to recognize the following individual for their code contribution discussion and other work to make the tf agent library if you use this code please cite it a this is not an official google product,"[('agent bandit agent example', 0.6862), ('welcome agent tutorial multi armed bandit example installation', 0.6511), ('tf agent library', 0.6473), ('bandit tutorial ipynb', 0.6436), ('example training agent', 0.5948), ('bandit environment', 0.5791), ('version tf agent', 0.5694), ('agent library', 0.5571), ('comprehensive multi armed bandit suite', 0.5571), ('agent directory e g tf agent', 0.4766)]","[-4.53304164e-02 -5.58791794e-02 -5.99413253e-02 -6.71096966e-02
  3.84930931e-02  1.55458534e-02  3.40241976e-02  1.09984372e-04
 -4.92361449e-02 -6.55305339e-03  5.82203381e-02  2.51686778e-02
  5.59344031e-02 -1.97873134e-02  8.12683776e-02  3.80770327e-03
  5.71977394e-03  6.93690777e-02  5.50174564e-02 -1.37285873e-01
 -1.14472985e-01 -2.39268076e-04  6.87576011e-02  8.24648945e-04
 -6.55633137e-02 -4.59539182e-02 -5.08633330e-02  2.71035731e-03
  1.69656333e-02 -1.66021399e-02 -2.76468340e-02  5.99094182e-02
  1.53595544e-02  2.41191685e-02  2.14348603e-02  2.61247568e-02
 -8.86062086e-02  4.53887284e-02 -6.45374879e-02  8.39750022e-02
 -2.78043523e-02  2.38142665e-02 -5.62189966e-02 -4.93252231e-03
  1.91347208e-02 -9.69042480e-02 -7.28580430e-02 -4.93363366e-02
  5.21240793e-02  5.96880622e-04 -7.06153139e-02 -4.99441735e-02
 -3.01425066e-02 -3.96583006e-02  3.46110854e-03 -5.10871895e-02
  2.97075283e-04  4.40243259e-02  7.26270373e-04 -1.37671465e-02
  5.20005859e-02 -2.72619147e-02 -6.63474798e-02  2.42263526e-02
  2.37999745e-02  2.10806075e-02 -4.39602546e-02  8.16976503e-02
  3.56102772e-02 -1.03212878e-01 -9.11784396e-02 -1.00750253e-02
 -5.12763336e-02 -4.55960864e-03 -5.08829691e-02  6.83248276e-03
 -2.14981362e-02  2.70477869e-02  4.19620723e-02 -8.76945183e-02
 -8.60875249e-02 -3.99140716e-02  3.29598449e-02  3.41553055e-03
 -8.00510962e-03  1.25805102e-02  1.98971983e-02 -1.10264868e-02
  6.89438358e-02 -3.61743346e-02  7.32571706e-02 -1.41742509e-02
 -1.99101251e-02  2.30630208e-02 -6.96565658e-02  3.86968106e-02
  3.26823327e-03  8.18144344e-03 -9.11368281e-02  6.39718845e-02
  1.29121332e-03 -1.05784141e-01  2.99347918e-02 -3.15389410e-02
 -7.00341836e-02  2.53388658e-03  3.41248251e-02  2.40797084e-02
  8.63548592e-02 -5.20806089e-02 -4.93485853e-02  1.47767803e-02
  8.41771252e-03 -2.94305477e-02  3.04114912e-02 -4.29113396e-02
 -9.39522311e-02  3.80377769e-02  3.87904383e-02  7.13593885e-02
  4.33747582e-02 -3.14387828e-02  4.53404337e-02  1.75652001e-03
  6.08264729e-02  7.93847628e-03 -9.38239601e-03  5.17779988e-33
  9.14719626e-02 -1.27161099e-02  8.64629634e-03  2.62543242e-02
  6.90499023e-02 -6.05401546e-02  6.77066743e-02 -1.82039812e-02
 -1.00884609e-01  6.65508807e-02  7.92389140e-02  6.75299838e-02
 -8.77882838e-02  1.22677878e-01  5.46984887e-03 -1.36678815e-01
 -5.22840694e-02  2.05765311e-02  9.65108946e-02 -5.13656475e-02
  3.29511240e-02  1.05324075e-01 -1.06925517e-02  2.81615090e-02
  4.69093509e-02  4.37042527e-02 -2.99372827e-03  9.23054200e-03
  5.18067628e-02  6.02716208e-02 -6.07791841e-02  2.55529713e-02
 -5.01641668e-02  3.09859049e-02 -1.11954659e-02 -2.34841742e-02
 -7.27419853e-02 -3.27318236e-02 -3.43590304e-02 -4.53005433e-02
 -1.93935577e-02  8.09617341e-03  6.38856813e-02 -8.27628821e-02
  2.25565629e-04 -5.39487638e-02  4.07599993e-02 -9.05598630e-04
  1.13380000e-01 -3.32384780e-02 -7.58183822e-02  1.27766114e-02
  3.70764993e-02  5.48772439e-02  4.61756811e-02  4.31784168e-02
 -2.24536993e-02  5.82251027e-02  2.66896095e-02  1.27648851e-02
  1.37635358e-02  7.00496510e-02  2.52832063e-02 -2.35465318e-02
  5.82388183e-03 -4.21664305e-03 -1.94713324e-02 -1.63017455e-02
  7.80052543e-02  3.97668630e-02 -3.02276225e-03  4.31901291e-02
  3.47901583e-02 -2.00973209e-02  1.40586225e-02 -8.39717835e-02
  4.28405823e-03  4.46245931e-02  1.43063590e-02 -1.15750529e-01
 -1.28611222e-01 -5.02893701e-02  2.13339296e-03  8.49985406e-02
 -3.58841307e-02 -8.62867944e-03  7.83022344e-02 -7.78810307e-02
  3.17009352e-02  9.25023109e-03 -6.37469590e-02  4.46341150e-02
 -1.02972083e-01  7.37071037e-02  1.82708986e-02 -5.61747518e-33
 -2.17742426e-03 -9.82690975e-03 -2.60158163e-03 -1.41274347e-03
  6.25464395e-02  3.02433372e-02 -7.34610716e-03 -4.65939417e-02
  4.57085595e-02  1.27788484e-02 -5.32794110e-02  6.32499605e-02
  4.08585789e-03 -5.21365702e-02  2.64764950e-03 -1.19673133e-01
  6.91775754e-02  8.56998842e-03 -2.37200409e-03 -1.95009448e-02
 -3.15576941e-02 -8.15677363e-03 -1.42831430e-02 -7.07871765e-02
  9.19992030e-02 -1.18033134e-03  2.39705127e-02  1.09264232e-01
  7.26869050e-03 -2.13147514e-02  6.11873195e-02 -2.89843068e-03
 -2.83848811e-02  3.26585434e-02 -6.35755435e-02 -9.52926651e-03
  7.67937154e-02  3.25988419e-02 -6.59179166e-02  2.35801358e-02
  8.53826255e-02 -3.84209827e-02  2.56275665e-02 -4.55598310e-02
 -1.59742422e-02  4.86972043e-03 -5.43883517e-02  1.72950551e-02
  1.55588258e-02 -4.90168855e-02 -7.28068082e-03 -1.31568229e-02
 -5.12804985e-02 -7.66000375e-02 -8.95472020e-02 -1.31537039e-02
  6.06353506e-02  2.39822511e-02  3.10342386e-02  4.11471874e-02
 -1.12286918e-02 -7.58817839e-03 -4.24329191e-02  5.33293001e-02
 -2.00617183e-02 -8.07902366e-02 -1.13967881e-01  7.85619020e-02
 -4.57432047e-02 -2.11433321e-03 -5.10897711e-02  6.39419034e-02
 -1.86111275e-02 -4.78793532e-02  2.62833387e-02  2.29257792e-02
  6.94115600e-03  1.96911301e-02  2.83493996e-02 -1.13840587e-01
  1.93114020e-02  2.53754091e-02 -3.88260260e-02  6.25304803e-02
 -2.19203513e-02  7.56934583e-02  6.66492898e-03  7.62774870e-02
  3.67759392e-02 -4.99721281e-02  2.95857694e-02 -1.56046776e-02
  1.72582977e-02  9.09455940e-02 -2.27580108e-02 -3.14663708e-08
 -6.52904212e-02  5.19382954e-02  7.14330673e-02  4.40633595e-02
  5.57302013e-02  5.01753874e-02  3.67429256e-02  4.23288457e-02
  3.08467075e-03  2.61837710e-02  6.66691884e-02 -2.52757519e-02
 -2.22026594e-02  2.99485214e-03 -5.72433472e-02  2.60478049e-03
 -7.82835633e-02 -3.50322090e-02 -2.47409288e-02 -3.64091843e-02
  5.27956486e-02 -2.30840277e-02  6.16550408e-02 -5.76114543e-02
 -1.63193867e-02 -2.23553833e-02 -8.87316614e-02 -2.37460737e-03
  1.24067217e-02  5.68958111e-02 -1.48331746e-02 -4.18816600e-03
  3.62508893e-02 -6.84915856e-02  1.31195486e-01  8.48631933e-02
 -5.58389649e-02 -4.95026214e-03  1.80305773e-03 -1.02420878e-02
 -7.25477636e-02 -4.46659587e-02  4.96099293e-02 -2.68665459e-02
  6.05665185e-02  7.84387346e-03 -4.60341126e-02 -5.56312464e-02
  5.17195873e-02 -1.23023324e-01 -3.93896848e-02 -3.79468221e-03
 -6.40016049e-02  3.23756188e-02  1.16162419e-01  3.35338078e-02
  2.49590948e-02 -3.37691568e-02  8.19757357e-02 -9.58160206e-04
 -2.27988064e-02 -1.73345506e-02 -4.83600758e-02  8.33856501e-03]",2,0
fcd_torch,1,pytorch implementation of fr chet chemnet distance ported from the original repository http github com bioinf jku fcd the ported model produce the same output a the original kera implementation and can be used for reproducible research the pytorch model of chemnet weight tenfold le resulting in faster loading other feature first install rdkit conda install yq c rdkit rdkit and then install fcd torch from pip pip install fcd torch or directly from the source import the module from fcd torch import fcd you can run calculation directly or precalculate statistic to reuse them on the test set see example below if you run fcd on gpu the gpu memory will be allocated only during calculation of fcd for the constructor you can pas the device a device cpu for cpu and device cuda n for gpu where n is the gpu device number n job parameter specifies the number of thread for parsing smile you can also vary the batch size parameter call parameter for fcd are fcd ref none gen none pref none pgen none where you should specify either ref smile list or pref precalculated statistic and the same for gen and pgen,"[('pytorch implementation', 0.5903), ('pytorch model', 0.5155), ('fcd torch import fcd', 0.4945), ('gpu device number n job parameter', 0.4458), ('fcd torch', 0.4351), ('gpu memory', 0.4285), ('chemnet weight tenfold le', 0.382), ('fr chet chemnet distance', 0.3816), ('gpu', 0.3799), ('device cuda n', 0.3313)]","[-7.53498226e-02 -6.01323843e-02 -9.41264778e-02 -1.86507162e-02
  9.36560985e-03 -5.44107845e-03 -1.65519882e-02  6.78266883e-02
 -3.45594250e-02 -9.88201499e-02  2.07381491e-02 -7.62158865e-03
 -5.16060181e-02  5.65261720e-03 -4.04005162e-02 -3.55697563e-03
 -2.20937245e-02  8.06621090e-03 -1.27349585e-01 -1.21275522e-01
  6.66731521e-02 -4.41811793e-02  7.84806833e-02  6.61077444e-03
 -6.04603477e-02 -6.60343990e-02  7.13145658e-02  1.13711134e-02
  7.00160116e-02  2.62273271e-02  1.13771670e-01  5.78758679e-02
 -1.69010628e-02  5.52013814e-02  6.29994795e-02 -5.76016828e-02
 -5.51593415e-02 -2.58536469e-02 -4.64117080e-02  4.10829186e-02
  1.39877554e-02 -2.78064199e-02 -1.96420830e-02 -2.83157709e-03
  2.27782857e-02 -9.89393517e-03  2.67582461e-02 -1.78745501e-02
 -1.74912475e-02 -9.54292864e-02  1.91864055e-02  5.40913455e-02
 -7.85338357e-02  3.51757072e-02  2.63362881e-02  1.28785232e-02
  2.59316210e-02 -9.03613865e-02 -7.18095386e-03 -7.65598789e-02
  5.72507046e-02 -3.10267098e-02 -5.17102471e-03 -6.22140523e-03
 -4.36668433e-02  1.77771728e-02 -9.11003631e-03 -5.09093069e-02
  1.12060748e-01 -1.14080124e-01 -2.53408700e-02 -1.01643614e-02
 -7.21274763e-02  4.60354537e-02 -7.56377541e-03 -4.13483754e-02
  9.52664018e-02  1.37980646e-02  1.90516319e-02 -1.24870121e-01
 -2.32239142e-02 -3.28308949e-03  1.10939600e-01  1.51789084e-03
  6.93913922e-02  1.80127490e-02 -1.75928399e-02  7.71819130e-02
  9.89564811e-04 -1.79841705e-02  2.25327499e-02 -1.57730617e-02
 -8.03018659e-02  4.78649214e-02 -4.26770337e-02 -2.35218331e-02
  5.40923551e-02 -5.05394824e-02 -7.73319900e-02  3.75233814e-02
 -1.59439910e-02 -6.49662316e-03  8.95914156e-03  5.14924377e-02
 -5.44789061e-02  6.29110262e-02 -7.99881481e-03  1.21875979e-01
 -2.81138215e-02 -4.13801521e-03  6.76991045e-02  3.91358770e-02
 -4.99820337e-02 -1.78266000e-02  1.07091442e-01 -2.50553507e-02
 -6.64404333e-02  3.33903953e-02  5.92764467e-02  5.47325090e-02
 -7.34622404e-02 -3.17456457e-03 -1.05378911e-01  8.30905735e-02
 -1.06497332e-01 -3.26160598e-03 -8.31768215e-02  8.15694191e-33
 -2.02413406e-02  2.42094304e-02 -1.09611023e-02 -5.60885966e-02
 -1.37592833e-02 -1.09582618e-02  4.30225171e-02 -6.11392548e-03
  9.74782335e-04  4.30976227e-03 -6.15562983e-02  3.78840836e-04
 -5.17025329e-02  4.72077355e-02 -8.29674825e-02 -4.55608666e-02
 -1.22732604e-02  4.48238477e-02  3.38717327e-02  1.56075787e-02
  5.59341572e-02  6.21654838e-02 -1.59686916e-02  3.65201868e-02
  4.86883242e-03  8.99190530e-02 -9.77751762e-02  2.22665612e-02
 -1.37494570e-02  1.91288106e-02  1.87776168e-03  5.53905917e-03
 -2.11440101e-02 -3.01304255e-02 -2.76227221e-02 -1.41009223e-02
 -1.26331020e-02 -4.62749153e-02 -4.08771727e-03 -1.06134787e-01
 -5.47569431e-02  5.58460131e-02  1.43367490e-02 -6.35316223e-02
 -7.62495920e-02  8.78424011e-03 -3.56367193e-02  5.25035486e-02
  4.64897901e-02  2.03102874e-03 -4.35045436e-02  3.78681831e-02
 -1.21026319e-02  5.00158407e-02  5.90482354e-02 -5.71234040e-02
  2.89657135e-02  1.16900140e-02  1.31974071e-01  6.67984411e-02
  3.16283219e-02  5.22132404e-02 -1.40141323e-02  2.99403891e-02
  3.94933252e-03  1.47742573e-02 -3.80384363e-02  2.44263764e-02
 -1.18150720e-02  5.16393818e-02 -9.66630802e-02  7.10794926e-02
 -1.14874654e-02 -1.02634011e-02  7.78293833e-02 -3.27869281e-02
  5.24271429e-02 -1.02567211e-01 -6.68510422e-02  4.06413712e-02
 -1.02899730e-01  4.72364202e-02 -1.81057919e-02 -6.75430149e-02
 -9.64112505e-02 -2.97067929e-02 -2.32055485e-02 -2.77522840e-02
  1.18009420e-02 -7.75411911e-03 -7.33417571e-02 -4.81211431e-02
  8.95725414e-02  4.95037325e-02 -2.05598976e-02 -7.45256806e-33
 -1.08877011e-02  3.23246866e-02  9.85171646e-03  3.86881195e-02
  6.90195616e-03 -1.31479055e-02 -5.04436344e-02 -4.36528102e-02
  9.96590219e-03 -1.92702152e-02  1.37928743e-02 -3.17403078e-02
 -2.65224949e-02 -2.06762645e-02  4.52289954e-02  4.72466536e-02
 -8.19321722e-03 -3.01929135e-02 -1.67323332e-02  4.85093854e-02
 -7.91849792e-02  1.06181882e-01 -9.35397148e-02 -3.56272198e-02
 -6.91147372e-02  1.54389469e-02  4.22216579e-02 -6.97849020e-02
  3.51230018e-02 -1.15016513e-02  2.32351590e-02 -2.42956169e-02
 -5.83069958e-02  9.27956104e-02 -5.76987825e-02 -2.35790312e-02
  7.95295909e-02 -9.20404494e-03  4.12809812e-02  2.55791191e-02
  1.28583446e-01  7.68544674e-02 -5.11843897e-02  6.44052923e-02
 -7.36787543e-02  6.55968860e-02 -5.42218201e-02 -1.50224371e-02
  5.22871548e-03 -1.37249343e-02  2.15692241e-02  1.18504539e-02
  4.33846563e-03 -6.19817991e-04  2.53073853e-02  3.88281755e-02
  8.79054070e-02  4.45435792e-02  2.13808962e-03 -3.11769340e-02
  3.35677504e-03 -1.27068833e-01  1.91241298e-02 -2.51784902e-02
 -9.71008558e-03 -1.85615681e-02 -6.56603724e-02  1.16401047e-01
  3.22017334e-02 -2.15965672e-03 -1.65196788e-02  6.35991916e-02
  9.06076059e-02  7.71216303e-02 -7.03828931e-02  7.65967183e-03
  2.41474472e-02 -1.18329655e-02  4.07725833e-02  4.79909368e-02
 -7.36105163e-03 -8.72284267e-03  3.92934717e-02  3.35360914e-02
  6.21018279e-03  1.70884188e-02  7.55914375e-02  4.95053343e-02
  1.64714586e-02 -6.09779097e-02  9.16732568e-03  4.05530185e-02
  1.16073608e-01  6.04383461e-02  5.83739132e-02 -3.53953595e-08
 -1.18659167e-02 -3.48572023e-02 -1.26253432e-02  6.98814727e-03
 -3.27539979e-04 -9.30499844e-03  4.48415391e-02  8.66908114e-03
 -2.57025324e-02 -7.70979887e-03  1.10227279e-02 -1.25688985e-02
 -1.01357186e-02 -7.04146107e-04  4.03911509e-02  1.02685325e-01
  2.58529559e-02  1.40597364e-02  5.21267094e-02 -2.18942445e-02
 -3.86867449e-02  2.44803280e-02 -1.94783006e-02  4.56359144e-03
 -5.39464653e-02 -4.08274531e-02  1.77467838e-02  8.37361533e-03
 -1.69049427e-02  2.14456152e-02 -1.61602609e-02 -4.51619513e-02
  3.62699032e-02  1.57794612e-03  1.57814577e-01  7.24292472e-02
 -5.94746973e-03 -2.55849827e-02  2.51799710e-02  3.31140496e-02
 -6.32683188e-02 -2.78155152e-02 -3.22359614e-02 -1.95158720e-02
  7.57607520e-02 -2.31698854e-03 -4.67234366e-02 -9.88643542e-02
 -1.02283575e-01  6.53247088e-02  3.43071222e-02  4.96027619e-02
 -1.62286703e-02  2.48464309e-02 -1.87752564e-02  6.63353503e-02
 -6.40975758e-02 -4.03240770e-02 -1.15618305e-02 -7.68306553e-02
 -1.98539346e-03  6.82838112e-02 -3.26847360e-02 -6.04502670e-02]",2,2
foolbox,1,foolbox is a python library that let you easily run adversarial attack against machine learning model like deep neural network it is built on top of eagerpy and work natively with model in pytorch tensorflow and jax foolbox ha been rewritten from scratch using eagerpy instead of numpy to achieve native performance on model developed in pytorch tensorflow and jax all with one code base without code duplication native performance foolbox is built on top of eagerpy and run natively in pytorch tensorflow and jax and come with real batch support state of the art attack foolbox provides a large collection of state of the art gradient based and decision based adversarial attack type checking catch bug before running your code thanks to extensive type annotation in foolbox guide the best place to get started with foolbox is the official guide tutorial if you are looking for a tutorial check out this jupyter notebook documentation the api documentation can be found on readthedocs foolbox is tested with python and newer however it will most likely also work with version to use it with pytorch tensorflow or jax the respective framework need to be installed separately these framework are not declared a dependency because not everyone want to use and thus install all of them and because some of these package have different build for different architecture and cuda version besides that all essential dependency are automatically installed you can see the version we currently use for testing in the compatibility section below but newer version are in general expected to work more example can be found in the example folder e g a full resnet example if you use foolbox for your work please cite our joss paper on foolbox native i e foolbox and our icml workshop paper on foolbox using the following bibtex entry we welcome contribution of all kind please have a look at our development guideline in particular you are invited to contribute new adversarial attack if you would like to help you can also have a look at the issue that are marked with contribution welcome if you have a question or need help feel free to open an issue on github once github discussion becomes publicly available we will switch to that foolbox is much faster than foolbox and a basic performance comparison can be found in the performance folder we currently test with the following version pytorch tensorflow jax numpy,"[('version pytorch tensorflow jax numpy', 0.5998), ('foolbox guide', 0.5154), ('pytorch tensorflow', 0.5121), ('native performance foolbox', 0.5081), ('foolbox', 0.5051), ('jax foolbox', 0.4892), ('art attack foolbox', 0.4556), ('python library', 0.4515), ('adversarial attack type', 0.4066), ('python', 0.3973)]","[-8.41074139e-02 -3.03547438e-02 -1.83269978e-02 -1.31181050e-02
  9.65538621e-02 -3.41226161e-02 -9.57983453e-03 -4.52752449e-02
 -6.83012605e-02 -1.01170145e-01  1.71038583e-02 -1.71500407e-02
 -5.43070119e-03 -5.02354130e-02 -1.02064647e-02  4.34611971e-03
 -6.67458251e-02  7.74714425e-02  3.22287157e-02 -5.24823032e-02
 -5.10077029e-02  7.49264238e-03  2.82075070e-02 -1.19600147e-02
 -3.20498534e-02 -1.73565540e-02  1.38386907e-02 -1.82355195e-02
 -4.72342856e-02 -3.35761383e-02 -2.31165774e-02  4.02324572e-02
  4.66519222e-02 -1.00336047e-02 -5.08299507e-02 -9.48307943e-03
 -5.25294803e-02 -2.43692790e-04 -4.34234273e-03 -2.11926661e-02
 -3.92394513e-02  3.08605712e-02 -2.88820229e-02 -3.59894559e-02
 -5.31747453e-02 -2.78373603e-02  6.83494508e-02 -6.12495467e-02
  5.19692898e-03 -2.64194068e-02 -4.11751643e-02 -4.11959440e-02
 -2.39502490e-02  1.16544394e-02 -7.89364986e-03 -5.70788197e-02
  2.21050419e-02  3.17571387e-02 -4.47131060e-02  1.79972388e-02
 -2.64903717e-02  3.42340283e-02 -2.17667837e-02  4.86190505e-02
 -3.81428259e-03  5.93718030e-02 -7.90439174e-03  2.37303916e-02
  1.05471149e-01 -9.92276967e-02 -3.62938009e-02 -1.46755390e-02
 -4.59607020e-02  4.76193987e-02 -1.17370095e-02  1.87351834e-02
  5.89047521e-02 -2.75954045e-02  5.27856946e-02 -6.13462776e-02
 -5.55755384e-02 -2.01149248e-02  3.92747968e-02  1.17914835e-02
  3.17628533e-02  9.76176839e-03 -8.44254717e-02  9.47987512e-02
  8.79330337e-02  3.35420035e-02  6.15528636e-02 -4.98368330e-02
  1.79391564e-03  7.06452131e-02  6.99091777e-02  3.08316288e-04
 -5.74131822e-03 -5.17791472e-02 -1.33887351e-01  6.21716939e-02
  3.78845562e-03 -8.05275142e-02  2.65622046e-02 -3.90727967e-02
  1.08964264e-01  6.95377737e-02  6.80441409e-02  1.71350082e-04
  5.79432724e-03  2.49733664e-02  1.06268702e-02 -4.09437828e-02
  2.47116983e-02 -1.07649274e-01  7.10779354e-02  1.44954557e-02
 -2.09512506e-02  8.33682641e-02  1.28605170e-03  6.11206442e-02
  3.90845276e-02 -1.50506897e-02  3.76369245e-02  4.09446210e-02
 -2.77331956e-02 -2.29366636e-03 -1.00150004e-01  5.90673605e-33
 -6.20906241e-03  7.68402312e-03  1.71971601e-03  1.16201350e-02
  5.82022555e-02 -1.20853834e-01  1.06521048e-01 -2.00879537e-02
 -3.75262424e-02 -5.50090149e-02 -4.92754020e-02  5.03157005e-02
 -2.89579723e-02  5.27997725e-02 -3.85839418e-02 -4.92437743e-02
  1.11204935e-02  4.93143871e-02  8.67814664e-03  5.40590510e-02
  7.25236908e-02  8.10241690e-05 -4.15936783e-02 -1.29319923e-02
 -3.11780698e-03  6.57420307e-02 -9.36813303e-04 -3.01483064e-03
  1.71038173e-02  3.76380272e-02 -9.64536443e-02 -7.43914098e-02
 -3.85096781e-02  2.77627259e-02  1.31046604e-02 -8.23042181e-04
 -5.25266565e-02 -7.70178512e-02 -4.32005338e-02 -1.01832844e-01
 -8.09443071e-02 -1.68422256e-02 -2.63224728e-02  8.88803042e-03
  6.82930201e-02 -8.32088012e-03 -5.69178239e-02  1.12809338e-01
  9.90048144e-03 -3.04878335e-02 -5.74517362e-02  1.21413497e-02
 -7.58290116e-05  4.28215377e-02  1.69731360e-02 -1.09563053e-01
  7.30307028e-02  7.55468011e-02  2.00544912e-02  8.29802752e-02
  2.99426820e-02  2.63990164e-02 -3.22033949e-02 -3.53682265e-02
 -4.42656167e-02  3.28423567e-02 -9.87455994e-02 -4.78767268e-02
 -5.42815924e-02  7.40514100e-02 -8.82148221e-02  7.56939501e-02
 -3.97056863e-02  5.73389679e-02 -3.09678484e-02 -4.90088873e-02
  5.49596101e-02 -5.61065525e-02  2.62016840e-02 -4.51042019e-02
 -8.03921670e-02 -2.54259836e-02  2.12700125e-02  1.76326204e-02
 -9.88153219e-02 -1.76351164e-02  1.81231946e-02  6.24901019e-02
  2.80387718e-02 -2.35198345e-02 -3.09231989e-02 -2.02530175e-02
  6.06362484e-02  4.03507613e-02 -6.47112429e-02 -4.28072835e-33
 -1.01813242e-01  6.06722943e-02 -7.74362162e-02  1.01096988e-01
 -7.89548084e-03  1.15385065e-02  3.77861690e-03  4.39278670e-02
  8.16781372e-02  2.03396697e-02 -4.18684408e-02 -6.70134500e-02
  6.67623729e-02 -2.93111037e-02  9.85650197e-02 -3.75492126e-02
  2.78792046e-02 -1.71531904e-02  1.31228869e-03  5.03638806e-03
 -7.14983419e-02  1.04120031e-01 -3.50889340e-02 -5.50587326e-02
 -5.77843562e-02 -9.46641061e-03 -7.15416595e-02  4.82698204e-03
 -2.57699657e-02  3.80178317e-02  7.38714589e-03  4.57905084e-02
 -1.25384927e-02  6.15798356e-03  3.20814252e-02  1.60339586e-02
  8.57378766e-02  1.37787340e-02 -3.06504499e-03 -1.90582424e-02
  9.52577814e-02  2.83782557e-02 -8.55206046e-03  4.42395322e-02
 -5.94094284e-02 -4.11021570e-03 -5.60825467e-02  1.43514555e-02
  1.77038014e-02 -3.08047906e-02 -3.49482219e-03 -1.27121760e-02
  1.00113368e-02 -3.40177715e-02 -7.78346211e-02  5.62956184e-02
  7.73613378e-02  2.73170862e-02  7.09780902e-02  7.82279521e-02
 -6.22312538e-02 -8.09989348e-02 -2.67890897e-02  1.11775175e-02
 -6.68847933e-03  3.25236912e-03 -8.58123749e-02  1.29326880e-01
 -5.78332879e-02 -1.10849673e-02 -1.63919234e-03  7.08880872e-02
  5.75214764e-03  7.39728883e-02 -3.19275968e-02  2.06724685e-02
  6.53995499e-02  5.23452181e-03 -5.77588473e-03  4.10941914e-02
  1.00298531e-01  1.65408589e-02  4.79064509e-02  4.80462164e-02
  2.59598922e-02  4.49633487e-02  7.25955963e-02  1.07912766e-02
 -7.83279072e-03 -1.19567299e-02  1.73460338e-02  5.36035164e-04
  1.96997705e-03  6.59447387e-02  1.54290767e-02 -2.88702360e-08
 -4.45148274e-02  5.75784519e-02  4.20274436e-02  2.65144538e-02
  4.44941558e-02  1.00545682e-01  3.81438322e-02  3.45226787e-02
 -5.44327311e-03  6.98789582e-03  2.55653337e-02 -2.04649940e-02
 -3.60070728e-02 -7.28085190e-02  4.44113724e-02  8.43981877e-02
 -6.79551139e-02  7.55295828e-02 -8.91716313e-03 -2.40529999e-02
  8.58351737e-02  2.11126003e-02 -1.40490485e-02 -4.14946079e-02
 -7.11805299e-02 -1.53735271e-02  2.27275155e-02  2.04429086e-02
 -4.78916196e-03  2.04283767e-03 -1.22419804e-01 -4.22782041e-02
  6.77210838e-02 -8.98731232e-04  3.64504643e-02  1.42455459e-01
 -3.56106833e-02 -1.60474908e-02 -4.92602102e-02  1.36334300e-01
 -1.14830695e-01 -1.51062733e-03  7.48988474e-03 -5.63352369e-02
  3.66518497e-02 -2.43674666e-02 -3.94929945e-02 -1.11379743e-01
  1.55601818e-02  4.34561726e-03  7.24694505e-02  3.33547778e-02
  8.05600453e-03  6.23924052e-03  4.41627949e-02  5.24855517e-02
 -7.91574568e-02 -5.80058172e-02 -1.24102905e-02  6.97788447e-02
 -1.98655929e-02  8.04832503e-02  7.08204508e-02  4.89362190e-03]",2,2
tensorflow_gpu,1,tensorflow is an open source software library for high performance numerical computation it flexible architecture allows easy deployment of computation across a variety of platform cpu gpus tpus and from desktop to cluster of server to mobile and edge device originally developed by researcher and engineer from the google brain team within google s ai organization it come with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domain tensorflow is licensed under apache,"[('tensorflow', 0.7401), ('many other scientific domain tensorflow', 0.6584), ('flexible numerical computation core', 0.4332), ('deep learning', 0.3829), ('google s ai organization', 0.3821), ('flexible architecture', 0.3764), ('google brain team', 0.3684), ('platform cpu gpus tpus', 0.3613), ('open source software library', 0.3378), ('edge device', 0.2987)]","[-4.41984236e-02 -1.09338418e-01  3.75523120e-02 -6.00057282e-02
  8.24890584e-02 -4.75711152e-02 -5.30565754e-02  2.51197536e-02
 -1.34402225e-02 -1.77784972e-02 -8.60479921e-02 -3.01086437e-02
 -6.02132641e-02  1.79548992e-03  1.93502568e-03 -4.11127694e-02
 -2.45012846e-02  3.20869386e-02 -2.12851036e-02 -1.30356878e-01
 -4.25707698e-02  3.98718528e-02 -1.30772097e-02  1.58899475e-03
 -9.78470664e-04  7.42263645e-02 -1.08599998e-02 -1.42022565e-01
  3.27437744e-02 -6.55014766e-03 -1.02071287e-02  9.64850187e-03
  4.71507711e-03  3.81811298e-02 -6.36580959e-02  3.31682228e-02
 -4.90938015e-02 -4.53656018e-02 -1.57287996e-02 -4.45531756e-02
 -3.31613086e-02 -6.49364218e-02  1.22887539e-02 -4.47792821e-02
  8.86408389e-02  1.80261284e-02  2.25173086e-02 -1.15306959e-01
 -2.31295135e-02  1.99361108e-02 -3.49466614e-02 -8.51285681e-02
 -5.48832156e-02  8.48228261e-02 -7.17121512e-02 -6.73733884e-03
  4.10806164e-02 -6.88320026e-03  1.62451004e-03 -8.22282210e-03
 -1.00759100e-02 -8.78835991e-02  4.05119406e-03  1.54486764e-02
 -3.63357663e-02  5.28295226e-02 -9.17309057e-03  1.02966493e-02
  6.14090227e-02 -1.00714304e-01  8.13344568e-02  3.17448117e-02
 -3.28305252e-02 -7.23914290e-03 -3.69609729e-03 -1.43599079e-03
  5.77681474e-02 -2.73799971e-02  8.20818990e-02 -6.62635267e-02
  2.97083259e-02 -1.71580762e-02 -3.10782306e-02  1.28447125e-02
  3.34661715e-02 -2.07291394e-02 -7.15484247e-02  8.80392119e-02
 -1.64926033e-02 -1.48727791e-03  4.25984226e-02 -3.23301293e-02
  2.54538245e-02 -3.26760523e-02  4.63619828e-02  8.25122185e-03
 -7.75806680e-02 -8.91252458e-02 -4.99539562e-02  3.41689922e-02
 -1.18737161e-01  1.49925379e-03  4.47053388e-02  4.81082126e-02
 -2.87359152e-02  8.46514031e-02  4.03921865e-02 -3.37678823e-03
  9.50822160e-02 -2.90156733e-02 -2.99208332e-02  6.71075732e-02
  4.06970084e-02 -7.54432976e-02  1.83600057e-02 -7.02635124e-02
 -3.98105867e-02  7.65025169e-02  3.50047797e-02  1.09249830e-01
 -1.12119086e-01  3.65649313e-02 -3.26654613e-02 -8.72930512e-03
 -2.04302394e-03  1.50718652e-02 -8.76195654e-02  4.60425342e-33
  1.02570942e-02  4.28824779e-03  2.68881787e-02 -6.87931105e-02
  6.11587837e-02 -6.28679469e-02  2.82790624e-02  1.04069605e-03
  1.19376676e-02 -2.09535728e-03 -6.10620193e-02  1.45414501e-01
 -2.77088694e-02  1.31631240e-01  5.33098839e-02 -4.76143509e-02
  1.41801238e-02  5.01978882e-02  7.20799938e-02 -1.55935355e-03
  2.45469920e-02  6.20573387e-02  4.07773815e-02  9.34710652e-02
  2.34671924e-02  6.27924548e-03 -4.57534008e-02 -4.82183322e-02
  6.20497251e-03  1.55118303e-02 -5.82717657e-02  2.86109298e-02
 -4.72726822e-02  8.19104631e-03  1.48596782e-02  1.40242409e-02
 -7.13818287e-03 -6.28968030e-02  7.68921385e-03  1.67898014e-02
 -2.04171371e-02  4.88140881e-02  2.21376605e-02 -4.13218699e-02
 -1.19940946e-02 -2.14574859e-02  2.20675208e-02  2.55821627e-02
  4.94300574e-02 -5.72271608e-02 -2.83469800e-02  2.06819158e-02
 -7.33109657e-03 -7.42880777e-02  4.45740707e-02 -1.86625645e-02
  1.66003243e-03  5.78348897e-02  4.94083539e-02  8.17290470e-02
 -1.52327828e-02  2.14482732e-02 -2.74007898e-02 -1.56885870e-02
  3.61333638e-02 -1.15680881e-02  2.34961193e-02  4.28035334e-02
  9.24402382e-03  2.92042568e-02 -8.23360160e-02  7.03407750e-02
  3.02753523e-02  7.97685049e-03 -4.55205850e-02 -1.75612886e-02
  3.93302739e-02 -1.34543687e-01 -4.39116172e-02 -5.35312574e-03
 -1.35161489e-01  1.69179011e-02  7.37877490e-05  1.96509007e-02
  2.84204120e-03  1.24324122e-02 -2.80277040e-02  1.61576960e-02
 -5.60108647e-02 -5.83440997e-02 -9.79904234e-02 -1.62681602e-02
  8.40131938e-02  6.26077726e-02 -9.34203938e-02 -4.11873715e-33
 -1.21871002e-01 -2.00116094e-02 -6.95795864e-02  9.35161188e-02
  1.06673934e-01  1.64793674e-02  5.29420041e-02 -4.99502346e-02
  7.84921634e-04  5.40840551e-02  3.24851312e-02 -1.49185723e-02
  7.49930143e-02 -3.41328681e-02  1.59471463e-02 -6.72609732e-02
 -9.23045427e-02 -7.28558823e-02 -1.41289271e-02 -1.31858299e-02
 -3.60742360e-02  5.88296913e-02 -1.11268647e-02 -1.11294286e-02
  4.44210209e-02  2.42709126e-02 -9.14493203e-02 -6.56116903e-02
  6.20851517e-02  7.87141398e-02  8.85193329e-03 -2.49410868e-02
  1.07943751e-02  5.60709536e-02  1.08070284e-01  2.54249573e-02
  4.70024757e-02 -3.27337794e-02  4.17715870e-02 -3.21090035e-02
  6.68326393e-02  2.09123977e-02  8.31178799e-02  5.75441495e-03
 -1.57177057e-02  5.13668582e-02 -1.00466989e-01  5.68194799e-02
 -7.79418200e-02 -7.36578256e-02  2.29164143e-03  1.09611116e-02
 -3.92328426e-02 -1.02612153e-01  2.16487814e-02  2.21487638e-02
  4.73303422e-02 -1.24629773e-02  1.68757718e-02 -1.58568248e-02
 -1.49172219e-03 -5.22242896e-02  2.37852894e-02  2.95598339e-02
 -4.57500108e-02  7.54220188e-02 -8.37374944e-03  2.92717386e-02
 -7.08715320e-02 -5.05744107e-02  3.37662622e-02  9.25058499e-03
  6.04491401e-03  5.77810109e-02 -5.38401343e-02  1.88186746e-02
  2.49538794e-02 -2.30869707e-02  4.80298363e-02 -1.32949762e-02
  6.71137720e-02  1.58789922e-02  3.83781157e-02  5.01072630e-02
  5.06595746e-02  7.17647970e-02  4.81168069e-02 -3.42952199e-02
  2.15225033e-02 -2.26974841e-02 -4.45528142e-02  1.36768846e-02
  3.95529121e-02  9.66346115e-02 -7.90077373e-02 -2.76889054e-08
  4.37600647e-05  1.28331250e-02  8.19380730e-02 -1.11790355e-02
  5.15342504e-02 -8.69946275e-03  3.87721956e-02  4.68982942e-02
 -2.15304736e-02  1.09144384e-02 -1.34722795e-02 -5.39821126e-02
 -5.36090359e-02  1.06498577e-04  9.06490088e-02  5.04027084e-02
 -4.07694206e-02 -2.80015147e-03  3.82598266e-02 -8.40364844e-02
  3.06539945e-02  7.10854530e-02 -5.35856653e-03 -4.30149212e-02
 -3.41456267e-03 -7.31946230e-02  4.09532413e-02 -5.65154618e-03
  9.22913849e-03  2.75571328e-02 -5.08137271e-02 -5.10621220e-02
  5.90167232e-02 -3.64067666e-02  9.62798372e-02  4.31413203e-03
  5.34966327e-02 -2.88944710e-02  2.93812994e-03  6.43181428e-02
 -2.65936963e-02  8.72200802e-02  4.80443090e-02 -4.97647300e-02
  7.63691915e-03 -4.89581898e-02  1.61224399e-02 -4.40009758e-02
 -1.88040975e-02  6.26380444e-02 -7.08308667e-02  7.13098049e-02
 -4.59356047e-02  8.05362687e-02  4.94703427e-02  2.27457359e-02
 -2.76032165e-02 -1.40729666e-01 -2.81152502e-02  2.60950923e-02
 -1.65610705e-02  1.23978546e-02  5.14509305e-02  7.77910324e-03]",2,2
niftynet,1,niftynet is a tensorflow based open source convolutional neural network cnn platform for research in medical image analysis and image guided therapy niftynet s modular structure is designed for sharing network and pre trained model using this modular structure you can niftynet is a consortium of research organisation bmeis school of biomedical engineering and imaging science king s college london wei wellcome epsrc centre for interventional and surgical science ucl cmic centre for medical image computing ucl hig high dimensional imaging group ucl where bmeis act a the consortium lead niftynet is not intended for clinical use niftynet release note are available here d volumetric image processed a a stack of d slice d co registered multi modal d volumesall other niftynet dependency are installed automatically a part of the pip installation process to install from the source repository please checkout the instruction the api reference and how to guide are available on read the doc if you use niftynet in your work please cite gibson and li et al bibtex entry the niftynet platform originated in software developed for li et al niftynet is released under the apache license version copyright the niftynet consortium this project is grateful for the support from the wellcome trust the engineering and physical science research council epsrc the national institute for health research nihr the department of health doh cancer research uk king s college london kcl university college london ucl the science and engineering south consortium s the stfc rutherford appleton laboratory and nvidia,"[('niftynet consortium', 0.6728), ('niftynet platform', 0.6193), ('consortium lead niftynet', 0.611), ('clinical use niftynet release note', 0.5894), ('niftynet', 0.5684), ('therapy niftynet', 0.5464), ('convolutional neural network cnn platform', 0.5152), ('other niftynet dependency', 0.478), ('tensorflow', 0.4746), ('medical image computing ucl hig', 0.4644)]","[-8.14165175e-02 -7.61680529e-02  1.67112220e-02 -3.37459221e-02
 -1.14060575e-02 -6.08717389e-02 -1.33495750e-02  1.76084060e-02
  3.30670998e-02 -9.10877213e-02 -4.48215641e-02  7.01271289e-04
 -6.09438233e-02  1.03757568e-01 -7.42762461e-02  3.50608863e-02
 -7.06892982e-02  4.34104465e-02 -4.91940789e-02 -4.57098745e-02
 -2.81790439e-02  3.28719430e-02  2.08775159e-02 -1.16852568e-02
  5.76014537e-03 -1.01969382e-02 -2.98886783e-02 -8.50204825e-02
  3.17343213e-02 -6.61892742e-02  2.74769366e-02  5.22152446e-02
 -5.43752685e-02  1.94746684e-02 -6.34342134e-02  4.83644269e-02
 -2.65634209e-02 -1.93678383e-02 -4.88899089e-02 -1.74483415e-02
  1.44225396e-02 -3.82911824e-02 -5.21641644e-03  7.72029608e-02
  8.05514082e-02  8.82797316e-03  7.28335418e-03 -2.04709079e-02
  6.41817153e-02  8.00533220e-02 -7.19116032e-02 -4.24837843e-02
 -2.53090933e-02  7.59464800e-02 -3.40204570e-03 -5.67713901e-02
 -1.52122183e-02 -6.93195462e-02 -5.01176715e-02 -2.41991840e-02
  7.28352219e-02  2.83410220e-04 -2.55631804e-02  1.52684823e-02
  3.80905680e-02  5.62035665e-02  2.89943870e-02 -1.94377191e-02
  6.82360784e-04 -1.14403836e-01  3.50466445e-02  1.98654123e-02
  1.22007066e-02  1.09951891e-01 -5.95273562e-02  3.37125137e-02
  1.46754414e-01  2.14526914e-02  9.54224840e-02 -6.44604787e-02
  6.10954575e-02  6.45171404e-02  1.00414552e-01 -1.46567049e-02
  1.00827083e-01  1.65474862e-02 -9.39904973e-02  1.18379638e-01
 -9.02253836e-02 -4.18898426e-02  1.13891799e-03  4.10394231e-03
  4.92785871e-02 -6.15112819e-02  3.87359993e-04 -4.50699627e-02
 -8.21101069e-02 -4.83611301e-02 -9.83014703e-02  7.49495253e-02
 -6.12787604e-02 -4.22567166e-02 -3.52356434e-02  7.83278942e-02
 -6.63615018e-02 -9.44333803e-03  7.16463998e-02 -6.51191268e-03
  2.47584488e-02 -5.78516768e-03  1.36454236e-02  1.29566193e-02
 -1.96003579e-02 -3.64592783e-02  5.58573864e-02  2.74568610e-02
 -9.05757621e-02  4.15045656e-02  7.19363913e-02  3.76770459e-02
 -1.04069673e-01 -1.52405845e-02 -5.14478758e-02 -3.90518606e-02
 -4.89142165e-02  5.38627841e-02 -7.07729682e-02  8.66802041e-33
 -1.69196185e-02 -1.00813210e-02  8.05478096e-02  3.26483460e-05
  3.02019194e-02 -4.15730588e-02 -3.71732190e-02 -3.10079269e-02
 -5.66722192e-02 -6.15451559e-02 -3.27530392e-02  5.34081459e-02
 -3.40368040e-02  3.25374119e-02 -1.85113214e-02 -6.32027015e-02
  3.58352959e-02  7.06443489e-02  3.45859230e-02  2.63119745e-03
 -3.18660773e-02  9.19263884e-02 -2.87077371e-02  4.80816774e-02
 -4.40428369e-02 -2.34884862e-02 -8.59889463e-02 -4.55989130e-02
  1.03026941e-01 -4.96001565e-04 -8.48963708e-02  3.09775639e-02
  1.32985124e-02 -1.23136891e-02 -6.77687749e-02 -5.39792553e-02
 -1.24801863e-02 -8.85647759e-02  1.78864412e-02  2.21449304e-02
 -2.04745103e-02  4.11183536e-02  2.70942580e-02 -4.48340736e-03
 -4.84867655e-02  2.39722263e-02 -7.57494068e-04  3.07069086e-02
  1.72877591e-02 -6.94293752e-02 -7.18005421e-03 -8.51565972e-03
 -1.14771754e-01 -5.24635054e-02  8.63802619e-03  2.75585968e-02
 -2.59658247e-02  1.95320025e-02  9.86796319e-02  7.25776656e-03
  4.40192409e-02 -2.50346716e-02 -1.06960796e-02  1.15455249e-02
  4.28085104e-02 -1.83021352e-02 -4.58229333e-03  1.09003959e-02
 -3.41648944e-02  9.61354673e-02 -8.50064233e-02  1.00927576e-01
  3.17470990e-02 -1.16719063e-02  5.81780598e-02 -1.83643494e-02
  1.66532136e-02 -4.75629829e-02 -4.22089361e-02 -9.92094050e-04
 -5.08957393e-02 -2.00866926e-02 -1.09331440e-02  2.72463039e-02
 -3.62756290e-02 -8.20014277e-04 -5.66903036e-03  2.03021411e-02
 -1.57579258e-02 -2.02888623e-02 -4.05765250e-02 -1.37625192e-03
  1.35561759e-02  2.49469168e-02 -1.45615842e-02 -6.91464201e-33
  7.53661850e-03  4.41037416e-02 -8.97648931e-02 -1.36949643e-02
  2.82955356e-02  6.28858879e-02  3.92271876e-02 -7.99636915e-02
  4.63358238e-02  8.58125687e-02  1.17225334e-01 -1.90721303e-02
  1.33022061e-02 -5.83168045e-02 -1.75422430e-02 -1.24059118e-01
 -1.68238431e-02 -9.39111561e-02 -3.67072672e-02  4.69575860e-02
 -3.73046584e-02  4.99622412e-02 -4.40897159e-02  1.58579908e-02
 -4.69840318e-02  2.34525949e-02 -2.03121826e-02 -8.57605319e-03
  1.07200481e-02 -4.47864495e-02 -5.19501232e-02 -9.17734113e-03
 -3.05180587e-02 -6.44219443e-02  7.18931109e-02  7.80582651e-02
  1.47604533e-02 -1.69403143e-02 -8.28495845e-02 -1.15303911e-01
  5.32209985e-02  4.58559878e-02 -5.18522412e-02  3.59789804e-02
 -3.10993716e-02  2.69934274e-02 -4.78951111e-02  2.31589377e-02
  2.51995102e-02  1.25436122e-02 -3.66546437e-02 -1.70478900e-03
 -4.04516011e-02 -1.44233266e-02  1.77638594e-03  3.34220827e-02
  1.13782035e-02  1.24201796e-03  4.50434834e-02  6.22998849e-02
 -6.17112070e-02 -6.06184714e-02  1.59970205e-02 -5.48227318e-02
 -1.18745409e-03  7.53580173e-03 -1.01115756e-01  9.64880139e-02
 -2.02857517e-02  1.06744781e-01 -5.77222519e-02  4.66169678e-02
  7.39587545e-02  1.04593046e-01 -4.83715385e-02 -3.52658145e-02
  5.32003641e-02 -9.22921449e-02  3.20642814e-02 -6.57539740e-02
  6.24948144e-02 -8.20626244e-02 -2.74369065e-02  1.06275760e-01
  5.98380938e-02  5.62434122e-02  7.70637468e-02 -2.04953142e-02
  9.91443545e-02  4.65985062e-03  2.50758417e-02  2.90823467e-02
 -8.81739799e-03  1.35248834e-02 -1.92385167e-02 -3.47694282e-08
 -2.46720500e-02 -2.43087038e-02  5.35731576e-02 -5.01257591e-02
  2.91511212e-02 -6.02097251e-02  2.34457012e-02  9.89734158e-02
  1.21864339e-03  2.82276981e-02  5.64751178e-02 -2.31410656e-02
 -5.54831438e-02 -1.07825592e-01  2.36569792e-02  8.33881944e-02
  3.13428529e-02  1.28611224e-02  4.33160216e-02 -5.27130999e-02
 -4.33478951e-02 -1.32242776e-02  1.72728803e-02 -8.29596445e-03
  1.29293595e-02 -4.30487618e-02  4.24825102e-02  3.51576954e-02
  2.66431887e-02 -3.68965678e-02 -6.01053331e-03 -1.53130223e-03
  8.48362446e-02 -3.92904058e-02  8.13141316e-02  6.11829348e-02
  1.44849357e-03  6.85214624e-02  1.21166173e-03  1.82452295e-02
 -3.45566822e-03  7.25536272e-02  1.81642585e-02 -1.56001002e-03
 -3.51481885e-02  1.46809435e-02  2.19812635e-02 -8.17360878e-02
  3.35408635e-02 -1.69168990e-02 -2.34377738e-02 -8.04829784e-03
  2.36195717e-02  8.69863927e-02  1.14626437e-02  6.71492368e-02
  9.33478645e-04 -5.20962663e-02  1.85701698e-02  7.44654983e-02
  4.97862250e-02 -1.00234628e-03  8.97520110e-02  3.09723839e-02]",2,2
dpu_utils,1,this contains a set of utility used across project of the dpu team stored in the python subdirectory published a the dpu utils package or via the community maintained conda recipe below you can find an overview of the utility included detailed documentation is provided at the docstring of each class unsorted segment operation following tensorflow s unsorted segment sum operation unsorted segment operation following tensorflow s unsorted segment sum operation these model have not been tested with tf you can use the deduplicationcli command to detect duplicate in pre processed source code by invokingwhere data path is a file containing tokenized jsonl gz file and out json is the target output file for more option look at help an exact but usually slower version of this can be found here along with code to tokenize java c python and javascript into the relevant format the resulting html file will be in htmlcov index html stored in the dotnet subdirectory generic utility code related utility this project welcome contribution and suggestion most contribution require you to agree to a contributor license agreement cla declaring that you have the right to and actually do grant u the right to use your contribution for detail visit http cla microsoft com when you submit a pull request a cla bot will automatically determine whether you need to provide a cla and decorate the pr appropriately e g label comment simply follow the instruction provided by the bot you will only need to do this once across all repos using our cla this project ha adopted the microsoft open source code of conduct for more information see the code of conduct faq or contact opencode microsoft com with any additional question or comment,"[('tensorflow', 0.4437), ('deduplicationcli command', 0.4304), ('jsonl', 0.3359), ('source code', 0.3345), ('generic utility code', 0.3304), ('microsoft open source code', 0.3276), ('json', 0.3139), ('detailed documentation', 0.3125), ('file', 0.2801), ('utility', 0.2777)]","[-1.30349904e-01 -8.87143686e-02 -4.16377652e-03 -2.42061615e-02
  2.71636620e-02 -4.23388667e-02 -3.37035246e-02  7.23342299e-02
 -4.33803797e-02 -2.50679459e-02  2.07037982e-02  2.51092184e-02
 -6.57561570e-02 -4.25593108e-02  5.84625192e-02  2.52732411e-02
 -7.92815983e-02  4.29926999e-02 -4.17450108e-02 -9.34595019e-02
  2.78574564e-02  3.53781469e-02 -3.73292752e-02  4.06469777e-02
  1.94660295e-02  5.14956042e-02  2.16625091e-02 -6.64262697e-02
  5.19761220e-02 -4.85198684e-02 -1.98826031e-03  5.39853796e-02
  7.18194470e-02  1.79710966e-02  5.33119123e-03  8.69455859e-02
  1.12327165e-03 -7.33039230e-02 -1.19924853e-02 -3.67603227e-02
 -4.55598161e-02  5.40867485e-02 -6.31147176e-02 -1.50376884e-02
  4.16147970e-02 -8.38106424e-02 -1.90276187e-02 -7.38198087e-02
 -1.38712442e-02  2.11386289e-02 -8.95432681e-02 -4.34283093e-02
 -8.19577277e-02  5.86299691e-04  4.79131714e-02 -9.48126167e-02
 -1.73080117e-02  4.07609902e-03 -3.61859165e-02 -2.17183623e-02
  1.85060389e-02  1.90725233e-02 -9.07829702e-02  7.69612119e-02
 -1.38603977e-03  3.76626067e-02  1.65926199e-02  4.41278182e-02
  1.29496843e-01 -8.82892981e-02 -5.27749248e-02 -1.65890064e-02
 -5.57309948e-02  1.03091560e-01 -5.85499499e-03 -6.96630101e-04
  5.11603393e-02 -2.11319793e-03 -4.49842401e-02 -5.66489995e-02
 -3.90053652e-02 -2.83246990e-02  3.37149501e-02  6.30481914e-02
 -2.45540962e-03  1.94537174e-02  5.07957190e-02  7.06634521e-02
  2.89592315e-02  2.68846918e-02  9.15548205e-03 -8.44070315e-02
  5.95498681e-02 -3.60509604e-02  6.15958460e-02  6.17567338e-02
 -1.96588747e-02 -5.58553971e-02  1.92979872e-02  2.74981279e-02
 -8.72038156e-02 -4.19052951e-02  6.80208728e-02  5.44359870e-02
 -7.33304583e-03  1.08884294e-02  6.51263148e-02 -4.72685769e-02
  3.22833359e-02  3.25335078e-02  4.71335807e-04  5.42080663e-02
 -5.51361442e-02 -1.07233286e-01  3.69226038e-02 -1.36372412e-03
 -1.91180483e-02  5.09238616e-02  3.31967138e-02  8.69726315e-02
 -7.97133427e-03  1.71800461e-02  1.69554539e-02 -1.08461985e-02
 -5.68554699e-02  9.24491882e-03 -6.65100217e-02  2.34755112e-33
  2.74699572e-02 -7.26649864e-03 -2.39218818e-03  9.53507144e-03
  4.61339429e-02 -1.12786904e-01  6.28092885e-02  5.21159805e-02
 -3.91625725e-02 -3.97272706e-02 -1.15375910e-02  1.62175342e-01
 -1.27603831e-02  1.38248384e-01 -5.92456236e-02 -6.83754534e-02
  4.39282693e-02  5.49954660e-02  1.77854653e-02  2.69183479e-02
  5.39165512e-02  3.92907858e-02  1.78908389e-02  8.99832100e-02
  3.65056768e-02  1.35382181e-02  1.78434774e-02  4.47203266e-03
  3.07326093e-02  1.24481404e-02 -3.99489366e-02 -9.86771584e-02
 -3.84386466e-03  4.92613465e-02  3.18762846e-02  7.94165358e-02
 -7.75155053e-02 -7.36493543e-02  2.77733617e-02 -8.03294331e-02
 -1.22220758e-02  1.70391724e-02 -8.06235299e-02 -3.07061188e-02
  1.75344162e-02 -6.86527491e-02 -4.75771911e-02 -1.76601037e-02
  2.06022188e-02 -4.52634878e-02 -2.38454342e-02  7.30787218e-02
  1.78082697e-02 -6.41136523e-03  4.32250788e-03 -1.00793310e-01
  2.43542939e-02 -1.31994556e-03  8.40428099e-02  1.52954012e-01
 -4.17994037e-02  7.34187886e-02  6.23646751e-03  9.15295910e-03
  5.14664724e-02 -2.81366259e-02 -7.22985715e-02 -2.86527090e-02
  1.48183387e-02  3.19020562e-02 -4.99605164e-02  6.24169111e-02
 -2.57412065e-02  8.91462062e-03 -1.66749079e-02 -3.70701402e-02
 -4.18912881e-04 -5.85123673e-02 -1.94754526e-02 -1.80313978e-02
 -1.17873795e-01  4.40948270e-03  7.46488629e-04  2.25977618e-02
 -7.34567121e-02  2.00739354e-02 -6.03726413e-03 -2.09364798e-02
  2.34914999e-02 -5.11059463e-02 -3.04973442e-02  2.62528714e-02
  2.15440430e-02 -4.87649217e-02 -1.74382422e-02 -2.96098707e-33
 -2.18445491e-02 -3.07697300e-02 -2.41497178e-02  6.72594607e-02
 -5.66644333e-02  4.44366895e-02  3.62022314e-04  5.23667783e-04
  3.89506184e-02  5.17509878e-02 -6.16795430e-03 -2.12724265e-02
  1.87779367e-02 -7.66603947e-02  2.13253107e-02 -2.68221851e-02
 -5.39080873e-02 -8.03202689e-02  1.64354239e-02 -4.19196876e-04
 -4.73093614e-02  1.33341983e-01  2.06579976e-02 -2.99508702e-02
  2.79382858e-02 -4.32550199e-02 -1.05662815e-01 -5.03678396e-02
  8.68622959e-03 -1.88418552e-02  2.26500574e-02  1.19114006e-02
 -1.27860550e-02 -4.47563604e-02  3.85598629e-03 -6.53612521e-03
  1.14529289e-01  9.72827002e-02  2.60924716e-02  3.27130407e-02
  7.38717988e-02 -1.16214091e-02  5.92724457e-02  5.24883196e-02
  3.60275316e-03 -2.76045296e-02 -1.00400247e-01  4.54111844e-02
 -1.96154546e-02 -5.53719252e-02  2.53899917e-02 -2.56719980e-02
 -2.67841369e-02 -8.99138600e-02 -1.27481623e-02  1.20347654e-02
  1.03538379e-01 -2.24356446e-02 -1.10372482e-03  5.58251143e-03
 -3.50564942e-02 -6.25178888e-02  3.98658663e-02 -5.92711056e-03
 -3.22045833e-02 -2.81469151e-02 -5.38022667e-02  1.94050407e-03
 -5.84546998e-02 -5.57101183e-02 -3.97286415e-02 -3.22910585e-03
  1.87755167e-03  2.66828667e-02 -3.15635763e-02 -1.49158062e-02
  1.57293323e-02  7.23389164e-03  2.25171391e-02  3.62156294e-02
  1.03320554e-01  2.10115127e-02  2.07245685e-02  6.97054863e-02
  5.35977930e-02  5.69354892e-02  3.62474248e-02 -2.23971009e-02
 -1.66272689e-02  1.01057645e-02 -9.17782038e-02  6.36509135e-02
 -1.39315624e-03  1.48950234e-01  3.84925446e-03 -2.60702837e-08
 -6.21247552e-02  1.46031147e-02  2.43550423e-03  8.48146230e-02
 -1.55485440e-02  1.13500832e-02 -5.48546873e-02  5.73344640e-02
  3.07142157e-02  5.62428730e-03  4.25875448e-02  2.38176305e-02
 -1.71178609e-01 -2.44911313e-02 -9.46897175e-03  5.67288250e-02
 -2.83802319e-02  3.51330154e-02  1.62526499e-02 -1.76315513e-02
  8.83880854e-02  7.47803599e-02 -3.95962549e-03  1.25541957e-02
 -1.81900356e-02 -1.48548828e-02  8.24892595e-02  7.63030574e-02
 -3.11002508e-02 -1.37238456e-02 -5.59123326e-03 -9.98143852e-03
  6.85535446e-02 -7.88562745e-02  2.69020181e-02  2.40573334e-03
  5.28628789e-02 -3.16939130e-02 -1.70935150e-02  5.50710112e-02
 -4.84144222e-03  5.58012053e-02 -1.99240986e-02 -5.02684899e-03
  6.09154254e-02  6.67336676e-03 -2.93241255e-02 -3.90158035e-02
 -5.71576916e-02 -4.60497402e-02 -6.50599971e-02  4.32426929e-02
 -1.70643646e-02  1.20891556e-01 -1.46990670e-02  7.55309388e-02
 -4.98153605e-02 -9.62766185e-02  5.20281121e-02 -1.60096854e-03
  5.91821736e-04  5.66600971e-02  4.91114222e-02  2.33512633e-02]",2,2
keras-tcn,1,kera temporal convolutional network paper tested with tensorflow and rc may for macos m user pip install no binary kera tcn kera tcn the no binary option will force pip to download the source tar gz and re compile it locally also make sure grpcio and h py are installed correctly there are some tutorial on how to do that online visualization of a stack of dilated causal convolutional layer wavenet the usual way is to import the tcn layer and use it inside a kera model an example is provided below for a regression task cf task for other example a ready to use tcn model can be used that way cf task for some example d tensor with shape batch size timesteps input dim timesteps can be none this can be useful if each sequence is of a different length multiple length sequence example here are some of my note regarding my experience using tcn nb filter present in any convnet architecture it is linked to the predictive power of the model and affect the size of your network the more the better unless you start to overfit it s similar to the number of unit in an lstm gru architecture too kernel size control the spatial area volume considered in the convolutional ops good value are usually between and if you think your sequence heavily depends on t and t but le on the rest then choose a kernel size of for nlp task we prefer bigger kernel size a large kernel size will make your network much bigger dilation it control how deep your tcn layer is usually consider a list with multiple of two you can guess how many dilation you need by matching the receptive field of the tcn with the length of feature in your sequence for example if your input sequence is periodic you might want to have multiple of that period a dilation nb stack not very useful unless your sequence are very long like waveform with hundred of thousand of time step padding i have only used causal since a tcn stand for temporal convolutional network causal prevents information leakage use skip connection skip connection connects layer similarly to densenet it help the gradient flow unless you experience a drop in performance you should always activate it return sequence same a the one present in the lstm layer refer to the kera doc for this parameter dropout rate similar to recurrent dropout for the lstm layer i usually don t use it much or set it to a low value like activation leave it to default i have never changed it kernel initializer if the training of the tcn get stuck it might be worth changing this parameter for example glorot uniform use batch norm use weight norm use weight norm use normalization if your network is big enough and the task contains enough data i usually prefer using use layer norm but you can try them all and see which one work the best the receptive field is defined a the maximum number of step back in time from current sample at time t that a filter from block layer stack tcn can hit effective history the receptive field of the tcn can be calculated using the formula where nstack is the number of stack nb is the number of residual block per stack d is a vector containing the dilation of each residual block in each stack and k is the kernel size the is there because there are two conv d layer in a single residualblock ideally you want your receptive field to be bigger than the largest length of input sequence if you pas a sequence longer than your receptive field into the model any extra value further back in the sequence will be replaced with zero note unlike the tcn example figure only include a single conv d per layer so the formula becomes rfield k nstack i di without the factor k dilation block k dilation block k dilation block making the tcn architecture non causal allows it to take the future into consideration to do it prediction a shown in the figure below however it is not anymore suitable for real time application non causal tcn k dilation block to use a non causal tcn specify padding valid or padding same when initializing the tcn layer once kera tcn is installed a a package you can take a glimpse of what is possible to do with tcns some task example are available in the repository for this purpose reproducible result are possible on nvidia gpus using the tensorflow determinism library it wa tested with kera tcn by lingdoc and he got reproducible result language modeling remains one of the primary application of recurrent network in this example we show that tcn can beat lstm without too much tuning more here wordptb tcn v lstm comparable number of weight the task consists of feeding a large array of decimal number to the network along with a boolean array of the same length the objective is to sum the two decimal where the boolean array contain the two s adding problem task the copy memory consists of a very large array the idea is to copy the content of the vector x to the end of the large array the task is made sufficiently complex by increasing the number of s in the middle copy memory task the idea here is to consider mnist image a d sequence and feed them to the network this task is particularly hard because sequence are element in order to classify correctly the network ha to remember all the sequence usual lstm are unable to perform well on this task sequential mnist testing is based on tox,"[('kera tcn', 0.5086), ('binary kera tcn kera tcn', 0.5035), ('tcn model', 0.503), ('tcn architecture non causal', 0.4808), ('tcn layer', 0.48), ('causal convolutional layer wavenet', 0.4765), ('convnet architecture', 0.4719), ('tensorflow', 0.469), ('tcn nb filter', 0.4596), ('temporal convolutional network paper', 0.4518)]","[-1.25807598e-01 -5.89724779e-02  7.92288333e-02 -4.20589373e-02
 -2.84503289e-02  2.68545467e-02 -2.83986125e-02 -7.57000223e-02
  2.25778366e-03 -7.26391301e-02 -6.75131753e-02 -6.45931363e-02
 -2.68212110e-02  1.85977072e-02 -1.01524502e-01 -6.40531555e-02
  6.26370981e-02  8.16549174e-03 -2.92540751e-02 -6.07015453e-02
 -5.00583928e-03 -2.77193729e-03 -2.92164069e-02 -4.33815731e-04
 -6.02258043e-03 -8.95114429e-03 -1.47629036e-02 -5.22522181e-02
  3.87857333e-02 -4.56851013e-02  2.32244655e-02  8.38999748e-02
 -7.32444227e-02  2.38847360e-02 -6.47917241e-02 -3.78210358e-02
 -4.31430340e-02 -2.72583980e-02 -3.44484150e-02 -6.10015094e-02
  2.97845174e-02 -1.61848962e-02 -5.65074123e-02  1.23988260e-02
  1.30547494e-01 -2.20984295e-02  5.38703538e-02 -4.43530306e-02
  1.40872225e-02 -6.19762614e-02 -4.16605733e-02 -3.36137116e-02
 -2.62008943e-02  1.01234384e-01  4.70507778e-02 -5.76061755e-02
 -2.62149964e-02 -3.76477130e-02 -2.50964463e-02  8.98094010e-03
  1.35680540e-02  3.62365656e-02 -1.16660133e-01  2.94285305e-02
  9.18660089e-02  6.18338548e-02 -1.02279395e-01  9.68046114e-02
  3.00194025e-02 -3.53392884e-02  1.01298885e-02  2.78623570e-02
 -4.36624251e-02  1.93823762e-02 -9.60260350e-03 -2.33545471e-02
  1.43882826e-01  4.26337384e-02  1.07353739e-02 -1.31800264e-01
  2.01286692e-02 -3.34631242e-02  2.16983501e-02 -2.15497091e-02
  2.63629425e-02 -9.40554217e-03 -1.12007551e-01  4.37464155e-02
 -3.41320373e-02  6.87632011e-03  3.33570279e-02  3.90376858e-02
  6.19991682e-02 -6.98300377e-02  5.39433695e-02 -1.77824777e-03
 -3.58238965e-02  1.08105661e-02  3.39432620e-02  8.35213810e-02
 -1.90795418e-02 -7.32715055e-02 -1.87572762e-02  4.20289561e-02
 -1.48496777e-03 -6.60453166e-04  5.04763573e-02  7.20316311e-03
  4.94305193e-02 -3.82265896e-02 -3.55126411e-02  3.19445170e-02
 -1.05259586e-02 -7.85622299e-02  4.63806801e-02  2.04343237e-02
 -1.83170345e-02 -3.75663606e-03  1.07963972e-01 -7.14705838e-03
 -1.31039321e-01 -5.13465181e-02 -7.31820688e-02  2.15928555e-02
 -2.91696507e-02  2.10972521e-02 -6.77653998e-02  6.93035911e-33
  9.84810991e-04 -5.35896420e-02  1.75703019e-02 -6.96720332e-02
  8.50523040e-02 -4.89540445e-03  2.87522078e-02 -5.43176569e-02
 -2.63808630e-02 -5.76955751e-02 -1.17544636e-01  3.40032130e-02
 -4.30230014e-02  3.58262286e-02  4.73809317e-02 -4.27211449e-02
 -9.87984911e-02  4.24639229e-03  2.26566959e-02  4.73622046e-03
  2.67454833e-02 -1.39828641e-02 -6.76142285e-03  1.09552927e-01
 -2.40453873e-02 -3.71728390e-02 -2.12591477e-02 -3.74482870e-02
 -4.61769328e-02  3.05180401e-02 -1.64494291e-02  5.17950878e-02
  5.70592210e-02 -3.03151924e-02 -4.41791043e-02 -3.18961926e-02
 -5.40752709e-02  2.37763524e-02 -1.90945491e-02 -2.18831431e-02
 -2.19714399e-02  7.40322238e-03 -4.53293510e-02 -1.35285305e-02
 -7.52743557e-02 -7.43357241e-02  3.53036076e-03  6.50862977e-02
 -5.46095136e-04 -9.97622777e-03  6.19677491e-02 -3.17404210e-03
 -2.26538405e-02 -1.15837470e-01  7.36159161e-02  6.63777813e-02
  1.00811049e-01  5.48889451e-02  2.99200919e-02  6.64583147e-02
  1.42582040e-02  1.26390653e-02 -2.56860014e-02 -2.67625712e-02
 -2.79032654e-04  4.29558195e-02 -5.55702634e-02  1.20167825e-02
  5.49167693e-02 -4.34677675e-02 -8.61007422e-02  8.57398733e-02
  5.08012297e-03 -4.35992423e-03  9.46155563e-02  1.43661490e-03
 -8.38379934e-02 -7.93145075e-02 -4.86874767e-02  8.74692872e-02
 -5.67511506e-02 -1.80144031e-02  1.71561372e-02  1.82896073e-03
 -4.58323322e-02  3.94482017e-02 -9.55209136e-03 -2.07366664e-02
  1.53775401e-02 -4.27392088e-02  3.66948247e-02  2.62885764e-02
  4.93136086e-02  3.49792652e-02 -1.22535760e-02 -6.82976544e-33
  1.40891057e-02  8.54676664e-02 -1.55497286e-02  2.36380193e-02
  1.85676049e-02  3.57197113e-02  1.67483126e-03 -3.33821140e-02
  1.09027855e-01  7.99190998e-02  6.95204809e-02 -5.77412434e-02
  7.29271993e-02 -1.76090728e-02  4.43481617e-02 -4.47808318e-02
 -5.44911064e-02 -5.84049523e-02 -5.08785583e-02 -3.56518105e-02
 -3.95330153e-02  4.06965315e-02 -6.24355413e-02 -2.53624246e-02
 -1.03337709e-02 -3.98674570e-02 -4.56613563e-02  9.67497602e-02
  5.00951782e-02 -4.41571139e-02 -6.88479468e-02 -3.55141386e-02
 -2.00809557e-02  5.42383604e-02  6.06262274e-02  8.36017057e-02
  1.22118302e-01 -1.68125778e-02  3.55785266e-02 -5.16346022e-02
  7.49112517e-02  6.63313791e-02  2.70057153e-02  2.53331382e-02
 -2.00298261e-02  2.16775741e-02 -7.00513497e-02  5.45116812e-02
 -5.98565787e-02 -2.98589598e-02  1.61512513e-02  1.61704943e-02
  3.21803391e-02  1.03379432e-02 -1.96959991e-02  9.77637470e-02
  8.89649168e-02  2.74792686e-03  1.44103900e-01  3.25637907e-02
  6.41708681e-03 -1.35401383e-01  2.94934660e-02 -3.13397162e-02
 -3.71108926e-03 -2.84252204e-02 -4.73774113e-02  7.70276263e-02
 -2.03940533e-02 -2.56599933e-02 -1.63708860e-03  5.21417148e-03
 -4.34300490e-02 -2.95637641e-03 -2.79198568e-02 -1.70842316e-02
 -9.99225583e-03  4.74341772e-02 -2.37712488e-02 -3.63156833e-02
 -3.47399004e-02  6.42914027e-02 -7.35928025e-03  5.14163300e-02
  7.15668648e-02  1.68028772e-01  3.83546837e-02 -7.15993121e-02
  9.01470929e-02  1.57574881e-02 -5.66397496e-02  3.85932960e-02
  2.56659407e-02  7.08293095e-02  2.15742970e-03 -3.50904230e-08
  1.02961268e-02 -1.87842920e-02 -1.51232872e-02 -2.00991780e-02
  6.17006943e-02  1.25667984e-02  6.83370829e-02  4.45614308e-02
  6.66656299e-03  1.16794715e-02 -2.82829273e-02  9.73824318e-03
 -8.71961638e-02 -3.30348685e-02 -1.76129695e-02  2.17861179e-02
 -6.40276921e-05 -3.32049355e-02  5.52691668e-02 -3.87251936e-02
  1.13202818e-01  2.53110956e-02 -1.13582546e-02  7.99209327e-02
 -7.59017013e-04 -2.15578880e-02 -1.52069852e-02  5.78204580e-02
 -1.13946600e-02 -2.71067061e-02 -3.73227783e-02  4.86377105e-02
  3.17724273e-02 -8.65223855e-02  2.82335170e-02  7.16007575e-02
  2.99479775e-02  2.19965689e-02 -5.91420308e-02  5.07656299e-02
 -3.15157548e-02 -1.16629023e-02 -3.51459682e-02 -1.26498165e-02
  4.67772596e-02 -3.08467844e-03  1.92629360e-02 -5.61770499e-02
 -1.65186562e-02  6.43281173e-03  1.94078311e-02  3.14829424e-02
 -3.62355672e-02  2.85712779e-02  2.92372168e-03  2.79875267e-02
  8.27070884e-03 -8.87165740e-02 -1.35350833e-02  2.66034040e-03
  1.86401866e-02  1.12197779e-01 -4.84533655e-03 -4.85380413e-03]",2,2
ogb,1,the open graph benchmark ogb is a collection of benchmark datasets data loader and evaluator for graph machine learning datasets cover a variety of graph machine learning task and real world application the ogb data loader are fully compatible with popular graph deep learning framework including pytorch geometric and deep graph library dgl they provide automatic dataset downloading standardized dataset split and unified performance evaluation ogb aim to provide graph datasets that cover important graph machine learning task diverse dataset scale and rich domain graph ml task we cover three fundamental graph machine learning task prediction at the level of node link and graph diverse scale small scale graph datasets can be processed within a single gpu while medium and large scale graph might require multiple gpus or clever sampling partition technique rich domain graph datasets come from diverse domain ranging from scientific one to social information network and also include heterogeneous knowledge graph ogb is an on going effort and we are planning to increase our coverage in the future you can install ogb using python s package manager pip if you have previously installed ogb please make sure you update the version to the release note is available here the recommended way to install ogb is using python s package manager pip you can also install ogb from source this is recommended if you want to contribute to ogb we highlight two key feature of ogb namely easy to use data loader and standardized evaluator we prepare easy to use pytorch geometric and dgl data loader we handle dataset downloading a well a standardized dataset splitting below on pytorch geometric we see that a few line of code is sufficient to prepare and split the dataset needle to say you can enjoy the same convenience for dgl we also prepare standardized evaluator for easy evaluation and comparison of different method the evaluator take input dict a dictionary whose format is specified in evaluator expected input format a input and return a dictionary storing the performance metric appropriate for the given dataset the standardized evaluation protocol allows researcher to reliably compare their method if you use ogb or ogb lsc datasets in your work please cite our paper bibtex below,"[('open graph benchmark ogb', 0.7347), ('graph machine learning datasets', 0.5948), ('deep graph library dgl', 0.5943), ('unified performance evaluation ogb', 0.5786), ('important graph machine', 0.5635), ('heterogeneous knowledge graph ogb', 0.5625), ('graph datasets', 0.5564), ('small scale graph datasets', 0.5427), ('graph machine', 0.5282), ('ogb data loader', 0.5134)]","[-6.99236467e-02  8.71842820e-03  2.38938499e-02  3.80155188e-03
  5.01147099e-02 -8.66127610e-02 -9.12332460e-02  1.47802550e-02
 -1.03675514e-01  3.02841067e-02 -4.41076495e-02 -2.55710986e-02
 -2.73478515e-02 -1.41839460e-02  4.93954942e-02  5.81905320e-02
  8.97638127e-02  4.10293601e-02 -5.35725318e-02 -6.78851381e-02
 -3.14305536e-02 -3.91115434e-03  2.01991536e-02 -7.70703405e-02
  9.73628610e-02 -4.81278263e-02 -1.73682868e-02 -4.04952243e-02
  3.53623331e-02 -5.20472303e-02  4.26086336e-02 -3.37871388e-02
 -1.84413958e-02  7.36193825e-03 -3.75006050e-02  3.04030050e-02
 -1.15544535e-02 -4.23045009e-02  1.19850356e-02  3.13136764e-02
  3.78617458e-02  2.73573380e-02  2.51779258e-02  4.74949032e-02
  2.19952781e-02  5.27003519e-02 -1.98212378e-02  2.42979899e-02
  1.79143958e-02 -4.37694862e-02 -6.42276108e-02 -1.80797055e-01
 -5.43882772e-02  3.62360291e-02  9.38363094e-03 -3.44501510e-02
  2.49663945e-02 -4.75902110e-02  2.91089397e-02 -3.19560841e-02
  8.18491168e-03 -6.37630075e-02 -5.69868982e-02  4.73684259e-02
 -1.13908749e-03  3.93061452e-02  3.73545066e-02 -1.66945383e-02
 -2.35583056e-02 -7.66054839e-02  2.32216995e-02  1.20519958e-02
 -1.62723213e-02  5.94505966e-02  1.77969839e-02  3.90621796e-02
  7.98367523e-03  3.36506478e-02  7.25955665e-02 -7.93369412e-02
  2.23743673e-02 -1.56603195e-02 -2.98230704e-02  3.98096675e-03
  1.01142880e-02 -3.47326212e-02 -7.89937824e-02  5.15928715e-02
 -4.60873507e-02 -1.53306350e-02  1.51265953e-02  3.65538597e-02
 -1.39908567e-02  2.93167923e-02 -5.87493293e-02  9.14018881e-03
  3.19131799e-02  2.48426031e-02 -6.39977977e-02  7.54428357e-02
 -2.90725213e-02 -4.84041544e-03  6.48047924e-02  5.79463951e-02
 -4.97177280e-02  7.56281614e-02  3.55931073e-02  6.43175468e-02
  1.44973875e-03  1.17106340e-03  1.38574010e-02  5.99688776e-02
  2.49145459e-02 -6.00344203e-02  2.58391071e-02 -8.25112462e-02
 -5.94957061e-02 -4.46853340e-02  1.62486732e-02  1.32886898e-02
 -1.44590870e-01  4.09097644e-03  3.96431535e-02 -3.90863651e-03
 -4.59972546e-02  5.96430413e-02 -6.62905499e-02  5.88853830e-33
  8.31695274e-02  7.44325528e-03  1.06271710e-02 -8.63254890e-02
  1.23757990e-02  2.10232064e-02 -4.84138988e-02  1.25998575e-02
 -7.29491711e-02 -8.88751354e-03 -6.58686832e-02  2.69072615e-02
 -1.54966416e-04  1.29113927e-01  1.07805766e-02  1.32652996e-02
  2.77263373e-02  6.02745451e-03  2.03578006e-02 -1.58249717e-02
  1.14277728e-01 -2.04540994e-02  3.82429361e-02  1.02429636e-01
  2.23161243e-02  6.68332577e-02 -1.22459484e-02 -2.05533430e-02
  2.66839191e-02  2.57744566e-02 -1.67417619e-02  9.39016510e-03
  7.01160356e-03  5.71920387e-02 -2.85574663e-02 -1.66770592e-02
 -5.03131188e-02 -4.74422947e-02  3.05023696e-02 -1.00945771e-01
 -5.70241138e-02  2.94099972e-02 -3.95584814e-02 -1.35584120e-02
 -4.05068174e-02 -4.81071211e-02  5.94937382e-03 -7.47375842e-03
  6.68222457e-02 -5.33028916e-02 -3.44446711e-02 -5.85737117e-02
 -5.39766103e-02  8.57546553e-02  3.77473161e-02  5.55161200e-02
  1.18505865e-01  5.10864928e-02  2.43938416e-02  8.57278481e-02
 -2.51157917e-02  5.15785553e-02  3.04039884e-02 -3.24945748e-02
 -2.96641663e-02  3.91649036e-03 -2.85399407e-02  3.85181494e-02
  2.25545689e-02 -2.20605265e-02 -1.25038309e-03 -1.52637726e-02
  2.67208554e-02  3.83327156e-03  1.41475564e-02 -7.95404240e-03
 -1.54694170e-03 -1.11345150e-01 -4.00096774e-02  4.54192273e-02
 -5.60185909e-02 -3.72132882e-02  1.28369010e-03 -3.15851942e-02
 -4.03176621e-02 -5.94200045e-02  3.15226689e-02 -2.16654595e-02
 -2.28171498e-02  6.95011541e-02 -7.54818246e-02 -6.61259983e-04
 -3.84948067e-02  2.80221496e-02 -3.46480459e-02 -4.03428229e-33
 -1.34068364e-02  1.17217332e-01  1.58582032e-02  1.36319607e-01
  1.21469691e-01 -4.33227494e-02 -2.63896026e-02 -5.51861897e-02
 -6.13781950e-03  6.89662099e-02  2.40138955e-02 -5.78992516e-02
  9.02684480e-02  6.26405422e-03  4.07452248e-02 -5.06003052e-02
 -7.81125575e-02 -5.14684729e-02 -9.56980214e-02  3.39008979e-02
 -5.44788353e-02  4.52550240e-02 -7.62857497e-02  2.73278840e-02
  1.31897137e-01  1.59435067e-02 -7.49129280e-02 -3.23493816e-02
 -8.69165920e-03  1.31484242e-02  2.26772316e-02  2.43916325e-02
 -3.75924110e-02 -1.12269251e-02  4.75884899e-02  3.12237684e-02
  1.06211253e-01 -4.06746119e-02  1.09760892e-02 -3.33640017e-02
  3.96314450e-02  1.71582289e-02 -6.96317777e-02 -4.88082208e-02
 -3.36510725e-02  6.92098811e-02 -1.33903250e-01  7.41652101e-02
 -8.03597718e-02 -5.81318587e-02  1.03289271e-02  6.05334938e-02
  5.05530797e-02 -8.40937626e-03  3.54212262e-02 -5.47540672e-02
  4.08879593e-02  5.22067249e-02 -1.07946843e-02  1.41930711e-02
 -2.97848266e-02 -2.99186725e-02 -1.33140404e-02  6.04620390e-02
 -4.87432107e-02 -1.12068830e-02 -1.64838620e-02 -1.88244823e-02
 -3.25223170e-02  3.07251830e-02 -3.72694712e-03  3.07245888e-02
 -4.44483906e-02  2.54994747e-03 -1.33718709e-02  7.22783655e-02
 -2.70768236e-02  5.21779917e-02 -9.31371562e-03  1.53706642e-02
  5.62875457e-02  8.53116531e-03  3.25253308e-02  8.22191015e-02
  4.98665236e-02  8.03886577e-02  3.75238955e-02 -2.38016970e-03
  3.85100283e-02  5.11347726e-02 -8.56995508e-02 -3.85997468e-03
 -7.39344209e-02  1.05206504e-01  1.38618816e-02 -2.75660881e-08
 -1.34107068e-01  1.26037057e-02  7.34312013e-02 -2.09529418e-02
  4.19732369e-02  2.48336163e-03  6.73676431e-02  1.03922434e-01
 -4.35895100e-02  1.22563690e-01  5.11295348e-02  7.97566399e-03
 -4.93455529e-02 -8.97589512e-03  1.33876153e-03 -5.09225344e-03
  2.67165732e-02 -5.48673198e-02  4.74165194e-02 -2.41743564e-03
  2.98869964e-02  1.48162441e-02  2.04197224e-02  4.11482230e-02
  5.76142631e-02 -1.14208944e-01 -2.71523297e-02 -8.02288949e-03
 -4.34271581e-02  9.56863444e-03 -2.56883353e-02  1.12982839e-02
  1.14523463e-01 -1.12660602e-01  6.10727929e-02  1.13167778e-01
  1.21764308e-02  2.75715087e-02 -5.78693673e-03  7.17476755e-02
  1.47353550e-02  8.55927542e-02 -3.59832197e-02 -3.58337797e-02
 -7.32305050e-02 -1.02785043e-03  9.73416120e-03  5.01094311e-02
 -1.39011405e-02 -1.94101539e-02 -6.54532313e-02 -1.78576130e-02
 -1.11020342e-01 -2.91988514e-02  3.39399911e-02  1.97475143e-02
 -6.45655543e-02 -5.94220124e-02  2.68045049e-02  2.52692662e-02
  4.07646485e-02 -8.33314955e-02 -5.11442646e-02 -9.99675039e-03]",2,0
tensorly,1,tensorly is a python library that aim at making tensor learning simple and accessible it allows to easily perform tensor decomposition tensor learning and tensor algebra it backend system allows to seamlessly perform computation with numpy pytorch jax mxnet tensorflow or cupy and run method at scale on cpu or gpu website http tensorly orgsource code http github com tensorly tensorlyjupyter notebook http github com jeankossaifi tensorly notebooksthe only pre requisite is to have python installed the easiest way is via the anaconda distribution with pip recommended with condadevelopment from git note tensorly depends on numpy by default if you want to use the mxnet or pytorch backends you will need to install these package separately for detailed instruction please see the documentation create a small third order tensor of size x x from a numpy array and perform simple operation on it you can also create random tensor you can also create tensor in tt format tucker etc see random tensor you can change the backend to perform computation with a different framework by default the backend is numpy but you can also perform the computation using pytorch tensorflow mxnet jax or cupy requires to have installed them first for instance after setting the backend to pytorch all the computation is done by pytorch and tensor can be created on gpu applying tensor decomposition is easy we have many more decomposition available be sure to check them out this is just a very quick introduction to some of the basic feature of tensorly for more information on getting started checkout the user guide and for a detailed reference of the function and their documentation refer to the apiif you see a bug open an issue or better yet a pull request testing and documentation are an essential part of this package and all function come with uni test and documentation the test are ran using the pytest package though you can also use nose first install pytest then to run the test simply run in the terminal alternatively you can specify for which backend you wish to run the test if you use tensorly in an academic paper please cite jean kossaifi yannis panagakis anima anandkumar and maja pantic tensorly tensor learning in python journal of machine learning research jmlr volume number,"[('numpy pytorch jax mxnet tensorflow', 0.5914), ('tensorlyjupyter notebook http github com jeankossaifi', 0.59), ('random tensor', 0.5828), ('tensor decomposition tensor learning', 0.5606), ('tensor learning', 0.5556), ('tensor', 0.5503), ('tensor decomposition', 0.5424), ('pytorch tensorflow mxnet jax', 0.5413), ('tensor algebra', 0.513), ('pytorch backends', 0.4208)]","[-1.21736594e-01 -9.81308222e-02 -5.90579305e-03 -5.87140955e-03
  9.03257504e-02 -3.05379624e-03 -2.55490523e-02 -3.19729261e-02
 -1.06167033e-01 -9.01953876e-02  1.25273261e-02  5.70435151e-02
 -3.21739614e-02  2.74496712e-03 -2.33229045e-02  5.43586351e-02
 -3.39770094e-02  1.08577982e-01 -1.78261176e-02 -7.52420276e-02
 -3.56540568e-02  1.26841133e-02  5.59074245e-02 -3.52091454e-02
  6.87771663e-02 -4.76814210e-02  3.10139917e-02 -5.31477407e-02
  1.12573034e-03  1.65175367e-02 -1.78757161e-02  6.93378225e-02
  2.84879375e-02 -5.28643578e-02  1.40863573e-02  2.47978568e-02
 -5.37043922e-02 -2.46949233e-02 -3.99342068e-02  1.95391756e-02
 -5.33525273e-03  3.45069617e-02 -1.45626133e-02 -2.44325511e-02
  5.08442707e-03 -2.51837187e-02  7.94997960e-02 -1.04437917e-01
  3.31031606e-02  1.59234721e-02 -7.09788203e-02 -7.19298497e-02
 -6.40671477e-02  2.57405415e-02 -4.84737270e-02 -4.96530160e-03
 -1.24271042e-04  4.24564211e-03 -8.84501562e-02 -8.17066729e-02
  2.19010208e-02  2.55162222e-03 -5.71035780e-02  3.53574147e-03
  1.10129369e-02  4.76722755e-02 -9.21931677e-03  5.22767156e-02
  8.70073438e-02 -1.19707420e-01 -5.13675325e-02 -1.88264903e-02
 -9.09394026e-02  7.95068219e-02 -1.13345692e-02 -1.86688267e-02
  1.01677023e-01 -2.22964343e-02  4.16541882e-02 -6.10647909e-02
 -7.63436481e-02 -4.64578085e-02  8.40312466e-02 -2.14023143e-02
  9.08396579e-03 -7.10884947e-03 -1.11741303e-02  1.02301687e-01
  5.34028411e-02  1.33369695e-02  5.40657304e-02 -2.27251519e-02
  5.54468716e-03  4.49666642e-02  1.06717898e-02  4.30357382e-02
  5.96812367e-02 -2.33457442e-02 -7.11173117e-02  5.69906645e-02
  2.47136708e-02 -7.51103237e-02  6.04419187e-02  1.71974823e-02
  3.03352885e-02  6.19066916e-02  4.09591235e-02  1.58634298e-02
 -6.81822421e-03 -4.55374038e-03  1.02113066e-02  3.15827690e-02
 -8.96451529e-03 -4.08130288e-02  8.32475871e-02  3.01697087e-02
  4.86969091e-02  4.51425202e-02  8.55856240e-02  7.78977796e-02
  6.79871347e-03  1.85353346e-02 -1.57132186e-02  3.44554968e-02
 -1.82731282e-02  6.00048192e-02 -6.88902587e-02  6.34339804e-33
  1.79470179e-03  5.93935931e-03  2.78412905e-02 -7.65037583e-03
  7.63527900e-02 -8.28670189e-02  1.02200001e-01 -3.63601483e-02
  2.10024975e-02 -2.62694750e-02 -8.02246109e-02  5.90223745e-02
  2.67233187e-03  3.90768796e-02 -6.12552278e-02 -1.46112517e-01
 -1.47361225e-02  8.02252162e-03  7.43166804e-02  2.47175284e-02
  4.17687483e-02  6.09623492e-02 -4.12260890e-02  1.08159566e-02
 -5.65123893e-02  3.32340486e-02  3.55241038e-02 -4.93471697e-02
 -1.35527896e-02  4.29893471e-02 -8.26650336e-02 -7.69946277e-02
 -3.95628996e-02  1.68760307e-02  1.09175313e-03 -1.19412011e-02
 -7.42374733e-02 -4.83258553e-02 -1.52359810e-02 -1.35556594e-01
 -7.48386532e-02  1.11538274e-02 -3.87981385e-02 -3.27908457e-03
 -3.66730355e-02 -4.20059673e-02 -5.52256452e-03  8.63689259e-02
  8.73769075e-02 -5.55830561e-02 -7.61558414e-02 -4.80519719e-02
 -1.16845807e-02  1.27059016e-02 -2.77984014e-04 -5.20290621e-03
  6.01982698e-02  7.25455955e-02  4.17200699e-02  1.01042949e-02
 -3.09828157e-03  2.61299331e-02  7.08113378e-03 -4.15139599e-03
 -2.15763021e-02  1.04932431e-02 -1.11291990e-01 -3.81136835e-02
 -4.32167575e-03  2.99007911e-02 -6.82089701e-02  9.60887745e-02
  3.67029123e-02 -2.33137365e-02 -1.51994769e-02 -3.58999148e-02
  5.36012910e-02 -8.60021040e-02 -2.10197344e-02 -2.78219022e-02
 -5.57664819e-02  5.07931896e-02  8.97135120e-03 -8.32600594e-02
 -6.21950068e-02  4.25640307e-02  9.69071127e-03 -1.90481804e-02
  3.34389918e-02 -5.01009002e-02 -6.29895180e-03 -2.75434721e-02
  1.09968722e-01  2.08318643e-02 -4.65431921e-02 -4.79921760e-33
 -9.89875495e-02  1.16884178e-02 -8.38639885e-02  7.32631683e-02
  1.34586236e-02 -2.46201195e-02 -4.04048897e-02  2.84041762e-02
  5.88620678e-02  8.34214222e-03 -4.52435352e-02 -2.52140649e-02
 -1.28209230e-03 -3.42782736e-02  6.45178109e-02  2.80187675e-03
  4.76657860e-02 -1.05726514e-02  2.02299152e-02 -3.24677490e-02
 -1.14296913e-01  6.93592057e-02 -2.37486344e-02 -4.88245254e-03
 -5.63988276e-02 -2.32605431e-02 -3.97464335e-02  1.40808588e-02
 -1.18042463e-02 -2.02208553e-02 -2.58723162e-02 -1.47438506e-02
 -1.93183850e-02  8.04904774e-02 -2.68679149e-02  2.62673367e-02
  4.70802933e-02  4.63005193e-02 -6.13458408e-03  1.12787848e-02
  9.58192199e-02  2.26867776e-02 -2.19780784e-02  5.64223751e-02
 -1.97977591e-02 -1.34587055e-03 -1.22560598e-01  2.68333089e-02
  3.63550568e-03 -1.80946325e-03 -3.07944790e-02 -2.64196526e-02
 -1.04359873e-02 -4.31031287e-02 -5.91403432e-02  2.25898270e-02
  8.41558799e-02  3.14251184e-02  6.41886294e-02  5.49892709e-02
 -5.55925183e-02 -6.78231642e-02 -2.33938936e-02  5.40564954e-02
  2.31484361e-02 -2.20021829e-02 -9.75905508e-02  9.24007297e-02
 -5.61729223e-02 -2.00942773e-02 -1.64541546e-02  3.77984531e-02
  2.72317436e-02  5.24818152e-02  1.65542923e-02 -8.68476648e-03
  6.42197579e-02  2.40128152e-02 -3.30800340e-02 -2.54499521e-02
  6.45235181e-02  1.27125615e-02 -6.24081166e-03  6.77456558e-02
  5.05887531e-02  1.93615314e-02  6.60148263e-02  8.42547715e-02
  7.13399053e-02 -4.30481620e-02  3.18780504e-02 -1.34826638e-03
  1.21949175e-02  7.08634853e-02  1.58381406e-02 -2.73652390e-08
 -3.05109825e-02  2.84268707e-02  2.72868294e-02  7.86147825e-03
 -3.71131673e-03  7.03856051e-02  7.80174732e-02  9.30997357e-02
  1.20758535e-02  1.12110879e-02  1.54434033e-02 -4.57686633e-02
 -6.51291534e-02 -2.24949345e-02 -6.63919421e-03  1.17045656e-01
 -4.47895229e-02  8.45934600e-02  5.63925738e-03 -3.80392075e-02
  7.68647790e-02  1.09360814e-02  9.67377215e-04  4.65941988e-02
 -1.84428096e-02 -4.77761356e-03 -7.83947017e-03  6.34851083e-02
  2.22038589e-02 -7.27565289e-02 -1.19391710e-01  1.56293269e-02
  8.30124393e-02 -5.84971420e-02  2.77257394e-02  1.04489073e-01
  5.81253171e-02 -4.43407036e-02 -8.16444755e-02  9.36718732e-02
 -1.07230611e-01  1.64746623e-02  1.48227643e-02 -4.11832109e-02
  9.35548469e-02 -1.03008329e-04 -8.27430002e-03 -8.19992647e-02
  5.60544729e-02  2.70445365e-03  2.86700036e-02 -2.36962754e-02
  1.62898991e-02 -1.67774353e-02  6.18741959e-02  4.14872058e-02
 -8.22675526e-02 -1.33149885e-02  1.68947764e-02 -6.27496559e-03
 -6.80882949e-03  1.17071100e-01  4.34296299e-03 -2.95664500e-02]",2,2
larq,1,larq is an open source deep learning library for training neural network with extremely low precision weight and activation such a binarized neural network bnns existing deep neural network use bit bit or bit to encode each weight and activation making them large slow and power hungry this prohibits many application in resource constrained environment larq is the first step towards solving this it is designed to provide an easy to use composable way to train bnns bit and other type of quantized neural network qnns and is based on the tf kera interface note that efficient inference using a trained bnn requires the use of an optimized inference engine we provide these for several platform in larq compute engine larq is part of a family of library for bnn development you can also check out larq zoo for pretrained model and larq compute engine for deployment on mobile and edge device to build a qnn larq introduces the concept of quantized layer and quantizers a quantizer defines the way of transforming a full precision input to a quantized output and the pseudo gradient method used for the backwards pas each quantized layer requires an input quantizer and a kernel quantizer that describe the way of quantizing the incoming activation and weight of the layer respectively if both input quantizer and kernel quantizer are none the layer is equivalent to a full precision layer you can define a simple binarized fully connected kera model using the straight through estimator the following way this layer can be used inside a kera model or with a custom training loop check out our example on how to train a binarized neural network in just a few line of code before installing larq please install you can install larq with python s pip package manager larq is being developed by a team of deep learning researcher and engineer at plumerai to help accelerate both our own research and the general adoption of binarized neural network,"[('deep neural network use bit bit', 0.5645), ('neural network qnns', 0.5499), ('deep learning library', 0.5319), ('neural network bnns', 0.5278), ('larq compute engine', 0.4637), ('quantizer', 0.4599), ('kernel quantizer', 0.4503), ('input quantizer', 0.4454), ('larq compute engine larq', 0.4308), ('neural network', 0.4275)]","[-5.81298061e-02 -7.69812018e-02  2.01975629e-02 -6.91945553e-02
 -5.37715442e-02  3.43191288e-02 -1.32019343e-02 -3.72453369e-02
  1.04295174e-02 -4.39689644e-02 -5.88413812e-02 -4.83318269e-02
 -6.55201301e-02 -3.62364249e-03 -1.97800044e-02  3.47108282e-02
 -8.26054160e-03  6.06597476e-02 -5.59035502e-02 -9.13278162e-02
  3.41223180e-02  3.40585932e-02  1.08119997e-03  4.61215200e-03
  5.97535353e-03  3.67165543e-02  3.21029834e-02 -3.24146114e-02
  6.01889826e-02 -2.04636678e-02 -2.66465861e-02 -1.90695340e-03
  4.04864177e-02  1.18746525e-02 -6.81965500e-02  1.21637527e-02
 -1.50849055e-02 -5.51626086e-02  3.03934049e-02 -4.47118245e-02
 -7.78223872e-02 -5.82489185e-02 -2.52723377e-02 -1.37977465e-03
  1.30959541e-01  5.44434749e-02  1.37074916e-02 -1.17446259e-02
  1.76741146e-02 -1.41567979e-02 -2.05463506e-02  1.46847591e-02
  2.64079645e-02  7.04661608e-02 -8.88654776e-03 -5.33352084e-02
 -3.81491557e-02  7.07736984e-02 -4.14894260e-02  6.30736500e-02
 -3.91485766e-02  1.89216193e-02  3.12463753e-02  5.49265475e-04
 -1.91037338e-02  5.62250055e-02 -3.55506642e-03  4.89074029e-02
  7.06532970e-02 -5.68889678e-02  3.00032180e-02 -2.66592819e-02
 -7.17511252e-02  2.57355403e-02 -2.13166717e-02  2.81537771e-02
  6.15287498e-02  1.02760934e-03  6.50866702e-02 -1.18918121e-01
  3.28237340e-02 -2.59736343e-03  1.02194063e-02  3.91863957e-02
  6.66935090e-03 -8.46505817e-03 -8.42331126e-02  4.78579365e-02
 -4.85869721e-02 -5.66598959e-02 -1.97524615e-02 -7.80641362e-02
  8.00739974e-03 -5.99915162e-02  9.12493393e-02 -4.95089144e-02
 -4.92647924e-02  2.65690982e-02  5.75486757e-03 -8.79562553e-03
 -4.36608680e-02 -6.36638552e-02  1.28768869e-02  3.52033526e-02
 -1.93596985e-02  6.19096383e-02  3.26965973e-02  1.90313403e-02
  1.40059799e-01 -2.44994438e-03 -6.00052625e-03  6.00298010e-02
 -4.37798649e-02 -1.19699724e-01  5.91722913e-02 -4.07765135e-02
 -6.80007413e-02  4.91345190e-02 -1.37464367e-02  4.09479849e-02
 -1.77231431e-01  1.64093208e-02 -8.38158727e-02  5.24934335e-03
 -1.30053898e-02 -3.80248600e-03 -1.15137607e-01  7.09350376e-33
 -3.49913142e-04 -1.11170858e-02  1.73981895e-03 -4.29125428e-02
  3.74277309e-02 -4.48400676e-02  4.50338684e-02  5.60853295e-02
  1.36742936e-02 -3.52225453e-02 -4.76602186e-03  5.60145378e-02
 -7.18523711e-02  4.46948968e-02  4.90307547e-02 -1.45035051e-03
  1.81829575e-02 -8.46232399e-02  2.17830781e-02 -2.09174231e-02
  1.71632562e-02 -2.53953282e-02  5.40218409e-03  1.41742721e-01
 -1.55266039e-02  1.11598121e-02  4.21705954e-02  1.41525250e-02
 -5.44536263e-02 -2.27902550e-02 -3.43625769e-02  6.34496240e-03
  1.43633112e-02 -6.47905692e-02  2.93577928e-02 -6.74882391e-03
  1.42848128e-02  4.81794812e-02 -1.08842235e-02 -1.19373426e-02
  1.07068652e-02 -1.61891710e-02 -6.20809309e-02 -2.91004740e-02
 -3.74150053e-02 -6.00632317e-02 -2.49498081e-03  1.11150201e-02
  3.70130651e-02 -2.10166778e-02  2.38301884e-02  1.60579663e-02
 -6.98129311e-02 -4.10968103e-02  1.22409888e-01 -2.89770085e-02
  7.05178753e-02  7.18294978e-02 -7.51509611e-03  9.91497934e-02
  1.27361575e-02 -1.93123352e-02  3.43797691e-02 -4.39962232e-03
  1.47784862e-03 -1.50087494e-02  3.80598530e-02  1.16968490e-02
 -9.54892784e-02  3.51575576e-02 -6.37814477e-02  2.50822455e-02
  7.76604787e-02  9.56634642e-04 -2.75140703e-02  1.32266954e-02
 -6.66761994e-02 -7.05967247e-02 -4.15065279e-03  8.41205344e-02
 -1.37282563e-02  3.08410898e-02  1.44892959e-02  3.13892923e-02
 -1.07735097e-02 -1.96689856e-03  3.69223878e-02 -6.65005073e-02
  3.57627459e-02  2.74740662e-02 -9.45998058e-02 -4.86725159e-02
  3.20574455e-02  3.35383415e-02 -6.25736341e-02 -7.66985232e-33
 -5.05034514e-02  1.21787757e-01 -1.08688071e-01  9.59870517e-02
 -1.59419735e-03  2.67759897e-03  4.78712702e-03 -5.86479716e-02
  7.60639235e-02  2.31476873e-03  2.76313890e-02  1.22929700e-02
  1.03284374e-01  1.72657855e-02  4.58118366e-03 -6.65951893e-02
 -8.68305266e-02 -8.39157850e-02 -3.94706167e-02  6.14399686e-02
 -3.47124003e-02  3.10253575e-02 -3.90248280e-03 -1.57336108e-02
  1.03446720e-02 -3.95636559e-02 -4.12975475e-02  3.37622352e-02
  6.25674129e-02  2.14524455e-02 -3.28766555e-02 -5.46347164e-02
  5.11514675e-03 -5.58056161e-02  5.69734536e-02 -2.82496586e-02
  1.06223993e-01 -2.92311609e-02  6.46200776e-03 -7.38043860e-02
  5.64310811e-02 -2.06454210e-02  4.72069271e-02  3.65905203e-02
 -1.69122647e-02 -2.70434935e-02 -7.84768835e-02  2.66799647e-02
 -5.41419834e-02 -2.86668297e-02  9.74971578e-02  1.45603286e-03
 -3.24922539e-02  1.05623305e-02 -1.53064635e-02 -3.44976992e-03
 -1.83414109e-02  2.74842530e-02  8.27309862e-02  4.71462831e-02
 -7.26837069e-02 -7.91561604e-02  4.06260183e-03  1.10610705e-02
 -9.54784974e-02  2.47663818e-03 -6.40535876e-02  3.88423055e-02
  4.37956955e-03 -6.69084713e-02  5.62095344e-02  6.57189339e-02
  8.17108676e-02  1.03820138e-01 -1.02617785e-01 -6.59492612e-02
 -6.10774122e-02 -1.07380683e-02  6.23663105e-02  1.09457793e-02
  7.94493556e-02  3.17515843e-02 -6.84476271e-02  3.14219519e-02
  6.45117536e-02  1.40525475e-01  1.30071547e-02 -3.61342058e-02
 -1.61832217e-02 -5.32856211e-03 -1.23416074e-02  3.52782607e-02
  9.83404368e-03  7.53573552e-02 -9.87908840e-02 -3.29992105e-08
 -4.10219934e-03  3.63849774e-02  7.04793558e-02 -6.26867339e-02
  1.20525301e-01 -1.74958482e-02  1.85923520e-02  1.23046815e-01
 -4.33236584e-02  5.20271584e-02  4.28045215e-03  2.93274503e-02
 -1.02326624e-01 -5.77176400e-02  4.70881835e-02  2.85008661e-02
  4.02802303e-02 -5.75586073e-02  8.97009969e-02 -5.47403507e-02
  8.17215219e-02  2.91634053e-02 -1.59542486e-02  3.25905681e-02
  4.95865755e-02 -6.58806786e-02 -1.66978966e-02 -3.99413966e-02
  5.69600239e-03  4.86770198e-02 -3.68774533e-02 -2.99935173e-02
  9.50708613e-02 -7.81525597e-02  9.65954512e-02  7.05413967e-02
 -1.23381922e-02 -2.08816454e-02 -7.02285348e-03  3.21890675e-02
 -2.81238910e-02  1.61104016e-02 -2.80546807e-02 -2.05491055e-02
 -3.94620262e-02 -4.32554409e-02 -1.44423507e-02  1.21111190e-02
  1.30320266e-02 -3.34435441e-02  4.61271126e-03  4.40905290e-03
 -1.85707938e-02  7.70389894e-03 -3.64531428e-02  4.48684543e-02
  2.69506127e-02 -1.23926222e-01 -2.07088776e-02  4.91826981e-02
 -6.83370158e-02  7.42170066e-02  4.07408364e-02 -2.95554753e-03]",2,2
face_recognition,1,find all the face that appear in a picture get the location and outline of each person s eye nose mouth and chin recognize who appears in each photo you can even use this library with other python library to do real time face recognition see this example for the code python or python macos or linux window not officially supported but might work first make sure you have dlib already installed with python binding how to install dlib from source on macos or ubuntuthen install this module from pypi using pip or pip for python raspberry pi installation instructionswhile window isn t officially supported helpful user have posted instruction on how to install this library masoudr s window installation guide dlib face recognition download the pre configured vm image for vmware player or virtualbox next you need a second folder with the file you want to identify if you are using python or newer pas in a cpu number of cpu core to use parameter you can also pas in cpu to use all cpu core in your system api doc http face recognition readthedocs io you can also opt in to a somewhat more accurate deep learning based face detection model all the example are available here find face in a photographfind face in a photograph using deep learning find face in batch of image w gpu using deep learning identify specific facial feature in a photographapply horribly ugly digital make upfind and recognize unknown face in a photograph based on photograph of known peoplecompare face by numeric face distance instead of only true false matchesrecognize face in live video using your webcam simple slower version requires opencv to be installed recognize face in live video using your webcam faster version requires opencv to be installed recognize face in a video file and write out new video file requires opencv to be installed recognize face on a raspberry pi w camerarun a web service to recognize face via http requires flask to be installed recognize face with a k nearest neighbor classifierhow face recognition worksthe face recognition model is trained on adult and doe not work very well on child it tends to mix up child quite easy using the default comparison threshold of issue illegal instruction core dumped when using face recognition or running example issue runtimeerror unsupported image type must be bit gray or rgb image when running the webcam example solution your webcam probably isn t set up correctly with opencv look here for more issue memoryerror when running pip install face recognitionissue attributeerror module object ha no attribute face recognition model v solution the version of dlib you have installed is too old you need version or newer upgrade dlib issue attribute error module object ha no attribute cnn face detection model v solution the version of dlib you have installed is too old you need version or newer upgrade dlib issue typeerror imread got an unexpected keyword argument mode solution the version of scipy you have installed is too old you need version or newer upgrade scipy many many thanks to davis king nulhom for creating dlib and for providing the trained facial feature detection and face encoding model used in this library for more information on the resnet that power the face encoding check out his blog post thanks to everyone who work on all the awesome python data science library like numpy scipy scikit image pillow etc etc that make this kind of stuff so easy and fun in python thanks to cookiecutter and the audreyr cookiecutter pypackage project template for making python project packaging way more tolerable you can now pas model small to face landmark to use the point face model instead of the point model now officially supporting python new example of using this library in a jupyter notebookadded the face detection cli commandremoved dependency on scipy to make installation easiercleaned up knn example and fixed a bug with drawing font to label detected face in the demofixed version numbering inside of module code fixed a bug where batch size parameter didn t work correctly when doing batch face detection on gpu updated opencv example to do proper bgr rgb conversionupdated webcam example to avoid common mistake and reduce support questionsadded a knn classification exampleadded an example of automatically blurring face in image or videosupdated dockerfile example to use dlib v which remove the boost dependency will use dlib s point face pose estimator when possible for speed instead of point face pose esimator dlib v is now the minimum required versionface recognition model v is now the minimum required versionadded support for dlib s cnn face detection model via model cnn parameter on face detecion calladded support for gpu batched face detection using dlib s cnn face detector modeladded find face in picture cnn py to examplesadded find face in batch py to examplesadded face rec from video file py to examplesdlib v is now the minimum required versionface recognition model v is now the minimum required versionadded show distance to clifixed a bug where tolerance wa ignored in cli if testing a single imageadded benchmark py to examplesadded tolerance to clithe cli can now take advantage of multiple cpu just pas in the cpu x parameter where x is the number of cpu to use added face distance py exampleimproved cli test to actually test the cli functionalityupdated facerec on raspberry pi py to capture in rgb not bgr format fixed a valueerror crash when using the cli on python raspberry pi support fixed face landmark wasn t returning all chin point fixed a minor bug in the command line interface minor pref improvement with face comparison test update fix minimum scipy version required fix missing pillow dependency first working release,"[('batch face detection', 0.6278), ('face detection cli', 0.6234), ('face detection', 0.6037), ('facial feature detection', 0.6017), ('face detection model', 0.5817), ('real time face recognition', 0.5778), ('face recognition', 0.5688), ('cnn face detector', 0.5618), ('cnn face detection model', 0.5585), ('face recognition model', 0.5484)]","[-6.05011620e-02 -7.76910409e-02 -3.84004391e-03 -6.57123253e-02
  6.32056966e-02  5.83856972e-03 -2.94449143e-02  1.36511130e-02
 -9.24447179e-02 -7.35894144e-02  5.88263012e-02 -1.30766407e-01
  6.08903952e-02  3.58427539e-02  2.03766171e-02 -1.01445287e-01
  9.24959872e-03  5.29468879e-02 -5.47813103e-02  1.94746219e-02
  8.34561605e-03 -9.69123654e-03  3.98166068e-02 -1.33665381e-02
 -7.09367022e-02 -1.18890582e-02  2.94369347e-02 -1.03334270e-01
 -1.80117541e-03 -5.53388707e-02 -6.82302266e-02 -7.08491029e-03
  1.10432923e-01  3.66714299e-02 -1.08391931e-02 -4.60709222e-02
  5.46319298e-02  3.05032590e-03  1.10563999e-02 -2.45658960e-02
 -6.38932213e-02 -5.24058193e-02 -1.64670739e-02 -3.18573266e-02
  1.42221510e-01  2.43671872e-02  2.06994899e-02 -3.16791758e-02
  5.96852452e-02 -5.74565940e-02 -2.91581862e-02 -6.55166656e-02
 -1.52991712e-02  5.99244349e-02 -5.16368411e-02 -2.29021236e-02
 -1.31130600e-02 -2.19186139e-03  5.35953566e-02  7.63374567e-02
 -1.71433017e-02 -3.98985408e-02 -5.89265348e-03  4.26838025e-02
 -2.01758202e-02 -2.97774300e-02 -1.77997991e-03 -4.45001461e-02
  1.32796213e-01 -3.56502719e-02 -2.60417126e-02  7.44808316e-02
 -3.66381705e-02 -8.83243885e-03 -2.11461112e-02 -2.28963457e-02
  9.71732289e-02  7.62664899e-03  6.70510381e-02 -6.63254634e-02
 -5.45472838e-02  4.72466322e-03  9.89510491e-02 -8.23123604e-02
  2.39284486e-02 -2.26827618e-02 -1.06445432e-01  1.10062733e-02
 -1.73793845e-02  2.90151462e-02 -1.57288294e-02 -5.69567904e-02
 -4.14884984e-02 -6.45378754e-02 -1.72608159e-02 -2.42595724e-03
 -1.01874731e-02  1.60902534e-02  3.72510329e-02 -1.07051414e-02
 -1.00310721e-01 -1.03929497e-01  1.56249804e-02  3.77268679e-02
  2.43090745e-02  1.55686224e-02  1.84890982e-02 -1.48436734e-02
  5.80072179e-02  6.05032220e-03  2.32117213e-02  2.68183313e-02
  6.39338419e-03 -1.41799673e-01  4.35910448e-02  3.46239656e-02
 -5.77398315e-02 -4.05812263e-02  4.44342047e-02  1.45691657e-03
 -5.90138771e-02 -2.11640541e-02  1.50876241e-02 -9.78733897e-02
 -1.80396717e-02 -3.84436920e-02 -7.01620951e-02  3.50170179e-33
 -2.10223366e-02  2.10369546e-02 -2.25423314e-02  3.66058876e-03
  1.46846799e-02 -2.63959486e-02 -1.94462982e-03  3.95494476e-02
  1.15382932e-02 -3.64200445e-03 -5.37984893e-02  6.99700117e-02
 -6.32999912e-02  8.49344656e-02  3.86502221e-02  2.99648792e-02
  2.08827946e-02  2.85043344e-02 -9.58884321e-03  6.20433362e-03
  1.02565279e-02 -6.74905032e-02  4.39387448e-02  4.88368198e-02
  1.58398980e-04  3.15380991e-02  5.28132319e-02 -9.12141986e-03
  1.27267034e-03 -3.83635587e-03  3.44025083e-02  1.14797547e-01
 -2.62008421e-02 -2.11003404e-02  5.31215295e-02 -3.40960757e-03
 -8.16785078e-03  2.75653694e-02 -2.31721438e-02 -1.72928423e-02
 -2.95526069e-02  6.16719350e-02 -1.04299352e-01 -3.58044468e-02
  3.39891165e-02  2.41669845e-02  1.66346580e-02  5.01721203e-02
  4.23263264e-04  5.41558899e-02  3.54872979e-02  4.43596914e-02
 -6.17390238e-02 -2.03288421e-02 -1.91532411e-02  1.03226915e-01
 -1.20739145e-02  4.00737487e-03  2.16893032e-02  8.19436274e-03
 -5.15766516e-02 -2.20518224e-02  7.84182549e-02  4.35395576e-02
 -4.55829464e-02 -9.14744064e-02  8.29607307e-04 -3.25472131e-02
 -4.27559763e-02  9.21107531e-02 -1.39910532e-02  5.18554375e-02
 -4.19430658e-02 -3.66448909e-02  5.00123799e-02 -1.92819927e-02
  5.78079298e-02 -1.73184369e-02 -7.66915604e-02  1.50060713e-01
 -1.08269521e-03  4.91179638e-02 -9.48749855e-03 -5.36624081e-02
  1.99025534e-02  1.08450688e-02 -1.96136278e-03 -3.04381773e-02
 -9.00232233e-03  2.91828974e-03 -3.49217132e-02 -2.30204314e-02
  5.25913723e-02  5.40519767e-02 -5.93523458e-02 -3.49629783e-33
  6.67795837e-02  7.58038089e-02 -3.56342979e-02 -4.17772420e-02
  1.10644498e-03 -1.64319742e-02 -5.10529662e-03 -9.41234920e-03
  3.36723328e-02  2.10648798e-03 -7.01911515e-03  3.51194777e-02
 -4.99760769e-02  1.80080663e-02 -1.28670754e-02 -1.30499685e-02
 -4.13591303e-02 -1.05368517e-01 -7.46344477e-02  6.53733388e-02
 -5.20896800e-02  7.40650818e-02 -1.22891575e-01 -1.23474216e-02
 -2.55279597e-02  3.68796550e-02 -3.95489559e-02  5.75693436e-02
  9.03669074e-02 -8.76531377e-03 -2.71196775e-02  7.77157396e-03
 -4.40007355e-03  1.01757906e-01 -1.93442330e-02 -2.39933766e-02
  5.54407090e-02 -7.95466080e-03  4.11777347e-02  6.16997294e-02
  1.15516864e-01  3.03765908e-02 -1.77107807e-02 -2.30812817e-04
 -4.33519529e-03 -1.51349399e-02 -2.57553700e-02  1.26052964e-02
 -1.27861321e-01 -6.42371504e-03 -4.56324555e-02 -1.52367475e-02
 -1.15507767e-01  3.32428031e-02 -4.64847945e-02  9.35794692e-03
  3.66042294e-02  3.45043503e-02  3.45080206e-03  6.27365429e-03
 -1.13857090e-02 -7.61159584e-02 -2.65419483e-02  3.13303769e-02
 -4.91643324e-02  1.66390892e-02 -4.40930240e-02 -6.29277388e-03
 -1.54982153e-02 -3.36102843e-02  3.23747806e-02  4.91740145e-02
  8.07224060e-05  7.01036006e-02 -6.68034852e-02 -1.76230054e-02
 -5.97504228e-02  4.84188311e-02  3.91044840e-02  5.06031606e-03
  2.41396558e-02 -7.82910436e-02  4.36189910e-03  1.19268045e-01
  4.40076999e-02  9.48047563e-02  1.76452212e-02 -8.51074755e-02
  7.08829761e-02 -7.23131448e-02 -9.71686468e-02  3.76101211e-02
  9.16069821e-02  6.88911229e-02 -4.42128116e-03 -2.41628406e-08
 -2.57597342e-02 -2.97765490e-02  3.04806675e-03 -2.33364236e-02
 -3.32219265e-02  4.19559069e-02 -6.75533414e-02  1.15061834e-01
 -3.01480014e-03  2.60432884e-02 -6.53573200e-02  5.66522870e-03
 -8.23640376e-02 -1.49536431e-02  4.04516272e-02  2.13130768e-02
 -5.60777634e-02 -2.22804192e-02  5.38094155e-02 -5.64951450e-02
  1.23721473e-02 -2.81372317e-03  2.04202589e-02  1.18517406e-01
  5.62412627e-02 -1.61291156e-02 -5.67392111e-02  6.31367788e-02
 -8.19453746e-02  8.97063501e-03 -2.98804510e-03  1.99601334e-02
  7.67967105e-02 -1.81453992e-02  1.45765886e-01  7.29342252e-02
  4.89663184e-02 -8.58766586e-02  4.33366485e-02  5.68243004e-02
  4.41969857e-02 -1.59020014e-02 -2.23673414e-02 -2.71809120e-02
 -2.14761728e-03  3.42289209e-02  7.73363337e-02 -5.60986027e-02
  1.44991782e-02  8.43204744e-03 -3.71273756e-02  3.67383212e-02
 -3.65929715e-02  7.02774525e-02 -2.45097280e-02  8.05801079e-02
  7.02244192e-02 -8.43832642e-02  4.17817309e-02  9.73711759e-02
 -2.78633405e-02  4.36862446e-02  1.44831243e-03 -5.17973937e-02]",2,2
nonechucks,1,nonechucks is a library that provides wrapper for pytorch s datasets sampler and transforms to allow for dropping unwanted or invalid sample dynamically what if you have a dataset of s of image out of which a few dozen image are unreadable because the image file are corrupted or what if your dataset is a folder full of scanned pdfs that you have to ocrize and then run a language detector on the resulting text because you want only the one that are in english or maybe you have an alternateindexsampler and you want to be able to move to dataset after dataset fails while attempting to load pytorch s data processing module expects you to rid your dataset of any unwanted or invalid sample before you feed them into it pipeline and provides no easy way to define a fallback policy in case such sample are encountered during dataset iteration you might be wondering why this is such a big deal when you could simply filter out sample before sending it to your pytorch dataset or sampler well it turn out that it can be a huge deal in many case in such case it s either simply too expensive to have a separate step to weed out bad sample or it s just plain impossible because you don t even know what constitutes a bad or worse both nonechucks allows you to wrap your existing datasets and sampler with safe version of them which can fix all these problem for you let s start with the simplest use case which involves wrapping an existing dataset instance with safedataset using something like torchvision s imagefolder dataset class we can load an entire folder of labelled image for a typical supervised classification task now if you have a sneaky fruit apple jpg that is corrupted sitting in your fruit folder to avoid the entire pipeline from surprise failing you would have to resort to something like this not only do you have to put your code inside an extra try except block but you are also forced to use a for loop depriving yourself of pytorch s built in dataloader which mean you can t use feature like batching shuffling multiprocessing and custom sampler for your dataset i don t know about you but not being able to do that kind of defeat the whole point of using a data processing module for me you can transform your dataset into a safedataset with a single line of code that s it seriously and that s not all you can also use a dataloader on top of this in this case safedataset will skip the erroneous image and use the next one in the place of it a opposed to dropping the entire batch the function of transorms in pytorch is restricted to modifying sample with nonechucks you can simply return none or raise an exception from the transform s call method and nonechucks will drop the sample from the dataset for you allowing you to use transforms a filter for the example we ll assume a pdfdocumentsdataset which read pdf file from a folder a plaintexttransform which transforms the file into raw text and a languagefilter which retains only document of a particular language to install nonechucks simply use pip pip install nonechucksor clone this repo and build from source with python setup py install all pr are welcome nonechucks is mit licensed,"[('pytorch dataset', 0.5059), ('datasets sampler', 0.4364), ('sampler', 0.3706), ('pytorch', 0.3689), ('custom sampler', 0.3455), ('pip pip install nonechucksor', 0.3418), ('language detector', 0.3208), ('languagefilter', 0.292), ('bad sample', 0.282), ('dataset instance', 0.277)]","[-4.24254797e-02 -4.79161255e-02 -2.86707450e-02 -2.26728357e-02
  8.11238140e-02 -7.41319358e-03  3.35255228e-02  1.03451014e-02
 -2.41589453e-02 -7.96022639e-02  2.96023563e-02 -9.49867591e-02
 -8.07140768e-02 -4.08218568e-03  4.58237436e-03 -1.17220469e-02
 -6.89321721e-04  5.85225224e-02 -3.56451608e-02 -1.47590607e-01
 -6.49165139e-02 -9.69954487e-03  6.43964261e-02  3.57054733e-02
  2.19814340e-03 -3.24517787e-02 -2.59410199e-02 -3.64696085e-02
  5.39256781e-02 -2.20576786e-02  2.05451716e-02  5.42992726e-02
  7.12824389e-02  5.86208748e-03  9.11168531e-02 -7.03765592e-03
 -1.53643144e-02  3.27843577e-02 -4.14990969e-02  1.28040770e-02
  3.23430425e-03 -1.49782244e-02 -3.69697921e-02 -4.41560782e-02
 -3.50287347e-03  4.99589369e-03  1.58216748e-02 -3.11700832e-02
  3.68676037e-02 -2.66890228e-03 -6.55106083e-02  6.06036466e-03
 -1.50353797e-02 -1.97893698e-02  4.92536975e-03 -9.93324220e-02
  7.17412606e-02  6.36263788e-02  2.13271454e-02 -3.83745283e-02
 -4.61195931e-02 -3.97037193e-02 -7.15167029e-03  6.19393140e-02
 -2.90263500e-02  6.44093305e-02 -3.37119065e-02  3.46614271e-02
  1.55613407e-01 -8.55689794e-02 -5.15698604e-02  1.09072169e-02
 -1.83949377e-02  1.24808356e-01 -4.95854020e-02 -3.26854959e-02
  4.38894657e-03 -1.40482858e-02  4.31389175e-02 -7.64956549e-02
 -5.84308356e-02 -3.80318090e-02  2.15859730e-02  4.96112406e-02
  5.07119149e-02  2.89649125e-02 -4.67458889e-02  4.12256829e-02
  2.20682647e-04 -1.06809950e-02  1.08514382e-02 -8.21933001e-02
  1.65336747e-02  9.98881757e-02 -1.64701492e-02  4.62816320e-02
  3.52232866e-02 -1.31760379e-02 -1.82776637e-02  1.09067857e-02
 -2.62359269e-02 -1.16058178e-02  3.23414393e-02  6.61381930e-02
 -7.16478825e-02 -3.71206217e-02 -2.28101201e-02 -4.35761996e-02
  2.00047996e-02 -1.68814063e-02  3.78047824e-02 -5.94538078e-02
  3.07522574e-03 -6.35309219e-02  1.07335806e-01 -4.36817715e-03
 -5.63526005e-02  1.45617947e-02 -6.80805296e-02  1.80535112e-02
 -1.20136909e-01  2.67595109e-02 -1.64787807e-02 -4.42536883e-02
  2.23870352e-02  6.14129491e-02 -1.45545658e-02  5.24717425e-33
  5.85008673e-02 -3.58863026e-02 -5.99694392e-03 -1.04185455e-02
  4.37019095e-02 -5.44364639e-02  1.12672083e-01  5.10779582e-03
  2.10426319e-02 -5.14252745e-02  5.48540279e-02 -5.85295595e-02
 -8.11031908e-02  1.23419054e-02  8.43060203e-03 -1.09179420e-02
 -3.20039690e-02  2.36083679e-02 -3.23288888e-02  3.79646942e-02
  1.00489974e-01  3.46950702e-02 -1.44484034e-02  8.57696962e-03
 -4.96750176e-02  6.47655427e-02 -3.56757501e-03 -5.50403222e-02
 -1.31733911e-02  2.14464962e-02 -1.10036470e-01  1.01671666e-02
  3.36640850e-02  8.32233429e-02 -8.54959898e-03 -5.40009737e-02
  2.11659800e-02  2.65461877e-02 -1.17101427e-02 -1.47005571e-02
 -3.44977938e-02  4.31240276e-02  5.25960326e-02 -3.99672873e-02
  6.79488182e-02 -5.87278865e-02  6.71915710e-03  5.28380945e-02
  3.89863178e-02  1.85733177e-02 -2.78667938e-02 -1.48694143e-02
 -7.67844124e-03  1.09996900e-01  4.88320142e-02  1.46332076e-02
  2.06365418e-02 -2.70784774e-04  9.16527435e-02  2.92670298e-02
 -3.21330549e-03  7.57130608e-02  3.74471322e-02  4.31394093e-02
  8.67236331e-02  1.04272109e-03 -5.99971078e-02 -4.93680015e-02
 -2.21980885e-02  4.33317088e-02 -6.68482631e-02 -4.52799490e-04
 -7.15372786e-02  4.57344539e-02  6.76281527e-02 -3.27884741e-02
 -1.64318532e-02  5.76327369e-03 -4.62823026e-02  4.34729196e-02
 -2.97810547e-02 -5.47462963e-02  1.89462639e-02 -6.19680388e-03
 -6.33973628e-02  3.15055773e-02  4.40720208e-02 -6.31511630e-03
  7.75501728e-02 -2.80796755e-02 -7.77702555e-02 -1.25612020e-02
 -5.85415922e-02 -2.96171121e-02 -3.41757052e-02 -5.10140708e-33
  2.81559173e-02  4.98692915e-02 -6.64305547e-03  1.07812375e-01
 -4.16767523e-02 -3.34749324e-03  2.32796278e-02 -3.39908227e-02
  1.15662880e-01 -1.83093697e-02 -1.73633136e-02 -8.64895210e-02
  1.01867288e-01 -1.68890078e-02  5.25303632e-02  4.29744534e-02
 -3.04053593e-02  4.01112847e-02  8.00014380e-03  4.52662446e-02
 -1.21643350e-01  7.73090124e-02 -7.79403672e-02 -2.57738382e-02
 -6.53481185e-02 -5.13664223e-02 -6.85420185e-02 -6.75003529e-02
  5.93827255e-02 -1.72047783e-02  1.16699999e-02  6.44153133e-02
 -6.19065538e-02 -1.14003029e-02 -6.78836554e-02 -3.46579589e-02
  8.56606662e-02 -1.26044741e-02 -2.07400471e-02  4.87083979e-02
  1.05802633e-01  9.12775919e-02 -8.14804137e-02  4.22625337e-03
 -7.07597937e-03 -1.87826101e-02 -1.07161582e-01  7.25976517e-03
 -5.21555729e-02 -1.41266128e-02 -5.84397689e-02  7.32157729e-04
 -1.50310472e-02  7.86516722e-03  1.68012994e-04 -2.62902789e-02
  1.13514051e-01  8.00594389e-02 -1.97073556e-02  1.07068662e-02
 -7.56638274e-02 -3.39767933e-02 -7.42853954e-02 -4.49825935e-02
  1.18034659e-02  2.81615718e-03 -2.90527306e-02  8.47660005e-02
  3.34691405e-02 -9.48487520e-02  4.86284383e-02 -3.95709388e-02
  6.84080273e-02  1.04816388e-02 -5.89547725e-03  5.85367456e-02
 -8.36264268e-02  9.95343924e-03  4.00198363e-02 -1.63361505e-02
 -1.01366891e-02  2.57299580e-02  4.66446877e-02  2.99949311e-02
  7.90190622e-02  4.24118116e-02 -2.33172048e-02 -2.29984740e-04
  2.74453480e-02 -1.12674236e-02  3.75426896e-02 -3.54796648e-02
  2.08940580e-02  1.38563737e-01  6.84490278e-02 -3.14521458e-08
 -4.87752073e-02  2.72668991e-02 -7.53306132e-03  6.92049116e-02
  4.41609472e-02  7.91954633e-04 -2.14237589e-02  1.26528099e-01
 -2.99715139e-02  1.06911326e-03  8.98838788e-03 -4.31690877e-03
 -9.45099369e-02 -4.76494208e-02  1.66051798e-02  4.40658443e-02
 -8.71474296e-03  1.12522624e-01  6.77345786e-03  2.04597251e-03
  3.35640088e-02  4.22146317e-04  3.31690349e-03 -6.18151352e-02
 -8.13703910e-02  2.15365868e-02  7.48817995e-02 -1.92090161e-02
 -6.65736757e-03 -9.77787971e-02 -4.34428155e-02 -3.18637453e-02
  1.67402234e-02 -5.58185577e-02  1.55442029e-01  5.67473248e-02
 -7.94312656e-02 -5.04089370e-02  3.86984497e-02  2.27663182e-02
 -1.72032546e-02  2.38628238e-02 -5.44573888e-02 -3.28139961e-02
  1.51649723e-02 -2.88007539e-02 -9.57650542e-02 -1.01532653e-01
 -4.45154822e-03 -4.21682699e-03  1.95937455e-02  1.83109138e-02
  1.34162288e-02  2.54263151e-02  6.94397688e-02  1.08530879e-01
 -4.36503775e-02 -3.02300639e-02 -6.43287897e-02 -6.22348674e-03
  1.51908696e-02  8.28303024e-02 -2.15529818e-02 -4.37096879e-02]",2,2
mabalgs,1,multi armed bandit mab is a problem in which a fixed limited set of resource must be allocated between competing alternative choice in a way that maximizes their expected gain when each choice s property are only partially known at the time of allocation and may become better understood a time pass or by allocating resource to the choice in the problem each machine provides a random reward from a probability distribution specific to that machine the objective of the gambler is to maximize the sum of reward earned through a sequence of lever pull the crucial tradeoff the gambler face at each trial is between exploitation of the machine that ha the highest expected payoff and exploration to get more information about the expected payoff of the other machine the trade off between exploration and exploitation is also faced in machine learning the main problem that the mab help to solve is the split of the population in online experiment if you are looking for a contextual bandit algorithm please go to my another repository onn thsis an algorithm for the multi armed bandit that achieves regret that grows only logarithmically with the number of action taken with no prior knowledge of the reward distribution required a strict improvement over both ucb solution can be made by tuning the upper bound parameter in ucb s decision rule ucb tuned empirically outperforms ucb and ucb in term of frequency of picking the best arm further indicate that ucb tuned is not very sensitive to the variance of the arm thompson sampling is fully bayesian it generates a bandit configuration i e a vector of expected reward from a posterior distribution and then act a if this wa the true configuration i e it pull the lever with the highest expected reward on the likelihood that one unknown probability exceeds another in view of the evidence of two sample produced the first paper on an equivalent problem to the multi armed bandit in which a solution to the bernoulli distribution bandit problem now referred to a thompson sampling is presented monte carlo simulation is the best way to debug test mab algorithm this simulation generates data in real time respecting a probability of delivery chosen by the executor of the simulation over time these probability may represent the taste of most user regarding a mab arm option over time for example example we want to test a arm mab that will be used in an ad problem and mab must choose which of the ad must receive the most click from user you can use the following probability setting for this each array element represents an arm and it probability of being clicked we can observe that ad index of array ha chance of click while others have chance of click these information can help u to analyze if the algorithm is performing well a simulation with the following setting wa made the key of the dictionary tell u the time which the probability must be activated in the passing time the total time of the simulation wa step and each point of the chart is an average of simulation with step each from this dictionary we can infer that you can check a full example of this simulation at this notebook remembering that all these analyzes were performed in a simulation environment and the result may vary according to the type of information the mab will perform on for a more sensible choice with real world data please perform an ab test between the algorithm in your scenario this is an online learning algorithm which can be used to the ranking problem in this algorithm we have a certain quantity of arm to be shown in a certain quantity of a ranked slot like a netflix film list each arm is best in some of the ranking position and this algorithm us mab instance to do that it s the same thing of the rba but with a modification in the arm collision approach this new collision approach wa made by me and my team partner f bio for more detail about this approach please see code documentation like before we will make a monte carlo experimentation using rba m algorithm to show it behavior example we want to test a arm and rank mab that will be used in a movie slot recomendation problem and mab must choose which of the movie banner must receive the most click from user in each slot position a simulation with the following setting wa made in this simulation we have something different now at a given time we need to set the probability of all available arm in each position of the rank and the last array is the rank position click probability velocity of convergence at given ranking position the total time of the simulation wa step and each point of the chart is an average of simulation with step each from this dictionary we can infer that you can check a full example of this simulation at this notebook remembering that all these analyzes were performed in a simulation environment and the result may vary according to the type of information the rmab will perform on for a more sensible choice with real world data please perform an ab test between the algorithm in your scenario if you want to see the behavior of the rba algorithm you can execute it using this notebook it s just change the rbam to rba in the run method the behavior between they is the same just in the same weight case for arm in same position is that rba m performs better,"[('contextual bandit algorithm', 0.6965), ('multi armed bandit mab', 0.6152), ('bernoulli distribution bandit problem', 0.5998), ('bandit configuration', 0.5963), ('reward distribution', 0.4991), ('armed bandit', 0.4808), ('random reward', 0.4571), ('thompson sampling', 0.4306), ('online learning algorithm', 0.4148), ('arm thompson sampling', 0.4123)]","[-4.36855219e-02 -1.52467396e-02 -4.75138761e-02 -1.30407205e-02
  2.80759800e-02  6.68649701e-03  7.44234398e-02  4.17300053e-02
 -5.47902361e-02  1.10016961e-03  1.32167432e-02  2.90352479e-02
  1.20238714e-01 -4.36583459e-02  7.67191947e-02  7.33684609e-03
  3.30649167e-02  3.09989303e-02  2.76362319e-02 -4.96964864e-02
 -4.30033393e-02 -3.67277749e-02  7.24943057e-02 -2.21620668e-02
  3.35109071e-03 -5.00302091e-02  3.94613435e-03 -2.67223436e-02
  6.42628372e-02  2.43739248e-03 -7.60626374e-03  2.92622373e-02
  4.67425995e-02 -8.16463754e-02 -3.92893516e-03  5.86483348e-03
 -9.51386094e-02 -1.81449354e-02 -5.53502655e-03  1.08792566e-01
 -2.57456284e-02  3.38673703e-02 -1.37253627e-02 -1.28650228e-02
  7.31833205e-02 -6.40384480e-02 -8.33024457e-02  1.36030680e-02
  1.95375625e-02  6.20879745e-03 -8.26146752e-02 -7.02844784e-02
 -5.50514646e-02 -3.09582166e-02 -2.54891459e-02 -6.22463077e-02
 -1.34522012e-02 -4.34341189e-03  3.28882076e-02 -3.12255993e-02
  7.12110251e-02 -1.05525054e-01 -5.64857051e-02  5.09555787e-02
 -2.20706202e-02  3.78839858e-02 -9.81979631e-03  8.05557594e-02
  4.26869206e-02 -2.73151193e-02 -2.28066836e-02  3.26511003e-02
 -6.77053407e-02  1.47149730e-02  6.09056242e-02 -6.24374077e-02
 -1.66434061e-03 -1.67926606e-02 -1.81772709e-02 -4.28495370e-02
 -6.41632974e-02 -2.71586590e-02  3.30030248e-02 -6.11598529e-02
  3.28469351e-02 -9.95899923e-03 -2.23339237e-02 -7.04586040e-03
  8.70414600e-02 -6.32647723e-02  4.53220196e-02 -4.05347422e-02
 -8.28105733e-02  8.25950969e-03 -3.68414111e-02  2.43968796e-02
 -1.49206333e-02  5.41639002e-03 -3.64632271e-02  6.88962340e-02
  6.66608736e-02 -2.65211686e-02 -2.11722068e-02 -6.52934909e-02
 -2.52593141e-02 -1.18093519e-02  1.31115019e-02  3.86361182e-02
  8.18437710e-02 -4.06513065e-02 -3.18208970e-02  3.03194206e-02
  6.58532009e-02 -1.08171469e-02  6.94473134e-03  2.10600253e-02
 -4.34162132e-02 -8.28171615e-03 -9.42700729e-03  1.48623675e-01
 -3.33610759e-03 -8.99085589e-03  1.52552696e-02 -3.67324948e-02
  2.25377898e-03 -4.87772264e-02 -4.14274000e-02  2.60950948e-33
  4.85927239e-02 -1.03487991e-01  6.40217727e-03 -4.36412543e-02
  5.20434380e-02  1.11170337e-02  3.58248800e-02 -4.38777581e-02
 -4.06865627e-02  1.32352086e-02  8.28203857e-02  1.03105277e-01
 -1.94010660e-02  1.44443542e-01  7.44823962e-02 -8.31428021e-02
 -1.37594948e-02  3.03628426e-02  9.32218060e-02 -4.63802740e-02
  7.84972161e-02  3.16764340e-02 -1.80791579e-02 -6.40153093e-03
  2.21860744e-02  7.82085285e-02 -4.67652082e-02  4.59334068e-02
 -1.42657699e-03  4.65995073e-02 -7.40172267e-02  4.44163289e-03
 -1.11379914e-01  2.63777990e-02  7.69939460e-03  1.25010600e-02
 -4.82865274e-02 -2.86932639e-03 -7.22681060e-02 -5.07303774e-02
 -6.13229349e-02  2.93034129e-02  1.95250474e-02  1.22574037e-02
 -5.00447676e-02 -8.29869285e-02  3.96671481e-02 -3.90369408e-02
  3.22845951e-02 -2.56400872e-02 -8.08554366e-02 -1.45695498e-03
  7.28970170e-02 -2.62771007e-02  5.23374043e-03  8.57842565e-02
 -5.95067553e-02  6.18495904e-02 -5.92518821e-02  5.89358695e-02
  9.35718641e-02 -1.25512769e-02  9.30195004e-02 -2.79753637e-02
  1.01470025e-02  6.22441396e-02 -5.12783974e-02 -5.30684479e-02
  6.77080154e-02  1.19024878e-02  4.69847880e-02  1.34463552e-02
  2.10458282e-02 -1.10276658e-02  5.94604835e-02 -3.79670598e-02
  4.83656190e-02  7.83311501e-02 -1.39123295e-03 -2.20180489e-03
 -8.64129234e-03 -9.08516720e-02 -2.04436742e-02  3.69839780e-02
 -8.09974596e-02  2.20034048e-02  6.61323816e-02 -1.10314749e-01
 -1.48081119e-04  1.15280580e-02 -8.50375593e-02  4.91027534e-02
 -7.27478862e-02  4.25896496e-02 -1.05893053e-03 -2.94607096e-33
 -7.78980320e-04  4.02057916e-02  5.30824997e-02 -1.05406679e-02
  5.53522771e-03  8.63379147e-03  9.33430530e-03 -6.58094808e-02
  4.36384976e-02 -1.98894795e-02 -9.61120576e-02  4.55004387e-02
 -4.71320301e-02 -9.86244902e-03 -5.54482676e-02 -7.96677098e-02
  6.88544139e-02  3.73785980e-02 -2.12487075e-02 -2.88592093e-02
 -1.33337257e-02  7.03191385e-02 -6.06165938e-02 -8.55163634e-02
  8.13278332e-02 -3.45020648e-03 -5.69265187e-02  3.88827622e-02
 -7.79848034e-03  2.69280616e-02 -1.95834171e-02 -7.28010610e-02
  1.38599603e-02  2.55245510e-02 -8.45206231e-02 -2.47450806e-02
  9.68537405e-02  3.87223288e-02 -1.86322145e-02  9.18672830e-02
  1.09302208e-01 -1.46939699e-02  5.44491038e-02 -2.66606100e-02
  3.88363525e-02  6.50925487e-02 -9.47833732e-02 -2.48062238e-02
  4.04186361e-02 -2.79464424e-02 -1.03197331e-02  4.03095223e-02
 -8.10834020e-02  1.93836652e-02 -6.86198398e-02 -5.49880564e-02
 -2.74948077e-03 -1.51083490e-03  5.17840534e-02  3.83258611e-02
 -2.64540706e-02  2.00509448e-02 -2.58661602e-02  5.15997000e-02
  8.40378087e-03 -4.55683172e-02 -7.64456242e-02  3.52464840e-02
 -2.24532019e-02  1.09747015e-02 -6.35895059e-02  7.73550346e-02
  5.07057048e-02 -4.64727245e-02  1.46065140e-02  8.51783901e-02
 -1.68081876e-02  1.80908237e-02 -3.42305526e-02 -1.21728308e-01
  1.92511976e-02 -3.12318206e-02 -2.91849989e-02  9.14801136e-02
 -3.23436670e-02  4.75591384e-02  7.56239966e-02  1.98807269e-02
  4.44307029e-02 -1.02221809e-01  1.66766290e-02 -9.36083347e-02
  5.24123013e-02  5.82195409e-02 -6.95819706e-02 -2.63155702e-08
 -4.36035059e-02  4.25876752e-02  7.58591294e-02  5.06266579e-02
  9.72145200e-02  1.64879411e-01 -4.10383232e-02  1.75221649e-03
 -5.20935357e-02  3.81828696e-02  2.07227748e-02  3.09407804e-02
 -4.63553369e-02 -5.47069460e-02 -4.34231050e-02 -1.34033067e-02
 -1.31885279e-02 -2.51553245e-02  8.41428619e-03 -7.26818517e-02
  1.02804802e-01  1.55976927e-02  5.91694266e-02 -5.34771048e-02
  3.36457752e-02 -3.15499566e-02 -2.78439559e-02  1.41168945e-02
  5.27053401e-02  2.71133948e-02 -4.13777344e-02 -2.65311357e-03
  1.42841004e-02 -7.09311515e-02  3.96972075e-02  1.02121551e-02
 -6.60708249e-02 -6.98096817e-03 -3.85856815e-02 -3.54038589e-02
 -3.22870426e-02 -1.78120919e-02  2.96352785e-02 -1.72186475e-02
  4.74365540e-02  7.23759159e-02 -1.36958817e-02  6.82229875e-04
  6.86033368e-02 -9.86814499e-02  2.79948357e-02 -1.70326307e-02
 -4.49071079e-02 -8.44031852e-03  1.21849932e-01  2.37247515e-02
  4.65403721e-02 -4.33168486e-02 -2.01431233e-02  3.03192642e-02
  1.94320325e-02  8.60400405e-03 -8.43592733e-02  1.61346216e-02]",2,0
gradient-utils,1,get started create account install cli tutorial docsresources website blog support contact salesgradient is an an end to end mlops platform that enables individual and organization to quickly develop train and deploy deep learning model the gradient software stack run on any infrastructure e g aws gcp on premise and low cost paperspace gpus leverage automatic versioning distributed training built in graph metric hyperparameter search gradientci click jupyter notebook our python sdk and more this is an sdk for performing machine learning with gradient it can be installed in addition to gradient cli this sdk requires python to install it run library for logging custom and framework metric in gradient usage example set the tf config environment variable for multi worker training you need to set the tf config environment variable for each binary running in your cluster set the value of tf config to a json string that specifies each task within the cluster including each task s address and role within the cluster we ve provided a kubernetes template in the tensorflow ecosystem repo which set tf config for your training task get tf config function to set value of tf config when run on machine within paperspace infrastructure it can raise a configerror exception with message if there s a problem with it configuration in a particular machine usage example get mongo conn str function to check and construct mongodb connection string it return a connection string to mongodb it can raise a configerror exception with message if there s a problem with any value used to prepare the mongodb connection string usage example data dir function to retrieve path to job space usage example model dir function to retrieve path to model space usage example export dir function to retrieve path for model export usage example worker host function to retrieve information about worker host usage example p host function to retrieve information about paperspace host usage example task index function to retrieve information about task index usage example job name function to retrieve information about job name usage example we use docker and docker compose to run the test locally,"[('tensorflow ecosystem repo', 0.4612), ('multi worker training', 0.4553), ('gradient software stack', 0.4371), ('gradient cli', 0.4068), ('metric hyperparameter search gradientci', 0.4032), ('training task', 0.4002), ('kubernetes template', 0.3772), ('end mlops platform', 0.3759), ('infrastructure e g aws gcp', 0.3535), ('training', 0.3272)]","[-4.09573540e-02 -1.25795007e-01  2.28774808e-02 -4.35301811e-02
  6.11991249e-02 -3.18239979e-03 -3.35275121e-02 -2.14114785e-02
 -3.29343192e-02 -4.05484065e-02 -8.21495354e-02 -4.33528535e-02
 -8.65802392e-02 -3.13308759e-04  1.06268963e-02 -6.45472482e-03
  2.50824057e-02  5.21989763e-02 -2.76810545e-02 -1.31400213e-01
 -5.38444631e-02 -1.71742383e-02 -4.45354767e-02  2.39507463e-02
 -6.56217411e-02 -2.28692498e-02 -6.11168519e-02 -5.95987290e-02
  2.37671640e-02 -2.92390361e-02 -1.63540337e-02 -2.52503008e-02
  5.35358749e-02  4.41246405e-02 -4.30882052e-02  1.26038596e-01
 -1.71774980e-02 -6.94303066e-02 -1.61274057e-02  3.48220840e-02
 -5.08076362e-02 -7.48461634e-02 -5.31112924e-02 -7.98155814e-02
  1.13824323e-01 -1.20597761e-02  4.49393205e-02 -1.30271837e-01
  1.88141372e-02 -1.00836232e-02 -2.67579816e-02 -1.75294623e-01
 -1.28734121e-02  5.31217232e-02 -9.47328098e-03 -4.50148582e-02
  4.46871072e-02  2.51381528e-02  2.69735157e-02 -8.06441829e-02
 -3.20810415e-02 -8.65563527e-02 -2.10813917e-02  5.33768255e-03
  1.97633654e-02  1.17875438e-03  2.95668287e-04  5.67042753e-02
  1.13010839e-01 -9.57497731e-02  2.66227726e-04 -2.26258710e-02
 -8.61500949e-02  8.44237918e-04 -2.20261775e-02  7.56433308e-02
  5.73161207e-02  1.23835374e-02  8.95234868e-02 -8.69491324e-02
  6.51797876e-02  5.68076968e-02  3.80760692e-02  2.08025798e-02
 -5.60458265e-02 -3.78646106e-02  6.72653317e-03  7.67583475e-02
  4.52255383e-02  1.80355518e-03  1.83303617e-02 -1.51205640e-02
  2.89457291e-02 -2.51835436e-02 -4.68761846e-03  4.09378707e-02
 -7.08036497e-02 -3.49236876e-02 -6.06342033e-03  1.95885394e-02
 -3.94055918e-02 -2.52025053e-02  2.69299597e-02  3.93935479e-02
 -1.16370115e-02  1.21157758e-01 -2.45813616e-02  6.25938177e-02
  1.09734572e-01  3.98585238e-02 -1.02518955e-02  7.13018030e-02
 -2.05585398e-02 -3.40574682e-02  1.74555350e-02  9.64614854e-04
 -2.10132003e-02  1.45423440e-02 -1.52881062e-02  1.73067853e-01
 -7.10451454e-02 -1.63359046e-02 -1.09028406e-02 -6.05871789e-02
 -3.10234316e-02  3.14938948e-02 -7.53709897e-02  4.81180825e-33
  1.47288572e-02 -1.08444998e-02  1.92567352e-02 -4.54735011e-02
  9.81284305e-02 -9.56788883e-02  1.52435871e-02  1.80006232e-02
 -1.45257926e-02 -6.31881207e-02 -6.38058707e-02  1.41780019e-01
 -2.94977147e-03  8.85545239e-02  4.25769761e-02 -8.45808387e-02
 -1.04113866e-03  5.22604175e-02  4.70340438e-02  6.02631830e-02
  7.41991699e-02 -6.99007064e-02 -1.35308467e-02  4.01550159e-02
  7.03169107e-02 -1.19210873e-02  6.44452795e-02 -1.13915019e-02
  1.63509380e-02  3.04936748e-02 -3.02732009e-02  2.90604588e-03
  1.03856027e-02  3.25399786e-02 -5.66779226e-02  3.11413445e-02
 -6.35866374e-02 -4.34175245e-02  2.74614356e-02  1.88519806e-02
 -1.56389419e-02  4.31270860e-02  1.77499140e-03 -5.08263148e-02
  5.11816330e-03 -7.72495344e-02  1.96214430e-02 -1.04484502e-02
  9.11865011e-02 -5.54665215e-02 -6.43123537e-02 -2.95433681e-02
  6.05056621e-02 -5.68992011e-02  6.13645799e-02  3.28311510e-02
  5.85956313e-02  7.06131943e-03  5.74500579e-03  4.42709178e-02
 -2.81992815e-02  2.80330256e-02  1.89701635e-02 -2.17474233e-02
  1.18773961e-02 -6.55356869e-02 -2.29889043e-02  3.93546931e-02
  1.36191454e-02  5.16081713e-02 -5.01659140e-02  3.08329444e-02
  4.41874079e-02 -2.22194977e-02 -4.41005416e-02 -4.77410667e-02
  1.52380150e-02 -5.19045740e-02 -2.74525471e-02  7.76482821e-02
 -7.34342039e-02  3.74386385e-02  1.81154162e-02  2.64730603e-02
 -9.63007275e-04 -7.86619820e-03 -3.20387296e-02  2.65698954e-02
  1.35910958e-02  6.22427883e-03 -1.22909032e-01 -3.05736382e-02
  1.78211704e-02  5.00779524e-02 -8.20506215e-02 -4.04853662e-33
 -3.05207111e-02  2.83381175e-02 -9.05773416e-02  7.84238428e-02
  4.29844707e-02  5.14147468e-02  2.69537829e-02 -7.08819460e-03
 -6.25749230e-02  9.03684646e-02  1.90035794e-02 -3.03194784e-02
  4.89564165e-02  1.03108585e-02 -3.40029709e-02 -6.19209632e-02
 -6.77521825e-02 -6.77137971e-02 -4.76645119e-03 -1.17645776e-02
 -3.09928637e-02  3.95536423e-02 -2.76559945e-02  3.16081196e-02
  5.01814000e-02 -1.93938594e-02 -5.53113259e-02 -3.63298841e-02
 -3.68527835e-03  1.25976400e-02 -1.91402884e-04 -2.23877020e-02
  8.14590231e-03  3.19997668e-02  4.04939651e-02  8.21040850e-03
  1.99377891e-02  2.05650032e-02  4.16066460e-02  5.03852963e-02
  7.71943256e-02 -3.17621678e-02  5.36902435e-02 -3.25667821e-02
 -7.92880803e-02  1.11227157e-02 -1.04130410e-01  1.58763863e-02
  3.73607222e-03 -1.15062334e-01 -5.68464547e-02  1.36443609e-02
 -7.84717575e-02 -2.25600321e-03 -4.66429405e-02  1.59562584e-02
  1.93584599e-02  5.19178994e-02 -1.44466162e-02 -2.83538061e-03
  2.14882009e-02 -5.79072116e-03  4.39320877e-02  3.00091989e-02
  1.15198987e-02  4.11188416e-02 -1.90690849e-02  2.72518154e-02
 -1.07908301e-01  3.89054813e-03  3.11751999e-02  3.44248228e-02
  3.25215980e-02  3.16973850e-02 -8.36381689e-02 -5.17050139e-05
 -1.35779921e-02 -2.83672828e-02  1.36040421e-02 -6.17642514e-02
  8.22262019e-02  1.18208220e-02  2.24143919e-02  8.22705626e-02
  7.30297714e-02  9.14276391e-02  3.95908058e-02  4.20937277e-02
  4.57639508e-02 -2.53835768e-02 -6.66760579e-02 -4.94212620e-02
  2.12639961e-02  5.20577878e-02 -1.82654671e-02 -3.07386578e-08
 -6.84847869e-03  4.60795946e-02  6.20776154e-02  2.08565239e-02
 -3.03324685e-02  1.78192891e-02  2.09841691e-03  9.41129997e-02
  7.89288338e-03  8.29948410e-02 -5.25388941e-02 -8.75416771e-03
 -1.00655384e-01  2.38160114e-03  1.06396936e-01  8.49177241e-02
 -2.42111031e-02  7.97585919e-02  1.17711062e-02 -8.94962624e-02
  6.31785765e-02  8.26095715e-02 -1.58807673e-02 -1.30875688e-02
  2.11008806e-02 -2.60508936e-02  6.33425787e-02  3.86706516e-02
  1.45542901e-03 -2.86198650e-02 -1.69168916e-02 -1.90079454e-02
  8.60389788e-03 -6.05953000e-02  5.42093888e-02  6.19028211e-02
 -2.19949558e-02  7.53144501e-03 -1.99481733e-02  3.48101296e-02
 -6.42503053e-02  1.28881605e-02  3.65906917e-02 -4.96525392e-02
  2.85566840e-02  2.56613828e-02 -5.36551289e-02  3.55248079e-02
 -5.60097173e-02 -1.39217516e-02 -7.73088401e-03 -1.83067080e-02
 -4.27631103e-02  1.13851495e-01  6.43550083e-02 -1.68223195e-02
  1.52545841e-02 -1.43609121e-01  3.66475023e-02  2.37997826e-02
  1.98421646e-02 -2.58241482e-02  4.52546850e-02 -3.24246213e-02]",2,2
numerapi,1,automatically download and upload data for the numerai machine learning competition this library is a python client to the numerai api the interface is programmed in python and allows downloading the training data uploading prediction and accessing user submission and competition information it work for both the main competition and the newer numerai signal competition if you encounter a problem or have suggestion feel free to open an issue pip install upgrade numerapinumerapi can be used a a regular importable python module or from the command line some action like uploading prediction or staking require a token to verify that it is really you interacting with numerai s api these token consists of a public id and secret key both can be obtained by login in to numer ai and going to account custom api key token can be passed to the python module a parameter or you can be set via environment variable numerai public id and numerai secret key to get started with the cli interface let s take a look at the help page each command ha it s own help page for example checkout the detailed api doc to learn about all available method parameter and returned value,"[('numerai api', 0.7053), ('numerai machine learning competition', 0.5604), ('numerai', 0.5084), ('newer numerai signal competition', 0.4705), ('api', 0.4509), ('training data uploading prediction', 0.4219), ('upgrade numerapinumerapi', 0.4188), ('python client', 0.3823), ('python module', 0.3626), ('python', 0.3447)]","[-1.12387463e-01 -4.71456200e-02 -5.24317697e-02 -2.43582651e-02
  6.79814117e-03 -2.39878017e-02 -5.13828173e-02  1.77125707e-02
 -4.87359725e-02  4.66185762e-03 -1.34064956e-02 -5.75579293e-02
  2.00595278e-02  4.65443842e-02  9.24592689e-02  1.56081822e-02
  1.09681196e-03  2.67908704e-02  1.22596289e-03 -1.49368539e-01
 -6.50325194e-02  7.15460107e-02  2.52089407e-02  4.70401198e-02
  3.81494127e-02 -5.42769618e-02 -1.37123708e-02 -1.51970126e-02
  2.69943383e-02 -5.50268404e-02 -5.65198027e-02  4.95873839e-02
  3.67061123e-02  3.05792950e-02 -2.20409185e-02 -4.89181168e-02
 -2.87862066e-02 -4.56104651e-02 -3.18040252e-02  1.38556100e-02
  1.33035714e-02 -6.71697706e-02 -1.99605543e-02 -2.06545182e-02
 -2.12960411e-02  4.45259735e-02 -6.12026602e-02 -1.79594290e-02
  4.74334173e-02  3.46029997e-02 -1.07130378e-01 -8.10569599e-02
 -5.95025271e-02  1.26919616e-02 -1.00385854e-02 -4.81643341e-02
 -5.30831181e-02 -3.02240327e-02 -4.15817462e-02 -8.84566456e-03
  1.73316356e-02 -3.84530164e-02 -3.10489605e-03  1.93012096e-02
 -4.23916429e-02  1.62261985e-02 -1.12347817e-02  1.93311442e-02
  7.93065876e-02 -7.53972307e-02  4.03989339e-03 -1.31944604e-02
 -2.81433724e-02  2.06296816e-02 -6.92825066e-03 -4.65738252e-02
  4.41688858e-02 -5.94981429e-05  4.02477719e-02 -6.05826043e-02
 -4.77684550e-02 -6.66513517e-02 -1.20768845e-02  3.24094705e-02
  2.99220979e-02  1.72225311e-02 -9.24954750e-03  6.84263930e-02
 -1.00925751e-03 -5.07394671e-02 -3.01787816e-03  3.67760658e-02
  3.48113067e-02 -1.04436381e-02  6.66142860e-03  4.65694182e-02
 -8.04407522e-02 -4.03828640e-03 -4.34963889e-02  5.93549460e-02
 -4.86732833e-03 -3.26543674e-02 -1.03030525e-01 -8.01860821e-03
 -2.43205316e-02  2.99547892e-02  1.29568592e-01 -7.46348500e-02
  1.20204419e-01  2.78102104e-02 -2.94873826e-02 -3.71114500e-02
 -2.79394574e-02 -9.29916576e-02  1.48392515e-02  6.84859529e-02
 -4.07080278e-02  8.72442946e-02 -1.89905372e-02  5.36237024e-02
 -1.16268219e-02 -4.51801345e-02 -2.23768270e-03  2.23664641e-02
 -1.66370291e-02  3.43631841e-02 -4.61535789e-02  4.70743868e-33
 -1.63628557e-03  2.16776058e-02  8.56131036e-03 -9.49684437e-03
  4.23136353e-02 -1.53587693e-02  1.23689743e-02  1.11681102e-02
 -6.18764088e-02 -8.56029168e-02 -9.49667767e-02  1.13564551e-01
 -5.30565232e-02 -4.21945192e-03  9.13425684e-02 -8.27420950e-02
 -3.67971463e-03  7.76182860e-02  2.20813230e-02 -2.66848858e-02
  2.47573722e-02  2.79903170e-02  1.57051254e-02  6.96785972e-02
  5.17012998e-02  8.27489123e-02 -3.49911414e-02  4.28287603e-04
  6.96831420e-02  2.24623103e-02 -4.75335196e-02 -1.61276590e-02
 -1.56743228e-02 -2.15959959e-02  6.07453845e-03 -9.63401701e-03
 -7.79161528e-02 -5.47538437e-02  2.69759097e-03  2.76217796e-02
 -9.16707516e-02  8.59695300e-02 -4.99495827e-02 -5.72107024e-02
 -3.38442102e-02  6.20486028e-03  2.61523016e-02  6.84781522e-02
 -5.72702289e-03 -1.20731676e-02 -9.00446177e-02  6.02167100e-03
 -2.62078922e-02 -1.96276680e-02  2.15055626e-02 -6.34347945e-02
  4.85778600e-02  1.99792590e-02 -4.31112200e-02 -1.76197011e-02
 -2.31438107e-03 -9.13283527e-02  4.96909805e-02 -6.68113530e-02
 -6.50967285e-03  5.09232506e-02  1.04007004e-02  1.29616028e-02
  5.17553315e-02  6.79540262e-02 -1.08371321e-02  9.90773588e-02
 -7.10228831e-02  3.01858913e-02  5.80453612e-02 -7.74759203e-02
 -1.81842800e-02 -2.62955204e-02 -5.34476107e-03 -2.58949827e-02
  5.29151410e-02  3.74430791e-02  2.29043234e-02  8.41759611e-03
  3.50467605e-03  1.83472540e-02  6.30343407e-02  2.69271564e-02
 -4.74144123e-04 -1.59539580e-02 -5.14023304e-02 -1.03292894e-02
  2.46219747e-02  1.67098455e-02 -2.87003033e-02 -3.63772229e-33
 -5.22916317e-02  2.25597005e-02 -1.22820705e-01  9.40108076e-02
 -1.11081023e-02 -2.48550735e-02  4.75416379e-03  1.36806900e-02
  1.47253303e-02  1.74038876e-02  7.22170994e-02 -7.32184127e-02
  8.39188695e-02 -1.39951203e-02 -5.24483100e-02 -1.50274877e-02
 -4.37999517e-02  2.83750035e-02 -1.11608673e-02 -8.83698985e-02
 -5.88581376e-02  1.08378425e-01 -5.68913259e-02 -6.57850355e-02
 -1.89065468e-02  2.03915518e-02 -1.04392491e-01  2.54296251e-02
  5.53918891e-02  3.11663207e-02 -4.90913503e-02 -1.14360722e-02
 -4.95247394e-02 -7.72011653e-02  2.51727849e-02  4.62206155e-02
  1.43219411e-01 -1.67650823e-02  8.58910475e-03  1.43600563e-02
  1.19980633e-01 -1.93852317e-02  8.64768550e-02  6.66851690e-03
 -5.76902460e-03 -8.51051435e-02 -7.66274780e-02  7.43595585e-02
 -5.81622608e-02 -2.91790999e-02  3.97216827e-02 -1.34158432e-02
  1.47348242e-02 -3.57453302e-02 -1.29054869e-02  1.09685354e-01
  5.32054827e-02  5.81224710e-02 -9.30067431e-03 -1.63673107e-02
 -6.90762177e-02 -4.66898233e-02  6.28802553e-03  3.88464592e-02
 -3.41542214e-02  5.44828773e-02 -2.71396562e-02  9.80348885e-02
 -3.86053659e-02 -3.77181880e-02 -1.85959991e-02  2.96628661e-02
  1.99922360e-02  2.17761528e-02 -1.17721513e-01 -5.96616194e-02
  1.93682034e-02  8.97337571e-02  2.67481972e-02 -4.09057830e-03
 -2.73270532e-02  2.13227998e-02  5.53178787e-02  5.72334155e-02
  4.61180396e-02  3.35456543e-02  1.86221033e-01 -1.14900963e-02
  2.98903305e-02 -4.94185835e-02 -7.03861937e-02  5.98782338e-02
 -3.54330093e-02  9.03010741e-02 -1.28394384e-02 -2.77119518e-08
  8.71801004e-03 -1.45629607e-02 -8.84944946e-03  5.47145978e-02
  8.91207680e-02  8.47020671e-02 -7.45748430e-02  9.53379199e-02
  3.49183828e-02  6.54697791e-02  2.71570869e-02  1.57552250e-02
 -8.02399665e-02  1.88764464e-02  3.31105068e-02  8.34930763e-02
  6.80355588e-03  2.93559190e-02  2.99960654e-02 -4.76076789e-02
  2.45822370e-02  1.05234630e-01  2.12643929e-02 -9.85178724e-02
 -6.11579716e-02 -3.84270512e-02  1.86132751e-02  6.95672929e-02
 -3.84605009e-05 -6.16711304e-02 -4.22934853e-02 -3.25152092e-03
  7.34610111e-02 -1.03432909e-01  2.38283556e-02  4.19439226e-02
 -3.17631289e-02  3.58905867e-02  2.04157154e-03  1.79610681e-02
 -3.75287086e-02  5.52503280e-02 -6.89698756e-02  2.10891739e-02
  7.91936144e-02 -2.70896852e-02 -6.52900059e-03 -9.45333168e-02
 -3.87104927e-03 -9.95724928e-03 -4.79387008e-02  1.63704082e-02
  3.70716974e-02 -1.67336948e-02  9.04852822e-02 -4.53175930e-03
 -2.59280168e-02 -6.06201440e-02 -2.82195341e-02  7.69640878e-02
  5.97689450e-02  3.41984145e-02  3.06088403e-02  6.15828969e-02]",2,2
pytorch-pretrained-bert,1,this repository contains op for op pytorch reimplementations pre trained model and fine tuning example for these implementation have been tested on several datasets see the example and should match the performance of the associated tensorflow implementation e g f on squad for bert f on rocstories for openai gpt and perplexity on wikitext for the transformer xl you can find more detail in the example section below here are some information on these model bert wa released together with the paper bert pre training of deep bidirectional transformer for language understanding by jacob devlin ming wei chang kenton lee and kristina toutanova this pytorch implementation of bert is provided with google s pre trained model example notebook and a command line interface to load any pre trained tensorflow checkpoint for bert is also provided openai gpt wa released together with the paper improving language understanding by generative pre training by alec radford karthik narasimhan tim salimans and ilya sutskever this pytorch implementation of openai gpt is an adaptation of the pytorch implementation by huggingface and is provided with openai s pre trained model and a command line interface that wa used to convert the pre trained numpy checkpoint in pytorch google cmu s transformer xl wa released together with the paper transformer xl attentive language model beyond a fixed length context by zihang dai zhilin yang yiming yang jaime carbonell quoc v le ruslan salakhutdinov this pytorch implementation of transformer xl is an adaptation of the original pytorch implementation which ha been slightly modified to match the performance of the tensorflow implementation and allow to re use the pretrained weight a command line interface is provided to convert tensorflow checkpoint in pytorch model openai gpt wa released together with the paper language model are unsupervised multitask learner by alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever this pytorch implementation of openai gpt is an adaptation of the openai s implementation and is provided with openai s pre trained model and a command line interface that wa used to convert the tensorflow checkpoint in pytorch this repo wa tested on python and example are tested only on python and pytorch pytorch pretrained bert can be installed by pip a follows if you want to reproduce the original tokenization process of the openai gpt paper you will need to install ftfy limit to version if you are using python and spacy if you don t install ftfy and spacy the openai gpt tokenizer will default to tokenize using bert s basictokenizer followed by byte pair encoding which should be fine for most usage don t worry clone the repository and run here also if you want to reproduce the original tokenization process of the openai gpt model you will need to install ftfy limit to version if you are using python and spacy again if you don t install ftfy and spacy the openai gpt tokenizer will default to tokenize using bert s basictokenizer followed by byte pair encoding which should be fine for most usage a series of test is included in the test folder and can be run using pytest install pytest if needed pip install pytest you can run the test with the command this package comprises the following class that can be imported in python and are detailed in the doc section of this readme eight bert pytorch model torch nn module with pre trained weight in the modeling py file three openai gpt pytorch model torch nn module with pre trained weight in the modeling openai py file two transformer xl pytorch model torch nn module with pre trained weight in the modeling transfo xl py file three openai gpt pytorch model torch nn module with pre trained weight in the modeling gpt py file tokenizers for bert using word piece in the tokenization py file tokenizer for openai gpt using byte pair encoding in the tokenization openai py file tokenizer for transformer xl word token ordered by frequency for adaptive softmax in the tokenization transfo xl py file tokenizer for openai gpt using byte level byte pair encoding in the tokenization gpt py file optimizer for bert in the optimization py file optimizer for openai gpt in the optimization openai py file configuration class for bert openai gpt and transformer xl in the respective modeling py modeling openai py modeling transfo xl py file the repository further comprises five example on how to use bert in the example folder one example on how to use openai gpt in the example folder one example on how to use transformer xl in the example folder one example on how to use openai gpt in the unconditional and interactive mode in the example folder these example are detailed in the example section of this readme three notebook that were used to check that the tensorflow and pytorch model behave identically in the notebook folder these notebook are detailed in the notebook section of this readme a command line interface to convert tensorflow checkpoint bert transformer xl or numpy checkpoint openai in a pytorch save of the associated pytorch model this cli is detailed in the command line interface section of this readme here is a quick start example using berttokenizer bertmodel and bertformaskedlm class with google ai s pre trained bert base uncased model see the doc section below for all the detail on these class first let s prepare a tokenized input with berttokenizerlet s see how to use bertmodel to get hidden statesand how to use bertformaskedlmhere is a quick start example using openaigpttokenizer openaigptmodel and openaigptlmheadmodel class with openai s pre trained model see the doc section below for all the detail on these class first let s prepare a tokenized input with openaigpttokenizerlet s see how to use openaigptmodel to get hidden statesand how to use openaigptlmheadmodelhere is a quick start example using transfoxltokenizer transfoxlmodel and transfoxlmodellmheadmodel class with the transformer xl model pre trained on wikitext see the doc section below for all the detail on these class first let s prepare a tokenized input with transfoxltokenizerlet s see how to use transfoxlmodel to get hidden statesand how to use transfoxllmheadmodelhere is a quick start example using gpt tokenizer gpt model and gpt lmheadmodel class with openai s pre trained model see the doc section below for all the detail on these class first let s prepare a tokenized input with gpt tokenizerlet s see how to use gpt model to get hidden statesand how to use gpt lmheadmodelhere is a detailed documentation of the class in the package and how to use them to load one of google ai s openai s pre trained model or a pytorch saved model an instance of bertforpretraining saved with torch save the pytorch model class and the tokenizer can be instantiated aswherebert class is either a tokenizer to load the vocabulary berttokenizer or openaigpttokenizer class or one of the eight bert or three openai gpt pytorch model class to load the pre trained weight bertmodel bertformaskedlm bertfornextsentenceprediction bertforpretraining bertforsequenceclassification bertfortokenclassification bertformultiplechoice bertforquestionanswering openaigptmodel openaigptlmheadmodel or openaigptdoubleheadsmodel andpre trained model name or path is either the shortcut name of a google ai s or openai s pre trained model selected in the list a path or url to a pretrained model archive containing if pre trained model name or path is a shortcut name the pre trained weight will be downloaded from aws s see the link here and stored in a cache folder to avoid future download the cache folder can be found at pytorch pretrained bert cache dir can be an optional path to a specific directory to download and cache the pre trained model weight this option is useful in particular when you are using distributed training to avoid concurrent access to the same weight you can set for example cache dir pretrained model format args local rank see the section on distributed training for more information uncased mean that the text ha been lowercased before wordpiece tokenization e g john smith becomes john smith the uncased model also strip out any accent marker cased mean that the true case and accent marker are preserved typically the uncased model is better unless you know that case information is important for your task e g named entity recognition or part of speech tagging for information about the multilingual and chinese model see the multilingual readme or the original tensorflow repository when using an uncased model make sure to pas do lower case to the example training script or pas do lower case true to fulltokenizer if you re using your own script and loading the tokenizer your self example this section explain how you can save and re load a fine tuned model bert gpt gpt and transformer xl there are three type of file you need to save to be able to reload a fine tuned model here is the recommended way of saving the model configuration and vocabulary to an output dir directory and reloading the model and tokenizer afterwards here is another way you can save and reload the model if you want to use specific path for each type of file model bert gpt gpt and transformer xl are defined and build from configuration class which containes the parameter of the model number of layer dimensionality and a few utility to read and write from json configuration file the respective configuration class are these configuration class contains a few utility to load and save configuration bertmodel is the basic bert transformer model with a layer of summed token position and sequence embeddings followed by a series of identical self attention block for bert base for bert large the input and output are identical to the tensorflow model input and output we detail them here this model take a input modeling pythis model output a tuple composed of encoded layer controled by the value of the output encoded layer argument pooled output a torch floattensor of size batch size hidden size which is the output of a classifier pretrained on top of the hidden state associated to the first character of the input clf to train on the next sentence task see bert s paper an example on how to use this class is given in the extract feature py script which can be used to extract the hidden state of the model for a given input bertforpretraining includes the bertmodel transformer followed by the two pre training head input comprises the input of the bertmodel class plus two optional label output if masked lm label and next sentence label are not none output the total loss which is the sum of the masked language modeling loss and the next sentence classification loss if masked lm label or next sentence label is none output a tuple comprisingan example on how to use this class is given in the run lm finetuning py script which can be used to fine tune the bert language model on your specific different text corpus this should improve model performance if the language style is different from the original bert training corpus wiki bookcorpus bertformaskedlm includes the bertmodel transformer followed by the possibly pre trained masked language modeling head input comprises the input of the bertmodel class plus optional label output bertfornextsentenceprediction includes the bertmodel transformer followed by the next sentence classification head input comprises the input of the bertmodel class plus an optional label output bertforsequenceclassification is a fine tuning model that includes bertmodel and a sequence level sequence or pair of sequence classifier on top of the bertmodel the sequence level classifier is a linear layer that take a input the last hidden state of the first character in the input sequence see figure a and b in the bert paper an example on how to use this class is given in the run classifier py script which can be used to fine tune a single sequence or pair of sequence classifier using bert for example for the mrpc task bertformultiplechoice is a fine tuning model that includes bertmodel and a linear layer on top of the bertmodel the linear layer output a single value for each choice of a multiple choice problem then all the output corresponding to an instance are passed through a softmax to get the model choice this implementation is largely inspired by the work of openai in improving language understanding by generative pre training and the answer of jacob devlin in the following issue an example on how to use this class is given in the run swag py script which can be used to fine tune a multiple choice classifier using bert for example for the swag task bertfortokenclassification is a fine tuning model that includes bertmodel and a token level classifier on top of the bertmodel the token level classifier is a linear layer that take a input the last hidden state of the sequence bertforquestionanswering is a fine tuning model that includes bertmodel with a token level classifier on top of the full sequence of last hidden state the token level classifier take a input the full sequence of the last hidden state and compute several e g two score for each token that can for example respectively be the score that a given token is a start span and a end span token see figure c and d in the bert paper an example on how to use this class is given in the run squad py script which can be used to fine tune a token classifier using bert for example for the squad task openaigptmodel is the basic openai gpt transformer model with a layer of summed token and position embeddings followed by a series of identical self attention block openai gpt use a single embedding matrix to store the word and special embeddings special token embeddings are additional token that are not pre trained sep cl special token need to be trained during the fine tuning if you use them the number of special embeddings can be controled using the set num special token num special token function the embeddings are ordered a follow in the token embeddings matrice where total token embeddings can be obtained a config total token embeddings and is total token embeddings config vocab size config n special you should use the associate index to index the embeddings the input and output are identical to the tensorflow model input and output we detail them here this model take a input modeling openai pythis model output openaigptlmheadmodel includes the openaigptmodel transformer followed by a language modeling head with weight tied to the input embeddings no additional parameter input are the same a the input of the openaigptmodel class plus optional label output openaigptdoubleheadsmodel includes the openaigptmodel transformer followed by two head input are the same a the input of the openaigptmodel class plus a classification mask and two optional label output the transformer xl model is described in transformer xl attentive language model beyond a fixed length context transformer xl use a relative positioning with sinusiodal pattern and adaptive softmax input which mean that this model take a input modeling transfo xl pythis model output a tuple of last hidden state new mem the new mem contain all the hidden state plus the output of the embeddings new mem new mem is the output of the hidden state of the layer below the last layer and last hidden state is the output of the last layer i e the input of the softmax when we have a language modeling head on top there are two difference between the shape of new mem and last hidden state new mem have transposed first dimension and are longer of size self config mem len here is how to extract the full list of hidden state from the model output transfoxllmheadmodel includes the transfoxlmodel transformer followed by an adaptive softmax head with weight tied to the input embeddings input are the same a the input of the transfoxlmodel class plus optional label output a tuple of last hidden state new mem gpt model is the openai gpt transformer model with a layer of summed token and position embeddings followed by a series of identical self attention block the input and output are identical to the tensorflow model input and output we detail them here this model take a input modeling gpt pythis model output gpt lmheadmodel includes the gpt model transformer followed by a language modeling head with weight tied to the input embeddings no additional parameter input are the same a the input of the gpt model class plus optional label output gpt doubleheadsmodel includes the gpt model transformer followed by two head input are the same a the input of the gpt model class plus a classification mask and two optional label output berttokenizer perform end to end tokenization i e basic tokenization followed by wordpiece tokenization this class ha five argument and three method please refer to the doc string and code in tokenization py for the detail of the basictokenizer and wordpiecetokenizer class in general it is recommended to use berttokenizer unless you know what you are doing openaigpttokenizer perform byte pair encoding bpe tokenization this class ha four argument and five method please refer to the doc string and code in tokenization openai py for the detail of the openaigpttokenizer transfoxltokenizer perform word tokenization this tokenizer can be used for adaptive softmax and ha utility for counting token in a corpus to create a vocabulary ordered by toekn frequency for adaptive softmax see the adaptive softmax paper efficient softmax approximation for gpus for more detail the api is similar to the api of berttokenizer see above please refer to the doc string and code in tokenization transfo xl py for the detail of these additional method in transfoxltokenizer gpt tokenizer perform byte level byte pair encoding bpe tokenization this class ha three argument and two method please refer to tokenization gpt py for more detail on the gpt tokenizer bertadam is a torch optimizer adapted to be closer to the optimizer used in the tensorflow implementation of bert the difference with pytorch adam optimizer are the following the optimizer accepts the following argument openaiadam is similar to bertadam the difference with bertadam is that openaiadam compensate for bias a in the regular adam optimizer openaiadam accepts the same argument a bertadam the optimization module also provides additional schedule in the form of schedule object that inherit from lrschedule all lrschedule subclass accept warmup and t total argument at construction when an lrschedule object is passed into bertadam or openaiadam the warmup and t total argument on the optimizer are ignored and the one in the lrschedule object are used an overview of the implemented schedule bert base and bert large are respectively m and m parameter model and it can be difficult to fine tune them on a single gpu with the recommended batch size for good performance in most case a batch size of to help with fine tuning these model we have included several technique that you can activate in the fine tuning script run classifier py and run squad py gradient accumulation multi gpu training distributed training and bit training for more detail on how to use these technique you can read the tip on training large batch in pytorch that i published earlier this month here is how to use these technique in our script to use bit training and distributed training you need to install nvidia s apex extension a detailed here you will find more information regarding the internals of apex and how to use apex in the doc and the associated repository the result of the test performed on pytorch bert by the nvidia team and my trial at reproducing them can be consulted in the relevant pr of the present repository note to use distributed training you will need to run one training script on each of your machine this can be done for example by running the following command on each server see the above mentioned blog post for more detail where this machine index is an sequential index assigned to each of your machine and the machine with rank ha an ip address and an open port we showcase several fine tuning example based on and extended from the original implementation we get the following result on the dev set of glue benchmark with an uncased bert base model all experiment were run on a p gpu with a batch size of some of these result are significantly different from the one reported on the test set of glue benchmark on the website for qqp and wnli please refer to faq on the webite before running anyone of these glue task you should download the glue data by running this script and unpack it to some directory glue dir where task name can be one of cola sst mrpc sts b qqp mnli qnli rte wnli the dev set result will be present within the text file eval result txt in the specified output dir in case of mnli since there are two separate dev set matched and mismatched there will be a separate output folder called tmp mnli mm in addition to tmp mnli the code ha not been tested with half precision training with apex on any glue task apart from mrpc mnli cola sst the following section provides detail on how to run half precision training with mrpc with that being said there shouldn t be any issue in running half precision training with the remaining glue task a well since the data processor for each task inherits from the base class dataprocessor this example code fine tune bert on the microsoft research paraphrase corpus mrpc corpus and run in le than minute on a single k and in second on single tesla v gb with apex installed before running this example you should download the glue data by running this script and unpack it to some directory glue dir our test ran on a few seed with the original implementation hyper parameter gave evaluation result between and fast run with apex and bit precision fine tuning on mrpc in second first install apex a indicated here then runthis example code fine tune bert on the squad dataset it run in min with bert base or min with bert large on a single tesla v gb the data for squad can be downloaded with the following link and should be saved in a squad dir directory training with the previous hyper parameter gave u the following result the data for swag can be downloaded by cloning the following repositorytraining with the previous hyper parameter on a single gpu gave u the following result the data should be a text file in the same format a sample text txt one sentence per line doc separated by empty line you can download an exemplary training corpus generated from wikipedia article and splitted into k sentence with spacy training one epoch on this corpus take about h on x nvidia tesla p with train batch size and max seq length thank to the work of rocketknight and tholor there are now several script that can be used to fine tune bert using the pretraining objective combination of masked language modeling and next sentence prediction loss these script are detailed in the readme of the example lm finetuning folder we provide three example of script for openai gpt transformer xl and openai gpt based on and extended from the respective original implementation this example code fine tune openai gpt on the rocstories dataset before running this example you should download the rocstories dataset and unpack it to some directory roc story dir this command run in about min on a single k an give an evaluation accuracy of about the author report a median accuracy with the tensorflow code of and the openai gpt paper report a best single run accuracy of this example code evaluate the pre trained transformer xl on the wikitext dataset this command will download a pre processed version of the wikitext dataset in which the vocabulary ha been computed this command run in about min on a v and give an evaluation perplexity of on wikitext the author report a perplexity of about on this dataset with the tensorflow code this example code is identical to the original unconditional and conditional generation code conditional generation unconditional generation the same option a in the original script are provided please refere to the code of the example and the original repository of openai the option we list above allow to fine tune bert large rather easily on gpu s instead of the tpu used by the original implementation for example fine tuning bert large on squad can be done on a server with k these are pretty old now in hour our result are similar to the tensorflow implementation result actually slightly higher to get these result we used a combination of here is the full list of hyper parameter for this run if you have a recent gpu starting from nvidia volta series you should try bit fine tuning fp here is an example of hyper parameter for a fp run we tried the result were similar to the above fp result actually slightly higher we include three jupyter notebook that can be used to check that the prediction of the pytorch model are identical to the prediction of the original tensorflow model the first notebook comparing tf and pt model ipynb extract the hidden state of a full sequence on each layer of the tensorflow and the pytorch model and computes the standard deviation between them in the given example we get a standard deviation of e to e on the various hidden state of the model the second notebook comparing tf and pt model squad ipynb compare the loss computed by the tensorflow and the pytorch model for identical initialization of the fine tuning layer of the bertforquestionanswering and computes the standard deviation between them in the given example we get a standard deviation of e between the model the third notebook comparing tf and pt model mlm nsp ipynb compare the prediction computed by the tensorflow and the pytorch model for masked token language modeling using the pre trained masked language modeling model please follow the instruction given in the notebook to run and modify them a command line interface is provided to convert a tensorflow checkpoint in a pytorch dump of the bertforpretraining class for bert or numpy checkpoint in a pytorch dump of the openaigptmodel class for openai gpt you can convert any tensorflow checkpoint for bert in particular the pre trained model released by google in a pytorch save file by using the convert tf checkpoint to pytorch py script this cli take a input a tensorflow checkpoint three file starting with bert model ckpt and the associated configuration file bert config json and creates a pytorch model for this configuration load the weight from the tensorflow checkpoint in the pytorch model and save the resulting model in a standard pytorch save file that can be imported using torch load see example in extract feature py run classifier py and run squad py you only need to run this conversion script once to get a pytorch model you can then disregard the tensorflow checkpoint the three file starting with bert model ckpt but be sure to keep the configuration file bert config json and the vocabulary file vocab txt a these are needed for the pytorch model too to run this specific conversion script you will need to have tensorflow and pytorch installed pip install tensorflow the rest of the repository only requires pytorch here is an example of the conversion process for a pre trained bert base uncased model you can download google s pre trained model for the conversion here here is an example of the conversion process for a pre trained openai gpt model assuming that your numpy checkpoint save a the same format than openai pretrained model see here here is an example of the conversion process for a pre trained transformer xl model see here here is an example of the conversion process for a pre trained openai s gpt model tpu support and pretraining scriptstpu are not supported by the current stable release of pytorch however the next version of pytorch v should support training on tpu and is expected to be released soon see the recent official announcement we will add tpu support when this next release is published the original tensorflow code further comprises two script for pre training bert create pretraining data py and run pretraining py since pre training bert is a particularly expensive operation that basically requires one or several tpus to be completed in a reasonable amout of time see detail here we have decided to wait for the inclusion of tpu support in pytorch to convert these pre training script,"[('basic bert transformer model', 0.6374), ('bert openai gpt', 0.6209), ('model bert gpt gpt', 0.6158), ('bert pytorch model torch nn module', 0.6056), ('tensorflow checkpoint bert transformer xl', 0.6052), ('pytorch model openai gpt', 0.6047), ('pytorch bert', 0.5933), ('bert language model', 0.5791), ('openai gpt pytorch model class', 0.5773), ('basic openai gpt transformer model', 0.5703)]","[-1.56760857e-01 -1.04763083e-01  2.75051072e-02  6.86431304e-03
  6.13270625e-02  2.80084810e-03  2.21043383e-03  9.81701985e-02
 -1.95977016e-04 -6.27615005e-02 -1.76243037e-02  5.72458357e-02
 -6.69730455e-02  5.52043989e-02  3.64507176e-02  2.37252861e-02
 -1.57855581e-02  4.13788408e-02 -6.59902319e-02 -6.35639131e-02
  3.41981426e-02  5.87642081e-02  6.55158758e-02 -1.71515364e-02
 -1.84968971e-02 -3.99368405e-02  5.10075912e-02 -4.23665866e-02
  6.30076900e-02  4.89567667e-02  5.61288372e-03 -2.54554860e-02
 -4.97062430e-02  8.06156769e-02  2.22479533e-02 -1.37920873e-02
 -3.96109447e-02 -3.96833159e-02  2.22979486e-02  3.67135480e-02
  6.05692454e-02 -1.90512538e-02 -2.86293998e-02 -1.66374147e-02
  6.44685328e-02 -6.38181195e-02  3.00800819e-02 -7.66972527e-02
 -3.72719802e-02 -7.77855515e-02  5.82213933e-03 -4.59646583e-02
 -3.72111169e-03  8.76708254e-02  1.68059263e-02  5.67967668e-02
  1.17918467e-02 -8.31273943e-02  1.82814766e-02 -1.36311054e-01
 -5.42348661e-02 -1.13908015e-03 -4.33791317e-02 -2.46776138e-02
 -1.16217658e-01  9.06308591e-02 -5.21037430e-02  4.09568399e-02
  5.54249585e-02 -1.07253306e-01 -7.35811815e-02  4.37467285e-02
 -4.56787869e-02  3.64189707e-02 -1.67461007e-03 -9.23845842e-02
  1.01047479e-01 -3.97080556e-03 -3.24825607e-02 -5.46898581e-02
  2.49046460e-02 -5.50255226e-03  8.43271539e-02  2.23744474e-02
  4.11471985e-02 -7.15800142e-03 -5.21930121e-03  6.92278594e-02
  3.17639001e-02 -1.68368611e-02 -2.25455035e-02 -5.64657897e-02
  5.99222891e-02  3.83674959e-03  1.06812259e-02 -2.42792889e-02
  2.55319737e-02 -6.49963915e-02 -3.78730521e-02  4.58375588e-02
 -1.97912417e-02 -4.51134853e-02  3.39895338e-02  2.87199393e-02
 -2.60367449e-02  5.15494533e-02  6.91515720e-03  4.34148647e-02
 -8.44568480e-03 -6.18627155e-03  4.56660837e-02 -5.76185398e-02
  4.39193845e-02 -1.20183416e-01  2.45935451e-02  1.33992522e-03
 -1.20354861e-01  4.49789576e-02 -3.04500349e-02  8.28828216e-02
 -3.43588158e-03  5.15334159e-02  2.78630070e-02  4.19535488e-02
 -7.82054663e-02  4.70642671e-02 -2.95580514e-02  1.00475343e-32
 -1.49319938e-03  1.43798562e-02 -4.21672175e-03 -2.21569370e-02
  1.83496680e-02  5.96240833e-02  6.84990138e-02  1.00344103e-02
  6.58738464e-02 -7.78019726e-02 -3.74784172e-02  3.67058478e-02
 -9.05090570e-02  3.37599330e-02 -8.02285373e-02  4.46148664e-02
 -6.17902204e-02  9.08940583e-02  4.79905605e-02  4.94772643e-02
  8.65426585e-02  8.94292220e-02  1.84588321e-02  2.89609167e-03
 -9.79477614e-02  4.64128777e-02  3.09042120e-03 -6.01398125e-02
 -2.24240121e-05  2.45298017e-02 -1.12234190e-01  5.11105135e-02
 -1.34919528e-02 -1.69717725e-02  8.28410499e-03  2.24332921e-02
  1.05519770e-02 -6.05571270e-02 -2.29985308e-04 -9.50263590e-02
 -7.73754269e-02  5.70429824e-02 -8.80776253e-03 -6.23848625e-02
  2.39295475e-02  6.86372072e-03  2.11480297e-02  6.26460239e-02
  7.90133029e-02  1.17348656e-02 -4.34875265e-02  2.99589653e-02
 -1.33643791e-01  5.74771091e-02  4.50505093e-02 -3.66810709e-02
  6.60987571e-02  3.12175695e-02  5.55722490e-02  1.09868245e-02
  2.06270162e-02  2.33930144e-02  3.37744281e-02 -9.93474852e-03
  1.88836288e-02 -2.35443451e-02 -8.94808546e-02 -1.09727094e-02
 -1.50878401e-03  3.83937992e-02 -3.21621820e-02  3.13292779e-02
 -1.62106976e-02  2.38826331e-02  2.19705515e-02 -3.61287594e-02
  3.37763131e-02 -8.64272751e-03 -6.30287454e-02  1.65406018e-02
 -8.17116052e-02 -7.58324284e-03 -3.19744982e-02 -3.66063751e-02
 -1.01870030e-01 -1.21630859e-02  3.57637033e-02 -5.46495066e-05
  6.94715977e-03 -3.42632597e-03 -6.68290034e-02 -6.73232824e-02
  8.01904069e-04  6.07944690e-02 -3.61571573e-02 -7.41496106e-33
  1.51505275e-02  8.39106813e-02 -6.99087381e-02  5.90682626e-02
 -1.28536206e-02 -9.50758234e-02  2.49161236e-02  8.62922594e-02
  2.05087811e-02  4.02252264e-02  3.00858822e-02 -3.89696509e-02
  4.19579726e-03 -3.43688354e-02  9.58596095e-02 -6.51267618e-02
 -3.37772295e-02  7.21130287e-03 -2.49682534e-02  2.57955678e-02
 -2.00174544e-02  2.07854640e-02 -1.23319529e-01 -1.31898718e-02
 -3.35094854e-02  6.28074184e-02 -7.07420483e-02  4.05191556e-02
  2.93655433e-02  1.13978591e-02  4.15337794e-02  6.05654791e-02
  2.51350156e-03  5.61516434e-02 -5.13864532e-02  6.34992793e-02
  7.66335875e-02 -6.34066155e-03 -1.22814509e-03 -1.27798086e-02
  8.58691037e-02 -2.51116361e-02  2.82137026e-03  7.08743408e-02
 -6.43639341e-02  1.90041233e-02 -7.31112733e-02 -2.38679000e-03
 -2.87633613e-02 -5.21618836e-02 -7.29872892e-03 -1.76198054e-02
 -3.64554115e-02 -2.80817933e-02 -1.40338130e-02 -5.24837673e-02
  1.21696293e-01 -1.28598288e-02 -2.85647083e-02 -5.04027354e-04
 -7.49328407e-03 -3.57659869e-02  7.18691647e-02 -2.04338320e-02
 -2.95922291e-02 -7.06823468e-02 -2.25089621e-02  5.60652241e-02
  4.04497311e-02  6.10051975e-02 -1.17455080e-01 -8.08540080e-03
  1.02700964e-01  2.24329568e-02  2.97051929e-02  3.03369407e-02
  5.73012559e-03 -1.96387693e-02  6.96671978e-02 -2.60033105e-02
 -3.43288891e-02 -5.93519807e-02  3.16971987e-02  2.75193807e-02
  1.29638575e-02  3.30383889e-02  8.72271284e-02  2.25940347e-03
  3.14075798e-02  5.96393421e-02 -7.99197108e-02  2.52373610e-02
  6.85296282e-02  9.70348194e-02  4.49820459e-02 -3.25279998e-08
 -7.42176026e-02 -4.09606751e-03 -2.80481596e-02  5.13317063e-02
 -3.24887745e-02 -4.34013195e-02 -1.27283530e-02  9.78274867e-02
 -5.74011132e-02  3.44519317e-02  1.68694612e-02 -3.05148978e-02
 -6.09170869e-02  7.86318444e-03  1.40303029e-02  8.54080394e-02
 -3.83132300e-03  2.00766511e-02  1.03129242e-02 -5.85900322e-02
 -2.93656960e-02  3.35293226e-02  2.52088048e-02 -4.04343382e-02
 -6.27574651e-03 -3.39768119e-02 -2.48016268e-02  9.57383588e-02
 -5.52710611e-03  3.62538211e-02  2.95867864e-02  6.85007684e-03
 -1.13831013e-02 -2.33965479e-02  8.13965797e-02  7.24046305e-02
  1.36177596e-02 -2.61435024e-02  1.42127192e-02  5.20633720e-02
  1.72421765e-02  4.41052625e-03 -9.38021764e-02 -1.33688739e-02
  5.63864037e-02 -4.43641003e-03 -6.37164637e-02 -1.27973065e-01
 -4.67351563e-02  6.07866459e-02 -2.32237857e-02 -2.62873098e-02
 -3.62876356e-02  5.71360737e-02 -8.68561566e-02  5.05250208e-02
 -5.61719872e-02 -6.73107896e-03 -2.14872062e-02 -1.90842114e-02
 -4.64982577e-02  9.53018889e-02  2.29367595e-02  5.18115014e-02]",2,2
nfp,1,kera layer for end to end learning on molecular structure based on kera tensorflow and rdkit source code used in the study message passing neural network for high throughput polymer screeningthis library extends kera with additional layer for handling molecular structure i e graph based input there a strong familiarity with kera is recommended an overview of how to build a model is shown in example solubility test graph output ipynb model can optionally include d molecular geometry a simple example of a network using d geometry is found in example model d coordinate ipynb the current state of the art architecture on qm published in is included in example schnet edgeupdate py this script requires qm preprocessing to be run before the model is evaluated with example preprocess qm py,"[('kera tensorflow', 0.5658), ('example solubility test graph output ipynb model', 0.5356), ('example model d coordinate ipynb', 0.4296), ('kera layer', 0.4286), ('high throughput polymer screeningthis library', 0.4192), ('example schnet edgeupdate py', 0.3818), ('molecular geometry', 0.3719), ('molecular structure', 0.3677), ('kera', 0.3098), ('rdkit source code', 0.2915)]","[-1.03108205e-01 -4.95397262e-02 -2.40060762e-02 -3.64169367e-02
  2.13385839e-02 -4.22211960e-02 -8.61413330e-02  2.25534122e-02
 -9.89351571e-02 -1.84293184e-02 -3.09303924e-02  1.98685639e-02
 -6.88765571e-02 -1.58530250e-02  2.87230097e-04 -9.29447729e-03
 -2.71630138e-02  4.92422730e-02 -7.96856359e-03 -7.67600685e-02
 -1.23329705e-03 -4.08607610e-02  4.78958152e-03  1.00007243e-02
 -1.84518192e-02  3.37186456e-02  5.16498014e-02 -1.57827679e-02
  4.62286733e-02 -8.93089473e-02  6.71809390e-02  7.80415833e-02
 -1.01062795e-02  5.86336181e-02  3.09821181e-02  2.05556694e-02
 -4.98815887e-02 -1.26337558e-02 -4.83271480e-02 -8.81172717e-03
  2.46630926e-02 -2.94256695e-02  3.08893528e-03 -4.81504798e-02
  6.49361238e-02 -9.17105377e-02 -8.98605585e-02 -3.19940858e-02
  1.09655438e-02 -7.77222514e-02 -6.68762922e-02 -1.08574107e-01
 -5.27959578e-02 -2.45456938e-02  5.52713871e-02 -3.92485149e-02
  1.10542718e-02 -6.34433329e-02  9.99915507e-03 -4.36591506e-02
  5.48280589e-02  2.40980014e-02 -5.92086650e-02  4.17483374e-02
  6.94332495e-02  1.03246951e-02  4.25333716e-02  7.86836371e-02
  7.84357414e-02 -2.15987042e-02 -6.77817464e-02  1.25818504e-02
 -7.03832805e-02  7.65621811e-02  3.72465551e-02  6.03700150e-03
  6.54284507e-02  1.92350941e-03  2.11172104e-02 -2.79521458e-02
 -7.12200776e-02 -7.17720948e-03  1.59022845e-02  3.66252474e-02
 -2.89614126e-02  1.34320641e-02 -3.21091190e-02  3.27129327e-02
  3.67306173e-02 -5.64994700e-02  7.84561113e-02  4.01481502e-02
 -1.12826422e-01 -3.01369671e-02 -1.56247420e-02  6.29372224e-02
  4.42314036e-02 -2.63725184e-02 -2.72461809e-02  3.56383733e-02
 -4.56661498e-03 -8.22577402e-02 -3.12232245e-02  6.33643148e-03
 -5.68073355e-02 -1.08351198e-03  6.25949204e-02 -4.74105682e-03
  5.06677628e-02  5.64198680e-02 -7.28019327e-02  3.37307304e-02
 -4.27303687e-02 -3.19240503e-02  7.63468668e-02 -3.67781371e-02
  2.95487954e-03  6.93813199e-03  5.09471409e-02  4.32210378e-02
 -9.72083136e-02  4.29510400e-02  1.79060306e-02  2.56959093e-03
 -4.30381894e-02  5.02680168e-02 -9.32211354e-02  7.73486450e-33
  4.07301486e-02 -3.93664427e-02  3.49987298e-02 -3.70906740e-02
  6.93413466e-02 -1.02443978e-01  3.43666002e-02 -3.57508920e-02
 -9.73365456e-02 -1.65819116e-02 -1.32739335e-01  3.02912835e-02
 -2.16441713e-02  6.35634363e-02 -4.18472961e-02 -4.69741784e-02
 -3.20300311e-02  6.97239265e-02 -2.69720028e-03  5.72346337e-02
  5.44074066e-02 -5.39650172e-02  4.01728377e-02  1.06693834e-01
  2.99129225e-02 -1.11784004e-02 -1.87513307e-02 -1.71232212e-04
 -1.53138610e-02  2.89017167e-02 -6.11096472e-02  3.92648652e-02
 -3.75343524e-02  3.47439088e-02 -7.36923590e-02  2.88443454e-03
 -7.50920698e-02 -5.05986512e-02 -7.53305899e-03 -7.29116052e-02
 -2.36641318e-02  1.13652563e-02  4.69731055e-02 -5.89023568e-02
 -5.09489849e-02  4.87099104e-02 -7.33835846e-02  7.31564090e-02
  5.58716245e-02  1.49826910e-02 -4.43287566e-02  3.79876643e-02
 -3.90209146e-02  1.57295149e-02 -7.09476089e-03  2.43387427e-02
  9.61271673e-02  7.12004863e-03  7.01329336e-02  1.26563266e-01
 -3.83877195e-02  5.60448393e-02 -1.73883908e-03 -1.88367702e-02
  2.49141268e-02  8.58494546e-03 -1.71729818e-01 -9.06130672e-03
  2.06211582e-02  3.97490934e-02 -8.76415893e-02  8.82383063e-02
 -1.36771402e-03 -6.30372670e-03  7.37493038e-02 -4.34096456e-02
 -1.90845858e-02 -3.16594020e-02 -3.56299765e-02  2.30383277e-02
 -5.42419553e-02 -1.32191479e-02  3.52872610e-02 -4.02135700e-02
 -2.04944331e-02 -1.00421257e-01  5.66776935e-03 -3.08120740e-03
  7.28209363e-03 -6.45960346e-02  1.27638886e-02 -5.13153076e-02
  3.58471572e-02 -4.12392020e-02  3.92664364e-03 -7.00788884e-33
  1.44176893e-02  5.40890321e-02 -8.01010523e-03  6.41777292e-02
  7.87314624e-02  5.78754069e-03  4.57581915e-02  2.74795424e-02
  7.06523731e-02  3.19668055e-02  5.25601879e-02 -2.46433113e-02
  8.65906999e-02 -1.78790819e-02  2.83703580e-03 -4.70967777e-02
 -4.46060076e-02 -8.15296844e-02  3.28942649e-02 -6.43180171e-03
 -9.17581692e-02  8.86002183e-02 -7.40083307e-02 -5.20675555e-02
 -2.75790300e-02 -2.28968766e-02 -3.17434967e-02  2.26794798e-02
 -4.85826144e-03 -4.58048619e-02  3.13895755e-02  4.07045968e-02
 -1.01429969e-01  6.78343102e-02  1.51364254e-02  3.52777354e-02
  7.62222940e-03 -1.90310497e-02  3.67731825e-02 -8.85685068e-03
  7.52626061e-02  7.23052919e-02 -6.04229644e-02 -7.99079333e-03
  1.65311508e-02  5.95588461e-02 -5.77426665e-02  3.72131877e-02
  6.86726184e-04 -8.17200541e-02 -1.04938885e-02  1.81100089e-02
  9.35878903e-02  1.09808641e-02 -6.35271706e-03  1.66749489e-02
  5.44985272e-02  4.38331924e-02  2.96492949e-02  3.38581055e-02
 -6.44987151e-02 -6.81867898e-02  9.19187441e-03  4.30823304e-03
 -4.41174619e-02 -7.45934919e-02 -5.41833863e-02  3.26233357e-02
 -7.67261256e-03 -8.30604415e-03  1.19150151e-02  4.60458808e-02
  7.04022869e-02  8.84182975e-02  4.53595407e-02 -4.50468101e-02
 -3.38065326e-02  6.25833943e-02 -8.43178015e-03  9.99320298e-03
  2.57667061e-02  3.51151191e-02 -5.78185031e-03  9.29344520e-02
  8.75005201e-02  5.63458577e-02  4.36721146e-02  1.00155547e-02
  5.08234650e-02  1.68307982e-02 -4.23022918e-03  4.68629822e-02
 -2.42467746e-02  1.65791661e-01  8.95781666e-02 -3.51073624e-08
 -1.79927982e-02 -2.21294686e-02  2.03029532e-02 -2.52976473e-02
 -4.34487686e-03  5.92692718e-02  9.15897340e-02  7.51987249e-02
  2.51234807e-02  1.17081981e-02 -3.72259393e-02  2.61399839e-02
 -7.16302842e-02  1.63132362e-02 -1.92700396e-03  7.41546825e-02
 -7.52412993e-03  5.19257672e-02  1.09503791e-02 -6.79999739e-02
  1.94748975e-02 -3.18795326e-03  1.95257664e-02  3.47578265e-02
 -7.10494956e-03  6.18695468e-03  5.65531515e-02  2.13166606e-02
 -4.76844907e-02 -6.10896572e-02 -5.35696931e-02  4.71668877e-03
  7.69209340e-02 -3.06172296e-02  9.46225449e-02  1.05102457e-01
 -2.33924016e-02  3.10081486e-02  2.24089995e-03  1.38815969e-01
 -8.43145475e-02  8.14280100e-03 -4.50547189e-02 -7.84670338e-02
  4.80274521e-02  4.13357317e-02 -5.15120551e-02 -1.24365352e-02
 -2.00166069e-02 -2.08818652e-02 -9.28327907e-03  1.12428749e-02
 -7.68121779e-02 -9.75784101e-03  1.14404904e-02  8.63837004e-02
 -1.06209710e-01 -6.95543811e-02  1.56645551e-02 -8.05242807e-02
  3.89109850e-02  5.32742329e-02  1.17212282e-02  2.22937949e-02]",2,2
ck,1,while machine learning is becoming more and more important in everyday life designing efficient ml system and deploying them in the real world is becoming increasingly challenging time consuming and costly researcher and engineer must keep pace with rapidly evolving software stack and a cambrian explosion of hardware platform from the cloud to the edge such platform have their own specific library framework apis and specification and often require repetitive tedious and ad hoc optimization of the whole model software hardware stack to trade off accuracy latency throughout power consumption size and cost depending on user requirement and constraint the collective knowledge framework ck is our attempt to develop a common plug play infrastructure that can be used by the community similar to wikipedia to learn how to solve above challenge and make it easier to co design benchmark optimize and deploy machine learning system in the real world across continuously evolving software hardware and data set see our acm techtalk for more detail ck aim at providing a simple playground with minimal software dependency to help researcher and practitioner share their knowledge in the form of reusable automation recipe with a unified python api cli and meta description ck help to organize software project and git repository a a database of above automation recipe and related artifact based on fair principle a described in our journal article shorter pre print see example of ck compatible github repository we collaborated with the community to reproduce ml and system paper and implement the following reusable automation recipe in the ck format portable meta package manager to automatically detect install or rebuild various ml artifact ml model data set framework library etc across different platform and operating system including linux window macos and android portable manager for python virtual environment ck repo portable workflow to support collaborative reproducible and cross platform benchmarking portable workflow to automate mlperf benchmark please contact grigori fursin if you are interested to join this community effort the latest version of the ck automation suite supported by mlcommons we plan to develop a new version of the ck framework v within the mlcommons design space exploration workgroup please contact grigori fursin to join this community effort follow this guide to install ck framework on your platform ck support the following platform here we show how to pull a github repo in the ck format and use a unified ck interface to compile and run any program image corner detection in our case with any compatible data set on any compatible platform you can check output of this program in the following directory you can now view this image with detected corner check ck doc for further detail we have prepared adaptive ck container to demonstrate mlops capability you can run them a follows you can create multiple virtual ck environment with template to automatically install different ck package and workflow for example for mlperf inference all ck module automation action and workflow are accessible a a micro service with a unified json i o api to make it easier to integrate them with web service and ci platform a described here we have developed the cknowledge io portal to help the community organize and find all the ck workflow and component similar to pypi the community provides docker container to test ck and component using different ml sw hw stack dse user can extend the ck functionality via ck module or external github reposities in the ck format a described here please check this documentation if you want to extend the ck core functionality and module note that we plan to redesign the ck framework to be more pythonic we wrote the first prototype without oo to be able to port it to bare metal device in c but eventually we decided to drop this idea please contact grigori fursin to join this community effort we would like to thank all contributor and collaborator for their support fruitful discussion and useful feedback see more acknowledgment in the ck journal article,"[('collective knowledge framework ck', 0.4967), ('ck automation suite', 0.4582), ('machine learning system', 0.4296), ('hardware platform', 0.4233), ('whole model software hardware stack', 0.4161), ('software project', 0.4155), ('software stack', 0.4151), ('ck framework', 0.4146), ('mlops capability', 0.4145), ('platform ck', 0.4023)]","[-2.9268624e-02 -6.6810623e-02  4.2178361e-03 -6.6662259e-02
 -5.4822350e-04  6.1989208e-03 -3.5923485e-02  2.7766703e-03
 -4.1090108e-02  3.1615075e-02 -1.9488771e-02 -3.4754120e-02
  2.6213951e-02 -3.8752530e-02  2.0523379e-02 -5.5332731e-02
  9.9693136e-03  3.7094221e-02 -3.9482225e-02 -1.1346661e-01
 -7.9729259e-02 -3.1807005e-02 -1.3397945e-02 -5.1419148e-03
  5.7038632e-03  4.4523157e-02 -4.9571659e-02 -3.3092685e-02
  7.9720333e-02 -4.9758438e-02 -8.0387294e-02  1.3626528e-02
  5.6853905e-02 -7.5196279e-03  3.1993292e-02  8.0107972e-02
  1.9995815e-03 -3.0743310e-02 -4.6875685e-02 -4.4528894e-02
 -6.8381965e-02 -7.6125905e-02 -1.3275697e-02 -1.0954567e-02
  8.1517801e-02  3.8342994e-02 -3.2824770e-02 -6.8018936e-02
 -2.4950715e-02 -5.5781193e-03 -6.5140821e-02 -1.6240063e-01
  4.6481218e-02  5.1777769e-02 -2.3510024e-02 -1.8440960e-02
 -4.9312748e-02  2.0819532e-03  4.4753723e-02 -2.0564765e-02
 -5.5294838e-02 -5.8810998e-02 -1.0975128e-01  6.9795541e-02
  7.0393835e-03  4.4569995e-02  7.8246212e-03  6.2690780e-02
  3.5160642e-02 -5.0388850e-02 -5.5396404e-02 -6.1436303e-02
 -8.2752388e-03  8.1093565e-02  5.5312581e-02  1.0023746e-02
  3.0656423e-02 -1.9158980e-02  6.4532652e-02 -7.7211872e-02
  4.4886284e-02  7.6743096e-02  4.7964007e-03  3.9882228e-02
 -4.6803676e-02  4.7755786e-03  5.6129578e-03  3.0472608e-02
 -3.9389949e-02 -3.1676888e-02 -2.4397008e-02 -5.2158318e-02
  1.3164550e-01 -7.4333794e-02  2.2452708e-02  1.8678477e-02
  3.5550773e-02 -2.0799505e-02 -1.4783311e-02  3.0246127e-02
 -8.4536716e-02  1.7581606e-02  4.2803921e-02 -1.8365214e-02
 -4.1399878e-02  2.0132188e-02 -6.5535672e-02  3.8217369e-02
  1.0872773e-01 -6.1099518e-02 -6.0271367e-02  1.5346785e-02
 -5.5822223e-02 -1.3292928e-01  8.3883613e-02 -8.1322499e-02
 -1.0773421e-02  7.7596465e-03  7.7585563e-02  7.4518599e-02
 -7.6926000e-02 -1.8530935e-02  1.1931906e-04 -4.5451103e-03
  2.0715669e-02  3.6834013e-02 -6.7560628e-02  8.9146241e-34
  2.4717050e-02  3.8897399e-02  3.7803143e-02 -5.8731265e-02
  9.2764921e-02 -1.3077809e-01 -1.0952577e-02  1.7095540e-02
 -7.6037318e-02 -1.2310992e-02  3.6291495e-02  9.1857217e-02
 -4.4595521e-02  5.1130792e-03  7.2775006e-02 -2.3254041e-02
  2.3364065e-02  3.3909123e-02  2.7517477e-02  1.5229783e-02
  7.6283999e-02  2.2700418e-02 -2.8220522e-03  7.1722105e-02
  2.8254922e-02 -2.6015267e-02  1.6152695e-02  4.9610943e-02
  2.8423583e-03  2.9099930e-02 -5.4723978e-02  3.5701312e-02
 -1.8852631e-02 -2.0205993e-02 -6.0286388e-02 -2.3411615e-02
 -2.3536845e-03 -9.9771790e-02  4.6081431e-02 -7.5721340e-03
 -3.2840386e-02 -1.5009265e-02 -2.9535480e-02 -5.1017571e-02
  4.7495451e-02 -1.0681775e-02  2.5403006e-03 -2.1250624e-02
  6.0661457e-02 -9.2927739e-03 -2.9256465e-02 -5.8052321e-03
  3.4421053e-02  2.0660890e-02  5.4739486e-02  7.4580945e-02
  3.6200736e-02 -2.3185389e-02  1.3771978e-02  7.1842149e-02
 -5.8714751e-02  1.5646996e-02  4.2117387e-02  3.4553371e-03
  1.0380724e-03 -6.1511137e-02 -1.7389337e-02 -4.0378258e-02
 -3.0468035e-02  7.2957046e-02 -5.1984940e-02 -8.7209931e-03
  7.1525253e-02 -1.1893859e-02  2.3331434e-02 -1.8313350e-02
 -1.2631421e-01 -3.6187474e-02 -8.1655435e-02  4.5432847e-02
 -6.6461243e-02  1.1274852e-02 -3.1403512e-02  2.0280411e-03
  1.0420531e-02 -3.4801520e-02 -3.4251098e-02 -3.0595886e-02
 -6.9182701e-02 -9.7681135e-02 -9.8260470e-02 -5.3003803e-02
  3.8643837e-02  5.0637390e-02 -7.8249618e-02 -1.7872356e-33
  4.5564324e-02  2.7437309e-02 -2.0326162e-02  1.2537408e-01
  8.3451256e-02  7.5963534e-02  3.8772210e-02 -2.2269113e-03
 -1.1148829e-02  7.7606663e-02  8.6321257e-02 -2.0405183e-02
  3.2798588e-02  1.6044868e-03 -2.3621492e-02  2.5910663e-03
 -8.6192295e-02 -7.5231396e-02  7.1564041e-02  6.1565556e-02
 -3.9528571e-02 -6.2245235e-02 -5.1099914e-03  1.3445921e-01
  9.2165239e-02 -9.2082117e-03 -9.5202982e-02  4.5256566e-02
  4.3816410e-02  6.3823752e-02  1.6165983e-02 -2.8473323e-02
  2.7347490e-02  1.7363807e-02  2.7019192e-02 -7.8097276e-02
  1.2809668e-02 -2.1881273e-02  2.3979438e-02 -4.3088924e-02
  9.8431572e-02 -3.8196992e-02 -1.3330747e-02 -1.4416590e-02
 -1.9645005e-02 -5.1665854e-02 -6.8796031e-02  1.1446056e-02
  5.7932980e-02 -1.4773050e-01  2.2492376e-03  2.6331916e-02
  2.1483791e-03 -2.5667015e-02 -9.0931967e-02  8.8625342e-02
  5.0964713e-02  9.8792367e-02  2.5284666e-04  6.9058731e-02
 -2.2202409e-03 -6.9727995e-02  5.2351777e-02  8.9134559e-02
  2.8139167e-02 -2.1790950e-02 -7.1023151e-02  9.1049425e-02
 -7.8902692e-02 -4.6672452e-02 -4.0544368e-02  4.3798473e-02
  4.6396535e-02 -2.8561065e-02 -3.3012580e-02 -1.8414203e-02
 -4.3193852e-03 -1.9006852e-02 -1.2048444e-02  2.6303001e-02
  3.8206574e-02 -2.2205602e-02 -1.4098739e-02  1.0505903e-01
 -1.3084131e-02  5.7348959e-02  6.2723160e-02 -6.9795540e-03
  7.9628276e-03 -2.9510438e-02 -3.7163589e-02 -1.2625131e-03
  4.2342499e-02  1.4571734e-01 -2.4067031e-02 -2.4728340e-08
  6.3372642e-02  1.8375874e-02 -6.6068741e-03  9.9121621e-03
  5.4153059e-02 -6.3708471e-03 -1.5352467e-02 -1.2025511e-02
 -4.0808521e-02  5.2206356e-02 -1.4099508e-02  1.5746498e-02
 -8.0117516e-02  4.2704012e-02  5.7940245e-02  1.0660584e-02
 -3.8989622e-02  2.9704427e-02  8.1056757e-03 -4.0369257e-02
  1.0169323e-01  2.3243528e-02  2.1056883e-02  1.5861005e-02
  2.3278324e-02 -1.8388929e-02  1.4370468e-02  5.9809085e-02
  3.8731430e-02  8.2077608e-02 -3.6822613e-02 -3.1048516e-02
  1.4711533e-02 -1.7642224e-02  7.2891705e-02  4.5569982e-02
 -2.1844994e-02 -3.3720881e-02 -1.5949124e-03  2.2439413e-02
 -1.2781707e-03  2.7941827e-02 -2.4376651e-02 -1.8791270e-02
 -5.6025840e-02  4.3233234e-02  3.6367741e-03 -2.8157621e-03
 -9.6934065e-03  1.9912399e-02 -3.1348009e-02  8.7647542e-02
 -6.5168746e-02  6.6742562e-02  6.0542531e-02  3.9059293e-02
  2.1446602e-02 -9.0915479e-02  5.6931622e-02 -9.8034106e-03
  5.8082700e-02 -5.3091746e-02  7.4248753e-02  1.4472985e-02]",2,0
aim,1,about feature demo example quick start documentation roadmap slack community twitter aim is an open source self hosted ml experiment tracking tool it s good at tracking lot s of training run and it allows you to compare them with a performant and beautiful ui you can use not only the great aim ui but also it sdk to query your run metadata programmatically that s especially useful for automation and additional analysis on a jupyter notebook aim s mission is to democratize ai dev tool follow the step below to get started with aim install aim on your training environment integrate aim with your codesee the full list of supported trackable object e g image text etc here run the training a usual and start aim ui or query run programmatically via sdksee documentation here see documentation here see documentation here see documentation here see documentation here see documentation here see documentation here see documentation here see documentation here training run comparisonorder of magnitude faster training run comparison with aimscalabilitybeloved tb visualization to be added on aimmlflow is an end to end ml lifecycle tool aim is focused on training tracking the main difference of aim and mlflow are around the ui scalability and run comparison feature run comparisonui scalabilityhosted v self hosted sparkle the aim product roadmapthe high level feature we are going to work on the next few monthsaim uisdk and storageintegrations,"[('ai dev tool', 0.5005), ('documentation roadmap slack community twitter', 0.4887), ('aimmlflow', 0.4456), ('great aim ui', 0.4358), ('end ml lifecycle tool aim', 0.4271), ('sdksee documentation', 0.4235), ('aim product', 0.4085), ('jupyter notebook aim', 0.3915), ('aim install', 0.3797), ('automation', 0.357)]","[-4.41242494e-02 -4.69292961e-02  4.17623296e-02 -2.30209734e-02
  9.91805121e-02 -5.30737266e-02  5.07480986e-02  6.91481680e-02
  4.07381579e-02  7.69490525e-02  1.71306320e-02 -1.46087781e-02
 -5.06416615e-03 -2.31701713e-02  8.61697942e-02  6.49217963e-02
 -3.12022995e-02 -5.16204648e-02  2.96254512e-02 -9.88732427e-02
 -3.05211022e-02  4.67479154e-02 -4.05978085e-03  1.37968399e-02
  2.68577915e-02  1.28360335e-02  2.11687796e-02 -3.99554744e-02
 -6.76524069e-04 -2.44164970e-02 -7.29192346e-02 -5.99207822e-03
  4.50144857e-02  2.52578896e-03  7.72832986e-03  2.38210112e-02
  3.21622659e-03 -7.62487482e-03  1.89707521e-02 -9.85715687e-02
 -4.86852080e-02 -7.61402547e-02 -3.50276828e-02 -5.78083806e-02
  6.20473772e-02  2.70421188e-02 -2.32512485e-02 -8.60224813e-02
 -1.31857069e-02  7.64166489e-02 -9.50159132e-02 -1.13248818e-01
 -1.81617513e-02  3.28280441e-02 -1.15640210e-02 -4.84979711e-03
 -5.60881337e-03  7.11782128e-02  9.42784846e-02  5.47034014e-03
 -1.07828714e-03 -2.13808734e-02 -1.50201749e-02  1.50654009e-02
 -1.86678059e-02 -5.67689491e-03 -4.09674691e-03  2.29936652e-02
  7.43532553e-02 -5.45664206e-02 -5.74973114e-02  1.73047557e-02
 -1.78972371e-02 -2.89975258e-04  2.57192049e-02 -1.60672199e-02
  2.63653621e-02 -6.23292997e-02  1.08580980e-02 -8.61240551e-02
 -5.39840534e-02 -3.76174389e-03 -6.20614886e-02  5.63749894e-02
 -5.32063320e-02  2.01378819e-02  2.62617320e-02  9.74102318e-03
  2.83756629e-02  5.14065325e-02  1.75513849e-02 -3.45572643e-02
  5.26828915e-02 -8.77518393e-03 -2.19603181e-02 -4.16771183e-03
 -1.73141691e-03 -1.05318710e-01 -6.73257038e-02  4.68911305e-02
 -8.60880390e-02  2.45143170e-03  8.73915292e-03 -5.38007505e-02
 -4.35925163e-02  4.98928800e-02  2.54985169e-02 -9.25216079e-02
  9.06019658e-02  5.08533381e-02 -3.62117067e-02 -5.90986237e-02
 -3.49365100e-02 -9.26195607e-02  1.27045676e-01 -1.44347884e-02
 -4.79383133e-02  7.25947842e-02  5.77285998e-02  1.02008116e-02
 -3.89255695e-02  4.60271984e-02  1.50997145e-02  2.78356764e-02
  4.80578393e-02  9.54045430e-02 -6.29530177e-02  4.56267838e-33
  6.68203011e-02  1.70042552e-02 -5.96585050e-02  6.42127022e-02
  3.40247341e-02 -1.20881215e-01  3.26495543e-02  3.16115469e-02
 -4.92675975e-02 -6.91023767e-02 -4.31118235e-02  3.04554999e-02
 -7.28632286e-02  6.90790713e-02  8.63717273e-02 -7.80811086e-02
 -1.50553212e-02  2.66505126e-02 -2.67000776e-02  5.82914650e-02
 -2.36899983e-02 -7.43939653e-02  2.59820968e-02  8.44392926e-02
  1.12329654e-01  2.38624979e-02  4.47514094e-02 -9.87098925e-03
  8.40350706e-03  1.65636279e-02 -5.70230074e-02  4.63479906e-02
 -1.23566324e-02 -3.29625793e-03 -1.64470132e-02  7.86196627e-03
 -6.91741481e-02 -3.97416055e-02  1.02224126e-02  5.31072579e-02
 -1.04153231e-01  7.14511052e-02  1.09545439e-02 -6.49261847e-02
  7.04957321e-02 -1.64762083e-02  4.68522264e-03 -2.25997567e-02
  1.60088897e-01 -3.22432294e-02 -5.79458922e-02  1.07452404e-02
  6.14544749e-02  3.33608948e-02 -3.19017805e-02 -9.43938345e-02
 -2.14681607e-02  1.18038906e-02  5.10843135e-02  6.50668889e-02
 -2.25760229e-02 -2.06065155e-03  3.02606858e-02  3.93173024e-02
  2.31024693e-03  4.89608198e-02  1.01184107e-01  4.96727265e-02
  5.51373623e-02 -6.50320761e-03 -5.55637106e-02  1.59671642e-02
  5.30001968e-02 -4.94889468e-02 -8.84693936e-02 -3.63311148e-03
 -4.46225842e-03 -7.51816481e-02  1.82621591e-02 -3.34277712e-02
 -6.68379515e-02 -1.97785925e-02 -1.38134593e-02 -2.69714128e-02
  6.28398061e-02 -8.70238990e-03 -1.13618118e-03 -3.35090868e-02
 -3.32531407e-02  1.80070698e-02 -6.72749355e-02  4.44459207e-02
 -4.82529774e-02  7.99188092e-02 -1.20049216e-01 -2.51996693e-33
 -2.58203242e-02 -2.08014678e-02 -7.21344650e-02  9.03441235e-02
  2.52701342e-02  3.97173315e-02  2.57996488e-02 -3.05265468e-02
  5.38062910e-03  8.88418406e-02  5.27098449e-03 -4.61746044e-02
 -2.81920433e-02  1.71144661e-02  3.40784565e-02 -4.42439131e-02
  1.33715337e-02 -8.49376842e-02  2.19343621e-02 -1.57485111e-03
 -3.41005214e-02  4.39106934e-02 -1.38354795e-02 -4.53049429e-02
  5.55012673e-02 -1.34307491e-02 -9.56236273e-02  2.94966716e-03
 -6.37340499e-03 -2.20460985e-02  9.30482894e-02  4.58152704e-02
  5.42045943e-03  7.13700138e-04  5.81634603e-02 -1.62988342e-02
  6.48942515e-02 -4.54833955e-02 -3.85449873e-03 -2.84567159e-02
  1.47613719e-01 -4.36876453e-02 -1.11830374e-02 -3.94652179e-03
 -5.49616031e-02  3.07506397e-02 -4.86138500e-02  4.07712013e-02
 -8.27530548e-02 -2.92528439e-02  3.81568074e-02 -1.98695567e-02
  1.82217602e-02 -1.13284513e-01 -8.59956965e-02 -3.46601717e-02
  4.27227691e-02  8.55435338e-03 -4.63256352e-02  5.76746697e-03
  1.63711775e-02  8.47186148e-03  6.42461553e-02  3.63724306e-02
 -1.13510191e-02  1.35811092e-02  4.08760794e-02  5.16720265e-02
 -1.62449360e-01 -3.97645794e-02  3.32210101e-02  9.20246728e-03
  2.02971790e-02 -1.20287505e-03 -1.67986769e-02 -2.07116436e-02
 -1.89539157e-02 -4.42765579e-02 -5.92392730e-03 -7.37140179e-02
  2.65609939e-02  3.41135897e-02 -1.40322559e-02  4.43111844e-02
 -2.63843983e-02  9.37544927e-02 -1.81827992e-02  6.37565702e-02
  4.16389219e-02  4.95436378e-02 -3.62330303e-02  5.06748073e-02
  2.10236944e-02  1.05672888e-01 -4.77199219e-02 -2.69485803e-08
  1.12698330e-02  8.55025370e-03  2.01840093e-03  6.05929531e-02
  4.99990862e-03  1.23946294e-02 -1.16305146e-02  1.29273668e-01
  3.19484361e-02  6.00891784e-02 -6.81494130e-03 -1.93999130e-02
 -1.00882277e-01  7.17433393e-02  2.92649064e-02 -2.02637650e-02
 -5.35373874e-02  3.14400941e-02 -9.16000269e-03 -1.14206597e-01
  6.32847026e-02  5.99149726e-02 -4.27895673e-02 -5.36247529e-02
  2.26218570e-02 -5.97052574e-02 -4.30516787e-02  6.59594685e-02
 -1.85098555e-02 -8.91632866e-03  1.69839133e-02 -1.49596371e-02
  4.91668545e-02 -1.22242663e-02  6.64675906e-02  2.63029821e-02
  2.60379277e-02 -1.34861730e-02 -2.67898627e-02  7.92814791e-02
 -4.38090041e-02  6.88697994e-02  1.69865172e-02 -7.61516765e-02
 -4.51913066e-02  3.33778467e-03 -7.23565519e-02 -7.62215778e-02
  6.44518482e-03 -6.79063052e-02 -5.00047319e-02 -1.37171363e-02
  1.16181746e-02  7.94267505e-02  7.97397494e-02  1.89019740e-02
  3.93984979e-03 -2.39142049e-02  9.37625095e-02  2.22714804e-02
  1.01260372e-01  7.37303356e-03 -1.04524372e-02  3.42318751e-02]",2,0
skorch,1,a scikit learn compatible neural network library that wrap pytorch documentationsource codeinstallationto see more elaborate example look here in an sklearn pipeline with grid search skorch also provides many convenient feature among others learning rate scheduler warm restarts cyclic lr and many more scoring using sklearn and custom scoring functionsearly stoppingcheckpointingparameter freezing unfreezingprogress bar for cli a well a jupyter automatic inference of cli parametersintegration with gpytorch for gaussian processesskorch requires python or higher you need a working conda installation get the correct miniconda for your system from here to install skorch you need to use the conda forge channel we recommend to use a conda virtual environment note the conda channel is not managed by the skorch maintainer more information is available here to install with pip run again we recommend to use a virtual environment for this if you would like to use the most recent addition to skorch or help development you should install skorch from source to install skorch from source using conda proceed a follows if you want to help developing run for pip follow these instruction instead if you want to help developing run pytorch is not covered by the dependency since the pytorch version you need is dependent on your o and device for installation instruction for pytorch visit the pytorch website skorch officially support the last four minor pytorch version which currently are however that doesn t mean that older version don t work just that they aren t tested since skorch mostly relies on the stable part of the pytorch api older pytorch version should work fine in general running this to install pytorch should work assuming cuda jakubczakon blog post creator and core contributor talk about their model training library from pytorch ecosystem benjaminbossan talk skorch a scikit learn compatible neural network library at pycon pydata githubnemo poster for the pytorch developer conference thomasjpfan talk skorch a union of scikit learn and pytorch at scipy thomasjpfan talk skorch a union of scikit learn and pytorch at pydata github issue bug report feature request install issue rfcs thought etc slack we run the skorch channel on the pytorch slack server for which you can request access here,"[('sklearn pipeline', 0.5845), ('pytorch website skorch', 0.5306), ('pytorch developer conference thomasjpfan talk skorch', 0.5213), ('run pytorch', 0.5206), ('sklearn', 0.5101), ('skorch maintainer', 0.4871), ('pytorch version', 0.4735), ('pytorch api', 0.4693), ('pytorch ecosystem benjaminbossan talk skorch', 0.4685), ('pytorch slack server', 0.4554)]","[-1.16318218e-01 -6.95200861e-02 -4.50753011e-02 -2.64313314e-02
  6.34452850e-02 -9.37607810e-02 -4.73179221e-02 -1.49981044e-02
 -7.70789236e-02 -5.57295829e-02 -7.01664388e-02  1.11532686e-02
 -6.95904642e-02  9.93797090e-03 -1.45833008e-03  6.09008856e-02
 -7.56993741e-02  1.43430427e-01 -4.03976031e-02 -1.64318100e-01
 -2.10097316e-03 -5.71219297e-03 -1.26765799e-02  4.00398038e-02
  4.79035303e-02 -6.89955279e-02  1.69246197e-02 -3.77815515e-02
 -2.13957559e-02  2.01742817e-02 -8.24134983e-03 -3.49170305e-02
  7.44985193e-02  1.08886920e-02  2.32643336e-02 -3.19583830e-03
 -1.79026481e-02  1.85711384e-02  1.64474696e-02  5.26571274e-03
  6.07402483e-03 -4.76504713e-02 -7.26926252e-02 -6.55266270e-03
 -3.24084498e-02  7.96490069e-03 -4.96679805e-02 -8.77511650e-02
 -3.86870727e-02 -2.79868599e-02 -6.97633624e-02 -7.67878890e-02
 -4.14754488e-02 -2.24826708e-02 -1.00161415e-02  3.05247698e-02
  1.90221686e-02  2.56047975e-02  5.19898906e-02 -1.58567987e-02
  2.98587251e-02 -6.93282709e-02 -4.30570506e-02  3.12626585e-02
 -4.10038186e-03  8.19166750e-02  3.82117741e-02  4.41734828e-02
  1.01626448e-01 -1.35276720e-01 -3.02530043e-02 -4.09966707e-02
 -1.03208475e-01  5.59295490e-02  1.12680281e-02 -1.15454234e-02
  6.39370754e-02  6.71166107e-02  6.63318159e-03 -9.17412117e-02
 -4.84815128e-02 -1.91331450e-02 -3.59774008e-02  3.13779898e-02
  2.23136880e-02  3.65478508e-02 -5.03049456e-02  8.26842412e-02
 -3.50177125e-03  7.98782241e-03  3.17381099e-02 -6.24177940e-02
  2.36404855e-02  3.84978689e-02 -3.24978605e-02  6.20450228e-02
  3.55244465e-02 -6.32962734e-02 -3.03996615e-02  5.93003668e-02
 -4.38008420e-02 -6.05252087e-02  4.93837036e-02 -4.62786108e-02
  1.01106698e-02  2.69261841e-02  1.45846664e-03  6.35282844e-02
  3.33414562e-02  7.73111582e-02  3.64013091e-02 -4.03046934e-03
  9.39590670e-03 -1.79183949e-02  1.44705951e-01  1.49904890e-02
 -1.01363584e-02  2.68865302e-02  3.81843522e-02  4.30961363e-02
 -3.93867604e-02  1.31082302e-02 -5.02722189e-02  2.43513808e-02
 -7.03648245e-03  3.14297155e-02 -5.28850295e-02  1.32394371e-32
  4.12255377e-02 -1.03557352e-02  4.82775159e-02 -4.21776436e-02
 -2.41059955e-04 -8.00387934e-02  5.58452494e-02 -8.32608901e-03
 -3.86255197e-02 -4.02416065e-02  1.95901506e-02 -5.20688258e-02
 -6.64126799e-02  2.00064536e-02 -8.58013183e-02 -9.60339382e-02
 -1.55915525e-02  5.52695096e-02  1.48281492e-02  2.96866037e-02
  1.23578217e-02 -3.99077870e-03 -3.89033556e-02  1.17143840e-01
  3.56431641e-02 -4.46629189e-02 -4.81241709e-03 -4.21711579e-02
 -1.19155832e-02  1.57690570e-02  3.21177766e-02 -2.25806087e-02
  2.84797307e-02 -7.28652254e-03 -3.76952291e-02 -8.26719254e-02
 -2.33626203e-03  7.87811168e-03 -3.30394804e-02 -3.46740559e-02
 -5.61552122e-02  5.85737340e-02 -9.38372780e-03 -1.95210241e-02
  2.67145671e-02 -5.78462146e-03 -2.11640075e-02  1.21292427e-01
  8.97906870e-02 -8.01183283e-02 -6.26330078e-02  1.70476586e-02
 -1.15189739e-02  1.18564665e-01  5.33652641e-02 -3.48950997e-02
  4.08423543e-02 -2.24070456e-02  1.19030252e-01  4.63670008e-02
  4.44462784e-02  8.57326835e-02  5.10947220e-02  8.12683720e-03
  4.79703546e-02 -3.92511971e-02 -7.74259940e-02  8.97223596e-03
 -4.59441356e-02  5.47975376e-02 -6.92073256e-02  8.47678483e-02
 -5.86726181e-02 -1.71352103e-02  5.00895688e-03 -1.42978141e-02
  5.18673286e-03 -6.23276345e-02 -1.41226659e-02  2.45563183e-02
 -4.89097126e-02 -1.56288501e-02  1.10657420e-02 -4.82272729e-02
 -6.59748688e-02 -8.61672452e-04 -2.53113396e-02 -2.90199313e-02
  5.39675960e-03 -1.44842183e-02 -8.13651234e-02 -1.73347332e-02
 -2.05555316e-02  5.55069819e-02  6.15383238e-02 -1.11546112e-32
  2.01946944e-02  4.37552482e-02 -3.72680575e-02  1.44047365e-01
  5.70093747e-03  1.94393322e-02 -5.40690087e-02 -6.93376288e-02
  8.14269949e-03  1.86934490e-02 -4.09365930e-02 -6.12671161e-03
 -1.95108876e-02  1.86089482e-02  8.78921077e-02 -2.29367930e-02
 -2.54490171e-02 -1.82584580e-02 -3.30509013e-03  8.56042728e-02
 -9.95220989e-02  9.68255922e-02 -9.04907137e-02 -7.34259095e-03
 -3.54209691e-02 -2.41174921e-02 -7.28147253e-02  9.26382933e-03
 -3.45712411e-03 -3.51974345e-03  1.07689351e-02  2.72315536e-02
 -3.38451229e-02 -1.75161976e-02 -3.01306453e-02 -3.86739597e-02
  2.43009459e-02  2.46587805e-02  6.71408186e-03 -1.04107466e-02
  1.00521281e-01  5.78441061e-02 -7.30806217e-02 -4.63651568e-02
 -9.01565235e-03  2.23027207e-02 -4.67707328e-02  4.53872085e-02
 -2.10719239e-02 -2.55645774e-02 -1.55845638e-02  2.88629737e-02
 -5.38536394e-03 -2.57520843e-02  3.51483747e-02  1.09603712e-02
  6.22660667e-02  7.10765198e-02 -5.71827218e-02 -1.08707137e-02
 -9.92625430e-02 -4.02407944e-02  1.21541526e-02 -6.23370660e-03
  1.63626131e-02 -2.56322920e-02 -4.73165661e-02  1.30911306e-01
 -4.14692573e-02 -3.88828069e-02  5.98580390e-02  2.03950778e-02
  3.18655446e-02  4.72721271e-02 -1.10642165e-02  3.76101956e-02
 -1.04072858e-02  5.05619161e-02 -3.25562060e-02  4.72414941e-02
  5.57138734e-02  6.17753454e-02  8.69859476e-03  7.91230202e-02
  4.68501672e-02  9.42356214e-02  4.84472923e-02  3.57723720e-02
  4.54968438e-02 -4.71500531e-02 -2.41431333e-02  2.42798906e-02
 -1.82648422e-04  3.93239036e-02  3.52002159e-02 -4.47434836e-08
  5.27567427e-05  4.59526777e-02  5.13291266e-03  3.47889401e-02
  4.27525528e-02  4.21239855e-03  7.56366029e-02  1.10453986e-01
 -1.40316309e-02  5.78974709e-02 -3.24093774e-02 -6.15581833e-02
 -5.88601679e-02  5.31378156e-03  7.89931118e-02  7.18116313e-02
  5.55339130e-03  4.40866947e-02  4.40425472e-03 -6.78644851e-02
  1.36308661e-02 -3.92081961e-02 -4.85827774e-02  2.62084529e-02
 -7.69933611e-02 -4.35405299e-02  1.31498688e-04  5.72916679e-02
  2.69357152e-02 -4.47834916e-02  1.65150070e-03 -3.31005119e-02
  3.89710441e-02 -6.72816113e-02  2.14778036e-01  7.66522139e-02
 -7.04519078e-02  2.50805472e-03  3.49689461e-03  9.40145850e-02
 -9.86836478e-02 -1.94180179e-02  1.27676548e-02 -2.01863181e-02
  2.93665566e-03  8.18741228e-03 -6.76436946e-02 -9.17001888e-02
 -6.54365495e-02  2.76887137e-02 -1.89148821e-02 -2.47853361e-02
  1.68995634e-02  3.02192550e-02  4.55176979e-02  1.01962171e-01
 -2.21482031e-02 -3.31419334e-02 -2.43862178e-02  3.16032842e-02
  1.74643677e-02  5.84861562e-02 -3.30758318e-02  2.88094184e-03]",2,2
grad_cam,1,pip install grad camdocumentation with advanced tutorial http jacobgil github io pytorch gradcam bookthis is a package with state of the art method for explainable ai for computer vision this can be used for diagnosing model prediction either in production or while developing model the aim is also to serve a a benchmark of algorithm and metric for research of new explainability method comprehensive collection of pixel attribution method for computer vision tested on many common cnn network and vision transformer advanced use case work with classification object detection semantic segmentation embedding similarity and more includes smoothing method to make the cam look nice high performance full support for batch of image in all method includes metric for checking if you can trust the explanation and tuning them for best performance you need to choose the target layer to compute cam for some common choice are if you pas a list with several layer the cam will be averaged accross them this can be useful if you re not sure what layer will perform best you can use this package for custom deep learning model for example object detection or semantic segmentation you will have to define object that you can then pas to the cam algorithm here you can find detailed example of how to use this for various custom use case like object detection these point to the new documentation jupter book for fast rendering the jupyter notebook themselves can be found under the tutorial folder in the git repository notebook tutorial xai recepies for the huggingface image classification modelsnotebook tutorial deep feature factorization for better model explainabilitynotebook tutorial class activation map for object detection with faster rcnnnotebook tutorial class activation map for yolo notebook tutorial class activation map for semantic segmentationnotebook tutorial adapting pixel attribution method for embedding output from modelsnotebook tutorial may the best explanation win cam metric and tuninghow it work with vision swint transformersto reduce noise in the cam and make it fit better on the object two smoothing method are supported aug smooth truetest time augmentation increase the run time by x applies a combination of horizontal flip and mutiplying the image by this ha the effect of better centering the cam around the object eigen smooth truefirst principle component of activation weightsthis ha the effect of removing a lot of noise usage python cam py image path path to image method method to use with cuda python cam py image path path to image use cudayou can choose between gradcam hirescam scorecam gradcamplusplus ablationcam xgradcam layercam fullgrad and eigencam some method like scorecam and ablationcam require a large number of forward pass and have a batched implementation you can control the batch size with cam batch size if you use this for research please cite here is an example bibtex entry http arxiv org ab grad cam visual explanation from deep network via gradient based localization ramprasaath r selvaraju michael cogswell abhishek da ramakrishna vedantam devi parikh dhruv batrahttps arxiv org ab use hirescam instead of grad cam for faithful explanation of convolutional neural network rachel l draelos lawrence carinhttps arxiv org ab grad cam improved visual explanation for deep convolutional network aditya chattopadhyay anirban sarkar prantik howlader vineeth n balasubramanianhttps arxiv org ab score cam score weighted visual explanation for convolutional neural network haofan wang zifan wang mengnan du fan yang zijian zhang sirui ding piotr mardziel xia huhttps ieeexplore ieee org abstract document ablation cam visual explanation for deep convolutional network via gradient free localization saurabh desai and harish g ramaswamy in wacv page http arxiv org ab axiom based grad cam towards accurate visualization and explanation of cnns ruigang fu qingyong hu xiaohu dong yulan guo yinghui gao biao lihttps arxiv org ab eigen cam class activation map using principal component mohammed bany muhammad mohammed yeasinhttp mftp mmcheng net paper tip layercam pdf layercam exploring hierarchical class activation map for localization peng tao jiang chang bin zhang qibin hou ming ming cheng yunchao weihttps arxiv org ab full gradient representation for neural network visualization suraj srinivas francois fleurethttps arxiv org ab deep feature factorization for concept discovery edo collins radhakrishna achanta sabine s sstrunk,"[('pytorch gradcam bookthis', 0.5221), ('example object detection', 0.4713), ('cam algorithm', 0.4602), ('grad camdocumentation', 0.4527), ('grad cam', 0.434), ('computer vision', 0.4339), ('gradcam hirescam scorecam gradcamplusplus ablationcam xgradcam layercam fullgrad', 0.4329), ('object detection', 0.4158), ('deep learning model', 0.4073), ('cuda python cam py image path path', 0.4051)]","[-5.53347170e-02 -1.02114886e-01  5.89992385e-03 -7.86115900e-02
  1.07842132e-01 -6.47199601e-02  1.72079038e-02  4.42969240e-02
 -2.22348422e-02  1.63333993e-02  1.05350958e-02  5.33963414e-03
 -2.60016862e-02  5.48477545e-02  3.73521298e-02 -3.66022810e-02
 -1.40473526e-02  7.01336190e-02 -9.03567020e-03 -6.88724592e-02
  4.31980230e-02 -4.47245501e-02  3.55188586e-02 -2.05269139e-02
 -2.46707071e-02 -3.25878002e-02  7.10049644e-02 -3.81221287e-02
  2.05686856e-02 -3.20951082e-02 -2.84705088e-02 -4.37731296e-02
  8.34800079e-02  8.38120133e-02 -6.92394841e-03 -1.91636570e-02
  1.58839952e-02 -3.39191556e-02 -4.16739844e-02  7.90757965e-03
 -4.57073599e-02  2.44756714e-02  7.30030565e-03 -1.59958657e-02
  5.52660562e-02  3.05602327e-02  5.73380664e-02 -5.53475358e-02
  1.34269027e-02 -9.54260156e-02 -2.86653209e-02 -4.02175561e-02
 -5.21623231e-02  1.81990433e-02 -3.58210057e-02  2.06910949e-02
 -2.97953375e-03 -3.77401896e-02  1.93989556e-02 -2.45736074e-02
  1.98526103e-02 -3.61782573e-02 -9.63429082e-03 -7.29837315e-03
 -7.91220367e-02  1.13535672e-04  2.36302540e-02  1.56434961e-02
  1.37878641e-01 -5.61401099e-02 -6.25107214e-02  3.94224152e-02
 -1.99469738e-02  3.96160856e-02 -2.51551569e-02 -5.05717956e-02
  6.75630197e-02  4.79400754e-02 -1.66889038e-02 -1.53183296e-01
 -5.94575610e-03  5.90993091e-03  4.47929874e-02  6.61127781e-03
  6.62865788e-02  1.97795611e-02 -3.27898376e-02  4.90896292e-02
 -7.44451256e-03 -4.47760522e-02  2.81926002e-02 -1.19941503e-01
 -7.85919577e-02  3.55420238e-03 -6.14329726e-02 -9.18424036e-03
  1.82628222e-02 -5.20012453e-02 -4.19415126e-04  1.22981996e-03
 -1.75493900e-02 -4.12329175e-02  1.39452901e-03  6.25575101e-03
  3.14473994e-02  6.95483834e-02 -8.04697443e-03  3.97364143e-03
  9.08690840e-02  1.79107301e-02 -5.03813550e-02  5.67489527e-02
  9.50890593e-03 -7.56105110e-02  6.74498007e-02  1.78050064e-02
 -1.33283287e-02  1.45487152e-02  1.38606979e-02  2.30523571e-02
 -4.22057137e-02 -1.71456095e-02 -3.00595947e-02 -5.07322047e-03
 -6.93671629e-02 -1.30872596e-02  8.91153142e-03  7.61924067e-33
 -3.71398078e-03  3.82650197e-02  1.84354540e-02 -3.71157960e-03
  7.94943348e-02 -2.21487116e-02  4.48438637e-02  7.87304565e-02
 -1.18253212e-02  8.79459083e-03 -3.02716102e-02 -4.28024940e-02
 -9.83194783e-02  1.09476194e-01  3.02330106e-02  1.64031573e-02
 -1.07848436e-01 -1.27965454e-02 -9.07579251e-03  7.20513314e-02
 -1.57981385e-02 -3.20569277e-02  4.85460944e-02  1.07044078e-01
  2.95221694e-02  3.44432667e-02  4.41716462e-02  1.37882708e-02
  6.23089038e-02  3.14327888e-03 -3.12021263e-02  8.61835107e-03
 -3.34687978e-02  1.02350218e-02  6.35254057e-03 -5.32664591e-04
 -4.47653942e-02  1.70123875e-02  5.86962886e-03 -6.76078871e-02
 -5.94620965e-02  6.92063719e-02 -5.23087308e-02 -6.35417625e-02
  2.49329358e-02 -2.86941081e-02 -2.68019233e-02  1.24420516e-01
 -4.05481122e-02  3.06184534e-02  1.83112361e-02 -1.31886164e-02
 -3.71649191e-02 -2.63618622e-02 -9.74969789e-02  1.57279912e-02
  5.09067923e-02  1.09099997e-02  2.28552837e-02 -6.53964207e-02
  4.89137173e-02  2.46169697e-02  5.12560904e-02 -4.29161917e-03
 -5.86590320e-02 -2.93985214e-02 -7.54145533e-03 -9.30158608e-03
 -3.37270834e-02  9.33972299e-02 -4.88770753e-02  8.16860124e-02
 -2.24086232e-02 -1.14710838e-01  4.87424321e-02  5.71068227e-02
 -7.30324313e-02 -7.24168792e-02 -5.12356050e-02  7.18994141e-02
 -1.23526797e-01  5.18460795e-02  6.12865537e-02 -3.11220177e-02
 -6.46762103e-02 -2.28168089e-02  3.56724411e-02 -3.66800018e-02
 -4.61352570e-03  6.42754277e-03 -7.46300444e-02  3.77076678e-02
  3.81679796e-02  2.66872458e-02 -4.55215666e-03 -5.74825113e-33
  5.52914403e-02  9.99316797e-02 -8.91295522e-02 -1.45584242e-02
  2.31506769e-02 -4.01482768e-02 -2.35962751e-03 -1.37758488e-03
  4.98776883e-02 -4.64851372e-02 -7.70936813e-03 -1.84728578e-02
  1.39907040e-02  3.01679950e-02  1.56835951e-02  5.58600156e-03
 -4.56262939e-02 -2.72442419e-02 -9.22913104e-02 -2.23638862e-02
 -4.08127457e-02  1.09027736e-01 -2.41234172e-02 -2.02441737e-02
 -3.29809673e-02 -7.99793110e-04 -1.20546287e-02  4.61538918e-02
  5.56888357e-02 -4.18845862e-02  8.89928490e-02 -8.17672350e-03
 -2.00425386e-02  2.75345072e-02 -5.68524897e-02 -1.60089738e-04
  7.53296241e-02 -9.12006013e-03 -1.46286155e-04  8.77721887e-03
  1.31447047e-01  3.93994562e-02  5.88554516e-02  5.59324864e-04
 -1.05838357e-02  4.08938201e-03 -5.83905131e-02  3.59391570e-02
 -3.59266289e-02 -2.05043401e-03 -3.06330454e-02  1.42038679e-02
 -6.96880445e-02  5.15995175e-02  4.63134721e-02  3.17843258e-02
  3.98627445e-02  4.82813008e-02  2.51104198e-02  4.38063815e-02
 -9.54763070e-02 -5.23115322e-02 -4.25304361e-02  1.75481457e-02
 -6.26018941e-02  1.90519169e-02 -6.79237098e-02  8.25914890e-02
 -5.29061742e-02 -1.98261719e-03  5.89378290e-02  6.28543422e-02
 -9.18891001e-03  6.93274736e-02 -4.02739160e-02  5.52900434e-02
  2.48505827e-02  2.63731629e-02  3.07002608e-02 -3.54946442e-02
  5.55428788e-02 -1.46214172e-01 -3.26869451e-02  1.23465329e-01
  1.38300164e-02  5.74582219e-02 -4.66321036e-03 -2.98861209e-02
  3.51639315e-02 -5.88264018e-02 -3.97243537e-02 -2.17859466e-02
  2.65981760e-02 -2.17256490e-02  6.81025814e-03 -3.00806136e-08
 -8.10881183e-02 -1.72020431e-04  3.22293700e-03  1.41441866e-04
 -6.34100474e-03 -3.22038028e-03  2.17830967e-02  1.95772171e-01
  3.05951815e-02  8.33567977e-03 -5.86910127e-03 -2.06120983e-02
 -1.37631772e-02  1.06073369e-03  5.76506695e-03  1.48113236e-01
  1.81980301e-02 -3.59083340e-03  5.30658960e-02 -2.03529652e-02
  3.97550315e-02 -3.99447046e-02 -4.56645265e-02  6.17833212e-02
 -2.08252035e-02 -7.16321543e-02 -3.24273817e-02  1.63464770e-02
 -1.83266774e-02 -3.25943753e-02 -2.21292861e-02  5.13920523e-02
  1.24093026e-01 -3.20990719e-02  1.80421710e-01  8.46729130e-02
 -5.91747127e-02 -2.07978357e-02  1.36121456e-02  5.85422143e-02
 -2.46521756e-02 -5.16725332e-02  4.79877070e-02 -4.48225960e-02
 -3.50598320e-02  6.69199750e-02  4.46934476e-02 -4.82591763e-02
 -7.17198104e-02  7.27073997e-02 -5.51511012e-02  4.62040864e-02
  1.46198682e-02  2.26952694e-02 -5.27698211e-02  7.74332583e-02
  2.23344080e-02 -1.52135849e-01 -2.93479115e-02 -2.90531982e-02
 -3.91039206e-03  8.99750665e-02 -1.17338095e-02  1.43447807e-02]",2,2
booster-pytorch,1,a lightweight library to ease the training and the debugging of deep neural network with pytorch data structure and paradigm a two level dictionary structure to store the model diagnostics compatible with tensorboard datastructure example a module to compute the running average of the diagnostics the output is a diagnostic object and can easily be logged to tensorboard the evaluator computes a forward pas through the model the loss and additional diagnostics the pipeline fuse the model forward pas with the evaluator and can be wrapped into a custom dataparallel class that handle the diagnostics,"[('pytorch data structure', 0.5421), ('deep neural network', 0.4812), ('tensorboard datastructure example', 0.4781), ('model diagnostics', 0.4555), ('lightweight library', 0.4063), ('diagnostic object', 0.399), ('diagnostics', 0.3805), ('additional diagnostics', 0.3378), ('debugging', 0.3281), ('module', 0.2985)]","[-5.80150783e-02 -9.48947519e-02 -2.79678646e-02  8.38474743e-03
  3.36732417e-02 -3.11911367e-02 -2.09923275e-02  6.76881969e-02
 -1.35095984e-01 -8.86335671e-02 -2.09429543e-02 -2.83146724e-02
 -8.40108022e-02  1.40128145e-02 -6.01549372e-02  1.34797050e-02
 -1.36992875e-02  8.45251232e-02 -2.48478595e-02 -7.43968636e-02
 -3.59463766e-02  1.36130499e-02 -2.06357129e-02  1.57931093e-02
 -4.86655980e-02  4.27952223e-02  2.15477515e-02 -5.53037785e-02
  1.20067531e-02 -3.78941521e-02  1.82178281e-02 -2.04443261e-02
  6.09436631e-02  3.72985527e-02  1.01311579e-02 -8.25328659e-03
  1.20586026e-02 -7.37453923e-02 -2.94718035e-02 -6.65850341e-02
 -1.33924987e-02  1.30266566e-02 -3.93342301e-02  1.46484431e-02
  5.04403822e-02 -4.40402608e-03  1.06270099e-02 -8.04630369e-02
  3.12662981e-02 -4.17565592e-02 -5.21067753e-02 -5.28328791e-02
 -7.14933649e-02  1.46336211e-02  6.81939442e-03 -1.19461427e-02
  2.79634912e-02 -4.36131731e-02 -1.65225584e-02 -3.34994085e-02
  6.47370005e-03 -5.86351426e-03 -1.37865189e-02  3.52147669e-02
  5.21619841e-02  5.03878109e-02 -3.10434606e-02  4.71943878e-02
  1.57532677e-01 -6.28197491e-02 -3.50352935e-02 -4.49062046e-03
 -6.09356090e-02  8.37734640e-02 -3.28564271e-02 -1.20615130e-02
  6.94196820e-02  2.94280872e-02  4.36879955e-02 -7.58014172e-02
 -7.36264884e-02 -1.14094256e-03  5.86217828e-02  2.04121154e-02
  3.98158208e-02 -5.03042806e-03 -2.49741282e-02  9.27807987e-02
 -5.19080572e-02 -3.45935971e-02  1.13554366e-01 -8.06313083e-02
 -2.46601384e-02  4.81609888e-02  1.60726067e-02  4.54795919e-02
  4.38676924e-02 -6.87380135e-02 -1.71978306e-02  9.59774852e-03
 -5.81001677e-02 -2.03591306e-02  7.22853914e-02  7.25625604e-02
 -8.97278562e-02  1.27968723e-02  3.88975479e-02 -2.46443767e-02
  2.79950332e-02 -9.50633362e-03 -4.09215540e-02  3.63243185e-02
 -1.39243342e-02 -1.04393013e-01  9.60427001e-02 -4.03768457e-02
 -8.33474472e-02  4.75903228e-02  5.75564150e-03  7.83967599e-02
 -5.50192036e-02  1.78156383e-02 -2.47787754e-03  3.60684171e-02
  4.89898911e-03  3.36275212e-02 -1.44958556e-01  4.28135099e-33
  7.68258527e-04 -1.21405488e-02 -3.43385451e-02  2.80111991e-02
  5.07709198e-02 -8.46973509e-02  4.35198471e-02  7.11934939e-02
  3.76257300e-02  4.47808541e-02 -6.05207644e-02  8.53967220e-02
 -4.63511050e-02  7.31519237e-02  5.62134245e-03  1.04553858e-02
 -7.56281242e-03  4.77560833e-02 -2.26643421e-02  7.69363120e-02
  3.14607434e-02 -1.28587540e-02  2.97971889e-02  7.25773796e-02
  5.45759872e-02  7.06374049e-02 -2.39019394e-02 -2.93382793e-03
 -3.44565324e-02  4.79327049e-03 -1.18538484e-01 -2.08475459e-02
 -3.17218304e-02  1.92482602e-02  7.74538610e-03 -1.43318614e-02
 -4.83002886e-02 -1.36842513e-02 -1.43955462e-02 -3.21112201e-02
 -3.02769858e-02  1.08403547e-04 -8.80842134e-02 -1.10865189e-02
  7.19910581e-03 -2.70089544e-02 -2.76078680e-03  9.53949541e-02
 -3.48163326e-03 -6.72229305e-02 -6.45976588e-02 -2.69628083e-03
 -2.37626880e-02  6.02305587e-03  6.21155661e-04  1.60653032e-02
  3.56829315e-02  1.56114697e-02  4.98890653e-02  9.80703756e-02
  2.19082786e-03  6.32121637e-02  1.51710995e-02 -1.51125081e-02
 -1.54094044e-02  1.84375476e-02 -1.07864738e-01  1.51622994e-02
  2.77703907e-02  4.70023155e-02 -8.70564282e-02  5.86554520e-02
 -1.46045154e-02 -2.81018321e-03  2.77240109e-02  1.02426652e-02
  6.38852315e-03 -8.05742666e-02 -4.88376953e-02  4.30960618e-02
 -7.20988140e-02  6.16434254e-02  4.09027860e-02  1.15221469e-02
 -4.78478484e-02 -3.08063719e-02 -8.85311235e-03  1.89762985e-04
 -3.92708518e-02 -3.23763937e-02 -6.06232323e-02  2.31052302e-02
  2.73452923e-02  1.80311054e-02 -3.76357362e-02 -3.37628132e-33
  4.22838260e-04  6.65142909e-02 -8.82309228e-02  6.76361993e-02
 -8.09909907e-05  1.84145812e-02 -3.61536294e-02  1.27098300e-02
  4.47810516e-02  6.38249591e-02  5.92156686e-02 -2.59581991e-02
 -1.10331345e-02 -6.13560379e-02  2.37146094e-02  6.89164624e-02
 -8.17704499e-02 -6.26016334e-02  2.58355904e-02 -3.18413600e-02
 -8.16396326e-02  1.44156635e-01 -9.45995077e-02 -6.87993243e-02
 -6.47588149e-02 -9.76987649e-03 -5.57062104e-02 -6.11107424e-02
  4.61074151e-02  1.04798805e-02  1.09776007e-02  1.94924918e-03
  5.06041087e-02  6.22099191e-02 -1.61070712e-02  2.05113646e-02
  1.15086034e-01 -7.40314499e-02  3.82652916e-02  3.74229699e-02
  1.51125729e-01  1.04573078e-01 -5.93564194e-03  1.27685303e-02
  3.32593778e-03 -5.77766867e-03 -1.13906801e-01  3.13277915e-02
 -2.08397359e-02 -3.91897932e-02  2.24316679e-02 -4.05471697e-02
  3.85084264e-02 -3.08437776e-02 -3.12042441e-02 -6.01393729e-03
  6.72773421e-02 -6.74202619e-03  6.18933104e-02  5.39121367e-02
 -4.84336615e-02 -9.08551142e-02  2.59330613e-03  1.06142499e-02
 -6.76217824e-02 -4.84010354e-02 -2.46103536e-02  1.59713626e-02
 -3.95927997e-03 -5.09753264e-02 -1.31689180e-02  3.73390988e-02
  6.26323968e-02  1.16959497e-01 -2.09661499e-02  1.04615912e-02
 -4.69585769e-02  3.12485397e-02  5.14694825e-02 -1.94214564e-02
  2.33048555e-02 -4.10026908e-02  1.59302223e-02  5.29961474e-02
  7.20110089e-02  7.78373852e-02  1.87035929e-02  4.87574562e-02
  3.41807269e-02 -5.66886552e-02 -5.85074397e-03  3.70134972e-03
  2.00570859e-02  1.10566013e-01  6.85178582e-03 -2.80246866e-08
 -4.94638868e-02  7.90263899e-03  5.34949116e-02 -7.83574395e-03
  1.54887531e-02 -1.94055475e-02  2.75793299e-02  1.42457932e-01
 -6.18667947e-03  6.01029806e-02 -1.06985830e-02 -4.26230729e-02
 -1.09811954e-01 -1.51315890e-02  2.82905567e-02  9.88454595e-02
  1.66721139e-02  2.94249393e-02  4.54241736e-03 -5.82715869e-02
  9.95941460e-02  3.42632346e-02  1.03672994e-02  6.17972650e-02
  2.57026125e-02 -1.14676759e-01  9.28073749e-03  3.00139803e-02
 -1.56548899e-02  4.65405080e-03 -3.24865393e-02  1.76706910e-03
  9.57242921e-02 -6.66292384e-02  7.57130459e-02  1.16497301e-01
  4.93914299e-02 -4.12281491e-02 -1.34001179e-02  1.01054400e-01
 -8.83340389e-02  1.43208932e-02 -3.29453312e-02 -3.08825821e-02
  4.53286469e-02 -3.65479514e-02 -5.20080663e-02 -3.93693857e-02
 -1.31265819e-02 -1.86656341e-02  2.69522388e-02  7.11211562e-02
 -1.69168673e-02  3.20191607e-02 -1.20437527e-02  9.88865495e-02
 -8.16612784e-03 -3.23264487e-02 -2.44773719e-02 -2.50572823e-02
 -7.81352911e-03  3.15170400e-02 -1.50634609e-02 -2.54331287e-02]",2,2
pytorchcv,1,this is a collection of image classification segmentation detection and pose estimation model many of them are pretrained on imagenet k cifar svhn cub pascal voc ade k cityscape and coco datasets and loaded automatically during use all pretrained model require the same ordinary normalization script for training evaluating converting model are in the imgclsmob repo to use the model in your project simply install the pytorchcv package with torch is recommended to enable disable different hardware support such a gpus check out pytorch installation instruction example of using a pretrained resnet model some remark some remark,"[('coco datasets', 0.5924), ('image classification segmentation detection', 0.4616), ('pytorchcv package', 0.4317), ('resnet model', 0.3986), ('model', 0.3378), ('pytorch installation instruction example', 0.3201), ('converting model', 0.3185), ('training', 0.3002), ('imgclsmob repo', 0.271), ('same ordinary normalization script', 0.2352)]","[-2.31075622e-02 -8.32857341e-02  1.66033320e-02  1.33031476e-02
  1.00638188e-01 -2.93506477e-02 -5.67921298e-03  4.61049043e-02
 -1.09764628e-01 -2.42211036e-02  1.05809011e-02 -2.87381206e-02
 -6.68645278e-02  2.64434479e-02 -2.33517680e-02 -3.19443978e-02
 -5.87930717e-03  1.19964384e-01 -8.98412243e-02 -1.35328010e-01
  4.13584942e-03 -2.61063259e-02  4.14901637e-02 -1.38836298e-02
 -3.45451012e-02 -2.22688559e-02  1.98108070e-02 -3.92029844e-02
  4.63236570e-02 -7.07803816e-02 -4.76579852e-02 -1.02414340e-02
  1.80815145e-01  6.01095073e-02  1.99240167e-02  7.85430148e-02
  6.42698631e-02 -6.32839054e-02 -1.74017102e-02 -7.00061694e-02
 -1.90852713e-02 -4.05285537e-04 -1.69379041e-02 -7.62589322e-03
  8.78112763e-02  3.03046079e-03 -3.24188881e-02 -6.17178753e-02
  3.03043574e-02 -7.39429072e-02 -4.89333123e-02 -4.43847887e-02
 -8.85686502e-02  3.38020511e-02 -1.85092110e-02  5.64627349e-03
  2.74522491e-02 -2.07034014e-02  8.74687135e-02 -1.76526867e-02
 -1.70563124e-02 -2.86226906e-02 -5.72088473e-02  7.77319297e-02
  9.54510295e-04 -1.94262601e-02 -2.11430322e-02  2.73539349e-02
  1.70280159e-01 -1.56354532e-01 -9.71816927e-02  6.08087424e-03
  3.03479331e-03  5.57484068e-02 -4.26176973e-02 -1.47041567e-02
  8.16233233e-02  5.39601482e-02  1.13201002e-03 -1.22490078e-01
 -2.65265126e-02 -3.31317894e-02  1.02926850e-01  3.54350694e-02
  5.29224575e-02  5.28051257e-02 -4.21961248e-02  9.29892063e-02
  2.58723907e-02  1.32488934e-02  3.26174945e-02 -3.53811719e-02
 -3.86987440e-02  8.26474465e-03 -1.52349202e-02 -8.49463802e-04
  6.13854281e-05 -5.27028181e-02  5.23073040e-03  5.18194139e-02
 -1.88628901e-02 -9.40875709e-02  5.85888624e-02  5.74406981e-03
  4.61128578e-02  6.07228465e-02  5.02852313e-02  7.15040776e-04
  9.23545957e-02  5.54612800e-02 -6.78552017e-02  4.51306589e-02
 -8.00983980e-02 -7.18478560e-02  9.71921906e-02 -8.88722949e-03
 -3.09594702e-02  3.79370824e-02 -2.51329900e-03 -4.61335061e-04
 -5.87542467e-02  2.38295197e-02 -2.44329888e-02 -4.22711670e-02
 -3.73095162e-02 -6.68274164e-02 -7.73466900e-02  6.81434663e-33
 -3.01934755e-03 -2.68098284e-02 -1.98964626e-02  2.39840895e-02
  2.48395391e-02 -3.77001129e-02  5.58020324e-02  2.65877414e-02
 -6.96624741e-02 -1.46192973e-02  6.64427727e-02  4.48293760e-02
 -7.75816217e-02  1.31529123e-01 -3.10236253e-02 -7.82679359e-04
 -3.47566083e-02  1.81640163e-02  4.15802523e-02  1.47106275e-02
 -7.65599031e-03  5.78795560e-03  1.80704966e-02  8.98415670e-02
  3.12540010e-02  7.67094567e-02 -2.18507610e-02 -2.50169393e-02
  3.70524116e-02 -1.04860449e-02 -2.35147979e-02  6.80624247e-02
  1.01466160e-02  7.49858795e-04 -3.91092375e-02  5.19402442e-04
  7.87841622e-03 -7.09475111e-03 -1.77576113e-02  1.10369734e-02
 -3.98116447e-02  6.47611022e-02 -3.07499673e-02 -2.36967597e-02
  1.40713016e-02  7.36704282e-03 -7.01962179e-03  9.11648199e-02
  4.56398353e-02  5.62412031e-02 -3.06048635e-02 -3.24154668e-03
  1.31800687e-02 -3.81640270e-02 -2.91234092e-03  5.01362747e-03
  4.39715311e-02 -1.21284723e-02  4.10415307e-02  3.36108990e-02
  4.71144952e-02  1.18732397e-02 -4.72819433e-03  1.24134347e-02
 -2.37468481e-02 -4.16387208e-02 -1.16650186e-01 -4.17591408e-02
 -3.54491221e-03  1.17937163e-01 -1.21786930e-01  4.53669280e-02
 -6.31820364e-03  8.92454293e-03 -3.73205962e-03 -3.68125141e-02
 -5.47546372e-02 -9.27624293e-03 -9.44884568e-02  7.60634020e-02
 -7.11793527e-02  6.67902231e-02  3.02085280e-03 -2.78414111e-03
 -3.28043960e-02  5.82509348e-03  1.03501612e-02 -3.35967145e-03
  1.07909385e-02  3.60142104e-02 -2.55011953e-03  1.17771327e-02
 -2.49749701e-02  3.61560509e-02 -5.44755394e-03 -5.30230531e-33
  1.85805727e-02  1.08680986e-01 -1.05206601e-01  6.31229952e-02
 -7.09805414e-02 -8.25633202e-03 -6.42255396e-02 -1.48747712e-02
  2.60940492e-02 -6.22589178e-02  4.29651439e-02 -1.27635384e-02
  2.72740778e-02 -3.46569046e-02 -1.37592843e-02 -2.00825203e-02
  7.57111469e-03 -6.31363504e-03 -1.79771911e-02  2.66306717e-02
 -4.32643779e-02  1.47425994e-01 -6.43652976e-02 -4.01568152e-02
 -3.18282023e-02  1.14571303e-02 -6.77685663e-02  7.75016844e-02
  3.36492360e-02 -4.51666936e-02  3.25613320e-02  1.09749716e-02
 -1.82985682e-02  1.75930541e-02 -4.00774777e-02 -4.50836904e-02
  3.11365779e-02 -5.02029015e-03  2.27101352e-02  5.46402037e-02
  1.02465771e-01  3.59000564e-02 -9.55188498e-02 -2.95646721e-03
 -5.06622940e-02  2.87258197e-02 -3.15447077e-02  4.86564599e-02
 -6.84164334e-05 -3.12450640e-02 -7.51285851e-02 -6.37306869e-02
 -7.80740231e-02 -3.37720141e-02 -1.50516769e-02  3.21263447e-02
  4.70898896e-02  6.24523424e-02  4.60072495e-02  6.54258474e-04
 -9.67716146e-03 -2.83092745e-02  2.85846647e-02 -5.95744476e-02
 -4.99621145e-02 -2.81108706e-03 -8.81446898e-02  1.32713839e-02
 -2.46822182e-02  8.39764325e-05  2.98012737e-02  3.46582085e-02
  5.55037595e-02  3.26525532e-02 -4.18669879e-02 -3.20935175e-02
 -1.62787512e-02  1.26775438e-02 -3.18787731e-02 -1.62966847e-02
  3.72899510e-02  1.95698962e-02  3.96746956e-02  6.04620017e-02
  6.56223819e-02  1.53550401e-01  4.62713614e-02 -4.49398644e-02
  7.73242861e-02 -3.14401090e-02 -3.84488441e-02  3.85314263e-02
  3.46321762e-02  1.52542308e-01 -1.26442602e-02 -3.07421182e-08
 -2.95413993e-02 -3.27138938e-02  4.66122059e-04  9.08521877e-04
 -3.17775384e-02  5.65680601e-02 -3.19444351e-02  1.19463593e-01
 -2.09435765e-02  4.57906052e-02 -2.79479567e-02 -9.95006412e-03
 -7.40077272e-02  2.29572002e-02  8.97736009e-03  8.97173360e-02
  3.15685421e-02  5.95289655e-02  2.30907463e-02 -2.24244036e-02
 -2.69563124e-02 -1.06021985e-02  1.12886848e-02  5.91603108e-02
 -2.49566184e-03 -6.20479807e-02  2.82557476e-02  1.76301729e-02
 -3.65543105e-02 -9.53567494e-03 -1.63749966e-04  2.80843815e-03
  1.15753867e-01 -3.96749675e-02  1.17365807e-01  5.94704859e-02
 -5.99169731e-02 -5.84752262e-02  2.24191858e-03  1.36023602e-02
  1.00497687e-02  2.35604495e-02  1.17478473e-02 -3.02646253e-02
  2.03985684e-02 -7.07749464e-03  4.00401689e-02 -8.54180530e-02
 -2.46420726e-02  6.90319983e-04  1.16863418e-02  3.42732742e-02
 -7.04584569e-02  3.06896865e-02  3.56546044e-02  4.31503654e-02
  2.77980417e-02 -4.13638651e-02  1.62043050e-02  4.74373139e-02
 -5.43820895e-02 -1.08054345e-02 -3.67053561e-02 -9.43984613e-02]",2,2
tensorflow-macos,1,tensorflow is an open source software library for high performance numerical computation it flexible architecture allows easy deployment of computation across a variety of platform cpu gpus tpus and from desktop to cluster of server to mobile and edge device originally developed by researcher and engineer from the google brain team within google s ai organization it come with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domain tensorflow is licensed under apache,"[('tensorflow', 0.7401), ('many other scientific domain tensorflow', 0.6584), ('flexible numerical computation core', 0.4332), ('deep learning', 0.3829), ('google s ai organization', 0.3821), ('flexible architecture', 0.3764), ('google brain team', 0.3684), ('platform cpu gpus tpus', 0.3613), ('open source software library', 0.3378), ('edge device', 0.2987)]","[-4.41984236e-02 -1.09338418e-01  3.75523120e-02 -6.00057282e-02
  8.24890584e-02 -4.75711152e-02 -5.30565754e-02  2.51197536e-02
 -1.34402225e-02 -1.77784972e-02 -8.60479921e-02 -3.01086437e-02
 -6.02132641e-02  1.79548992e-03  1.93502568e-03 -4.11127694e-02
 -2.45012846e-02  3.20869386e-02 -2.12851036e-02 -1.30356878e-01
 -4.25707698e-02  3.98718528e-02 -1.30772097e-02  1.58899475e-03
 -9.78470664e-04  7.42263645e-02 -1.08599998e-02 -1.42022565e-01
  3.27437744e-02 -6.55014766e-03 -1.02071287e-02  9.64850187e-03
  4.71507711e-03  3.81811298e-02 -6.36580959e-02  3.31682228e-02
 -4.90938015e-02 -4.53656018e-02 -1.57287996e-02 -4.45531756e-02
 -3.31613086e-02 -6.49364218e-02  1.22887539e-02 -4.47792821e-02
  8.86408389e-02  1.80261284e-02  2.25173086e-02 -1.15306959e-01
 -2.31295135e-02  1.99361108e-02 -3.49466614e-02 -8.51285681e-02
 -5.48832156e-02  8.48228261e-02 -7.17121512e-02 -6.73733884e-03
  4.10806164e-02 -6.88320026e-03  1.62451004e-03 -8.22282210e-03
 -1.00759100e-02 -8.78835991e-02  4.05119406e-03  1.54486764e-02
 -3.63357663e-02  5.28295226e-02 -9.17309057e-03  1.02966493e-02
  6.14090227e-02 -1.00714304e-01  8.13344568e-02  3.17448117e-02
 -3.28305252e-02 -7.23914290e-03 -3.69609729e-03 -1.43599079e-03
  5.77681474e-02 -2.73799971e-02  8.20818990e-02 -6.62635267e-02
  2.97083259e-02 -1.71580762e-02 -3.10782306e-02  1.28447125e-02
  3.34661715e-02 -2.07291394e-02 -7.15484247e-02  8.80392119e-02
 -1.64926033e-02 -1.48727791e-03  4.25984226e-02 -3.23301293e-02
  2.54538245e-02 -3.26760523e-02  4.63619828e-02  8.25122185e-03
 -7.75806680e-02 -8.91252458e-02 -4.99539562e-02  3.41689922e-02
 -1.18737161e-01  1.49925379e-03  4.47053388e-02  4.81082126e-02
 -2.87359152e-02  8.46514031e-02  4.03921865e-02 -3.37678823e-03
  9.50822160e-02 -2.90156733e-02 -2.99208332e-02  6.71075732e-02
  4.06970084e-02 -7.54432976e-02  1.83600057e-02 -7.02635124e-02
 -3.98105867e-02  7.65025169e-02  3.50047797e-02  1.09249830e-01
 -1.12119086e-01  3.65649313e-02 -3.26654613e-02 -8.72930512e-03
 -2.04302394e-03  1.50718652e-02 -8.76195654e-02  4.60425342e-33
  1.02570942e-02  4.28824779e-03  2.68881787e-02 -6.87931105e-02
  6.11587837e-02 -6.28679469e-02  2.82790624e-02  1.04069605e-03
  1.19376676e-02 -2.09535728e-03 -6.10620193e-02  1.45414501e-01
 -2.77088694e-02  1.31631240e-01  5.33098839e-02 -4.76143509e-02
  1.41801238e-02  5.01978882e-02  7.20799938e-02 -1.55935355e-03
  2.45469920e-02  6.20573387e-02  4.07773815e-02  9.34710652e-02
  2.34671924e-02  6.27924548e-03 -4.57534008e-02 -4.82183322e-02
  6.20497251e-03  1.55118303e-02 -5.82717657e-02  2.86109298e-02
 -4.72726822e-02  8.19104631e-03  1.48596782e-02  1.40242409e-02
 -7.13818287e-03 -6.28968030e-02  7.68921385e-03  1.67898014e-02
 -2.04171371e-02  4.88140881e-02  2.21376605e-02 -4.13218699e-02
 -1.19940946e-02 -2.14574859e-02  2.20675208e-02  2.55821627e-02
  4.94300574e-02 -5.72271608e-02 -2.83469800e-02  2.06819158e-02
 -7.33109657e-03 -7.42880777e-02  4.45740707e-02 -1.86625645e-02
  1.66003243e-03  5.78348897e-02  4.94083539e-02  8.17290470e-02
 -1.52327828e-02  2.14482732e-02 -2.74007898e-02 -1.56885870e-02
  3.61333638e-02 -1.15680881e-02  2.34961193e-02  4.28035334e-02
  9.24402382e-03  2.92042568e-02 -8.23360160e-02  7.03407750e-02
  3.02753523e-02  7.97685049e-03 -4.55205850e-02 -1.75612886e-02
  3.93302739e-02 -1.34543687e-01 -4.39116172e-02 -5.35312574e-03
 -1.35161489e-01  1.69179011e-02  7.37877490e-05  1.96509007e-02
  2.84204120e-03  1.24324122e-02 -2.80277040e-02  1.61576960e-02
 -5.60108647e-02 -5.83440997e-02 -9.79904234e-02 -1.62681602e-02
  8.40131938e-02  6.26077726e-02 -9.34203938e-02 -4.11873715e-33
 -1.21871002e-01 -2.00116094e-02 -6.95795864e-02  9.35161188e-02
  1.06673934e-01  1.64793674e-02  5.29420041e-02 -4.99502346e-02
  7.84921634e-04  5.40840551e-02  3.24851312e-02 -1.49185723e-02
  7.49930143e-02 -3.41328681e-02  1.59471463e-02 -6.72609732e-02
 -9.23045427e-02 -7.28558823e-02 -1.41289271e-02 -1.31858299e-02
 -3.60742360e-02  5.88296913e-02 -1.11268647e-02 -1.11294286e-02
  4.44210209e-02  2.42709126e-02 -9.14493203e-02 -6.56116903e-02
  6.20851517e-02  7.87141398e-02  8.85193329e-03 -2.49410868e-02
  1.07943751e-02  5.60709536e-02  1.08070284e-01  2.54249573e-02
  4.70024757e-02 -3.27337794e-02  4.17715870e-02 -3.21090035e-02
  6.68326393e-02  2.09123977e-02  8.31178799e-02  5.75441495e-03
 -1.57177057e-02  5.13668582e-02 -1.00466989e-01  5.68194799e-02
 -7.79418200e-02 -7.36578256e-02  2.29164143e-03  1.09611116e-02
 -3.92328426e-02 -1.02612153e-01  2.16487814e-02  2.21487638e-02
  4.73303422e-02 -1.24629773e-02  1.68757718e-02 -1.58568248e-02
 -1.49172219e-03 -5.22242896e-02  2.37852894e-02  2.95598339e-02
 -4.57500108e-02  7.54220188e-02 -8.37374944e-03  2.92717386e-02
 -7.08715320e-02 -5.05744107e-02  3.37662622e-02  9.25058499e-03
  6.04491401e-03  5.77810109e-02 -5.38401343e-02  1.88186746e-02
  2.49538794e-02 -2.30869707e-02  4.80298363e-02 -1.32949762e-02
  6.71137720e-02  1.58789922e-02  3.83781157e-02  5.01072630e-02
  5.06595746e-02  7.17647970e-02  4.81168069e-02 -3.42952199e-02
  2.15225033e-02 -2.26974841e-02 -4.45528142e-02  1.36768846e-02
  3.95529121e-02  9.66346115e-02 -7.90077373e-02 -2.76889054e-08
  4.37600647e-05  1.28331250e-02  8.19380730e-02 -1.11790355e-02
  5.15342504e-02 -8.69946275e-03  3.87721956e-02  4.68982942e-02
 -2.15304736e-02  1.09144384e-02 -1.34722795e-02 -5.39821126e-02
 -5.36090359e-02  1.06498577e-04  9.06490088e-02  5.04027084e-02
 -4.07694206e-02 -2.80015147e-03  3.82598266e-02 -8.40364844e-02
  3.06539945e-02  7.10854530e-02 -5.35856653e-03 -4.30149212e-02
 -3.41456267e-03 -7.31946230e-02  4.09532413e-02 -5.65154618e-03
  9.22913849e-03  2.75571328e-02 -5.08137271e-02 -5.10621220e-02
  5.90167232e-02 -3.64067666e-02  9.62798372e-02  4.31413203e-03
  5.34966327e-02 -2.88944710e-02  2.93812994e-03  6.43181428e-02
 -2.65936963e-02  8.72200802e-02  4.80443090e-02 -4.97647300e-02
  7.63691915e-03 -4.89581898e-02  1.61224399e-02 -4.40009758e-02
 -1.88040975e-02  6.26380444e-02 -7.08308667e-02  7.13098049e-02
 -4.59356047e-02  8.05362687e-02  4.94703427e-02  2.27457359e-02
 -2.76032165e-02 -1.40729666e-01 -2.81152502e-02  2.60950923e-02
 -1.65610705e-02  1.23978546e-02  5.14509305e-02  7.77910324e-03]",2,2
imantics,1,image understanding is widely used in many area like satellite imaging robotic technology sensory network medical and biomedical imaging intelligent transportation system etc recently semantic analysis ha become an active research topic aimed at resolving the gap between low level image feature and high level semantics which is a promoting approach in image understanding with many image annotation semantics existing in the field of computer vision it can become daunting to manage this package provides the ability to convert and visualize many different type of annotation format for object dectection and localization currently support format if you enjoy my work please consider supporting me,"[('many image annotation semantics', 0.6245), ('image understanding', 0.5999), ('low level image feature', 0.5377), ('computer vision', 0.4921), ('high level semantics', 0.4356), ('object dectection', 0.4), ('semantic analysis', 0.3705), ('robotic technology', 0.3502), ('annotation format', 0.3458), ('sensory network', 0.3238)]","[ 7.11422786e-02 -5.78678623e-02  7.12058917e-02 -9.32679921e-02
  8.09306279e-02 -2.94495299e-02  2.52085589e-02  4.79725711e-02
 -1.51588889e-02 -8.64399597e-03 -5.09652868e-03 -5.34940809e-02
  3.37407216e-02  9.68233645e-02  3.31691876e-02  2.96390168e-02
  4.30585630e-02  7.09602386e-02 -8.63669887e-02 -1.72272697e-02
  7.26905987e-02 -2.80631846e-03  4.78322618e-02 -2.94242576e-02
 -4.11490798e-02  4.13352391e-03  4.27035205e-02 -1.48321599e-01
 -1.95336924e-03 -4.77857850e-02 -5.42561039e-02  3.47498283e-02
  5.95027097e-02  4.46125604e-02  1.28648402e-02  3.45948562e-02
  4.37092520e-02 -2.57104933e-02  1.50790215e-02 -3.01615968e-02
 -5.42296879e-02 -2.22101156e-03  3.00882617e-03 -4.03970443e-02
  1.37250870e-01  8.49452764e-02 -2.49203462e-02 -9.07094181e-02
 -1.41031472e-02  8.27303715e-03 -7.86842033e-02 -1.72248781e-02
 -7.69586638e-02  2.03075558e-02  1.51074762e-02 -4.71926033e-02
  1.49062918e-02 -2.92124767e-02  2.76233573e-02  1.94895957e-02
  4.53818552e-02 -2.96281464e-02 -2.46612150e-02  3.01942769e-02
  3.54941301e-02 -2.79123690e-02 -1.66503973e-02 -7.02659264e-02
  9.34195817e-02 -9.56813097e-02 -4.65503894e-03 -6.67644199e-03
  3.92580256e-02 -9.75986663e-03 -3.13972011e-02 -1.41153848e-02
  6.99621290e-02  1.31805241e-02  3.86242755e-03 -1.60733506e-01
  1.41230803e-02  2.18562521e-02  5.48499376e-02 -4.50859815e-02
  3.34712192e-02  3.55988415e-03 -7.73597956e-02  2.18989830e-02
 -4.04699221e-02  5.06185368e-02 -1.14006829e-02 -9.71916094e-02
 -7.08372816e-02 -1.64077468e-02  4.04923186e-02 -4.01927345e-02
 -2.77852863e-02 -1.21093281e-01  1.95476878e-02  2.40019523e-02
 -1.72835551e-02 -3.13408263e-02  1.47340922e-02  2.66512409e-02
  2.00498775e-02  1.67348571e-02 -8.75184778e-03 -5.90084083e-02
  6.77198693e-02  2.62731388e-02 -1.15045523e-02 -1.83457527e-02
 -2.85720490e-02 -6.06048927e-02  4.46794834e-03 -1.34869963e-02
 -5.66903092e-02  2.91152541e-02  9.50069129e-02 -3.39973830e-02
 -5.93683012e-02 -7.54320025e-02  3.34741846e-02 -6.03564568e-02
  7.27443118e-03 -3.20797861e-02 -8.89759660e-02  3.90997449e-33
  2.80623958e-02 -1.83844585e-02  3.40148099e-02 -1.12962462e-02
 -4.37513599e-03 -2.21739057e-02  2.67135585e-03 -2.62745675e-02
  1.43248606e-02 -3.86056192e-02 -5.74042322e-03  3.79973017e-02
 -9.12907198e-02  1.22465611e-01  1.66212600e-02 -1.25123688e-03
  3.37856784e-02  3.41828391e-02 -4.77117486e-02  5.74801117e-02
  1.89306214e-02  3.23794149e-02  3.08671091e-02  9.53730270e-02
  2.36957539e-02  6.18852535e-03 -6.28142478e-03 -6.78591058e-02
 -9.36582685e-03  3.72045999e-03 -3.94554213e-02  3.58740464e-02
  5.41831069e-02  3.57779227e-02  1.61147974e-02 -7.32304063e-03
  8.20741244e-03  9.10351635e-04  1.19157042e-02 -2.31090374e-02
 -5.97036257e-03  4.52954173e-02 -5.20327240e-02 -5.69266938e-02
  2.71403529e-02  1.69273391e-02 -1.64433606e-02  5.42318113e-02
 -6.42953888e-02  2.68172808e-02 -3.31794377e-03 -2.05149818e-02
  1.73935387e-03 -7.43916780e-02 -2.28704698e-02  1.54315708e-02
  1.95922403e-04 -1.26853473e-02  3.78721394e-02 -6.97672442e-02
 -1.31773297e-02  2.48602722e-02  8.38665888e-02 -2.74763480e-02
  1.39170722e-03  1.84272714e-02  6.05641454e-02  1.43003883e-02
 -1.30175417e-02  7.00960904e-02 -5.01153655e-02  6.27855808e-02
 -3.59634459e-02 -5.98987639e-02  1.71661342e-03 -1.85792451e-03
 -1.81381069e-02 -7.23912567e-02 -4.40077810e-03  1.13174073e-01
 -1.23952553e-01  3.39097902e-03  9.28323832e-04 -2.91158818e-02
  4.88188723e-03  3.59055027e-02  2.07740515e-02 -5.52005768e-02
  4.92929630e-02 -1.68802980e-02 -6.71260059e-02  2.13863067e-02
 -2.46842485e-02  3.92649062e-02 -4.81093153e-02 -3.13738597e-33
 -3.79924886e-02  5.19649014e-02 -1.17062055e-01  4.53357920e-02
 -4.01775204e-02 -5.82766756e-02  3.49441096e-02  3.33791040e-02
  2.02378090e-02  2.16394402e-02 -1.11575006e-02  4.82614525e-03
 -4.85985540e-02 -1.42347822e-02 -5.57534806e-02 -8.07465985e-02
 -1.67082194e-02 -8.78337864e-03  1.04574999e-03  8.33670869e-02
 -3.74349393e-02  9.04520527e-02 -1.17475092e-01  5.16467355e-02
 -8.86025652e-03  8.57750997e-02 -1.01485275e-01 -8.01913068e-02
  5.43801002e-02 -4.50150762e-03 -4.32952791e-02 -4.52963337e-02
  1.14179052e-01  4.20303456e-02  2.51102727e-02 -2.92378683e-02
  5.55116050e-02 -8.31676871e-02 -4.18091491e-02 -1.65661704e-02
  1.04444526e-01  4.90167597e-03 -2.68630050e-02  9.58692096e-03
 -2.12031715e-02 -7.45843872e-02 -9.93169695e-02  5.94836473e-02
 -1.30632035e-02  5.26329279e-02 -1.31760752e-02 -1.67067070e-02
 -4.06996673e-03 -3.85222696e-02 -1.52899157e-02 -8.38481821e-03
  6.01931922e-02  1.16681363e-02  3.51490974e-02  3.55316806e-05
 -2.51869094e-02 -8.38431269e-02  2.47792602e-02  7.30991811e-02
 -7.97636900e-03  3.69402096e-02 -5.45826554e-02  2.23719124e-02
 -7.87238702e-02 -1.60593912e-02  5.10290377e-02  6.09601438e-02
  2.38660686e-02  6.69709593e-02 -5.12812287e-03 -2.85423491e-02
 -1.50396479e-02  5.49719855e-02  7.39179552e-02 -2.10232567e-02
  5.10979369e-02 -3.64722461e-02  1.31716952e-02  1.55491963e-01
  4.88312356e-02  1.24297783e-01 -4.22286876e-02 -8.29126686e-02
  3.76335122e-02 -2.82185040e-02 -3.39783803e-02  2.55650142e-03
  7.46704219e-03  8.31451789e-02 -8.74214247e-02 -2.70120548e-08
 -4.65390272e-02 -3.86017151e-02  9.39808115e-02 -7.28411749e-02
  3.82987820e-02 -6.13756850e-03 -1.55820679e-02  1.68242469e-01
 -2.32610330e-02  1.48876384e-02  1.89086224e-03  1.67791843e-02
 -1.01822637e-01 -5.93207814e-02  9.33813602e-02  3.66754900e-03
  2.37839818e-02  2.05005556e-02  4.48694229e-02 -2.74513494e-02
 -6.71148393e-03 -1.61657277e-02  7.05169467e-03  6.90990780e-03
  2.98555177e-02 -2.08809320e-02 -2.77710687e-02  5.81975132e-02
 -7.12407893e-03  1.11601017e-02 -1.18724518e-02 -3.43511365e-02
  6.04000986e-02 -2.66830553e-03  7.21495897e-02  1.00607164e-01
 -9.35771037e-03 -4.69358936e-02 -5.21329045e-02 -3.90852802e-03
  1.16510969e-02  1.46242930e-02  2.26631276e-02 -2.00690422e-02
  7.71887824e-02 -4.80998820e-03  2.58044470e-02 -2.84232255e-02
 -7.14520440e-02  1.83458962e-02 -1.30752236e-01  1.12325691e-01
 -7.45894536e-02  9.55979675e-02 -1.76412966e-02  3.44947353e-02
  7.51067549e-02 -4.06686217e-02  3.76686938e-02  1.06850453e-01
  2.19414625e-02  5.25537096e-02  2.91511677e-02 -1.96443666e-02]",2,2
nnmnkwii,1,library to build speech synthesis system designed for easy and fast prototyping the latest release is availabe on pypi assuming you have already numpy installed you can install nnmnkwii by if you want the latest development version run or this should resolve the package dependency and install nnmnkwii property at the moment nnmnkwii autograd package depends on pytorch if you need autograd feature please install pytorch a well the library is inspired by the following open source project logo wa created by gloomy ghost,"[('speech synthesis system', 0.6283), ('moment nnmnkwii autograd package', 0.3663), ('pytorch', 0.3229), ('library', 0.2747), ('nnmnkwii', 0.2701), ('pypi', 0.2552), ('latest development version run', 0.2144), ('nnmnkwii property', 0.183), ('autograd feature', 0.1797), ('gloomy ghost', 0.1616)]","[-9.12217423e-02 -6.61896989e-02 -2.37682881e-03 -5.03530987e-02
 -3.17501873e-02 -1.36852097e-02 -2.90504713e-02  3.95363313e-04
 -5.23342341e-02 -6.74697906e-02  2.55243294e-03 -1.15221739e-02
 -2.65293103e-02 -3.08622103e-02 -1.87984928e-02  2.32527070e-02
 -4.96501476e-02  6.44358844e-02  1.91303045e-02 -6.46334812e-02
  7.68551044e-03  7.15676546e-02  4.57709096e-02  8.09389260e-03
  1.00451529e-01  1.97127983e-02  1.17457239e-02  1.26894601e-02
  7.30651841e-02 -8.02186430e-02  1.33760367e-02  8.24480951e-02
  1.01293735e-01 -6.12307265e-02  1.86045785e-02 -1.97129659e-02
 -2.21682470e-02  8.83349217e-03 -1.27766673e-02 -3.46692726e-02
 -3.42194252e-02  2.31554281e-04 -1.00986743e-02 -1.13953790e-02
  4.58379090e-02 -5.09931296e-02 -5.80952540e-02 -5.50523438e-02
  2.63277832e-02 -2.17368770e-02 -5.64844236e-02 -7.75253624e-02
 -2.61602104e-02  2.17660628e-02  3.17320824e-02  3.07742110e-03
  2.65265908e-02  4.51336391e-02 -3.53848352e-03 -4.10248712e-02
  2.98633613e-02  4.66370909e-03 -5.32942340e-02 -4.43771221e-02
  6.91817030e-02  6.15279265e-02  4.49383035e-02  1.43514127e-02
  8.94121900e-02 -2.01183800e-02 -1.13617279e-01  4.19650786e-03
  1.29256037e-03  6.02525100e-02 -1.35720698e-02  7.39303650e-03
  4.97290827e-02 -7.98424520e-03  5.18736281e-02 -5.41757308e-02
  1.82318073e-02 -1.23249991e-02  4.88305576e-02 -2.60076076e-02
  4.92430627e-02  4.76288795e-02 -5.86390384e-02  5.11891581e-03
 -4.67344075e-02 -1.51684145e-02 -4.79356619e-03 -1.17888652e-01
 -3.24562266e-02  5.17001599e-02  1.46348597e-02 -4.54622461e-03
 -4.58528325e-02 -2.86695082e-02 -1.04003428e-02  5.25710583e-02
 -4.69838921e-03 -7.15545490e-02  7.47727649e-03 -2.13002712e-02
 -4.79451083e-02 -2.09886488e-03  3.59457135e-02  1.07753323e-03
  6.77166283e-02 -3.63026261e-02 -1.05708547e-01 -7.23604634e-02
 -2.65713781e-02 -1.03111178e-01  8.23020563e-02 -1.57298800e-02
 -6.54271767e-02 -3.00284754e-03  7.01320693e-02  4.95457090e-03
 -5.23443967e-02  9.96216107e-03 -4.95026149e-02  9.73385945e-02
 -4.10773195e-02 -2.10111495e-04 -5.39174937e-02  6.48653578e-33
  2.69190632e-02  6.05888255e-02 -5.31433001e-02  4.13022228e-02
  8.32346305e-02 -1.14736758e-01 -3.30147240e-03 -2.00840868e-02
 -3.21881808e-02 -7.90716112e-02  1.23305731e-02 -4.79093939e-02
 -1.04658037e-01  2.45792829e-02  1.06241377e-02 -9.24852863e-02
 -3.90675440e-02  3.87013778e-02  4.35186736e-02  3.02358950e-03
  5.13770692e-02  6.54517114e-02  4.68435250e-02  1.64777786e-02
  1.43584926e-02  5.28781451e-02  8.25783759e-02  1.83793344e-03
  1.80156855e-03 -4.22483170e-03 -3.79022546e-02 -3.09283976e-02
  2.92004738e-02 -4.84393612e-02 -1.23423776e-02 -6.82941154e-02
 -6.04705550e-02 -8.75500124e-03  1.71767883e-02 -4.58584875e-02
 -7.04988018e-02  2.03855764e-02 -2.91846823e-02 -5.08425012e-02
  2.47516837e-02  6.20005652e-03 -1.11990506e-02  1.43838212e-01
  5.04797995e-02 -7.42504075e-02 -1.83517579e-02  5.15086129e-02
 -1.18394673e-01  5.11110090e-02  6.05020225e-02 -4.48384993e-02
  9.02393386e-02 -8.10196623e-03  6.12106696e-02  5.15381712e-03
  3.25252078e-02  2.89360359e-02  5.71938902e-02 -9.88877416e-02
  5.23622148e-02  2.90240478e-02 -1.34139895e-01 -3.55096199e-02
  4.63488996e-02 -1.89462223e-03 -4.44632284e-02 -3.30363140e-02
  1.36140443e-03  4.55124080e-02  2.33353190e-02  3.28518264e-02
 -2.29785033e-02 -1.93489715e-02  1.58719532e-02 -1.04904315e-02
 -5.26091903e-02  7.53367841e-02 -5.22100590e-02 -6.84378669e-02
  1.70869399e-02 -4.49880473e-02  9.55071114e-03 -8.61833394e-02
 -5.28643876e-02  3.08232792e-02 -3.03618331e-02 -5.35113923e-02
 -4.30258317e-03  9.22539383e-02 -4.19300683e-02 -6.94339019e-33
 -1.02501847e-02  9.76025090e-02 -9.04949233e-02  8.54051262e-02
 -4.81889062e-02  3.60106416e-02 -1.63157284e-02  3.70116010e-02
  4.21491265e-02  4.48015444e-02  2.25987080e-02 -6.25419151e-03
  1.29079998e-01  5.32358140e-03  8.87669250e-02 -1.64476000e-02
  4.27889749e-02 -1.35479430e-02  4.19290625e-02  3.17768939e-02
 -6.09460101e-03  1.00414239e-01 -6.85396940e-02 -2.03439388e-02
 -6.57067746e-02 -1.13213090e-02 -5.28751872e-02  5.31366542e-02
  7.95802288e-03  2.48894747e-02 -2.49437485e-02  1.10915201e-02
 -1.41440779e-01  3.04104518e-02 -4.58180159e-02  1.59534942e-02
  6.91640750e-02 -4.88591716e-02 -3.23982611e-02 -2.90181655e-02
  1.46717504e-01  1.66458916e-02  3.96649241e-02 -4.45711799e-02
  9.19612683e-03  2.28714421e-02 -1.04835488e-01  6.64318204e-02
  3.31856892e-03 -4.08877432e-02  8.24227035e-02 -3.83064011e-03
  7.09059909e-02 -2.19721254e-02 -4.27131802e-02  3.55841927e-02
  4.53740582e-02  7.54891662e-03  7.35543817e-02  2.88307257e-02
 -9.05910730e-02 -5.60105778e-02  7.14285858e-03 -1.34765610e-01
 -2.79617738e-02  5.29338941e-02 -4.08546478e-02  7.93152303e-02
  5.33579066e-02 -2.51659825e-02  4.18448560e-02  8.16887319e-02
  1.82443541e-02  5.70622571e-02 -4.71599586e-02  1.93129722e-02
 -3.26492079e-02 -2.61429343e-02 -4.56065387e-02 -7.32658431e-02
 -1.62132662e-02  4.07893062e-02  2.34436486e-02  3.97557691e-02
 -1.74489226e-02  7.55007789e-02  6.95199668e-02  2.53913626e-02
  9.54269618e-02 -3.39113153e-03  1.93577036e-02  1.05831258e-01
 -8.97164736e-03  5.53166717e-02 -1.89299546e-02 -3.66539368e-08
 -3.83827202e-02 -2.63626716e-04  2.56045777e-02 -3.95105518e-02
  5.19854650e-02 -9.75561440e-02  5.15603870e-02 -3.98319727e-03
 -1.56968292e-02 -9.55391955e-03  2.21421383e-02 -3.90260927e-02
 -6.72489777e-02  2.85973456e-02 -4.35190828e-04  3.04567572e-02
 -2.83815283e-02  1.15761749e-01 -6.50368258e-03 -1.46147966e-01
  4.10319082e-02  4.38388027e-02  1.20908786e-02  5.63325658e-02
  1.75670590e-02 -4.78216931e-02  7.02725276e-02 -1.43969189e-02
  2.45555639e-02  4.91617173e-02 -1.06249768e-02  8.62833187e-02
  5.05355895e-02 -7.32732937e-02  4.18950245e-02  6.36377111e-02
  1.34925237e-02 -6.63745403e-03 -1.75221134e-02  1.12100914e-02
 -4.18529175e-02 -8.43332615e-03 -6.84040114e-02  4.64483909e-03
  2.16489267e-02 -1.62003282e-02 -2.65226569e-02 -1.35931656e-01
 -1.73085984e-02  4.48490260e-03  3.12979147e-02  6.17740154e-02
 -3.57093476e-02 -1.68733466e-02  4.07455564e-02  6.57892972e-02
 -3.74286398e-02 -1.48851341e-02  2.41066683e-02  4.64221649e-03
 -3.52528803e-02  3.55409496e-02  1.67971142e-02 -1.64964795e-02]",2,2
torch-pitch-shift,1,pitch shift audio clip quickly with pytorch cuda supported additional utility for searching efficient transformation are included view on pypi view documentation this package includes two main feature also check out torch time stretch a sister project for time stretching check out example py to see torch pitch shift in action see the documentation page for detailed documentation please feel free to submit issue or pull request,"[('pitch shift audio clip', 0.6767), ('torch pitch shift', 0.571), ('pytorch cuda', 0.3818), ('pypi view documentation', 0.3323), ('time stretching check', 0.3239), ('torch time', 0.3227), ('example py', 0.2927), ('efficient transformation', 0.2586), ('additional utility', 0.1451), ('view', 0.0937)]","[-1.04787625e-01 -4.64549810e-02 -6.30890206e-02 -8.63501504e-02
  2.46204752e-02  2.64798366e-02 -6.55502379e-02  2.57042628e-02
 -1.71070471e-02 -5.58559634e-02  6.38036802e-02 -2.64201667e-02
 -1.29393741e-01 -3.21903219e-03  2.81539783e-02 -2.41192877e-02
 -3.91084626e-02  8.94704834e-02 -9.22614895e-03 -9.98241529e-02
  8.50263238e-02 -1.95585620e-02  4.56891023e-02  4.11616042e-02
  2.28922889e-02  4.92640510e-02 -8.02382454e-03  2.10295524e-02
  4.85083461e-02 -3.45447473e-02 -9.42770205e-03  1.02120765e-01
  1.00374065e-01  2.05518678e-02 -4.65033278e-02 -4.52616811e-03
  6.62590098e-03  1.31700244e-02 -1.97975002e-02 -1.04391631e-02
 -4.37706988e-03  6.48835395e-03 -3.50590162e-02 -3.47906314e-02
 -4.17320654e-02 -1.06750196e-02 -9.52809583e-03 -3.21648084e-02
 -7.16375485e-02 -4.71622422e-02 -6.45702928e-02 -5.03826179e-02
 -2.58722901e-02  5.88593222e-02  5.65300742e-03  1.25746643e-02
  6.36660978e-02 -1.26948068e-02  8.38690549e-02 -1.23617671e-01
  6.29853159e-02  4.11780439e-02 -6.26925677e-02  2.78859166e-03
 -6.01221621e-02  3.29918675e-02  1.73285473e-02 -2.33223103e-02
  1.13221921e-01 -6.74757734e-02 -1.26059860e-01 -1.63961407e-02
  1.12566026e-02  5.31432889e-02 -2.52268091e-02 -9.12477374e-02
  9.07208249e-02 -5.58933802e-02 -6.48712888e-02 -8.47915635e-02
  1.55043276e-02 -2.06054654e-02  4.00381684e-02  2.67380774e-02
  7.76419044e-02 -2.50013024e-02  7.34788552e-03  6.34239987e-02
 -1.10419756e-02 -2.13015769e-02  9.03661922e-03 -6.75267801e-02
 -1.91221554e-02 -1.86631177e-02  2.39711963e-02 -3.43723642e-03
 -1.21996561e-02 -7.36878291e-02  3.08246892e-02  8.75981152e-03
  3.40071432e-02 -5.73796593e-02 -1.42118251e-02  1.01303505e-02
 -3.21044624e-02 -7.98344892e-03  2.21072268e-02  2.47534811e-02
  4.13142331e-02  5.28635271e-02  1.91794336e-02  3.71927544e-02
 -7.65405819e-02 -1.15829475e-01  1.25630051e-01  3.40467878e-02
 -7.26783425e-02  6.98636919e-02  2.68758088e-02  5.76963872e-02
 -2.81934496e-02  4.33743969e-02 -5.94019927e-02  7.29161650e-02
 -3.48436683e-02  1.87987201e-02 -5.30449525e-02  3.10200102e-33
  4.00480395e-03  6.42048195e-02 -8.94628987e-02 -2.96011716e-02
  5.31504974e-02 -6.73639700e-02 -8.67350958e-03  1.04170796e-02
  5.43054715e-02 -2.43793484e-02  6.80545764e-03  8.84560719e-02
 -1.49065014e-02  3.16823423e-02 -5.63448705e-02 -4.79464121e-02
 -6.08285964e-02  1.31511122e-01  3.65804583e-02  2.36680564e-02
  6.26468041e-04  1.02182664e-02  3.88139710e-02  3.55981551e-02
  5.80705777e-02  9.02081206e-02 -3.72016504e-02 -2.64956877e-02
  3.60679291e-02 -5.02450019e-03 -5.39533235e-02 -1.51883205e-02
 -5.32690203e-03 -8.30697790e-02  1.58167277e-02  2.72222850e-02
 -4.32705544e-02 -7.80146495e-02  1.18527114e-02 -6.33849278e-02
 -4.65141386e-02  1.87816303e-02 -1.91818587e-02 -9.13213566e-02
 -2.78426912e-02 -1.97249688e-02 -6.01578243e-02  1.66571125e-01
  3.32568623e-02  3.88498418e-02  9.40087531e-03  3.07773538e-02
 -6.34460077e-02  8.01680516e-03 -1.99417304e-03 -5.86481765e-02
  7.54598454e-02  1.15060834e-02  1.01270817e-01  1.77374564e-03
  6.27996549e-02  3.22839059e-02  2.29467899e-02 -5.79676665e-02
 -1.27814310e-02  5.10057108e-03 -4.04732525e-02 -8.24657641e-03
  7.33558554e-03  3.95158790e-02 -1.74647257e-01  3.64950411e-02
 -4.80347499e-02  1.12579362e-02 -1.69392284e-02 -2.22396050e-02
 -1.44498516e-02 -2.40689721e-02 -3.01665068e-02  4.25500944e-02
 -6.75820783e-02  5.98553307e-02  9.74275470e-02 -2.12993398e-02
 -5.63182645e-02 -5.39156459e-02  2.61789095e-02 -4.26903032e-02
 -2.97967736e-02 -3.98531221e-02 -6.54188097e-02 -2.20855661e-02
  8.50771740e-03  9.71488096e-03  2.67924387e-02 -2.44123581e-33
  1.79262422e-02  1.50423134e-02 -1.46213286e-02  8.16716552e-02
  1.46509511e-02  1.74758267e-02  2.68065166e-02  1.20341917e-02
  2.59139035e-02 -6.65694326e-02  1.87024996e-02 -7.24997446e-02
 -2.33083628e-02 -7.86581710e-02  1.55817019e-02 -2.36908104e-02
 -1.79943386e-02  1.43007480e-03 -3.91132943e-03  3.13705876e-02
  2.67278366e-02  8.08029920e-02 -5.21528088e-02  5.18110208e-03
 -7.39564076e-02  1.19609311e-02  1.02727581e-02 -3.30510996e-02
  1.08491806e-02 -3.56009603e-02  7.28914188e-03  3.27247791e-02
 -8.53924602e-02 -2.06610207e-02 -5.66226989e-02  4.63504791e-02
  1.41452756e-02  5.16860895e-02 -2.62902807e-02  4.75990847e-02
  1.30819529e-01  1.11248426e-01  7.35749006e-02 -1.40765905e-02
 -9.75655094e-02  5.09908162e-02 -2.31758114e-02  8.58817473e-02
 -5.40663004e-02  3.20507698e-02  8.93865675e-02  1.25458594e-02
  3.48130502e-02 -6.34126663e-02 -1.36090508e-02 -1.13236224e-02
  6.80249417e-03 -3.65838557e-02  4.30885208e-04  3.84456408e-03
 -8.93937275e-02 -5.15804514e-02 -1.29653215e-02 -3.68950292e-02
 -2.32308675e-02  4.08403063e-03 -4.56832955e-03 -3.14955786e-02
  4.50049266e-02 -8.34105350e-03  7.40037262e-02 -9.07330308e-03
  3.74004617e-02  1.73915457e-02 -2.53218412e-02  1.01964725e-02
 -1.59534961e-02 -2.90780328e-02 -4.70409505e-02  1.80874933e-02
  6.54373169e-02 -4.48450595e-02  8.47073644e-02  3.29999849e-02
  8.17251392e-03  1.48711905e-01 -1.01190656e-02  3.43330726e-02
  5.06656170e-02 -5.67126041e-03 -3.78471427e-02  5.38175143e-02
  7.33565465e-02  8.33333880e-02  7.14943707e-02 -2.24864394e-08
 -3.18040885e-02  4.69585098e-02  4.73833345e-02 -1.15093682e-02
 -4.88664024e-02 -1.68258995e-02  5.40168770e-02  5.46079092e-02
  6.86825812e-03 -4.16742228e-02  4.69984338e-02 -2.19133329e-02
  4.34373580e-02 -2.43113725e-03 -1.26809757e-02  8.78833979e-02
  2.68981308e-02  8.02979842e-02  3.07086797e-04 -6.25703707e-02
 -8.41619354e-03  7.42196366e-02  1.97030846e-02 -6.27983268e-03
 -1.89034380e-02  2.44865865e-02  1.78823471e-02  8.86433944e-02
  3.42199951e-02 -4.80847359e-02  2.39300635e-02  5.52910492e-02
  8.69411826e-02 -2.56874468e-02  9.22307670e-02  3.09825130e-02
 -5.15487902e-02  4.27879160e-03 -2.00908184e-02  9.75140035e-02
 -4.06644791e-02 -9.01162811e-03 -9.36376229e-02  8.43197480e-03
 -2.96512656e-02  1.83509607e-02 -1.93195709e-03 -6.59867376e-02
 -8.90528038e-02  4.05312702e-02  1.50487544e-02  6.61651418e-02
 -4.21227515e-02  7.69864535e-04  4.85041924e-03  1.37270093e-01
  4.33676038e-03 -4.23533767e-02 -1.32224495e-02 -5.95424883e-02
 -3.34479325e-02  2.17060000e-02 -2.47702170e-02  1.13343839e-02]",2,2
geffnet,1,a generic implementation of efficientnet mixnet mobilenetv etc that cover most of the compute parameter efficient architecture derived from the mobilenet v v block sequence including those found via automated neural architecture search all model are implemented by genefficientnet or mobilenetv class with string based architecture definition to configure the block layout idea from here implemented model include i originally implemented and trained some these model with code here this repository contains just the genefficientnet model validation and associated onnx caffe export code i ve managed to train several of the model to accuracy close to or above the originating paper and official impl my training code is here http github com rwightman pytorch image modelsmore pretrained model to come the weight ported from tensorflow checkpoint for the efficientnet model do pretty much match accuracy in tensorflow once a same convolution padding equivalent is added and the same crop factor image scaling etc see table are used via cmd line args important to run validation for tf efficientnet b python validate py path to imagenet validation model tf efficientnet b b img size crop pct interpolation bicubicto run validation w tf preprocessing for tf efficientnet b python validate py path to imagenet validation model tf efficientnet b b img size tf preprocessingto run validation for a model with inception preprocessing ie efficientnet b advprop python validate py path to imagenet validation model tf efficientnet b ap b num gpu img size crop pct mean std tfp model validated with tf preprocessing pipelinegoogle tf and tflite weight ported from official tensorflow repositoriesall development and testing ha been done in conda python environment on linux x system specifically python x x x user have reported that a python anaconda install in window work i have not verified this myself pytorch version have been tested with this code i ve tried to keep the dependency minimal the setup is a per the pytorch default install instruction for conda model can be accessed via the pytorch hub apithis package can be installed via pip install after conda env install eval use train use create in a nn sequential container for fast ai etc script are included toas an example to export the mobilenet v pretrained model and then run an imagenet validation these script were tested to be working a of pytorch and onnx w onnx runtime caffe compatible export now requires additional args mentioned in the export script not needed in earlier version,"[('efficientnet mixnet mobilenetv', 0.6157), ('genefficientnet model validation', 0.5792), ('imagenet validation', 0.5784), ('efficientnet model', 0.5534), ('tensorflow', 0.5052), ('efficientnet b python', 0.5026), ('official tensorflow', 0.496), ('genefficientnet', 0.4897), ('mobilenet v', 0.4678), ('http github com rwightman pytorch image modelsmore', 0.4487)]","[-6.45042509e-02 -7.85086155e-02  4.72709313e-02 -2.70248279e-02
  1.35162473e-01 -2.08807644e-02 -3.09299193e-02 -2.54362803e-02
 -5.84751889e-02 -3.03766411e-02  5.26503893e-03  1.38204759e-02
  3.24083231e-02  4.88636456e-02  1.65353585e-02  2.44809464e-02
  8.65353178e-03  9.82591584e-02 -8.40785801e-02 -9.04003233e-02
 -2.20545959e-02 -4.27007035e-04  5.14771827e-02 -5.73654324e-02
  2.93423161e-02 -2.88078561e-02 -2.72791106e-02 -3.13137881e-02
  5.84630333e-02 -4.96778414e-02  2.77784467e-02  1.14101330e-02
  9.82960500e-03  6.16277866e-02 -3.90905514e-02 -1.91258099e-02
 -3.12440097e-03 -1.40335853e-03 -2.56048031e-02 -1.00582698e-03
 -2.25027725e-02 -1.77245215e-02 -1.08421948e-02 -1.67453885e-02
  5.90984672e-02  1.31127387e-02  9.33502801e-03 -9.35565606e-02
  4.27804701e-02 -4.26197313e-02 -2.57137045e-02 -7.74617270e-02
 -5.99257164e-02  4.77538444e-02  9.28749330e-04 -2.46992968e-02
 -2.72909962e-02 -4.31022421e-02  1.50053529e-02 -2.11736001e-02
  4.07953523e-02  2.05029431e-03 -9.60445553e-02  5.91330193e-02
  1.27011014e-03  2.92129200e-02  1.66525207e-02  1.54343690e-03
  1.00401938e-01 -1.14976868e-01 -1.83045361e-02  1.32549526e-02
 -5.19230254e-02  4.50684913e-02 -2.15303414e-02  5.38367662e-04
  1.56875968e-01  8.93013366e-03  8.81742500e-03 -9.02946666e-02
 -2.49690725e-03  3.94566404e-03  3.48810479e-02 -3.08376253e-02
  6.53927401e-02  2.53921263e-02 -1.01453960e-01  9.88103077e-02
 -4.52003628e-02 -5.31620979e-02 -4.84711118e-03  1.57675911e-02
  6.46393467e-03  1.88598502e-02  6.80704638e-02  1.85553730e-02
 -4.17989679e-03 -1.56535357e-01 -6.92579448e-02  9.71096829e-02
 -7.61306286e-02 -8.22541118e-02  2.44984366e-02 -1.62424631e-02
  7.86933526e-02  3.83672640e-02  9.13256928e-02  3.18424478e-02
  9.82176661e-02  1.60998907e-02  2.44425163e-02  1.11695947e-02
 -2.69383397e-02 -6.11724518e-02  4.06924188e-02  2.31910795e-02
 -5.86736500e-02  5.21650612e-02  5.41866980e-02  3.83617356e-02
 -1.11972511e-01  1.13543484e-03 -4.11367649e-03 -2.95572355e-02
  1.14227238e-03 -3.78346555e-02 -1.04886159e-01  7.23178304e-33
 -4.37213816e-02  8.38060305e-03  2.76662186e-02 -6.08854508e-03
  9.66723263e-02 -4.80573401e-02  5.11637218e-02 -2.22982001e-02
 -1.10943178e-02 -8.48446488e-02 -1.24972425e-01 -4.31433599e-03
 -4.05853353e-02  1.37904719e-01  4.39772978e-02 -1.02057785e-01
 -3.22374552e-02  4.63402420e-02  6.52764142e-02  1.01457335e-01
  2.96513527e-03 -8.13063309e-02 -1.05617512e-02  7.62619972e-02
 -2.49544363e-02 -3.34987715e-02  4.11892198e-02  2.49682385e-02
  7.37014264e-02  3.58179398e-02 -7.11266994e-02  7.94988312e-03
  4.52548563e-02  1.13283433e-02  1.01740658e-02 -5.55184819e-02
 -7.48994946e-02 -3.47591862e-02  1.61770042e-02 -1.58494692e-02
 -3.80583666e-02  4.08852659e-02 -4.96449657e-02 -8.35145544e-03
 -5.12086116e-02 -3.44900116e-02 -6.96026012e-02  1.00118503e-01
  1.67532340e-02  3.70097980e-02  3.49486805e-02  1.15312124e-02
 -2.20148880e-02 -7.61002526e-02 -9.20497254e-02 -4.46667436e-05
  5.30088544e-02  2.58388687e-02  5.26504070e-02  2.86052432e-02
  1.76357105e-02  1.20698940e-02 -4.30279598e-02  6.96437154e-03
  1.00776507e-02 -2.69152485e-02 -2.15054508e-02 -6.27453849e-02
 -3.46461050e-02  4.14526165e-02 -4.90714200e-02  6.21179119e-02
 -3.26861888e-02 -4.06387784e-02  6.69532493e-02 -3.92880626e-02
 -7.49531016e-03 -2.86117028e-02 -4.87558357e-02  9.57361758e-02
 -1.17087685e-01  7.59339482e-02 -3.38633619e-02 -5.07484972e-02
 -2.33556312e-02 -1.71885584e-02 -1.51127335e-02 -1.68738831e-02
  7.11167753e-02  6.37273788e-02  2.38450803e-02 -1.79458670e-02
  1.44754816e-02  2.77423169e-02 -2.19832025e-02 -4.04410611e-33
 -1.05024483e-02  1.39193490e-01 -5.72250038e-02  1.13417923e-01
 -1.26451654e-02 -5.52935489e-02  5.30298129e-02  1.12821618e-02
 -1.48978559e-02 -4.55860235e-02  9.28975046e-02  7.33545609e-03
 -9.93559230e-03 -6.70306087e-02 -1.53177995e-02 -3.81051935e-02
 -4.00186665e-02 -9.36905220e-02  4.66684923e-02  1.71789825e-02
 -1.22524723e-02  4.87734079e-02 -4.47558761e-02 -3.12986202e-04
 -7.43086860e-02 -6.70446828e-03 -4.34829295e-02  5.41981980e-02
 -1.86125794e-03 -7.14914175e-03 -1.00380350e-02 -3.09830885e-02
 -1.67148635e-02 -1.40061583e-02  4.58807200e-02  3.43958512e-02
  1.08768083e-01 -1.38784852e-02  7.71210669e-03  6.73750043e-02
  1.36413008e-01  3.22586745e-02 -7.81411603e-02  7.18208477e-02
 -3.76852565e-02 -4.72785812e-03 -5.49028404e-02 -5.76430466e-03
  2.28792342e-04 -5.22140972e-02 -1.28077529e-03  1.20856054e-02
 -7.54587650e-02  5.90150652e-04  5.33506423e-02  3.86519022e-02
  6.10101558e-02  4.62241508e-02  5.47362380e-02  1.72597598e-02
 -5.00598550e-02 -1.07152268e-01 -6.27448857e-02  4.25465703e-02
 -3.42374970e-03  6.08986169e-02 -9.33955461e-02  4.39905152e-02
 -1.06139649e-02  2.75011901e-02 -2.72847414e-02 -3.47815524e-03
  3.71132195e-02  8.82631615e-02 -4.54740003e-02 -5.49766272e-02
  8.21539462e-02  3.23517923e-03  3.25252302e-02 -3.89286615e-02
  3.04233544e-02  3.05799320e-02 -8.11946392e-03  3.92527208e-02
  3.85765806e-02  7.88498074e-02  3.45486663e-02 -3.54611687e-02
  3.65398005e-02  2.80291010e-02 -9.88272857e-03  1.77825950e-02
  4.96056452e-02  6.71565533e-02  2.45931596e-02 -2.81568493e-08
 -2.19437405e-02  5.49574979e-02  6.45229518e-02 -3.21140140e-02
 -2.33058464e-02  3.58458352e-03  6.49828166e-02  4.80219116e-03
  8.53910521e-02  5.26108453e-03 -2.46837568e-02  1.24436151e-02
 -5.49271926e-02 -5.07827774e-02  3.42970155e-02  9.36508849e-02
 -8.92362930e-03  6.72088712e-02  2.22468190e-02 -4.14869525e-02
  1.74876433e-02  1.67819820e-02 -1.44768581e-02  9.64004546e-03
  1.96330305e-02 -7.13555096e-03 -2.29836348e-02  6.32657036e-02
 -1.96024566e-03 -6.41371533e-02 -3.95222865e-02 -4.22267476e-04
  3.03188022e-02 -7.12012351e-02  5.55263124e-02  6.81439415e-02
 -4.65208776e-02 -2.91416217e-02  2.13782135e-02  1.76704787e-02
 -4.38914634e-02  5.20241894e-02  5.55942347e-03 -7.38451704e-02
  7.42443576e-02 -3.71794365e-02 -1.14459125e-02 -8.71440694e-02
 -4.15489674e-02  4.21223324e-03  4.99512926e-02  1.55119104e-02
 -2.37481631e-02  7.53519237e-02 -1.38508119e-02  4.52670502e-03
  1.94985056e-04 -9.56448317e-02  1.82649847e-02  3.49926054e-02
  2.16048653e-03  8.44439343e-02  4.63244468e-02 -6.80505857e-02]",2,2
torchtuples,1,torchtuples is a small python package for training pytorch model it work equally well for numpy array and torch tensor one of the main benefit of torchtuples is that it handle data in the form of nested tuples see example below torchtuples depends on pytorch which should be installed from here next torchtuples can be installed with pip or via conda for the bleeding edge version install directly from github consider adding force reinstall or by cloning the repo make a data set with three set of covariates x x and x and a target y the covariates are structured in a nested tuple x create a simple relu net that take a input the tensor x tensor and the tuple x tuple note that x tuple can be of arbitrary length the tensor in x tuple are passed through a layer lin tuple averaged and concatenated with x tensor we then pas our new tensor through the layer lin cat we can now fit the model withand make prediction with either the net predict methodor with the net forward methodfor more example see the example folder,"[('torch tensor', 0.6028), ('pytorch model', 0.5497), ('torchtuples', 0.5472), ('next torchtuples', 0.4982), ('simple relu net', 0.4908), ('pytorch', 0.4692), ('tensor', 0.3902), ('x tensor', 0.3814), ('new tensor', 0.3789), ('numpy array', 0.3196)]","[-2.25101728e-02 -1.20981492e-01  5.20970905e-03  2.28501447e-02
  2.55964436e-02  3.97923104e-02 -8.55735037e-03 -1.90106370e-02
 -5.76898940e-02 -9.39563587e-02  3.95009853e-03  1.20939398e-02
 -1.25840604e-01  7.98081234e-02 -3.72284204e-02  4.47175428e-02
 -5.41453697e-02  6.30076602e-02 -6.23675659e-02 -8.25407952e-02
  2.19031554e-02 -3.68161798e-02  3.96610275e-02  8.73706012e-05
 -1.02795176e-02  1.01462193e-02  4.51199934e-02 -4.75749746e-03
  6.68620393e-02  3.69476229e-02  8.50681041e-04  1.47740217e-02
 -7.21947253e-02  1.40536530e-02 -1.60770293e-03 -2.53438968e-02
 -3.67447212e-02 -2.52753496e-02 -6.48456141e-02  5.22224531e-02
  2.82902904e-02 -2.61356384e-02 -2.95678079e-02  3.61921825e-02
  1.77871045e-02  1.32468129e-02  8.75594541e-02  8.59628338e-03
  1.99561026e-02 -5.43184616e-02 -1.02618907e-03 -3.50035392e-02
 -8.98552909e-02  5.00658937e-02  7.24641308e-02 -1.20622627e-02
 -1.93798337e-02 -6.26862198e-02 -4.66980180e-03 -9.62374732e-02
  5.95151931e-02 -6.08453935e-04 -1.09554427e-02 -2.96979956e-02
 -2.41605863e-02  3.49664576e-02 -4.08507884e-02  2.13901177e-02
  1.07703291e-01 -6.58130422e-02 -2.07021963e-02 -3.35114188e-02
 -9.87166837e-02  6.70660660e-02 -2.54291240e-02 -6.05820678e-02
  1.85849473e-01  1.85519028e-02  2.34152302e-02 -6.68606758e-02
 -3.66718136e-02  5.20205218e-03  5.15783615e-02 -1.66831873e-02
  2.80892942e-02 -2.60533094e-02 -2.36260165e-02  6.97387010e-02
  3.01880185e-02 -5.87883443e-02  7.11869961e-03 -2.78262068e-02
 -3.49269845e-02  3.72078605e-02  2.84627695e-02  4.99239890e-03
  5.08482903e-02 -5.65207042e-02 -7.53582865e-02  1.95257664e-02
  2.27319971e-02 -6.81329146e-02 -1.43371094e-02  3.47705893e-02
 -3.01934574e-02  7.48053789e-02 -3.21138464e-02  7.13854358e-02
 -7.32100308e-02 -1.72121581e-02  2.03885790e-02  3.28837670e-02
  1.26770884e-02 -7.85167664e-02  4.93502431e-02 -3.27188671e-02
 -8.48522708e-02  8.09246600e-02  5.51345423e-02  9.06067044e-02
 -3.22696678e-02  6.78094253e-02 -1.14631781e-03  6.75404370e-02
 -3.97626385e-02 -3.40494961e-02 -1.19742565e-01  3.71497431e-33
 -1.57765523e-02  7.50881657e-02 -5.05722165e-02  1.25220427e-02
  1.96293704e-02 -7.88646564e-03  9.88099426e-02  1.76797230e-02
  7.08881393e-03  9.75903310e-03 -6.51231334e-02  1.03206933e-02
 -1.05181122e-02  4.24671657e-02 -6.89301193e-02 -5.77647164e-02
  3.56235430e-02  1.85356513e-02  8.96974932e-03  2.79013254e-02
 -1.62378252e-02  1.15775704e-01  1.11142360e-02  3.76667306e-02
 -8.42666402e-02  1.63372736e-02 -3.38453874e-02 -4.11510728e-02
 -2.26375014e-02 -4.78204805e-03 -6.64286762e-02  1.63957896e-03
  4.17486280e-02  2.79524340e-03 -3.46013233e-02 -1.23220021e-02
  2.76570208e-02 -4.14542742e-02  5.35968924e-03 -1.37997016e-01
  1.30056893e-03  1.18479654e-01 -4.80686165e-02 -6.43760860e-02
 -3.95710319e-02  2.17806268e-02 -4.10047211e-02  5.18080480e-02
  1.28151430e-02 -2.32555903e-02 -8.49134699e-02  2.12098286e-02
 -4.36806642e-02  5.89865148e-02  6.65988307e-03 -6.10420294e-02
  4.46913578e-03  8.60790685e-02  1.11726493e-01 -6.04420342e-02
  2.69909743e-02  8.15328583e-02  2.01589614e-02  3.60016972e-02
 -1.40680745e-02  2.21932475e-02 -3.40983197e-02  7.29305949e-03
  1.16683524e-02  1.55021194e-02 -1.13953419e-01  1.31209254e-01
 -5.19541912e-02 -2.99239922e-02  2.29227003e-02 -5.01893535e-02
  5.03419153e-02 -8.02131891e-02 -8.66031740e-03  3.23334038e-02
 -9.30071697e-02  7.74627030e-02  1.76089741e-02 -4.82573509e-02
 -9.46197957e-02 -1.05232056e-02 -9.83797759e-03 -1.71599369e-02
  2.97288373e-02 -2.25288030e-02 -8.10999200e-02 -1.28132999e-02
  7.48945251e-02 -1.51072592e-02 -2.88365688e-02 -2.45293529e-33
  1.01276278e-03  6.30637482e-02 -4.33805026e-02  4.85876501e-02
 -2.79602911e-02 -2.99966093e-02 -3.90768331e-03 -2.48617567e-02
  3.21528222e-03  1.26702078e-02  4.62391973e-02 -6.17527850e-02
 -2.05240417e-02 -5.97978160e-02  9.12138447e-02 -1.81330275e-02
 -2.33953185e-02 -1.01868240e-02  5.26433811e-02 -4.57196161e-02
 -5.82450181e-02  9.44189802e-02 -1.27003580e-01 -3.50922570e-02
 -1.28020465e-01  1.12881381e-02  8.69202707e-03  6.25584857e-04
 -1.74431149e-02 -1.65889189e-02  4.94063050e-02 -4.51772958e-02
 -3.06672044e-02  7.27430210e-02 -9.41209868e-03  5.42931631e-02
  4.47418429e-02 -2.19733641e-02 -3.62362131e-03  5.71745522e-02
  1.06212899e-01  5.93611933e-02 -2.13913489e-02  7.73031637e-02
 -8.03724229e-02 -2.72425637e-03 -4.33286615e-02  1.95993087e-03
  4.80910623e-03 -3.94090936e-02  5.19002741e-03 -9.87519100e-02
 -5.49305975e-02 -6.39980361e-02 -1.69772729e-02  3.08938958e-02
  5.99415600e-02  1.39799295e-02  5.45418970e-02  2.31627598e-02
 -3.85409817e-02 -1.21578224e-01  2.81818714e-02 -1.55769615e-02
  6.18908228e-03 -1.20024169e-02 -6.60289228e-02  6.00335971e-02
  5.42637892e-02  5.05611580e-03 -4.30607283e-03  7.86017552e-02
  7.96527788e-02  4.32572924e-02  5.90983685e-03 -1.98106542e-02
  5.35045303e-02  2.50649881e-02  4.63451743e-02 -2.95971371e-02
 -1.53340632e-02 -1.75885372e-02  1.11195222e-02  7.50607550e-02
  1.10060930e-01  5.11982962e-02  5.36882058e-02  6.50012493e-02
  1.25826355e-02 -3.32666934e-02  1.98768452e-02  1.83804613e-02
  9.72191244e-02  1.12708837e-01  9.68621001e-02 -2.41285267e-08
 -6.00902177e-02  4.47570942e-02 -1.08137503e-02  4.58461381e-02
  1.58455204e-02  1.71517476e-03  5.36085516e-02  9.25742909e-02
 -2.17929613e-02  2.88803037e-03  8.67674686e-03 -4.84442636e-02
  1.77263711e-02  2.00880095e-02  1.24186119e-02  1.09961450e-01
 -2.96922363e-02 -4.99095721e-03  4.56330031e-02 -2.09744424e-02
  3.16725112e-02  1.36775505e-02  2.66654138e-02  1.11543285e-02
 -4.22204547e-02 -4.11196984e-02  2.16721483e-02  5.17331138e-02
 -2.36949027e-02  3.00431065e-02 -7.18477881e-03 -3.61920297e-02
  9.69793350e-02  1.42034935e-02  7.43280500e-02  5.36434054e-02
 -3.28185782e-02  2.45150668e-03 -5.02884462e-02  1.35881584e-02
 -7.74499327e-02  5.67297172e-03 -9.95402690e-03  2.14013457e-03
  8.68009776e-02 -6.10691831e-02 -4.26215678e-02 -8.05392787e-02
 -2.03815382e-02  9.21835843e-03  2.62198262e-02  5.95735386e-02
 -5.89988427e-03  2.73442771e-02  2.67237518e-02  2.86861807e-02
 -3.27225998e-02  4.28529456e-02  1.56976674e-02 -4.50197831e-02
 -1.66715886e-02  4.30263467e-02 -6.88266754e-02 -6.11041449e-02]",2,2
kgcnn,1,a set of layer for graph convolution in tensorflow kera that use raggedtensors general requirement installation documentation implementation detail literature datasets training issue citing referencesthe package in kgcnn contains several layer class to build up graph convolution model some model are given a an example a documentation is generated in doc focus of kgcnn is batched graph learning for molecule kgcnn mol and material kgcnn crystal if you want to get in contact feel free to discus for kgcnn usually the latest version of tensorflow is required but is listed a extra requirement in the setup py for simplicity standard python package requirement are placed in the setup py and are installed automatically package which must be installed manually for full functionality clone repository or latest release and install with editable mode or latest release via python package index auto documentation is generated at http kgcnn readthedocs io en latest index html the most frequent usage for graph convolution is either node or graph classification a for their size either a single large graph e g citation network or small batched graph like molecule have to be considered graph can be represented by an index list of connection plus feature information typical quantity in tensor format to describe a graph are listed below a major issue for graph is their flexible size and shape when using mini batch here for a graph implementation in the spirit of kera the batch dimension should be kept also in between layer this is realized by using raggedtensors graph tensor for edge index or attribute for multiple graph is passed to the model in form of ragged tensor of shape batch none dim where dim denotes a fixed feature or index dimension such a ragged tensor ha ragged rank with one ragged dimension indicated by none and is build from a value plus partition tensor for example the graph structure is represented by an index list of shape batch none with index of incoming or receiving node i and outgoing or sending node j a i j note an additional edge with j i is required for undirected graph a ragged constant can be easily created and passed to a model model can be set up in a functional way example message passing from fundamental operation or via sub classing of the message passing base layer where only message function and update node must be implemented a version of the following model and variant thereof are implemented in literature how to construct ragged tensor is shown above moreover some data handling class are given in kgcnn data graph are represented by a dictionary of numpy tensor graphdict and are stored in a list memorygraphlist both must fit into memory and are supposed to be handled just like a python dict or list respectively the memorygraphdataset inherits from memorygraphlist but must be initialized with file information on disk that point to a data directory for the dataset the data directory can have a subdirectory for file and or single file such a a csv file a base dataset class is created with path and name information the subclass qmdataset moleculenetdataset and graphtudataset further have function required for the specific dataset type to convert and process file such a txt sdf xyz etc most subclass implement prepare data and read in memory with dataset dependent argument an example for moleculenetdataset is shown below for more detail find tutorial in notebook in data datasets there are graph learning benchmark datasets a subclass which are being downloaded from e g popular graph archive like tudatasets or moleculenet the subclass graphtudataset and moleculenetdataset download and read the available datasets by name there are also specific dataset subclass for each dataset to handle additional processing or downloading from individual source downloaded datasets are stored in kgcnn datasets on your computer please remove them manually if no longer required a set of example training can be found in training training script are configurable with a hyperparameter config file and command line argument regarding model and dataset you can find a table of common benchmark datasets in result some known issue to be aware of if using and making new model or layer with kgcnn if you want to cite this repo please refer to our paper,"[('kgcnn data graph', 0.5756), ('graph convolution model', 0.5555), ('tensorflow kera', 0.522), ('numpy tensor graphdict', 0.5178), ('kgcnn datasets', 0.5115), ('tensorflow', 0.49), ('graph convolution', 0.4748), ('raggedtensors graph tensor', 0.4635), ('graph learning', 0.4468), ('graph implementation', 0.4288)]","[-1.22169971e-01 -6.95177987e-02  5.28932996e-02 -2.45432518e-02
 -2.27713790e-02 -4.21400778e-02 -5.94520867e-02 -3.76154855e-02
 -1.27326757e-01  3.75444293e-02 -5.83059760e-03 -3.26738693e-02
 -1.35076232e-02  2.65272129e-02 -2.77455971e-02 -2.81559974e-02
  1.81563962e-02  7.14157000e-02 -4.93641011e-02 -8.73776078e-02
 -2.85089705e-02  1.01379268e-02 -6.77606557e-03 -7.25608598e-03
  6.85577840e-02  3.02394945e-02  1.44392224e-02 -6.61287010e-02
  6.25070184e-02 -2.57200189e-02  7.95037206e-03  8.17607157e-04
 -2.69698282e-03  5.42986207e-02 -3.10462210e-02  1.54620949e-02
 -3.09180301e-02 -3.85971293e-02  1.47924921e-03  4.13991511e-02
 -2.94296741e-02 -1.11903581e-04 -2.55890395e-02 -2.10460648e-03
  9.10726637e-02  3.28797251e-02 -3.21835913e-02 -5.26582636e-02
  2.42439061e-02  1.91658698e-02 -6.65510595e-02 -6.93362728e-02
 -3.27620879e-02  1.47980358e-03 -3.99276391e-02 -1.11480467e-01
 -3.14803235e-02  1.08669838e-02  9.42279696e-02  6.14044769e-03
  4.63291816e-02 -4.51934412e-02 -8.68720561e-02  4.95963879e-02
 -1.05183702e-02  4.52448195e-03  7.39407865e-03  4.25998531e-02
  7.51286298e-02 -2.80800834e-02  3.38925160e-02  6.87790150e-03
 -8.85209292e-02  4.16090228e-02  1.15522789e-02  1.31745432e-02
  9.17231441e-02  1.02852816e-02  4.75162966e-03 -8.79236460e-02
  4.35819067e-02 -7.07675656e-03  1.84223279e-02 -1.70832649e-02
  2.11765375e-02 -7.79553577e-02 -1.17510922e-01  2.45716963e-02
  2.64003826e-03 -6.76900614e-03  9.40443110e-03  2.02008430e-02
 -2.82892603e-02 -4.03914303e-02  1.14749735e-02  4.99864668e-02
 -2.03213841e-02  2.89014038e-02 -6.41829446e-02  5.01691364e-02
 -3.85546647e-02 -5.09682707e-02  4.01986800e-02  6.10958040e-02
 -1.24279987e-02  3.59009393e-02 -2.67380662e-02  2.62481701e-02
  4.48210686e-02  2.07356066e-02 -2.63306834e-02  9.16899815e-02
 -3.32614332e-02 -4.97173406e-02  9.43400711e-02 -2.21478157e-02
 -1.20714586e-02 -2.12231893e-02  6.89455643e-02  7.54681900e-02
 -8.60628188e-02 -1.39091369e-02 -1.20289307e-02 -4.00853343e-02
 -4.80934680e-02  2.08783727e-02 -9.50381085e-02  3.98787377e-33
  2.17815153e-02 -1.84395239e-02  4.93177362e-02 -5.44734634e-02
  5.01991250e-02 -4.43471596e-02 -6.85314909e-02 -1.47819528e-02
 -9.64142475e-03 -4.81923670e-02 -9.09790173e-02  8.82360265e-02
  1.30064087e-02  9.41953063e-02 -3.21426541e-02 -3.30760586e-03
  5.16841970e-02 -3.15506645e-02  4.07769009e-02 -1.14144967e-03
  7.24235922e-02 -2.86514163e-02  1.01700544e-01  8.79113972e-02
  2.19162107e-02  2.90851872e-02 -6.27092505e-03  1.77973090e-03
  7.68210273e-03  2.02676151e-02 -3.28183211e-02  3.40010375e-02
  8.24854150e-03 -8.70396383e-03 -8.17200094e-02 -1.89052727e-02
 -6.53502271e-02 -3.10259312e-02  3.77117959e-03 -1.37078360e-01
 -2.98722200e-02  2.62149405e-02 -1.76589563e-02 -7.62395412e-02
 -4.43812571e-02  9.74111725e-03  1.88592135e-03 -1.34174759e-02
  6.81512505e-02  6.22945093e-03 -3.47989127e-02 -3.42918485e-02
 -6.66195303e-02  4.16533090e-02  5.49179763e-02  1.70333143e-02
  1.11824371e-01  3.24195586e-02  1.43801421e-02  5.50199859e-02
  2.28095407e-04  4.10710275e-02  4.75114435e-02 -9.07132775e-03
 -3.63062471e-02 -5.63300885e-02 -1.00049272e-01  4.48118076e-02
  3.76343578e-02 -3.29680964e-02 -8.81601498e-02  1.13823734e-01
 -3.60407121e-02 -1.26385633e-02  7.33267923e-04 -4.24718820e-02
 -2.98635662e-02 -8.78869146e-02 -7.53111914e-02  7.78990611e-02
 -5.39440215e-02 -4.74217944e-02  3.61030549e-02  1.51760289e-02
 -7.59034231e-02 -3.57960202e-02  2.46351771e-02 -2.57643070e-02
  5.02713211e-02 -7.14779347e-02 -7.64319524e-02 -3.30911577e-03
  3.02973036e-02  4.82677482e-02 -3.76673751e-02 -3.51617838e-33
  2.34611910e-02  1.27878502e-01 -1.18046461e-04  1.22707225e-01
  5.56079634e-02  3.07833049e-02  3.84524018e-02 -2.92579178e-02
  2.27556285e-02  7.33404756e-02  4.93018627e-02 -1.02922574e-01
  4.00490202e-02 -2.10782792e-02  7.43715391e-02 -3.43950652e-02
 -5.88713512e-02 -6.53815493e-02 -4.14488986e-02 -3.96950450e-03
 -3.09273452e-02  5.53768165e-02 -1.16102085e-01 -6.31856918e-02
  5.42237088e-02 -1.94050372e-02 -9.29986015e-02  7.53984461e-03
  9.73500870e-03  3.74282151e-02 -1.41351698e-02  2.71718893e-02
 -3.55029330e-02 -2.19020294e-03  6.99683726e-02  2.19028294e-02
  7.34255761e-02  1.43962549e-02  3.87458727e-02 -2.81501580e-02
  9.35496241e-02  8.11873674e-02 -4.83011780e-03  3.35006602e-02
 -1.61915440e-02  3.62385549e-02  1.37644801e-02  7.20006675e-02
 -8.40369835e-02 -5.84941618e-02 -2.45991424e-02  2.89384363e-04
  3.32542472e-02 -1.23753930e-02 -7.17773521e-03 -3.40174185e-03
  9.16371346e-02  9.09455419e-02  9.85344127e-03 -2.41342168e-02
 -6.84309006e-02 -1.02856442e-01  4.93589267e-02 -4.85726371e-02
 -3.86884771e-02 -2.90085399e-03 -4.91290179e-04  3.39262746e-02
 -1.59229661e-04  2.94569153e-02 -3.15014599e-03  1.83093864e-02
  2.91393921e-02  4.66731284e-03 -2.92889914e-03 -1.38627440e-02
 -3.76200974e-02  7.44382516e-02  5.31495921e-03 -2.10504886e-02
  4.73585799e-02  2.17387904e-04 -1.43072577e-02  1.20421767e-01
  1.12276979e-01  1.00529075e-01  3.24583985e-02  1.98278036e-02
  7.47776255e-02  2.61089113e-02 -4.29633670e-02  6.96381833e-03
 -2.29912437e-02  1.44415200e-01  2.73240730e-02 -2.40776998e-08
 -7.78291523e-02 -2.51143575e-02  7.20170932e-03 -3.43847647e-02
  4.91449237e-02  1.57333184e-02  1.05265401e-01  1.21054739e-01
 -3.16816159e-02  7.53327459e-02 -5.08242287e-03  2.30627619e-02
 -8.25540200e-02 -2.30280422e-02 -4.71795090e-02  2.43449025e-02
 -1.28821246e-02  9.29691084e-03  5.78610934e-02  4.76278029e-02
  1.69506706e-02 -1.49418591e-02 -1.09990928e-02  5.28225638e-02
 -2.22294573e-02 -8.66623968e-02 -3.11608668e-02  1.61880739e-02
 -3.91795114e-02 -7.17426650e-03 -7.48421997e-02 -4.35879081e-03
  1.26469582e-01 -8.70175436e-02  5.03993928e-02  3.60610932e-02
  4.06174548e-02 -1.89475436e-02 -4.06641364e-02  4.81261984e-02
 -2.83248201e-02  1.15485214e-01 -3.81326228e-02 -3.77719216e-02
  1.51614612e-02  4.36783619e-02  5.28711751e-02  1.48610678e-03
  1.00024119e-02  7.73292314e-03 -7.29468539e-02  3.12916450e-02
 -9.53504294e-02 -2.40427572e-02  4.32155579e-02  2.21468098e-02
 -3.49316783e-02 -7.43995160e-02  1.12377936e-02  1.72141548e-02
 -6.46953646e-04  3.64146382e-02 -6.65853396e-02 -2.79457569e-02]",2,2
gymnasium,1,gymnasium is an open source python library for developing and comparing reinforcement learning algorithm by providing a standard api to communicate between learning algorithm and environment a well a a standard set of environment compliant with that api this is a fork of openai s gym library by the maintainer openai handed over maintenance a few year ago to an outside team and is where future maintenance will occur going forwardthe documentation website is at gymnasium farama org and we have a public discord server which we also use to coordinate development work that you can join here http discord gg bnj kubtg gymnasium includes the following family of environment along with a wide variety of third party environment,"[('gymnasium', 0.6315), ('gym library', 0.6083), ('http discord gg bnj kubtg gymnasium', 0.538), ('gymnasium farama org', 0.4904), ('open source python library', 0.4668), ('openai', 0.4184), ('reinforcement learning algorithm', 0.4144), ('maintainer openai', 0.4132), ('api', 0.3882), ('environment', 0.3448)]","[-2.95176674e-02 -6.75119236e-02 -1.80098097e-04  1.38043892e-02
  1.89369526e-02 -5.67062087e-02 -1.05922390e-02 -5.93889551e-03
  1.49731827e-03  1.50116747e-02 -5.84915932e-03 -3.41357701e-02
  3.87419425e-02 -3.15909125e-02  1.54348210e-01  3.00078336e-02
  1.39027997e-03  6.67705983e-02 -2.44294293e-02 -1.74405009e-01
 -5.43420017e-02  4.37554643e-02  5.38932681e-02  3.20279039e-02
  6.72167214e-03  8.42904672e-03  3.21440678e-03 -1.34856449e-02
  3.05584371e-02 -4.45968322e-02 -5.67076355e-02 -3.07626594e-02
  3.83172333e-02  1.22807119e-02 -8.81539434e-02  1.11885458e-01
 -3.42779085e-02 -8.11327770e-02 -6.73122033e-02  5.73975518e-02
 -8.28666463e-02  2.75017247e-02  3.64289843e-02 -7.35142780e-03
  4.78487685e-02  3.18711326e-02 -2.75202002e-02 -8.54555294e-02
  9.47775412e-03  5.29503357e-03 -4.82787117e-02 -1.09221801e-01
  4.88819219e-02  5.58739267e-02  8.01368617e-03 -3.28584984e-02
 -5.13526686e-02  5.29526966e-03  2.07384379e-04 -5.96873723e-02
  7.81853944e-02 -1.47338174e-02 -4.22343388e-02  2.26350706e-02
 -5.39501496e-02  9.26486403e-03  1.39757013e-02  1.12816267e-01
  1.27900109e-01 -1.49675757e-01 -3.02580446e-02 -3.23579349e-02
 -1.49124162e-03  6.23495737e-03 -2.39651240e-02  2.74333730e-02
 -5.63172773e-02  1.95783395e-02  2.75285766e-02 -9.13742557e-03
 -2.84225997e-02 -4.28060517e-02 -6.73257094e-03 -2.45390367e-03
 -4.21297969e-03 -1.96072869e-02 -6.95519475e-03  3.44132632e-02
  2.05423404e-02  9.33796540e-03  3.67809385e-02  2.07368918e-02
  2.33255997e-02 -9.06424224e-03 -4.76526767e-02  1.79247055e-02
 -2.21646149e-02 -7.67914057e-02 -5.71497306e-02  8.48726630e-02
 -3.17666866e-02  2.91620120e-02  2.51359008e-02  1.01181135e-01
 -3.82453464e-02 -9.36574023e-03  3.05770012e-03  4.82534170e-02
  1.09344348e-01  4.86667827e-02 -4.75302190e-02 -3.79780717e-02
  3.89358066e-02 -3.92955467e-02  7.02626631e-02  7.32820332e-02
  2.09589731e-02  2.15637907e-02 -3.21150832e-02  9.10721347e-02
 -1.23851988e-02 -4.71078008e-02  7.15560019e-02 -4.71848994e-04
  1.08316094e-02  2.23737769e-02 -4.14904095e-02  4.04115562e-33
  7.84773827e-02 -6.26961812e-02  3.70454639e-02 -6.39333576e-02
  6.19752742e-02 -6.10346124e-02  2.75479667e-02 -2.40497272e-02
 -4.39579552e-03 -1.14430957e-01  1.92390953e-03  1.66694015e-01
  3.23862978e-03  8.86699930e-03  9.80195701e-02 -5.17882220e-02
 -4.28930521e-02  7.54619241e-02  7.79964030e-02 -1.31913852e-02
  3.19778137e-02 -4.78929691e-02  2.58626770e-02  1.08944289e-01
  2.12358292e-02  9.07540619e-02  3.98450904e-02 -3.80895808e-02
 -1.09145090e-04  4.20541270e-03  4.05927524e-02 -1.46170612e-02
 -8.52464437e-02 -1.95533084e-03  3.84965092e-02 -1.95154250e-02
 -7.50383958e-02 -1.45232091e-02  8.21798854e-03 -4.60675731e-02
 -3.00206132e-02  3.97697836e-03  1.99833512e-02 -1.40775479e-02
  1.48591902e-02 -5.02173156e-02 -1.71101224e-02 -3.37503478e-02
  3.77348773e-02 -6.45607114e-02 -4.58453707e-02  1.56236952e-02
  3.58280875e-02 -3.02620456e-02  2.86436197e-03 -1.58151817e-02
  2.20763292e-02  7.51738995e-02 -4.81990092e-02  1.56891141e-02
  8.54610801e-02  5.50693013e-02 -1.92560498e-02 -3.76226604e-02
 -6.85809776e-02  3.53363156e-02 -4.95663211e-02 -3.04895677e-02
  6.91572428e-02  2.58352403e-02  1.46926269e-02  6.75924346e-02
  1.18248509e-02  7.19553977e-02 -5.05525917e-02 -2.44156010e-02
 -5.46862744e-02 -6.66677877e-02 -5.15328981e-02 -6.48961589e-03
  2.91158794e-03 -1.06048323e-02 -4.27914523e-02  4.33083773e-02
 -2.80005727e-02 -4.88777421e-02  2.30501890e-02 -3.96362208e-02
 -3.52328271e-02  5.87982163e-02 -1.32193163e-01  2.38623247e-02
 -1.78639516e-02  1.23899527e-01  6.03018422e-03 -2.30309667e-33
 -1.01616932e-02 -6.80660233e-02 -1.51646538e-02  4.54388037e-02
  6.68710023e-02  1.30173322e-02 -1.01017989e-02 -3.43189240e-02
  4.61035706e-02  3.63822952e-02  9.21446085e-03  4.12614234e-02
  7.20252693e-02  4.15914208e-02 -8.76047090e-03 -5.67607246e-02
 -1.34068644e-02 -6.86006993e-02 -5.97953983e-02  2.77855601e-02
 -5.75924367e-02  5.31240515e-02 -5.34813404e-02 -5.33067696e-02
  6.68644905e-02  3.00882850e-02 -6.65801316e-02 -4.88241669e-03
  5.48571674e-03  4.58979309e-02  7.73994019e-03  2.21972298e-02
 -1.20857898e-02 -2.40394529e-02 -3.61541822e-03 -1.57027245e-02
  4.40133661e-02  3.99396569e-02 -3.21981646e-02 -4.35691662e-02
  1.26823038e-01 -1.12564182e-02 -1.76933724e-02 -2.78208572e-02
 -4.01266199e-03  1.48799540e-02 -1.32915705e-01  4.68891673e-02
 -1.10662237e-01 -1.09972067e-01  6.86665401e-02 -6.95616603e-02
 -3.87989450e-03 -1.02471977e-01  4.09921668e-02  1.67268552e-02
 -8.70195404e-03  4.97599915e-02 -2.46222392e-02 -6.47428185e-02
 -1.13295488e-01 -5.28381020e-02 -4.30788398e-02  1.64224714e-01
 -3.58785354e-02 -8.25878698e-03  1.01406928e-02  1.14844263e-01
 -7.57365823e-02 -3.47869061e-02  4.01628204e-03  5.38062751e-02
  6.85996935e-02  3.54157574e-02 -4.41988707e-02  3.42564918e-02
  1.05811609e-02  6.57714978e-02  4.49156836e-02 -2.97833979e-02
  4.65868041e-02 -9.87928128e-04  5.10401838e-02  4.42317016e-02
  5.03118262e-02  4.95678708e-02  1.00400046e-01  5.05810902e-02
 -1.31493350e-02 -1.11148590e-02 -2.29030829e-02  4.73757721e-02
 -1.67166796e-02  7.37442896e-02  1.12742074e-02 -2.48532075e-08
 -2.85218377e-02 -4.21534404e-02  4.54953499e-02  7.64678791e-02
  3.44659202e-02  1.74046718e-02  2.77320538e-02  5.57253100e-02
 -5.87030190e-05  4.98911254e-02  3.29164555e-04  2.01116391e-02
 -2.13562138e-02 -2.42310427e-02 -2.66744476e-02  5.24637736e-02
 -3.31698656e-02  1.61446687e-02 -1.80966081e-03 -3.54927815e-02
  3.77764478e-02 -2.05239691e-02  5.53968037e-03  1.20834424e-03
 -3.71530503e-02 -8.77540335e-02 -3.21751870e-02 -8.63000378e-03
 -2.12279707e-02 -3.50502878e-02  1.47789475e-02 -4.63422835e-02
  9.74139348e-02 -7.20912367e-02  4.96879779e-02  5.84127940e-02
 -7.28862081e-03 -7.75524080e-02 -6.06492609e-02  6.10243417e-02
 -8.31795186e-02 -3.10249645e-02  4.45795916e-02 -4.84256260e-02
  4.51471023e-02  2.51684655e-02 -2.18973905e-02 -4.82508875e-02
  1.47994887e-02 -4.20613699e-02 -4.13375795e-02 -4.49014194e-02
  1.28042717e-02 -2.36768126e-02  5.32629713e-02  5.63538633e-02
  4.04717354e-03 -3.71706150e-02 -4.62508248e-03  8.86965469e-02
  2.53993869e-02  2.78430488e-02 -3.01360544e-02  3.98395136e-02]",2,0
segmentation-models-pytorch,1,python library with neural network for image segmentation based on pytorch the main feature of this library are visit read the doc project page or read following readme to know more about segmentation model pytorch smp for short librarysegmentation model is just a pytorch nn module which can be created a easy a all encoders have pretrained weight preparing your data the same way a during weight pre training may give your better result higher metric score and faster convergence it is not necessary in case you train the whole model not only decoder congratulation you are done now you can train your model with your favorite framework the following is a list of supported encoders in the smp select the appropriate family of encoders and click to expand the table and select a specific encoder and it pre trained weight encoder name and encoder weight parameter ssl swsl semi supervised and weakly supervised learning on imagenet repo docspytorch image model a k a timm ha a lot of pretrained model and interface which allows using these model a encoders in smp however not all model are supportedtotal number of supported encoders input channel parameter allows you to create model which process tensor with arbitrary number of channel if you use pretrained weight from imagenet weight of first convolution will be reused for channel case it would be a sum of weight of first convolution layer otherwise channel would be populated with weight like new weight i pretrained weight i and than scaled with new weight new in channel all model support aux params parameter which is default set to none if aux params none then classification auxiliary output is not created else model produce not only mask but also label output with shape nc classification head consists of globalpooling dropout optional linear activation optional layer which can be configured by aux params a follows depth parameter specify a number of downsampling operation in encoder so you can make your model lighter if specify smaller depth pypi version latest version from source segmentation model package is widely used in the image segmentation competition here you can find competition name of the winner and link to their solution project is distributed under mit license,"[('segmentation model pytorch smp', 0.6428), ('short librarysegmentation model', 0.4888), ('imagenet', 0.4628), ('docspytorch image model', 0.4519), ('source segmentation model package', 0.4394), ('encoder', 0.422), ('image segmentation', 0.3952), ('image segmentation competition', 0.3927), ('specific encoder', 0.391), ('imagenet weight', 0.388)]","[-4.60413992e-02 -7.27303028e-02  5.23559675e-02 -2.21943036e-02
  1.22512743e-01 -1.45043368e-02 -4.04696213e-03  6.20216765e-02
 -1.91378668e-02 -2.59258617e-02 -3.19169578e-03  2.10936684e-02
  2.03960519e-02  6.35517836e-02 -6.30887807e-04 -2.28156038e-02
 -7.18028694e-02  1.34243116e-01 -8.55933875e-02 -6.70227483e-02
  4.47204933e-02 -2.72934698e-02 -2.18450669e-02 -3.07426900e-02
  1.58961378e-02 -3.70184220e-02  1.95194315e-02 -2.70783175e-02
  3.68051343e-02 -1.75108444e-02  4.92430618e-03  8.05686042e-03
  1.40867054e-01  6.80969283e-02 -1.25081465e-02  9.16957564e-04
  4.55647288e-03 -4.55956198e-02 -1.63024515e-02 -4.76029180e-02
 -5.86633720e-02 -1.47708403e-02 -5.74489124e-02 -1.84485987e-02
  6.45149350e-02 -1.75230987e-02 -1.00433929e-02 -5.22904471e-02
 -2.25295853e-02 -3.23050767e-02 -8.34335610e-02 -8.05259943e-02
 -8.57373700e-02  2.27245167e-02  8.32945667e-03 -3.97766791e-02
 -1.38249211e-02 -6.57774061e-02  4.48695868e-02  1.74379547e-03
  1.35626337e-02 -2.42081974e-02 -1.07513167e-01  7.40699768e-02
  3.34747359e-02  2.18096580e-02  1.39412824e-02 -8.33913870e-03
  9.60141346e-02 -1.04586080e-01 -6.68087527e-02 -4.45561782e-02
 -3.40749472e-02  2.78044250e-02 -2.03764178e-02 -7.18054175e-02
  1.34806290e-01  6.88862801e-02  3.04864682e-02 -1.08425759e-01
  1.59033164e-02 -4.32335306e-03  4.36301455e-02  1.01301037e-02
  2.10594456e-03 -1.81806907e-02 -7.48566315e-02  6.84311762e-02
  2.12152395e-03  6.79975096e-03  1.17815146e-02 -4.59020622e-02
 -7.63051435e-02  3.69458310e-02  3.38136554e-02  2.37733983e-02
 -4.20922600e-02 -9.26041901e-02 -2.22316124e-02  6.21601790e-02
 -3.28052044e-02 -4.74342369e-02  6.80385381e-02 -2.44974755e-02
  4.82893065e-02 -4.59303055e-03  5.49205244e-02 -1.78630347e-03
  3.84264477e-02  5.00805527e-02  3.27068754e-02  1.48710946e-03
 -5.48464805e-02 -9.70258713e-02  1.01001680e-01 -2.80805733e-02
 -5.26457615e-02  7.75914872e-03  3.77653204e-02  1.83318648e-02
 -1.34532094e-01  6.27567396e-02 -1.61763059e-03  1.10991960e-02
 -2.82748193e-02 -1.41716693e-02 -1.18022345e-01  5.87873210e-33
 -2.56156549e-02 -5.96789345e-02  5.10307476e-02 -5.61156981e-02
  7.56785795e-02 -6.62960038e-02  2.07755994e-02  2.99945697e-02
 -3.41444053e-02 -8.18162709e-02 -6.29514977e-02 -3.30347344e-02
 -5.75069301e-02  1.16065562e-01  4.62228097e-02 -3.27714868e-02
 -2.08752844e-02  6.04045391e-02  3.27871330e-02  7.73935467e-02
 -1.49982115e-02 -5.65835275e-02  1.77562144e-02  1.10727482e-01
  4.85470891e-03  7.22414290e-04  2.06012977e-03 -7.20671713e-02
 -2.16148403e-02  1.96649809e-03 -2.01729909e-02  4.41349372e-02
  4.09569740e-02 -2.07373109e-02 -3.69765423e-02  1.03256050e-02
 -1.67816039e-02  2.57107224e-02  1.97058916e-03 -3.73230949e-02
 -4.19339649e-02  6.70289546e-02 -1.63123179e-02  7.22785993e-03
 -2.56730132e-02  7.26089021e-03 -3.64240520e-02  1.15637802e-01
 -4.09725541e-03  4.96567525e-02  4.39774282e-02  8.41613207e-03
 -4.94735828e-03 -1.16314124e-02 -4.63229381e-02  3.56690139e-02
  5.52111827e-02  1.52735338e-02  2.89998837e-02  2.67429296e-02
  1.85841005e-02  3.71366516e-02  6.83540404e-02 -1.18311811e-02
  4.20805626e-02 -7.63634369e-02 -7.15345070e-02 -3.00303847e-02
 -5.99492453e-02  6.24637492e-02 -6.53374419e-02  4.46616970e-02
 -5.14946654e-02  9.53039899e-03  1.03777424e-01 -2.49665827e-02
 -7.45296776e-02 -9.11021419e-03 -7.84186721e-02  1.15574650e-01
 -1.24771059e-01  4.96841334e-02 -3.23546082e-02 -6.67812526e-02
 -7.05557838e-02  2.22935881e-02  3.93083580e-02 -6.41697869e-02
  5.61431460e-02  8.39211792e-03 -1.03355087e-02 -1.36025681e-03
 -2.79470850e-02  1.09851882e-01  4.69986126e-02 -3.64473558e-33
  2.50487253e-02  9.41368043e-02 -5.46832271e-02  1.00137010e-01
 -2.65141260e-02 -1.05557395e-02 -1.74384210e-02  3.87868695e-02
 -1.79323554e-02  5.54959802e-03  8.77305865e-02 -1.04416767e-02
 -1.14866591e-03 -1.42424079e-02 -4.72123222e-03 -6.11949600e-02
  2.24214736e-02 -6.27255440e-02  3.29529941e-02  6.03569038e-02
 -5.39156003e-03  9.56658199e-02 -8.18042010e-02 -3.47745493e-02
 -4.74232621e-02  5.32176308e-02 -2.58496739e-02  1.92737225e-02
  1.78597663e-02  1.47532520e-03 -2.01998111e-02  1.60495024e-02
 -7.18211802e-03 -4.49334737e-03 -5.17692342e-02  7.98407942e-03
  2.78002415e-02 -1.19727124e-02  4.47740965e-02  3.57834436e-02
  1.25256330e-01  3.72652300e-02  5.53901531e-02  6.54607564e-02
 -5.68158627e-02 -7.41281882e-02 -1.49171427e-02  3.81890163e-02
 -1.85735710e-02 -1.70640908e-02 -7.06171766e-02  2.82260869e-02
 -4.98296656e-02 -1.91760939e-02 -6.26202067e-03  3.89845893e-02
 -4.03384678e-02  1.92112587e-02  7.98299462e-02 -1.71644092e-02
 -5.44149764e-02 -8.10011327e-02 -7.70208016e-02  5.99599583e-03
  2.30210442e-02 -1.60090178e-02 -6.00661747e-02  3.34899873e-02
 -7.90541060e-03 -2.27724873e-02 -2.10991377e-04  5.37765808e-02
  8.81807730e-02  1.12596639e-01  1.35991126e-02 -1.46745658e-02
 -1.32975830e-02  6.95347339e-02  3.72950546e-02 -5.90956397e-02
  3.59550752e-02 -1.99996624e-02 -2.71122623e-02  8.00825357e-02
  1.18599795e-02  7.93437138e-02  4.95029576e-02 -4.97542247e-02
  3.48011330e-02 -1.51658971e-02 -3.19558345e-02  3.65302972e-02
  9.35145989e-02  9.44681764e-02 -1.81317236e-02 -2.78438428e-08
 -1.54140871e-03 -8.95039458e-03  3.26903760e-02 -7.50055071e-03
 -5.69794793e-03  1.31960986e-02  2.13839225e-02  1.03716604e-01
 -1.52994590e-02  5.09838387e-02 -2.40004677e-02 -4.04365771e-02
 -1.17213115e-01  8.02282942e-04  2.27949000e-03  9.33053493e-02
  6.52340800e-02  2.56361384e-02  4.36253436e-02 -5.92623986e-02
 -3.11584268e-02 -1.56846046e-02 -5.36584586e-04  2.44378597e-02
  1.74131524e-02 -4.81674122e-03 -3.91841568e-02  4.31483760e-02
 -8.77564424e-04 -9.48107392e-02  1.11363670e-02  5.47759309e-02
  3.04587297e-02 -2.88502350e-02  1.07779287e-01  8.16380009e-02
 -6.69849738e-02 -5.29646687e-02 -2.84155388e-03  1.65223256e-02
 -2.36821696e-02 -9.13343765e-03  3.01160272e-02 -7.37784281e-02
  4.59099449e-02  3.79300937e-02  7.24134073e-02 -5.08738421e-02
 -2.40643602e-02  2.71657519e-02  4.77334037e-02  5.11241611e-03
  8.89658183e-03  4.01503704e-02 -5.85655719e-02  4.21396829e-02
  6.84543326e-02 -4.06830907e-02  6.24446236e-02  2.33249459e-02
 -7.31570125e-02  6.25222176e-02 -4.66443822e-02 -1.85960960e-02]",2,2
pytorch-ignite,1,ignite is a high level library to help with training and evaluating neural network in pytorch flexibly and transparently click on the image to see complete codeless code than pure pytorch while ensuring maximum control and simplicitylibrary approach and no program s control inversion use ignite where and when you needextensible api for metric experiment manager and other componentsignite is a library that provides three high level feature no more coding for while loop on epoch and iteration user instantiate engine and run them the cool thing with handler is that they offer unparalleled flexibility compared to for example callback handler can be any function e g lambda simple function class method etc thus we do not require to inherit from an interface and override it abstract method which could unnecessarily bulk up your code and it complexity event can be stacked together to enable multiple call custom event related to backward and optimizer step call metric for various task precision recall accuracy confusion matrix iou etc regression metric user can also compose their metric with ease from existing one using arithmetic operation or torch method from pip from conda from source from pip from conda this suggests to install pytorch nightly release instead of stable version a dependency pull a pre built docker image from our docker hub and run it with docker v basevision nlp for more detail see here few pointer to get you started inspired by torchvision reference we provide several reproducible baseline for vision task feature the easiest way to create your training script with pytorch ignite github issue question bug report feature request etc discus pytorch category ignite pytorch ignite discord server to chat with the communitygithub discussion general library related discussion idea q a etc we have created a form for user feedback we appreciate any type of feedback and this is how we would like to see our community thank you please see the contribution guideline for more information a always pr are welcome see other project at used by if your project implement a paper represents other use case not covered in our official tutorial kaggle competition s code or just your code present interesting result and us ignite we would like to add your project to this list so please send a pr with brief description of the project if you use pytorch ignite in a scientific publication we would appreciate citation to our project pytorch ignite is a numfocus affiliated project operated and maintained by volunteer in the pytorch community in their capacity a individual and not a representative of their employer see the about u page for a list of core contributor for usage question and issue please see the various channel here for all other question and inquiry please send an email to contact pytorch ignite ai,"[('project pytorch ignite', 0.5166), ('pytorch ignite', 0.4803), ('metric experiment manager', 0.4313), ('pytorch ignite discord server', 0.4217), ('pytorch community', 0.3984), ('ignite', 0.3976), ('pytorch', 0.3817), ('pure pytorch', 0.3696), ('needextensible api', 0.3693), ('discus pytorch category', 0.356)]","[-8.24035481e-02 -6.72677010e-02 -5.19373268e-02  2.91340277e-02
  3.94706577e-02 -4.76648696e-02  4.11659330e-02 -2.06271769e-03
 -6.41817078e-02 -4.82570082e-02 -2.53923275e-02 -7.81564862e-02
 -6.38249591e-02  1.85198002e-02  2.97621042e-02  2.47298144e-02
  2.05573328e-02  5.69931678e-02 -4.76405360e-02 -1.13418028e-01
 -6.73891976e-03 -2.77079083e-02  7.84174576e-02  3.00992485e-02
  7.99832284e-04 -5.05945347e-02  1.28609492e-02 -7.18607043e-04
 -1.17393257e-02 -1.56800635e-02 -3.09912469e-02  4.77038836e-03
  5.68479765e-03  3.22151072e-02  2.93582398e-02  8.74536391e-03
 -3.57968882e-02 -2.51239375e-03 -5.32221459e-02  4.90811281e-03
 -2.46609803e-02 -2.84085367e-02 -2.46753395e-02  7.57783502e-02
 -9.14273560e-02 -4.47782204e-02 -1.78910568e-02 -5.11550792e-02
 -4.26053964e-02 -4.38213311e-02 -5.26869185e-02 -4.24478427e-02
 -5.58279939e-02  1.94675736e-02  3.53565961e-02 -3.97654623e-02
  3.36714052e-02 -3.31091359e-02  1.79884154e-02 -1.15747921e-01
  5.33022135e-02 -1.01263756e-02 -4.95753102e-02 -5.19008338e-02
  5.03886817e-03  2.06326153e-02  6.82967715e-03  4.11922671e-02
  1.35666132e-01 -1.31969213e-01  5.91905601e-03 -1.29785424e-03
 -8.96535143e-02  6.21526539e-02 -2.25629546e-02  5.12359627e-02
  2.55255383e-02  8.65938887e-02  9.07445233e-03 -6.95367530e-02
 -3.52989286e-02  1.45573523e-02 -5.47736650e-03  4.85091396e-02
  4.25580032e-02  3.49849015e-02 -6.01772033e-02  9.16849822e-02
 -1.54388649e-02 -1.76315811e-02  4.14416492e-02  3.36903445e-02
  4.76336256e-02  1.31110316e-02 -1.92626100e-02  1.36777209e-02
  1.01128601e-01 -6.08233996e-02 -2.81413943e-02  2.17430741e-02
 -4.94781211e-02  9.64918267e-03 -1.42669901e-01  2.59956550e-02
 -5.00679761e-02 -8.56309477e-03 -8.51957221e-03  7.57690668e-02
  7.88240954e-02  5.38879596e-02  4.62615602e-02 -5.37038036e-02
 -3.00367968e-03 -4.36793678e-02  6.80288225e-02  2.60073040e-02
 -4.02548127e-02  2.49176361e-02  7.47880116e-02  3.94565165e-02
 -3.80007029e-02  5.23368604e-02 -2.01522950e-02  2.26846971e-02
 -5.32484800e-02  3.42635289e-02 -8.50413367e-02  9.39249814e-33
  1.28348451e-02  5.36097474e-02 -5.58470860e-02  6.62288889e-02
  1.16448831e-02 -5.77998944e-02  1.67562682e-02  6.13468178e-02
 -2.53317468e-02 -6.93662390e-02 -2.77506839e-02  3.36900428e-02
 -5.04018441e-02  9.70491208e-03  1.95396785e-03 -8.28890949e-02
  5.69088981e-02  9.43183750e-02  2.80648638e-02 -5.91882467e-02
 -4.18583155e-02  4.70032636e-03 -1.27747739e-02  8.40329900e-02
  1.89723819e-02  5.94087802e-02 -6.88620657e-02 -2.55821869e-02
 -6.84751477e-03  2.33055204e-02  3.99640799e-02  1.84365176e-02
 -3.65835428e-02  9.71959252e-03 -2.05736998e-02 -4.25937437e-02
 -7.60777816e-02 -1.16171577e-04 -6.94135055e-02 -2.91284788e-02
 -8.38033929e-02  6.53950274e-02 -2.19585802e-02 -3.74666192e-02
  2.48299702e-03 -1.44727575e-03 -2.67226249e-02  6.21882267e-02
  1.10971712e-01 -1.11759238e-01 -4.15927842e-02  7.35254362e-02
  3.57128344e-02  1.07857496e-01  5.39040677e-02  4.64245863e-03
  4.39713895e-02 -2.36492883e-02  6.93134069e-02  7.19421580e-02
  6.57824194e-03  8.53628069e-02  2.56665535e-02  9.28229559e-03
  1.34719703e-02  3.04878671e-02 -7.50459582e-02 -2.00313870e-02
 -1.10327895e-03  1.29807159e-01 -1.48316082e-02  4.05766293e-02
  2.81979423e-02 -5.48577914e-03 -9.02488653e-04 -2.04563737e-02
  1.27416383e-02 -6.32846579e-02 -5.18845394e-02  1.73841361e-02
 -6.46171048e-02 -9.02832672e-02 -1.97433047e-02 -6.44413233e-02
 -8.47183615e-02 -4.42857742e-02 -3.70611291e-04  3.32207270e-02
 -3.67616564e-02  4.74359766e-02 -6.62917271e-02 -2.74892598e-02
 -3.65326949e-03  3.92872021e-02 -1.00409677e-02 -7.50615150e-33
 -1.72198694e-02 -1.02304912e-03 -1.53693100e-02  5.63707426e-02
  6.80301487e-02 -3.14424105e-04 -6.16379678e-02 -1.08722232e-01
 -1.17493269e-03  9.53800604e-02 -1.71693545e-02 -3.49583477e-02
  3.62738110e-02 -1.25848688e-02 -8.23182147e-03  3.63017321e-02
 -2.45405156e-02 -2.45836172e-02 -1.55058515e-03  7.01806173e-02
 -1.13623641e-01  9.43568945e-02 -1.18287951e-01 -2.85853278e-02
 -2.96605173e-02 -1.06199821e-02 -2.09709182e-02 -3.20046842e-02
 -1.50058009e-02 -5.67077063e-02  4.06685881e-02  3.04151159e-02
 -4.91683371e-02 -6.61164448e-02 -7.36895949e-02  2.67012622e-02
  1.02550529e-01 -2.06447430e-02 -3.40977460e-02  4.35863063e-03
  1.20702580e-01  5.79795614e-02 -6.73661754e-02  4.42845672e-02
  5.93188824e-03  8.13656952e-03 -8.02436471e-02  4.01056148e-02
 -4.79738141e-04  4.51401062e-02  4.32635248e-02 -3.48012596e-02
  1.75889023e-02 -3.93404998e-02  5.23153692e-02  6.52397564e-03
  4.09705415e-02  4.36844975e-02 -3.93762365e-02  6.41992837e-02
 -1.46001242e-02 -8.75727460e-02  4.95515531e-03  6.70574307e-02
  5.33087738e-02 -5.11267781e-02  9.46512632e-03  1.31312877e-01
  2.14643218e-02 -3.80345620e-02  6.00741766e-02  9.33591872e-02
  5.05719855e-02  4.41043191e-02 -1.33612808e-02  5.52651957e-02
  5.60941137e-02  4.46290411e-02  1.19506614e-02  1.30618485e-02
  4.14582863e-02  7.03053847e-02  4.74381223e-02  9.60507151e-03
  5.40571697e-02  1.68832950e-02  7.95728937e-02  1.04598820e-01
  1.75244771e-02 -2.05101538e-02 -2.12660711e-02 -3.52644026e-02
  4.46148030e-02  6.92743734e-02  8.44394490e-02 -3.74717430e-08
 -4.76563722e-02  5.66656180e-02 -2.82899179e-02  3.70005444e-02
  7.13763468e-04  5.39388731e-02 -3.83502361e-03  9.04471502e-02
 -1.50439302e-02  4.17587087e-02 -7.43143633e-02 -5.47594763e-02
 -3.32704373e-02  7.75155202e-02  3.09728552e-02 -6.24883808e-02
  1.71460565e-02  4.52688970e-02 -6.62917783e-03 -1.74978618e-02
  2.39843577e-02  3.49520072e-02  2.61701755e-02 -2.46046465e-02
 -6.36963323e-02 -7.46888742e-02  3.15622985e-02  1.50081264e-02
  7.19041005e-03 -4.45571579e-02 -3.48026641e-02 -3.54865268e-02
  5.24890646e-02 -4.30320241e-02  7.05937818e-02  7.08073378e-02
 -6.24577627e-02 -3.67221329e-03 -3.07913627e-02  4.72920649e-02
 -6.90293759e-02 -5.35818189e-02 -5.82932830e-02 -1.48455519e-02
 -1.73502380e-03 -1.31928865e-02 -1.06949836e-01 -6.35130331e-02
 -6.84463158e-02  7.24294558e-02 -1.92607269e-02 -3.26578096e-02
  2.60879211e-02  4.15995717e-03  5.03818654e-02  8.40684921e-02
 -6.24675117e-02  2.19856519e-02  1.30578140e-02 -2.96682753e-02
  1.14006996e-01  4.33768928e-02 -7.08084181e-02 -3.44166234e-02]",2,2
captum,1,captum is a model interpretability and understanding library for pytorch captum mean comprehension in latin and contains general purpose implementation of integrated gradient saliency map smoothgrad vargrad and others for pytorch model it ha quick integration for model built with domain specific library such a torchvision torchtext and others captum is currently in beta and under active development with the increase in model complexity and the resulting lack of transparency model interpretability method have become increasingly important model understanding is both an active area of research a well a an area of focus for practical application across industry using machine learning captum provides state of the art algorithm including integrated gradient to provide researcher and developer with an easy way to understand which feature are contributing to a model s output for model developer captum can be used to improve and troubleshoot model by facilitating the identification of different feature that contribute to a model s output in order to design better model and troubleshoot unexpected model output captum help ml researcher more easily implement interpretability algorithm that can interact with pytorch model captum also allows researcher to quickly benchmark their work against other existing algorithm available in the library the primary audience for captum are model developer who are looking to improve their model and understand which feature are important and interpretability researcher focused on identifying algorithm that can better interpret many type of model captum can also be used by application engineer who are using trained model in production captum provides easier troubleshooting through improved model interpretability and the potential for delivering better explanation to end user on why they re seeing a specific piece of content such a a movie recommendation installation requirementsthe latest release of captum is easily installed either via anaconda recommended or via pip with condayou can install captum from any of the following supported conda channel channel pytorchchannel conda forgewith pipmanual dev installif you d like to try our bleeding edge feature and don t mind potentially running into the occasional bug here or there you can install the latest master directly from github for a basic install run to customize the installation you can also run the following variant of the above to execute unit test from a manual install run captum help you interpret and understand prediction of pytorch model by exploring feature that contribute to a prediction the model make it also help understand which neuron and layer are important for model prediction let s apply some of those algorithm to a toy model we have created for demonstration purpose for simplicity we will use the following architecture but user are welcome to use any pytorch model of their choice let s create an instance of our model and set it to eval mode next we need to define simple input and baseline tensor baseline belong to the input space and often carry no predictive signal zero tensor can serve a a baseline for many task some interpretability algorithm such a integrated gradient deeplift and gradientshap are designed to attribute the change between the input and baseline to a predictive class or a value that the neural network output we will apply model interpretability algorithm on the network mentioned above in order to understand the importance of individual neuron layer and the part of the input that play an important role in the final prediction to make computation deterministic let s fix random seed let s define our input and baseline tensor baseline are used in some interpretability algorithm such a integratedgradients deeplift gradientshap neuronconductance layerconductance internalinfluence and neuronintegratedgradients next we will use integratedgradients algorithm to assign attribution score to each input feature with respect to the first target output output the algorithm output an attribution score for each input element and a convergence delta the lower the absolute value of the convergence delta the better is the approximation if we choose not to return delta we can simply not provide the return convergence delta input argument the absolute value of the returned delta can be interpreted a an approximation error for each input sample it can also serve a a proxy of how accurate the integral approximation for given input and baseline is if the approximation error is large we can try a larger number of integral approximation step by setting n step to a larger value not all algorithm return approximation error those which do though compute it based on the completeness property of the algorithm positive attribution score mean that the input in that particular position positively contributed to the final prediction and negative mean the opposite the magnitude of the attribution score signifies the strength of the contribution zero attribution score mean no contribution from that particular feature similarly we can apply gradientshap deeplift and other attribution algorithm to the model gradientshap first chooses a random baseline from baseline distribution then add gaussian noise with std to each input example n sample time afterwards it chooses a random point between each example baseline pair and computes the gradient with respect to target class in this case target resulting attribution is the mean of gradient input baseline outputdeltas are computed for each n sample input shape example the user can for instance average them in order to get per example average delta below is an example of how we can apply deeplift and deepliftshap on the toymodel described above the current implementation of deeplift support only the rescale rule for more detail on alternative implementation please see the deeplift paper outputdeeplift assigns similar attribution score a integratedgradients to input however it ha lower execution time another important thing to remember about deeplift is that it currently doesn t support all non linear activation type for more detail on limitation of the current implementation please see the deeplift paper similar to integrated gradient deeplift return a convergence delta score per input example the approximation error is then the absolute value of the convergence delta and can serve a a proxy of how accurate the algorithm s approximation is now let s look into deepliftshap similar to gradientshap deepliftshap us baseline distribution in the example below we use the same baseline distribution a for gradientshap outputdeepliftshap us deeplift to compute attribution score for each input baseline pair and average it for each input across all baseline it computes delta for each input example baseline pair thus resulting to input shape baseline shape delta value similar to gradientshap in order to compute example based delta we can average them per example in order to smooth and improve the quality of the attribution we can run integratedgradients and other attribution method through a noisetunnel noisetunnel allows u to use smoothgrad smoothgrad sq and vargrad technique to smoothen the attribution by aggregating them for multiple noisy sample that were generated by adding gaussian noise here is an example of how we can use noisetunnel with integratedgradients outputthe number of element in the delta tensor is equal to nt sample input shape in order to get an example wise delta we can for example average them let s look into the internals of our network and understand which layer and neuron are important for the prediction we will start with the neuronconductance neuronconductance help u to identify input feature that are important for a particular neuron in a given layer it decomposes the computation of integrated gradient via the chain rule by defining the importance of a neuron a path integral of the derivative of the output with respect to the neuron time the derivative of the neuron with respect to the input of the model in this case we choose to analyze the first neuron in the linear layer outputlayer conductance show the importance of neuron for a layer and given input it is an extension of path integrated gradient for hidden layer and hold the completeness property a well it doesn t attribute the contribution score to the input feature but show the importance of each neuron in the selected layer outputssimilar to other attribution algorithm that return convergence delta layerconductance return the delta for each example the approximation error is then the absolute value of the convergence delta and can serve a a proxy of how accurate integral approximation for given input and baseline is more detail on the list of supported algorithm and how to apply captum on different type of model can be found in our tutorial captum provides a web interface called insight for easy visualization and access to a number of our interpretability algorithm to analyze a sample model on cifar via captum insight runand navigate to the url specified in the output to build insight you will need node x and yarn to build and launch from a checkout in a conda environment runcaptum insight also ha a jupyter widget providing the same user interface a the web app to install and enable the widget runto build the widget from a checkout in a conda environment runif you have question about using captum method please check this faq which address many common issue see the contributing file for how to help out neurips the slide of our presentation can be found herekdd the slide of our presentation from kdd tutorial can be found here you can watch the recorded talk heregtc opening up the black box model understanding with captum and pytorch you can watch the recorded talk herexai summit using captum and fiddler to improve model understanding with explainable ai you can watch the recorded talk herepytorch developer day model interpretability you can watch the recorded talk herenaacl tutorial on fine grained interpretation and causation analysis in deep nlp model you can watch the recorded talk hereiclr workshop on responsible ai more detail about the above mentioned algorithm and their pro and con can be found on our web site captum is bsd licensed a found in the license file,"[('machine learning captum', 0.6701), ('model developer captum', 0.6242), ('model captum', 0.6025), ('pytorch model captum', 0.5842), ('captum insight runand', 0.5254), ('tutorial captum', 0.5249), ('pytorch captum', 0.5212), ('model interpretability algorithm', 0.5176), ('unexpected model output captum help ml', 0.5025), ('model interpretability', 0.5021)]","[-3.12993973e-02 -1.48962647e-01  1.03006617e-03  2.37915956e-04
  5.82862496e-02 -4.45030443e-02 -3.48830707e-02  4.86204633e-03
 -9.60204676e-02 -1.08069405e-02 -4.63483036e-02 -5.29494360e-02
 -3.08845676e-02 -3.62660959e-02 -5.42632528e-02  1.21195856e-02
 -3.29468213e-02  3.98446657e-02 -7.79862255e-02 -7.02369809e-02
  3.60220149e-02  4.43273522e-02 -7.27080833e-03  3.60765718e-02
 -3.40230390e-02  3.49596678e-03  4.18022275e-02  4.99346405e-02
  2.80810129e-02 -6.68355450e-02 -6.18836023e-02 -2.66992077e-02
  5.42358309e-02  3.56051922e-02 -1.11716371e-02  1.17932353e-02
  2.30018869e-02  6.79240329e-03  3.55650224e-02 -6.02425747e-02
 -4.40785363e-02 -4.92702350e-02  2.99705677e-02 -4.50162888e-02
  1.18245780e-01 -3.63895744e-02 -6.34406880e-02 -1.43427685e-01
 -1.60698639e-03 -2.64057312e-02 -8.18650648e-02 -6.64129779e-02
 -2.82606352e-02 -2.29284856e-02 -6.20672330e-02 -5.03115170e-03
  5.29546291e-02 -3.94433290e-02  7.89006650e-02 -6.05362952e-02
 -1.07491866e-01 -6.68787956e-02 -3.31178531e-02  3.24973017e-02
  3.76704074e-02  3.09079401e-02 -6.14883080e-02  3.46985497e-02
  2.12713052e-02 -5.24820900e-03 -9.74812880e-02  4.32048179e-02
 -5.60402200e-02  3.58854681e-02 -4.52507325e-02  3.31990048e-02
  4.11134697e-02 -9.08268429e-03  5.57834506e-02 -6.68524206e-02
 -8.51884335e-02 -2.73828134e-02 -1.19531555e-02  8.53246525e-02
  1.17254471e-02 -2.17936970e-02 -5.65531552e-02 -6.43872935e-03
  2.58907978e-03  2.60517048e-03  5.31839579e-02 -5.85571118e-02
  3.68033238e-02  4.03213874e-02  1.86485518e-02  6.85340166e-02
  2.95789074e-02 -1.20727107e-01 -7.95827154e-03  6.63385494e-03
 -3.74642424e-02  8.35689455e-02  1.12681054e-02 -3.65198553e-02
  5.78034576e-03 -1.85479950e-02  5.52012995e-02  8.74513207e-05
  9.89545658e-02 -3.65057737e-02 -2.50482224e-02  1.02380887e-02
 -8.52294192e-02 -5.22759818e-02  8.85430276e-02 -1.24172568e-02
 -4.66039516e-02 -9.47132707e-04 -5.10931276e-02  3.63461040e-02
 -9.12524834e-02  7.78850392e-02 -7.91138131e-03  3.44430208e-02
 -1.07432269e-02 -5.96331544e-02 -6.65063038e-02  6.42861404e-33
  8.65961425e-03 -6.30782768e-02  2.69647259e-02  3.33457440e-02
  1.24426603e-01 -7.20333233e-02 -7.45096244e-03  1.85346659e-02
  1.57135893e-02  1.04733109e-02 -3.59174125e-02  2.58484185e-02
 -7.60318115e-02  3.70238461e-02  3.19541874e-03  9.17899162e-02
  3.07151563e-02  4.13519107e-02 -1.47164330e-01  3.84409726e-02
  8.42539743e-02  2.99018379e-02  5.42967431e-02  4.14822698e-02
  2.92530544e-02  1.04434744e-01  1.80320106e-02 -4.41748183e-04
 -4.98971157e-03  5.52832596e-02 -5.19746915e-02 -3.76156211e-04
 -3.77191193e-02  2.63684406e-03  4.24062498e-02 -4.47761528e-02
 -2.90433839e-02 -1.94117017e-02 -2.91234604e-03 -1.64503139e-03
 -9.38000306e-02 -1.22100888e-02 -1.27799092e-02 -2.91464552e-02
  1.76929422e-02 -1.18970210e-02 -2.13659741e-03  1.71655789e-02
  3.92281637e-02  4.82876822e-02 -7.47733787e-02  7.59887882e-03
 -8.34560208e-03  1.44715589e-02 -4.54514027e-02 -7.67108286e-03
  4.28490788e-02  2.54053418e-02 -8.76872614e-03  3.53843682e-02
  1.81537662e-02  3.66383046e-02  3.51849981e-02 -1.47270914e-02
 -5.05247936e-02  2.80088000e-02 -3.29773650e-02  4.29204442e-02
  3.18668745e-02  6.71248436e-02 -5.07056974e-02  2.44483408e-02
 -2.57444456e-02  3.35627422e-02  2.54861172e-02 -8.58426653e-03
  1.64496340e-02 -4.13656384e-02  4.21739034e-02  4.17191386e-02
 -5.33378907e-02  6.51323348e-02 -5.47497682e-02 -5.15462458e-02
 -4.86820526e-02 -6.20061010e-02 -4.39854106e-03 -2.97851097e-02
 -2.96060126e-02  2.84093358e-02 -5.29586412e-02  3.84262539e-02
 -1.00634899e-02  5.80547117e-02 -7.76200518e-02 -7.45142636e-33
 -3.36740352e-02  1.01475917e-01 -6.23461716e-02  4.58465405e-02
  5.92366210e-04  8.58929660e-03 -4.91615459e-02  2.40461761e-03
  1.21922716e-01  1.78325344e-02 -7.07466602e-02 -3.05743665e-02
  1.10332454e-02  4.11291867e-02  5.12392493e-03  2.00753398e-02
 -1.28262071e-02 -3.98006327e-02 -4.50635701e-02  5.25232404e-02
 -4.02179807e-02  1.53295174e-01 -7.96089098e-02 -2.78134253e-02
  4.76672724e-02  5.31197079e-02 -4.37815040e-02  2.24308055e-02
  1.74767133e-02  1.37153100e-02  1.64736547e-02  9.49380919e-02
 -6.56124763e-03  2.89761722e-02 -2.87305508e-02 -1.42200878e-02
  1.01306453e-01 -8.43677744e-02  3.82771827e-02  5.32546043e-02
  9.27180797e-02  1.74497105e-02  3.09934970e-02 -3.50528657e-02
  1.76211558e-02 -6.28440529e-02 -7.83825964e-02 -1.10411150e-02
 -2.53869817e-02 -3.43782865e-02 -4.24369052e-02 -1.85257606e-02
 -1.17496587e-02 -1.29515789e-02 -5.23294769e-02  3.66980396e-03
  2.82887351e-02  3.71464132e-03  6.82202503e-02  4.84764874e-02
 -5.04241176e-02 -5.10754157e-03  3.29758376e-02  4.73449230e-02
 -3.79788093e-02 -3.15165147e-02 -1.21445712e-02  1.76691753e-03
 -3.64960320e-02 -4.36982773e-02 -4.07668017e-02  2.74555944e-02
  3.49826328e-02  6.74588829e-02 -4.13831808e-02  2.00343858e-02
 -8.80994573e-02 -4.89095077e-02 -7.42705837e-02 -9.34216529e-02
  5.18733598e-02 -5.31600602e-02  1.58955213e-02  5.87743036e-02
  5.81164425e-03  3.53185423e-02  8.00261274e-02  5.07283583e-02
 -8.38423148e-03 -4.93456312e-02 -4.92597409e-02 -2.60858680e-03
 -7.15356916e-02  1.46962985e-01 -5.26538417e-02 -3.39265860e-08
 -6.20591035e-03  3.23164300e-03  5.31192608e-02  5.64212501e-02
  3.65743414e-02  2.96359863e-02 -1.04805753e-01  1.18059583e-01
 -4.81238365e-02  3.49614322e-02  1.61419176e-02  1.93270314e-02
 -7.67139494e-02 -9.10119899e-03  1.79440528e-02  6.94239363e-02
 -1.45612154e-02  1.40831694e-02 -1.10875228e-02 -7.61538520e-02
  4.96614985e-02 -1.15312182e-03 -1.06067816e-02 -1.15319407e-02
  6.38078749e-02 -9.35728103e-02 -6.25946969e-02  1.18850939e-01
 -5.88211045e-02 -3.19831376e-03  6.40848204e-02  2.45855544e-02
  5.64929321e-02 -2.02060472e-02  8.88388306e-02  1.05981879e-01
  9.29411426e-02 -8.01336691e-02 -1.99455544e-02  8.78984258e-02
 -4.77557145e-02  6.35142997e-02 -7.91775063e-02 -2.59058666e-04
  4.95186746e-02  3.36725004e-02 -1.09344823e-02 -6.63050786e-02
  1.57173164e-02 -1.14463093e-02  1.95529200e-02 -1.74266323e-02
 -2.55326298e-03  5.43990955e-02  3.18402573e-02  9.00003314e-02
  4.24072407e-02 -5.73658235e-02  2.15847249e-04 -7.83466920e-03
  8.12843442e-02  7.68568814e-02  3.68671641e-02  4.05322164e-02]",2,0
tensor-annotations,1,tensorannotations is an experimental library enabling annotation of data type and semantic shape information using type annotation for example this annotation state that the data type of frame is uint and that the dimension are time like batch like etc while saying nothing about the actual value e g the actual batch size why two reason to do this the library provides three thing tensorannotations is being developed for jax and tensorflow here is some code that take advantage of static shape checking this code contains shape annotation in the signature of sample batch and train batch and in the line calling reduce max it is otherwise the same code you would have written in an unchecked program you can check these annotation for inconsistency by running a static type checker on your code see general usage below for example running train batch directly on batch will result in the following error from pytype similarly changing the the call to reduce max from axis to axis result in these message were shortened for readability the actual error will be more verbose because fully qualified type name will be displayed we are looking into improving this see example tf time batch py for a complete example tensorannotatations requires python or above due to the use of typing literal to install custom tensor type then depending on whether you use jax or tensorflow if you use pytype you ll also need to take a few extra step to let it take advantage of jax tensorflow stub since it doesn t yet support pep stub package first make a copy of typeshed in e g your home directory next symlink the stub into your copy of typeshed first import tensor annotation and start annotating function signature and variable assignment this can be done gradually next run a static type checker on your code if you use mypy it should just work if you use pytype you need to invoke it in a special way in order to let it know about the custom typeshed installation we recommend you deliberately introduce a shape error and then confirm that your type checker give you an error to be sure you re set up correctly tensorannotations provides tensor class for jax and tensorflow these class can be parameterized by semantic axis label below using generic similar to list int different class are needed for each rank because python currently doe not support variadic generic but we re working on it tensorannotations also provides it own data type type this is because for various reason the native data type type like tf uint and jnp uint are unsuitable for use in type annotation see tensorflow py and jax py for more information axis label are used to indicate the semantic meaning of each dimension in a tensor whether the dimension is batch like feature like etc note that no connection is made between the symbol e g batch and the actual value of that dimension e g the batch size the symbol really doe only describe the semantic meaning of the dimension see ax py for the list of axis label we provide out of the box to define a custom axis label simply subclass tensor annotation ax axis you can also use typing newtype to do this using a single line in the future we intend to support axis type that are tied to the actual size of that axis currently however we don t have a good way of doing this if you nonetheless want to annotate certain dimension with a literal size e g for documentation of interface which are hardcoded for specific size we recommend you just use a custom axis for this purpose just to be clear though these size will not be checked neither statically nor at runtime by default tensorflow and jax are not aware of our annotation for example if you have a tensor x array uint time batch and you call jnp sum x axis you won t get a array uint batch you ll just get an any we therefore provide a set of custom type annotation for tensorflow and jax packaged in stub pyi file our stub currently cover the following part of the api all operation are supported for rank and tensor unless otherwise noted unary operator are also supported for rank scalar tensor see coverage tensor unary operator for tensor x ab x x xtensor binary operator for tensor a and b a b a b a b a b a b a b a b a b a b yet to be typed a float a int for tensor broadcasting where one axis is see coverage tensor unary operator for tensor x ab x x xtensor binary operator for tensor a and b a b a b a b a b a b a b a b a b a b yet to be typed a float a int for tensor broadcasting where one axis is some of your code might be already typed with existing library tensor type if this is the case and you don t want to change these type globally in your code you can cast to tensorannotations class with typing cast note that this is only a hint to the type checker at runtime it s a no op an alternative syntax emphasising this fact is use tuples for shape axis specificationsfor type inference with tensorflow and jax api function we often have to match additional argument i e the rank of a tf zero tensor depends on the length of the shape argument this only work with tuples and not with list whileruntime v static checksnote that we do not verify that the rank of a tensor at runtime match the one specified in the annotation if you were in an evil mood you could create an untyped any tensor and statically type it a something completely wrong this is in line with the rest of the python type checking approach which doe not enforce consistency with the annotated type at runtime value consistency not only do we not verify the rank we don t verify anything about the actual shape value either the following will not raise an error note that this is by design shape symbol such a batch are not placeholder for actual value like or symbol only refer to the semantic meaning of a dimension in the above example say x might be a train batch and y might be a test batch and therefore they have different size even though both of their dimension are batch like this mean that even element wise operation like z x y would in this case not raise a type check error why doesn t e g tjax arrayn subclass jnp devicearray we d like this to be the case but haven t figured out how to yet because of circular dependency we ultimate solution to this will hopefully be to upstream our arrayn class such that they can be defined in jax numpy too until then we ll just be trying to make e g tjax arrayn look a close to jnp devicearray a possible through dummy method and dummy attribute so that autocomplete still work if there are particular method attribute you d like added please do let u know why are so many method annotated a any in the jax stub we don t yet have a good way of automatically generating stub in general for the method where we do generate stub automatically all the one not annotated a any we ve checked their signature manually and written stub generator for each method individually ideally we d start from stub generated by e g pytype and then customise them to include shape information but we haven t got around to setting this up yet this library is one approach of many to checking tensor shape we don t expect it to be the final solution we create it to explore one point in the space of possibility other tool for checking tensor shape include to learn more about tensor shape checking in general see the tensor annotation package contains four type of thing,"[('tensorannotations class', 0.5995), ('tensorannotations', 0.5961), ('tensor annotation package', 0.571), ('complete example tensorannotatations', 0.5411), ('first import tensor annotation', 0.5301), ('thing tensorannotations', 0.5235), ('custom tensor type', 0.491), ('type annotation', 0.4731), ('library tensor type', 0.4728), ('tensor shape checking', 0.4626)]","[-2.63065770e-02 -4.07627188e-02 -5.46481786e-03 -3.34597602e-02
  4.71990220e-02  1.36303818e-02  2.98970640e-02  2.34514605e-02
 -1.40342906e-01 -2.14098841e-02 -3.73775139e-02 -1.18848234e-02
 -8.49645659e-02  4.21149097e-02  9.94711183e-03 -2.47634258e-02
 -4.33148928e-02  8.24870393e-02 -3.00568547e-02  7.04484247e-03
 -2.96840128e-02 -1.78469922e-02  4.03156467e-02  5.50394505e-02
 -6.48508295e-02 -3.24785970e-02  6.58510849e-02 -4.51753736e-02
 -1.21163726e-02 -2.84738448e-02 -6.88513890e-02  7.32480884e-02
  5.94090261e-02  1.06455833e-02 -2.25954819e-02 -2.18046224e-03
 -1.95911489e-02 -2.34829523e-02 -2.31281612e-02  1.03108576e-02
 -2.58228127e-02 -2.25956906e-02  9.18325130e-03  4.52823415e-02
  2.77985763e-02  2.32631024e-02  2.78462307e-03 -1.72990386e-03
  3.63076329e-02  3.63434129e-03 -3.80365811e-02 -9.30937380e-02
 -8.74182358e-02 -5.66057675e-02  8.41285568e-03 -1.63601786e-02
  2.32156143e-02 -2.64372397e-02 -5.64137548e-02 -3.76422554e-02
  2.97964960e-02  2.28742901e-02 -4.57417890e-02  2.58159265e-02
 -1.37708588e-02  5.18626198e-02  1.25955855e-02  6.28476441e-02
  9.96184275e-02 -7.34617263e-02 -7.36981630e-02 -4.11753729e-02
 -3.72977890e-02  9.50125456e-02 -5.67063913e-02 -2.08542738e-02
  5.93638793e-02  4.59520556e-02  3.79865132e-02 -4.42120954e-02
 -1.34763241e-01  7.20708519e-02  8.25322568e-02 -3.18890736e-02
 -2.04400253e-02  5.26736602e-02  1.22258998e-02  1.80241019e-02
  7.24023499e-04 -2.13861614e-02  1.07810907e-01 -6.51553124e-02
 -4.91284728e-02  1.51483752e-02  6.81044534e-02  8.68117064e-02
 -3.87521125e-02  4.22716290e-02  3.17487530e-02  2.10260227e-02
  1.48002971e-02 -6.42515346e-02 -7.22179376e-03  3.38987485e-02
 -3.33041027e-02  5.15788049e-02  1.31605063e-02 -3.41453180e-02
 -1.68505125e-02  8.07420164e-03 -2.37731985e-03  1.64314825e-02
  2.98020411e-02 -1.19151928e-01  2.47692019e-02  3.27146426e-03
 -8.26930553e-02  3.97547595e-02  3.11003886e-02  3.83729152e-02
 -2.38932949e-02 -2.90859141e-03  9.51698050e-02 -1.07650394e-02
 -2.41645668e-02  3.32756341e-02 -8.42351094e-02  6.27384478e-33
  1.06287282e-02  4.71392870e-02 -4.39918526e-02  6.11848868e-02
 -7.02083707e-02 -5.73938675e-02 -5.92844561e-03  9.73745715e-03
 -1.27723049e-02 -4.64608185e-02  3.80994729e-03  5.90517744e-02
  3.20919231e-02  5.82287572e-02 -5.43631501e-02 -2.63687298e-02
  2.47869398e-02 -1.33397945e-04  1.13731893e-02  2.90747210e-02
  3.65177505e-02  4.26752083e-02  6.94064945e-02  9.10294950e-02
  1.12673198e-03  1.18099853e-01 -1.05283018e-02 -4.44755815e-02
 -3.80975306e-02  5.45428554e-03 -2.02521775e-02 -1.15855165e-01
 -5.93876047e-03 -8.20303615e-03 -1.54035846e-02 -1.08767822e-02
 -1.26992147e-02 -9.91038419e-03 -6.40466884e-02 -1.28596723e-01
  1.94988661e-02  4.64382693e-02 -6.06984645e-02 -9.52661037e-03
 -3.07124183e-02  1.68571360e-02  3.76121700e-02  6.71200901e-02
  9.05273855e-02 -1.63258880e-03 -1.50187146e-02 -7.46323615e-02
 -1.63746849e-02 -3.54003720e-03 -3.55688743e-02  3.25850211e-02
  1.05219912e-02  1.13856455e-03  6.62205592e-02 -7.10076615e-02
 -1.98738985e-02  1.84332766e-02  7.52779096e-02 -5.73913753e-02
 -4.67634425e-02 -1.34067424e-02 -1.25851691e-01 -5.70606673e-03
  3.23561244e-02 -1.33279962e-02 -3.38558070e-02  5.92968315e-02
 -6.82378411e-02  4.73094732e-02 -3.70239752e-04 -1.99792329e-02
  3.37827355e-02 -7.43046105e-02  1.92642473e-02  8.57534818e-03
 -8.24734196e-02 -1.41686369e-02  5.50568663e-02 -6.89156651e-02
 -6.49062917e-02 -5.30507863e-02  3.44557092e-02 -2.57242639e-02
  5.44434041e-02 -3.42174172e-02 -5.01140356e-02  1.96464192e-02
 -2.38112058e-03  2.72744875e-02  1.93677191e-02 -6.82776343e-33
 -3.09227756e-03  2.11116988e-02 -1.05456509e-01  3.24382037e-02
 -2.84500420e-02  4.44195680e-02 -3.73097546e-02  5.32949120e-02
  6.86600506e-02  4.11190055e-02 -2.43540038e-03 -1.48927979e-02
  5.09731360e-02 -4.81572859e-02 -1.52411060e-02 -3.45844850e-02
 -6.48882315e-02 -1.11988988e-02  6.31128019e-03  1.00522144e-02
 -6.33451641e-02 -4.97649536e-02 -8.31241235e-02  5.47026331e-03
  7.40035623e-03  6.45942092e-02 -1.48362210e-02 -8.97549167e-02
  4.77597117e-02  2.66960915e-02  3.62535119e-02  1.26775226e-03
 -1.13566872e-03  4.23447117e-02 -7.19689205e-02 -3.09769735e-02
  9.32700858e-02  1.28176762e-02 -1.35317361e-02  3.73624526e-02
  7.40434006e-02  1.11210249e-01  1.98051371e-02  2.33057756e-02
 -7.12707415e-02 -5.37287407e-02 -9.01423246e-02  5.95859773e-02
 -5.23699215e-03 -6.62420392e-02 -1.84219219e-02 -6.32130727e-02
  5.16310409e-02 -6.41240329e-02 -8.08907300e-03 -3.00499494e-03
  1.33147061e-01  3.08331400e-02  6.35855496e-02  2.66934372e-02
 -6.99626431e-02 -9.49223191e-02  6.89541735e-03  1.17257321e-02
  2.75883693e-02 -2.97344681e-02 -8.92036110e-02 -1.05931424e-02
  7.36224838e-03 -1.65259168e-02 -3.81285809e-02  9.61145386e-02
 -1.29589187e-02  1.52846016e-02 -7.37678027e-03  5.49467094e-03
  8.35908875e-02  1.00516267e-01  9.01874751e-02 -1.16714202e-02
  8.86193514e-02 -5.67737455e-03  6.14341609e-02  1.50911629e-01
  1.89831816e-02  3.52976955e-02  3.64667810e-02  3.76397893e-02
  1.84532441e-02  4.01324444e-02 -1.77625902e-02  3.20594683e-02
  4.17545997e-02  1.01907276e-01  1.66194793e-02 -3.21303162e-08
 -1.13954999e-01 -6.19479753e-02  6.59846067e-02  3.13270837e-02
 -7.06709595e-03  5.81225380e-02  1.24023287e-02  4.62309271e-02
 -2.49975286e-02  6.62124855e-03 -6.08684644e-02  2.01134216e-02
 -1.08593911e-01 -3.53481174e-02  6.75763786e-02  3.11875679e-02
 -3.49291079e-02  7.57585913e-02 -4.93471324e-02  3.49118449e-02
 -1.38437031e-02 -1.01310210e-02  1.15088383e-02  1.36270151e-02
  2.70084720e-02 -9.87109821e-03  5.38963377e-02  7.83470497e-02
 -2.12308281e-04 -3.51067744e-02 -7.16975927e-02  9.66715720e-03
  2.35064272e-02 -1.01557128e-01  2.50836071e-02  1.25916213e-01
  4.92718033e-02 -5.48805818e-02 -1.52198151e-02  6.25912398e-02
 -9.64835361e-02  1.28722528e-03 -6.54116645e-02 -6.86493963e-02
  1.06881797e-01 -1.53652625e-02 -4.93082441e-02 -3.12985368e-02
 -1.04989216e-01 -6.25134166e-03 -1.57466475e-02  3.79036441e-02
 -7.61562660e-02  6.54207245e-02 -7.65438452e-02  8.43811110e-02
 -2.56727524e-02  1.01825176e-02  5.29435314e-02 -5.00882193e-02
 -3.97694930e-02  6.31934926e-02  2.70377435e-02 -6.55321702e-02]",2,2
crafter,1,status stable releaseopen world survival game for evaluating a wide range of agent ability within a single environment crafter feature randomly generated d world where the player need to forage for food and water find shelter to sleep defend against monster collect material and build tool crafter aim to be a fruitful benchmark for reinforcement learning by focusing on the following design goal research challenge crafter pose substantial challenge to current method evaluating strong generalization wide and deep exploration representation learning and long term reasoning and credit assignment meaningful evaluation agent are evaluated by semantically meaningful achievement that can be unlocked in each episode offering insight into the ability spectrum of both reward agent and unsupervised agent iteration speed crafter evaluates many agent ability within a single env vastly reducing the computational requirement over benchmark suite that require training on many separate envs from scratch see the research paper to find out more benchmarking the spectrum of agent capabilitiesto install crafter run pip install crafter the environment follows the openai gym interface observation are image of size and output are one of categorical action agent are allowed a budget of m environmnent step and are evaluated by their success rate of the achievement and by their geometric mean score example script for computing these are included in the analysis directory of the repository reward the sparse reward is for unlocking an achievement during the episode and or for lost or regenerated health point result should be reported not a reward but a success rate and score success rate the success rate of the achievemnts are computed a the percentage across all training episode in which the achievement wa unlocked allowing insight into the ability spectrum of an agent crafter score the score is the geometric mean of success rate so that improvement on difficult achievement contribute more than improvement on achievement with already high success rate baseline score of various agent are available for crafter both with and without reward the score are available in json format in the score directory of the repository for comparison the score of human expert player is the baseline implementation are available a a separate repository please open an issue on github,"[('reward agent', 0.5683), ('meaningful evaluation agent', 0.5437), ('agent crafter score', 0.5338), ('unsupervised agent iteration speed crafter', 0.5259), ('categorical action agent', 0.5001), ('many agent ability', 0.4941), ('agent ability', 0.4876), ('design goal research challenge crafter', 0.4377), ('various agent', 0.4091), ('openai gym interface observation', 0.4074)]","[-1.63717531e-02 -2.99470853e-02 -6.47220835e-02  1.32824257e-02
 -6.41575432e-04  2.60536652e-02  2.04209574e-02 -1.50585491e-02
 -6.22141659e-02 -1.86817115e-03 -2.32327394e-02 -1.06073029e-01
 -3.47229943e-04  4.71271873e-02  6.58333749e-02  2.12731250e-02
  9.59297642e-02  7.62427524e-02 -4.46182303e-02 -4.92892377e-02
  5.85512370e-02  1.90111883e-02  4.33646217e-02  1.33097833e-02
 -6.88534677e-02 -1.68681284e-03 -3.15539502e-02 -4.66224626e-02
  2.45175567e-02 -4.56598885e-02 -1.65819861e-02  7.83360302e-02
  7.72677436e-02  6.50414750e-02 -1.08050741e-02  6.21161126e-02
 -5.43614067e-02  3.67488787e-02  9.21744574e-03 -6.83675474e-03
 -7.56118149e-02 -5.08312471e-02  2.00661947e-03 -4.09494936e-02
  6.16204403e-02 -4.11982164e-02 -2.89440751e-02 -7.50467554e-02
 -7.06119910e-02 -8.89677089e-03 -1.25752926e-01 -1.03402168e-01
  2.87485309e-03  6.99177710e-03 -2.82709319e-02 -1.41016981e-02
 -8.10866617e-03 -3.32174450e-02  3.94040812e-03 -4.18315008e-02
 -1.78173948e-02 -2.76952777e-02 -7.07937405e-03  5.61823547e-02
 -2.95645911e-02 -3.69274355e-02 -8.74694437e-02  5.88572724e-03
  3.77690196e-02 -9.75499675e-02 -2.58557266e-03 -2.15475764e-02
 -4.49776591e-04 -4.41219695e-02  7.04331473e-02  1.28740622e-02
 -1.54524622e-02 -2.34203935e-02  3.86494473e-02 -1.25044174e-02
 -4.75813672e-02 -4.34809923e-02 -3.64093930e-02  7.30053410e-02
 -3.39647979e-02  4.43679765e-02  1.03799235e-02  4.87966500e-02
  1.62614696e-02  5.60145862e-02  2.41814069e-02  1.03312628e-02
  4.39452492e-02  4.30578738e-03  2.75976099e-02  7.67501891e-02
 -6.84929341e-02 -1.05148688e-01 -1.61494669e-02  4.39770967e-02
 -3.11312284e-02 -2.10990408e-03 -3.67260613e-02  3.94015945e-02
 -6.10642843e-02 -1.24149816e-02  4.79578301e-02 -8.33002664e-03
  6.15414865e-02  2.00436097e-02  2.41031148e-03 -4.79973592e-02
  1.27486493e-02 -3.39402631e-02  8.90597999e-02  4.75922339e-02
 -8.29981267e-02  1.00560442e-01  3.95780504e-02  3.13836373e-02
  2.75041927e-02 -4.14038114e-02  8.56218264e-02 -2.06574518e-02
  4.78933416e-02  8.59105736e-02 -1.34313470e-02  6.56761257e-33
  4.08674851e-02  7.11104786e-03  5.06231487e-02  5.09417914e-02
  3.60325463e-02 -5.91876358e-02  1.84287168e-02  4.40686643e-02
  2.56629605e-02 -5.38879186e-02  1.45686762e-02  1.87097594e-01
 -4.22094613e-02  1.64082095e-01  7.16698617e-02 -3.15832123e-02
  8.54630489e-03  3.84159945e-02 -1.52592100e-02 -2.09614653e-02
  6.26663938e-02  7.64970155e-03  3.82267833e-02  6.69906065e-02
  2.50144508e-02  2.19059158e-02 -1.48063675e-02 -8.79447907e-02
 -2.15996709e-02 -4.91861673e-03 -3.33443433e-02  1.81056540e-02
 -1.25856083e-02  2.57761031e-02 -7.00714365e-02  3.95573862e-02
 -2.36780327e-02 -8.41988176e-02  2.66209226e-02 -2.35578399e-02
 -1.04724996e-01  3.33761387e-02  6.49111439e-03 -3.63968164e-02
  4.42549959e-03 -5.28274029e-02  4.29751761e-02  4.24405262e-02
  7.66082034e-02  6.40318245e-02 -1.33420303e-02  6.96207434e-02
  5.19757420e-02 -3.38123143e-02  3.37649211e-02  2.09670141e-02
  1.60915013e-02  3.01326681e-02 -5.20283692e-02  4.08547334e-02
 -3.82077955e-02  1.25477156e-02  5.50435074e-02 -3.90169770e-02
  1.54496618e-02  1.72439292e-02  9.28586721e-03 -1.98773481e-03
  3.94515581e-02  4.44566496e-02 -4.10468169e-02  6.72742873e-02
 -1.32116545e-02 -2.26849597e-02 -3.26049030e-02 -9.44253150e-03
 -2.80296262e-02 -1.06621362e-01 -2.15883367e-02 -7.54231401e-03
 -8.53096992e-02  5.75111695e-02 -5.70792258e-02  2.81246901e-02
 -4.31647412e-02 -3.23017612e-02 -8.42811354e-03 -4.84981723e-02
  2.89775748e-02  3.15748081e-02 -9.33029279e-02  3.82408015e-02
 -9.09210816e-02  8.24126750e-02 -1.47632342e-02 -5.31985213e-33
 -7.85169005e-02 -4.81234118e-02 -7.99150839e-02  6.39812499e-02
  1.14850804e-01 -2.29151491e-02 -3.30928825e-02 -3.45063321e-02
  7.46614859e-02  7.27227405e-02 -1.27896073e-03  2.38127750e-03
  4.24998021e-03 -1.31184710e-02 -2.92505529e-02 -9.51771364e-02
 -5.08180894e-02  1.15698772e-02  1.77885848e-03  4.38088365e-02
  6.79838657e-02  1.18372545e-01 -8.07802826e-02 -8.85185301e-02
  5.09755909e-02  4.04240265e-02 -1.53835891e-02  6.69235550e-03
  6.89487830e-02  3.98436822e-02  1.03286155e-01 -1.74113512e-02
  2.28170697e-02  4.26012725e-02 -5.29545185e-04  5.37195168e-02
  4.08623926e-02 -6.69441447e-02 -6.77137927e-04  6.10624291e-02
  7.81415105e-02 -5.53959906e-02  1.01756239e-02  1.98495556e-02
 -1.50889475e-02 -1.88380498e-02 -9.25213918e-02  3.21192145e-02
  2.60058139e-03 -1.74813736e-02 -1.63642205e-02 -8.80816951e-03
 -8.39539617e-02 -6.12611771e-02 -3.44701596e-02  5.29354671e-03
 -4.37065633e-03 -5.05590215e-02  8.02929327e-03 -2.47731842e-02
  3.25408578e-02  4.79407161e-02 -1.51434843e-03  1.51792243e-01
 -3.01141217e-02 -1.27392784e-02  2.32937466e-02  4.30286191e-02
 -1.06542304e-01 -8.66661668e-02  4.51025367e-02  1.61510408e-02
 -5.90604823e-03  7.23188445e-02  1.82870161e-02 -5.19313011e-03
 -5.49551733e-02  4.60038297e-02  5.18531203e-02 -6.04414120e-02
 -4.82645929e-02 -2.61365552e-03  6.95876603e-04  1.83574203e-02
  2.25343127e-02  6.36003390e-02  2.07678825e-02  5.32307960e-02
  4.77013737e-02  3.58245373e-02  1.47838490e-02  2.96843145e-02
  3.46020758e-02  6.91483319e-02 -3.53359133e-02 -3.50018148e-08
 -5.89155592e-02 -4.87119928e-02  6.88057542e-02  2.71700118e-02
 -4.20127437e-02  1.03640668e-02 -2.17217412e-02  8.30183458e-03
 -2.75624488e-02  2.09306236e-02  8.94528851e-02 -7.03786081e-03
 -8.18613991e-02  5.89191541e-03  6.35065436e-02 -3.18744145e-02
 -2.03786790e-03  6.60758791e-03 -3.02837230e-02 -7.04639927e-02
  5.51345013e-02 -8.21811613e-03 -4.11005467e-02 -5.66675700e-02
 -8.92861858e-02 -4.98335883e-02 -8.50146487e-02 -2.83391811e-02
 -4.06875573e-02  6.94122687e-02  5.04179038e-02  2.17455090e-03
  1.20041169e-01 -7.02123195e-02  1.46691473e-02  1.13382250e-01
 -6.63089752e-02 -8.04494545e-02 -4.79535386e-02  5.68999127e-02
 -8.47617835e-02  6.76304800e-03  7.50287389e-03 -5.96974418e-02
  4.49706875e-02 -4.67707664e-02 -1.03135079e-01 -8.18811134e-02
  3.00835148e-02 -4.35227565e-02 -9.66118947e-02 -3.21866162e-02
 -6.57161549e-02 -1.17744198e-02  5.44572510e-02  6.34453148e-02
  5.88102564e-02 -5.04128262e-02  3.89297828e-02 -5.67226729e-04
  5.73515892e-02  1.45569341e-02 -2.93762051e-02 -1.67807508e-02]",2,0
dice-ml,1,how to explain a machine learning model such that the explanation is truthful to the model and yet interpretable to people ramaravind k mothilal amit sharma chenhao tanfat paper doc example notebook live jupyter notebook blog post explanation for ml using diverse counterfactualscase study towards data science hotel booking analytics vidhya titanic dataset explanation are critical for machine learning especially a machine learning based system are being used to inform decision in societally critical domain such a finance healthcare education and criminal justice however most explanation method depend on an approximation of the ml model to create an interpretable explanation for example consider a person who applied for a loan and wa rejected by the loan distribution algorithm of a financial company typically the company may provide an explanation on why the loan wa rejected for example due to poor credit history however such an explanation doe not help the person decide what they do should next to improve their chance of being approved in the future critically the most important feature may not be enough to flip the decision of the algorithm and in practice may not even be changeable such a gender and race dice implement counterfactual cf explanation that provide this information by showing feature perturbed version of the same person who would have received the loan e g you would have received the loan if your income wa higher by in other word it provides what if explanation for model output and can be a useful complement to other explanation method both for end user and model developer barring simple linear model however it is difficult to generate cf example that work for any machine learning model dice is based on recent research that generates cf explanation for any ml model the core idea is to setup finding such explanation a an optimization problem similar to finding adversarial example the critical difference is that for explanation we need perturbation that change the output of a machine learning model but are also diverse and feasible to change therefore dice support generating a set of counterfactual explanation and ha tunable parameter for diversity and proximity of the explanation to the original input it also support simple constraint on feature to ensure feasibility of the generated counterfactual example dice support python the stable version of dice is available on pypi dice is also available on conda forge to install the latest dev version of dice and it dependency clone this repo and run pip install from the top most folder of the repo if you face any problem try installing dependency manually dice requires the following package jsonschemanumpyscikit learnpandash pytqdm optional tensorflow pytorch work with tensorflow with dice generating explanation is a simple three step process train mode and then invoke dice to generate counterfactual example for any input for any given input we can now generate counterfactual explanation for example the following input lead to class low income using dice we can now generate example that would have been classified a class high income you can save the generated counterfactual example in the following way for more detail check out the doc source notebook folder here are some example notebook getting started generate cf example for a sklearn tensorflow or pytorch binary classifier and compute feature importance score explaining multi class classifier and regressors generate cf explanation for a multi class classifier or regressor local and global feature importance estimate local and global feature importance score using generated counterfactuals providing constraint on counterfactual generation specifying which feature to vary and their permissible range for valid counterfactual example dice can generate counterfactual example using the following method model agnostic methodsrandomized samplingkd tree for counterfactuals within the training data genetic algorithmsee model agnostic notebook for code example on using these method gradient based methodsan explicit loss based method described in mothilal et al default for deep learning model a variational autoencoder vae based method described in mahajan et al see the basevae notebook the last two method require a differentiable model such a a neural network if you are interested in a specific method do raise an issue here datadice doe not need access to the full dataset it only requires metadata property for each feature min max for continuous feature and level for categorical feature thus for sensitive data the dataset can be provided a modelwe support pre trained model a well a training a model here s a simple example using tensorflow check out the getting started notebook to see code example on using dice with sklearn and pytorch model explanationswe visualize explanation through a table highlighting the change in feature we plan to support an english language explanation too we acknowledge that not all counterfactual explanation may be feasible for a user in general counterfactuals closer to an individual s profile will be more feasible diversity is also important to help an individual choose between multiple possible option dice provides tunable parameter for diversity and proximity to generate different kind of explanation additionally it may be the case that some feature are harder to change than others e g education level is harder to change than working hour per week dice allows input of relative difficulty in changing a feature through specifying feature weight a higher feature weight mean that the feature is harder to change than others for instance one way is to use the mean absolute deviation from the median a a measure of relative difficulty of changing a continuous feature by default dice computes this internally and divide the distance between continuous feature by the mad of the feature s value in the training set we can also assign different value through the feature weight parameter finally some feature are impossible to change such a one s age or race therefore dice also allows inputting a list of feature to vary it also support simple constraint on feature that reflect practical constraint e g working hour per week should be between and using the permitted range parameter for more detail check out this notebook being truthful to the model counterfactual explanation can be useful to all stakeholder for a decision made by a machine learning model that make decision decision subject counterfactual explanation can be used to explore actionable recourse for a person based on a decision received by a ml model dice show decision outcome with actionable alternative profile to help people understand what they could have done to change their model outcome ml model developer counterfactual explanation are also useful for model developer to debug their model for potential problem dice can be used to show cf explanation for a selection of input that can uncover if there are any problematic in dependence on some feature e g for of input changing feature x and y change the outcome but not for the other we aim to support aggregate metric to help developer debug ml model decision maker counterfactual explanation may be useful to decision maker such a doctor or judge who may use ml model to make decision for a particular individual dice allows probing the ml model to see the possible change that lead to a different ml outcome thus enabling decision maker to ass their trust in the prediction decision evaluator finally counterfactual explanation can be useful to decision evaluator who may be interested in fairness or other desirable property of an ml model we plan to add support for this in the future ideally counterfactual explanation should balance between a wide range of suggested change diversity and the relative ease of adopting those change proximity to the original input and also follow the causal law of the world e g one can hardly lower their educational degree or change their race we are working on adding the following feature to dice support for using dice for debugging machine learning modelsconstructed english phrase e g desired outcome if feature wa changed and other way to output the counterfactual examplesevaluating feature attribution method like lime and shap on necessity and sufficiency metric using counterfactuals see this paper support for bayesian optimization and other algorithm for generating counterfactual explanationsbetter feasibility constraint for counterfactual generationif you find dice useful for your research work please cite it a follows ramaravind k mothilal amit sharma and chenhao tan explaining machine learning classifier through diverse counterfactual explanation proceeding of the conference on fairness accountability and transparency bibtex this project welcome contribution and suggestion most contribution require you to agree to a contributor license agreement cla declaring that you have the right to and actually do grant u the right to use your contribution for detail visit http cla microsoft com when you submit a pull request a cla bot will automatically determine whether you need to provide a cla and decorate the pr appropriately e g label comment simply follow the instruction provided by the bot you will only need to do this once across all repos using our cla this project ha adopted the microsoft open source code of conduct for more information see the code of conduct faq or contact opencode microsoft com with any additional question or comment,"[('machine learning model', 0.539), ('machine learning classifier', 0.4963), ('diverse counterfactual explanation proceeding', 0.4952), ('counterfactual examplesevaluating feature attribution method', 0.4926), ('counterfactual explanationsbetter feasibility constraint', 0.4808), ('most explanation method', 0.4643), ('machine learning', 0.461), ('ml model developer', 0.4306), ('prediction decision evaluator', 0.4223), ('counterfactual generation', 0.4033)]","[-4.71174493e-02 -8.48731920e-02  2.20431108e-02  7.35001813e-05
  7.78359398e-02 -1.55913308e-02 -2.95898970e-02  2.02665813e-02
 -8.01286623e-02  1.42080123e-02 -6.30721822e-02 -8.70999694e-02
  2.06846390e-02 -5.97618707e-02 -6.96923360e-02  1.06910616e-02
  2.62702373e-03  3.64301801e-02 -2.47466806e-02 -1.12915747e-01
  5.73502332e-02  1.70463044e-02 -1.67090613e-02  8.38355869e-02
 -5.90041578e-02 -3.47077698e-02  1.02413535e-01  3.65252905e-02
  6.50408641e-02 -6.11230079e-03 -5.75691685e-02 -4.09902148e-02
 -4.02728952e-02  2.54070070e-02 -8.86236429e-02 -5.74390311e-03
 -2.10969709e-02  6.17576949e-02  6.71217516e-02 -1.16703641e-02
 -7.47196376e-02 -6.78242296e-02 -1.57378726e-02 -9.72442999e-02
  1.41652077e-01  8.60781875e-03 -5.69126755e-02 -8.23134035e-02
 -5.19894287e-02  2.14605406e-02 -1.08220026e-01 -5.12693226e-02
 -1.53441885e-02 -6.77897334e-02 -3.79413180e-02 -7.05429539e-02
  6.04946427e-02  2.54119691e-02  6.51239902e-02 -1.21374028e-02
 -3.39850150e-02 -8.80170017e-02 -1.21089861e-01  5.03917858e-02
  4.83363383e-02  3.90314236e-02 -6.76990971e-02  4.54562493e-02
 -4.11118940e-03  1.02888523e-02 -4.81658615e-02  7.50374570e-02
 -8.65286309e-03  4.87152003e-02 -1.27740102e-02  4.44185808e-02
  4.11654860e-02  5.37366010e-02  6.14598915e-02 -2.57144831e-02
 -7.23224059e-02  8.50606803e-03  1.00795217e-02  2.13940311e-02
 -4.64753341e-03 -9.81350914e-02 -8.19233209e-02  1.69447269e-02
  4.20712829e-02  4.40484211e-02  4.32197675e-02 -7.31448457e-02
  2.19803657e-02  4.45933954e-04 -6.76489319e-04  6.65626377e-02
 -5.18066771e-02 -7.53497481e-02  5.83592355e-02 -4.67866324e-02
 -3.68258804e-02  6.85853437e-02  1.74687132e-02  4.79025883e-04
  2.32282095e-02 -1.30287614e-02  1.67342531e-03 -1.69574022e-02
  1.18010454e-01 -1.38308648e-02  2.16775271e-03  1.38468081e-02
 -3.21766324e-02 -6.83091357e-02  6.85954988e-02  2.69370899e-03
 -3.75620164e-02  1.55354803e-03 -1.04096802e-02  5.69117442e-02
 -8.62582251e-02  4.51938175e-02  6.32905886e-02  2.50870921e-02
 -2.06715092e-02 -3.83913592e-02 -8.17153156e-02  8.32860670e-33
  1.49669638e-02 -3.91833670e-02 -4.96909895e-04  4.68802778e-03
  5.29207475e-02 -1.11356556e-01 -6.81318492e-02  2.44612824e-02
  8.27216506e-02  9.04413778e-03  3.10023446e-02  9.09794942e-02
 -8.11346546e-02  7.93738514e-02  9.52340290e-03  5.94738834e-02
  6.41706288e-02  6.23113476e-02 -9.77802575e-02 -2.19073258e-02
  3.41543779e-02 -2.24065315e-02  4.41503376e-02  1.69408170e-03
 -2.99896970e-02  4.30657007e-02  1.74795203e-02  4.66310745e-03
 -4.96265516e-02  3.37850899e-02 -1.19091123e-02 -4.20881845e-02
 -1.89706348e-02  1.47169093e-02  3.76028828e-02  2.24591364e-04
 -2.58175414e-02  6.91769179e-03 -3.14453500e-04 -4.19536000e-03
 -4.80591767e-02 -4.49646264e-02 -1.35111809e-02 -2.72767544e-02
  5.79925552e-02 -7.23791495e-02 -2.77479216e-02 -6.75973371e-02
  2.16248948e-02  2.70983856e-02 -1.07091516e-02  5.99808525e-03
 -1.27345724e-02 -2.93402690e-02 -1.71558335e-02  6.17546067e-02
  1.16315577e-02 -3.19390148e-02 -2.54276525e-02  6.39460981e-02
 -3.43498997e-02  3.26587670e-02  1.67430621e-02 -4.12256233e-02
 -7.22412989e-02 -8.14223848e-03  2.69799680e-02 -6.35801395e-03
 -8.51402525e-03  6.61275163e-02 -2.41627619e-02  3.60567607e-02
  3.95617522e-02 -2.18443386e-02  2.72718389e-02 -4.93351254e-04
 -2.42884122e-02 -4.32016095e-03  4.84241918e-02  3.94123755e-02
 -5.22931106e-02  5.12549654e-02  4.11470002e-03  9.80501994e-03
 -6.04095757e-02  4.69759339e-03 -9.78468172e-03  3.03116236e-02
  6.46690186e-03 -2.62607783e-02 -7.31626078e-02  1.58457004e-03
  4.07616049e-02  8.25080350e-02 -1.06254593e-02 -8.35816965e-33
 -5.20046614e-02  5.32969199e-02 -2.10563000e-02  4.37249318e-02
  5.90551049e-02  4.20892127e-02 -4.16871086e-02 -2.71576922e-02
  7.92105272e-02  5.55463741e-03 -8.19924697e-02  1.01880039e-04
  2.40836907e-02  7.42314234e-02 -5.10605462e-02 -2.65687108e-02
 -4.01226394e-02 -3.30478582e-03 -3.47625688e-02  9.19433534e-02
 -3.52368951e-02  8.31289813e-02 -3.92278917e-02 -3.16884853e-02
  8.09630156e-02  1.90266538e-02 -4.78406288e-02  3.83322723e-02
  1.38558596e-01  1.36947604e-02 -4.42533614e-03  3.06550805e-02
  5.54328933e-02 -4.59604226e-02 -1.14248425e-03 -2.96784323e-02
  7.68848360e-02 -4.25966270e-02  3.42863016e-02  9.88604724e-02
  1.16290711e-01 -1.16757979e-03  1.49170859e-02 -8.70612040e-02
 -1.16028264e-02 -3.74409780e-02 -7.42082074e-02 -2.20705140e-02
  3.71128805e-02 -1.11622393e-01  1.06717842e-02  2.67838426e-02
 -1.40825482e-02  7.54162818e-02 -4.49000336e-02 -5.30560361e-03
  3.74290161e-02 -2.97792088e-02  7.31431171e-02  4.09737006e-02
 -6.75147325e-02 -1.53050292e-02  9.37014073e-02  5.10480888e-02
  4.02899608e-02  4.90499623e-02 -1.35282129e-02  3.07354629e-02
 -5.39865009e-02 -4.83141690e-02 -3.31795984e-03  6.28246889e-02
 -1.52539471e-02  3.85321230e-02 -5.87386824e-03 -3.19850594e-02
 -9.10797268e-02 -3.72039750e-02 -4.85512763e-02 -5.93215302e-02
  8.69365409e-02 -5.07922880e-02 -1.48463883e-02  6.65292293e-02
 -3.39675806e-02  6.10512774e-03  2.61205491e-02  8.13917257e-03
 -6.67863572e-03 -8.75087678e-02 -7.53346011e-02 -3.87467253e-06
 -7.56505281e-02  1.08261563e-01 -3.95801179e-02 -3.62805856e-08
 -1.73946694e-02  1.69551605e-03  1.24543987e-01  1.43773584e-02
  4.58023995e-02  3.77302840e-02 -7.17524514e-02  5.77740222e-02
 -4.30763960e-02  2.15005483e-02 -1.81214809e-02  1.01312734e-02
 -3.07420529e-02 -1.10372705e-02  7.14579772e-04  2.26143580e-02
 -3.10761500e-02 -1.64104346e-02  3.43446583e-02 -4.84565012e-02
  6.94033206e-02 -2.45466772e-02 -1.82134770e-02 -1.99523810e-02
  1.06173836e-01 -8.57288092e-02 -8.31068680e-02  4.62025218e-02
 -3.73918526e-02  1.56311058e-02 -7.10263327e-02  1.33544337e-02
  4.57669385e-02 -4.35134545e-02  5.01818173e-02  9.99571681e-02
  7.66004622e-02 -1.02635287e-01 -3.84472869e-02  1.46765951e-02
 -1.03409905e-02  9.64220911e-02 -6.27404675e-02  2.19959803e-02
  7.11092427e-02 -2.09437367e-02  4.57860120e-02 -5.98879196e-02
 -4.96342890e-02  3.41212889e-03 -6.90674558e-02 -1.18811326e-02
  2.62125842e-02  4.16496769e-02  4.88307402e-02  3.43683697e-02
  5.31999953e-03 -7.86424577e-02  1.15065984e-02  4.04050760e-02
  9.99894217e-02  1.15193978e-01  3.57043780e-02  3.54630500e-02]",2,0
pytorch-pretrained-biggan,1,an op for op pytorch reimplementation of deepmind s biggan model with the pre trained weight from deepmind this repository contains an op for op pytorch reimplementation of deepmind s biggan that wa released with the paper large scale gan training for high fidelity natural image synthesis by andrew brocky jeff donahuey and karen simonyan this pytorch implementation of biggan is provided with the pretrained x x and x model by deepmind we also provide the script used to download and convert these model from the tensorflow hub model this reimplementation wa done from the raw computation graph of the tensorflow version and behave similarly to the tensorflow version variance of the output difference of the order of e this repo wa tested on python and pytorch pytorch pretrained biggan can be installed by pip a follows if you simply want to play with the gan this should be enough if you want to use the conversion script and the imagenet utility additional requirement are needed in particular tensorflow and nltk to install all the requirement please use the full requirement txt file this repository provide direct and simple access to the pretrained deep version of biggan for and pixel resolution a described in the associated publication here are some detail on the model please refer to appendix b of the paper for detail on the architecture all model comprise pre computed batch norm statistic for truncation value between and see appendix c in the paper for detail here is a quick start example using biggan with a pre trained model see the doc section below for detail on these class and method to load one of deepmind s pre trained model instantiate a biggan model with from pretrained a wherepre trained model name or path is either the shortcut name of a google ai s or openai s pre trained model selected in the list a path or url to a pretrained model archive containing if pre trained model name or path is a shortcut name the pre trained weight will be downloaded from aws s see the link here and stored in a cache folder to avoid future download the cache folder can be found at pytorch pretrained biggan cache dir can be an optional path to a specific directory to download and cache the pre trained model weight bigganconfig is a class to store and load biggan configuration it s defined in config py here are some detail on the attribute biggan is a pytorch model torch nn module of biggan defined in model py this model comprises the class embeddings a linear layer and the generator with a series of convolution and conditional batch norm the discriminator is currently not implemented since pre trained weight have not been released for it the input and output are identical to the tensorflow model input and output we detail them here biggan take a input biggan output an array of shape batch size resolution resolution where resolution is or depending of the model we provide a few utility method to use the model they are defined in utils py here are some detail on these method truncated noise sample batch size dim z truncation seed none create a truncated noise vector convert to image obj convert an output tensor from biggan in a list of image save a image obj file name output convert and save an output tensor from biggan in a list of saved image display in terminal obj convert and display an output tensor from biggan in the terminal this function use libsixel and will only work in a libsixel compatible terminal please refer to http github com saitoha libsixel for more detail one hot from int int or list batch size create a one hot vector from a class index or a list of class index one hot from name class name batch size create a one hot vector from the name of an imagenet class tennis ball daisy we use nltk s wordnet search to try to find the relevant synset of imagenet and take the first one if we can t find it direcly we look at the hyponym and hypernym of the class name script to download and convert the tensorflow model from tensorflow hub are provided in script the script can be used directly a,"[('input biggan output', 0.6407), ('model weight bigganconfig', 0.6302), ('biggan model', 0.6253), ('large scale gan training', 0.6135), ('biggan configuration', 0.6125), ('attribute biggan', 0.5486), ('pytorch implementation', 0.514), ('biggan', 0.5108), ('particular tensorflow', 0.4889), ('biggan cache dir', 0.4847)]","[-3.55854146e-02 -4.33633216e-02  1.76525041e-02 -1.70992420e-03
  7.55022913e-02  3.59306261e-02 -9.43222269e-02  4.53662127e-02
 -1.02099508e-01 -5.97041622e-02  2.34777573e-02  2.07660757e-02
 -7.51239881e-02 -6.39658794e-02 -2.35210098e-02 -6.38058595e-03
  1.79138314e-02  6.45129383e-02 -4.06333059e-02 -6.45447746e-02
 -1.19292699e-02  2.45047212e-02  2.14531291e-02  2.82522999e-02
 -2.61504650e-02  2.69259848e-02 -4.81436262e-03 -2.41161529e-02
  5.81185184e-02 -3.24300788e-02 -8.45652074e-03  3.27026024e-02
  7.55000934e-02 -4.10642941e-03 -7.36326650e-02  4.37410101e-02
 -1.33355008e-02 -2.43270956e-02  1.42685184e-02 -1.02535337e-01
 -8.57163593e-03 -8.33333358e-02  2.80208029e-02 -7.01904446e-02
  6.58290312e-02 -4.06821407e-02  5.15721217e-02 -4.45765704e-02
 -5.05777672e-02 -4.72617596e-02 -1.06044775e-02 -6.37292415e-02
 -7.62437731e-02  1.07713744e-01  9.23759565e-02 -6.24920279e-02
  1.06819365e-02 -3.27744596e-02 -1.80438964e-03  3.85728404e-02
 -4.87504750e-02  7.33596552e-03 -6.43117353e-02  6.05458133e-02
  2.00257618e-02  4.51089740e-02  1.90800577e-02  1.06338840e-02
  1.12426177e-01 -1.37529612e-01 -4.24594656e-02  2.04994511e-02
 -5.99959269e-02  4.32261154e-02  2.58465186e-02  9.21949185e-03
  2.92339958e-02  4.54775132e-02  6.77179769e-02 -1.19939506e-01
 -7.87195284e-03 -8.72201659e-03  1.16965011e-01  1.24880802e-02
 -3.16308364e-02  3.30578685e-02 -5.05043678e-02  7.41639137e-02
  3.69786136e-02 -3.39411981e-02  7.58150965e-02  7.65511859e-03
 -9.42453146e-02  7.49964565e-02  1.06105655e-02  3.57578918e-02
 -5.05072773e-02 -1.45111633e-02  3.13485111e-03 -8.29605677e-04
  2.04545837e-02 -2.75225714e-02  5.47661707e-02  9.50209517e-03
  3.77746671e-02 -2.58082012e-03  9.43528935e-02  4.10591215e-02
  7.85179809e-02 -2.70023849e-02  2.75132153e-02  2.99867596e-02
 -2.95202266e-02 -6.41921312e-02  5.48048913e-02 -4.25968133e-02
 -3.26596461e-02  7.72126690e-02 -8.05013478e-02  6.30246550e-02
 -1.41319379e-01  9.45447162e-02 -4.55255508e-02 -3.43656279e-02
 -8.14985111e-02 -3.32909115e-02 -8.48182663e-02  5.61356776e-33
  4.21150364e-02 -1.28036402e-02 -4.31651287e-02  1.21129109e-02
  3.85403037e-02 -1.10889357e-02  1.46037349e-02  9.48312227e-03
  3.68115753e-02  1.60819311e-02  4.89036785e-03  1.18843010e-02
 -1.02013335e-01  1.11297593e-01  6.37012571e-02 -5.48560508e-02
 -8.94827861e-03 -5.82337333e-03  5.10028675e-02 -2.74574216e-02
  6.32261112e-02  3.19222510e-02 -4.30604070e-02  6.71301484e-02
  6.99878410e-02  1.08142048e-01  4.79126386e-02  1.58522949e-02
 -5.23642786e-02  1.62362829e-02 -4.42251638e-02 -5.76052964e-02
  8.67703278e-03 -4.50349227e-02 -3.62123363e-02 -6.12768121e-02
 -2.73498818e-02 -3.38556692e-02 -4.09264006e-02 -6.75745085e-02
 -4.81855907e-02  3.51431146e-02 -1.55837208e-01 -8.53565522e-03
 -5.70935272e-02  1.42151611e-02 -8.24204274e-03  2.61220336e-02
  4.08766419e-02  2.40343772e-02 -2.08485164e-02 -4.03440632e-02
  2.31002290e-02  1.84910093e-02 -1.64432917e-02 -2.08393857e-03
  2.24854853e-02 -8.11326841e-04  1.01189770e-01  4.53190990e-02
 -7.40493461e-02  5.31760007e-02  4.88968566e-02  1.37158968e-02
 -5.73227666e-02 -6.59730472e-03 -5.46451584e-02 -3.81739624e-02
 -7.36844679e-03  5.43007292e-02 -2.07631495e-02  1.83206126e-02
 -1.53555162e-02 -6.54007941e-02 -1.02185477e-02 -6.79148361e-02
 -7.22611323e-02 -4.08871099e-02 -4.80367914e-02  8.23160857e-02
 -6.40868098e-02  6.40295669e-02  5.01269028e-02  4.06281799e-02
 -4.15959023e-02 -3.52974683e-02 -1.41394725e-02 -1.85698289e-02
  7.40699321e-02 -1.99061688e-02 -5.36039583e-02  1.30983274e-02
  6.73884302e-02 -5.82600534e-02 -3.36880274e-02 -4.26977935e-33
  1.86518766e-02  7.42622912e-02 -1.67929754e-02  2.39517801e-02
  6.00649975e-04  1.51966652e-02 -6.84798285e-02 -2.75673121e-02
  3.51891443e-02  1.17503060e-02 -1.40671944e-02  1.94706041e-02
  1.97288282e-02 -7.74989799e-02  6.16554776e-03 -8.74902681e-02
 -1.98132675e-02 -4.00668383e-02 -3.79048940e-03  6.36824779e-03
  6.15922082e-03  5.31757139e-02 -7.73487687e-02  6.05787858e-02
  2.41335179e-03 -2.58308779e-02  1.33301709e-02  1.93757601e-02
  5.56636751e-02 -1.70958266e-02  8.60706940e-02 -3.20886858e-02
  4.03939001e-02  7.01870173e-02 -4.13263030e-02 -3.40745300e-02
  1.43884033e-01  4.44464684e-02 -2.66535245e-02  5.93578501e-04
  1.04457520e-01 -9.98803531e-04 -4.01456766e-02  2.98033487e-02
 -6.57187030e-02  5.65242535e-03 -5.23931384e-02  2.42985412e-02
 -2.95653641e-02 -3.36853764e-03 -2.10967697e-02 -4.12876345e-02
 -4.67792489e-02 -2.96715889e-02 -2.68140119e-02 -2.04769392e-02
  9.38774347e-02  2.22123303e-02  2.47757770e-02 -4.07249480e-02
 -5.18492460e-02 -1.21752553e-01 -1.59121864e-02 -5.00796642e-03
 -4.27284427e-02 -4.98797931e-03 -3.49805355e-02 -5.42115420e-02
  2.82677561e-02 -1.55281080e-02 -5.45411669e-02 -9.31846648e-02
  1.97364595e-02  9.64287668e-02 -2.20648367e-02  3.44475321e-02
  7.25222975e-02 -1.39582315e-02  6.57653883e-02 -1.73674859e-02
  5.57195880e-02 -5.07235294e-03  8.06099027e-02  4.74065132e-02
  7.04975277e-02  2.20922735e-02  2.55354010e-02  1.97447836e-02
  2.64215108e-04 -1.96189545e-02 -1.82115659e-02  1.17502108e-01
  5.78257665e-02  6.86188638e-02 -1.62741095e-02 -2.69436473e-08
 -5.86016364e-02  7.01985508e-03  5.89530915e-02  7.96905160e-02
  8.58324673e-03  2.15083268e-02  6.00429215e-02  1.54060736e-01
  5.71367927e-02  1.14661403e-01 -2.68372148e-02 -5.71056595e-03
 -9.13536549e-02 -5.14512360e-02  6.37637377e-02 -1.83019286e-03
 -8.26771464e-03 -2.60879975e-02 -3.87194119e-02 -5.78041822e-02
  3.65535286e-03  2.03422233e-02 -4.22969498e-02 -1.59026161e-02
  6.91012889e-02 -6.54653236e-02  2.60704197e-02  6.06556125e-02
 -7.12230653e-02  3.01797558e-02 -5.02326712e-03  1.35565223e-02
 -9.14506707e-03 -7.83373863e-02  1.02563925e-01  7.55929798e-02
  1.09650893e-02 -1.61321424e-02  7.85633475e-02  5.24803847e-02
  6.68844488e-03 -1.22925127e-03  6.09397255e-02 -3.39429639e-02
 -1.45313824e-02 -2.67135017e-02 -5.23551088e-03 -3.43455300e-02
 -3.66670793e-05  6.61315769e-02 -4.82991524e-02  3.40623595e-02
 -1.16180861e-02  2.99331974e-02 -2.71520559e-02  5.26611172e-02
  1.01102944e-02 -2.86744311e-02  4.90941145e-02 -5.45888804e-02
  5.15063144e-02  4.76826429e-02 -1.14039928e-02 -3.89762936e-05]",2,2
mlserver,1,an open source inference server for your machine learning model mlserver aim to provide an easy way to start serving your machine learning model through a rest and grpc interface fully compliant with kfserving s v dataplane spec watch a quick video introducing the project here you can read more about the goal of this project on the inital design document you can install the mlserver package running note that to use any of the optional inference runtimes you ll need to install the relevant package for example to serve a scikit learn model you would need to install the mlserver sklearn package for further information on how to use mlserver you can check any of the available example inference runtimes allow you to define how your model should be used within mlserver you can think of them a the backend glue between mlserver and your machine learning framework of choice you can read more about inference runtimes in their documentation page out of the box mlserver come with a set of pre packaged runtimes which let you interact with a subset of common framework this allows you to start serving model saved in these framework straight away however it s also possible to write custom runtimes out of the box mlserver provides support for to see mlserver in action check out our full list of example you can find below a few selected example showcasing how you can leverage mlserver to start serving your machine learning model both the main mlserver package and the inference runtimes package try to follow the same versioning schema to bump the version across all of them you can use the hack update version sh script for example,"[('open source inference server', 0.5692), ('available example inference runtimes', 0.5234), ('mlserver sklearn package', 0.5096), ('machine learning framework', 0.467), ('inference runtimes', 0.4474), ('mlserver package', 0.3873), ('grpc interface', 0.3838), ('machine learning model', 0.3728), ('main mlserver package', 0.3706), ('custom runtimes', 0.3462)]","[-3.69807668e-02 -1.45176202e-01 -8.89197364e-03 -2.20467821e-02
  1.07148334e-01 -3.72319706e-02 -9.38678458e-02  5.18631339e-02
  6.88230945e-03 -2.52206158e-02 -5.16249575e-02  1.65338516e-02
 -7.95990750e-02 -5.09884162e-03  4.46220152e-02 -1.67821962e-02
  1.43840013e-03  3.06923836e-02  1.34653570e-02 -8.53865817e-02
 -1.77651439e-02  7.66979251e-03 -4.18368317e-02 -1.63671970e-02
 -1.10486634e-02 -6.39916062e-02  4.12404910e-02  3.16899307e-02
 -2.86107883e-02 -5.75862303e-02  1.19439866e-02 -6.46917149e-02
  5.49409091e-02 -3.32555287e-02  1.62125099e-02  2.84347422e-02
  2.30713543e-02 -1.92107726e-02 -1.17099797e-02  1.75791420e-02
 -5.57054579e-03 -4.63754535e-02  4.19970863e-02 -1.56121617e-02
  6.12126663e-02  1.30482465e-02  5.65871447e-02 -6.58398345e-02
 -4.09884378e-02 -2.15974171e-02 -7.15627670e-02 -1.12411059e-01
 -1.39950030e-02  2.72786245e-02 -1.30698495e-02 -6.15300834e-02
 -2.83767357e-02  3.57626490e-02  2.46928143e-03 -3.63208428e-02
 -3.38950232e-02 -6.13232069e-02 -9.20555219e-02  2.10804529e-02
 -6.09781295e-02  2.13849079e-02  2.62945499e-02  6.62921071e-02
  1.14475206e-01 -1.17318787e-01 -7.51456916e-02  7.28960731e-05
 -4.53177700e-03  6.14269301e-02 -5.25564328e-02  1.24314521e-02
  1.09603733e-01 -2.50250902e-02  1.42866760e-01 -6.92313612e-02
 -4.31837551e-02  9.52167623e-03 -7.61820376e-02  2.88785947e-03
 -9.51668248e-03  1.82793457e-02 -8.49795118e-02  9.09061506e-02
 -1.94488745e-02 -1.88765470e-02  5.88630922e-02 -4.47817892e-02
 -6.11365847e-02 -2.02502850e-02 -1.43341022e-02  2.37653279e-04
  1.80081674e-03 -3.89969684e-02 -6.76349364e-03  1.04701258e-01
 -1.00363821e-01 -1.06987334e-03  1.08164728e-01 -5.89315919e-03
 -2.02097986e-02  1.29984040e-02  5.43625578e-02  3.98843549e-02
  8.24557319e-02 -4.29050885e-02  1.69943906e-02 -5.55638922e-03
 -1.75313484e-02 -8.14861134e-02 -4.94633848e-03 -6.77085817e-02
 -4.81661595e-02 -5.63121065e-02 -4.64195013e-02 -1.84583161e-02
 -1.06723890e-01  2.21413858e-02 -3.16962823e-02  2.32704841e-02
  6.03824221e-02 -9.79091506e-03 -3.22453156e-02  5.02064621e-33
  1.75027605e-02 -2.09497269e-02  1.89278256e-02 -4.13677134e-02
  4.27797884e-02 -2.88922135e-02  1.48048401e-02  1.57347284e-02
 -4.99314293e-02 -2.69970186e-02  6.90627769e-02  4.69364710e-02
 -4.38207723e-02  7.51077905e-02  6.59514368e-02  1.59493983e-02
 -3.02550644e-02  5.70022911e-02  5.56115657e-02 -4.54905741e-02
  3.53519730e-02 -2.01751781e-03  1.08278273e-02  1.22204192e-01
  2.43410934e-02 -3.40982787e-02  2.94899456e-02 -3.05456109e-02
 -4.99930978e-03  7.38669420e-04  6.56043142e-02  4.34512496e-02
 -4.04753685e-02  9.81998071e-02  3.41813304e-02 -1.46698905e-02
 -4.84016761e-02 -6.34240955e-02  1.81015730e-02  1.01796435e-02
 -4.83409576e-02  3.58682126e-02  7.43141174e-02 -6.78005163e-03
 -4.88045812e-02 -1.19653136e-01 -5.72875813e-02 -1.46162380e-02
  4.39622551e-02 -3.43018733e-02  4.04122137e-02 -3.67278643e-02
  5.25317006e-02 -4.51789564e-03  3.67758684e-02  9.59460661e-02
  1.23053677e-02  1.88920964e-02  8.01042765e-02  9.79312286e-02
 -5.09799831e-02  1.23123992e-02 -6.57181861e-03 -1.34603888e-01
  3.35430391e-02 -4.93690223e-02 -6.47403523e-02 -6.83124736e-02
  1.42250955e-02  5.87095805e-02 -1.77040026e-02 -1.01867858e-02
  2.11110581e-02 -1.79875828e-02  4.36925888e-02  1.27941398e-02
 -4.73305434e-02 -8.83565936e-03 -5.01963217e-03  3.02174371e-02
 -6.83959201e-02  4.57199886e-02 -8.93849880e-03  6.13152422e-02
  4.06782068e-02  1.63751654e-02 -5.93012124e-02  1.67439058e-02
  1.78774074e-02 -4.51218300e-02 -8.74336958e-02  3.03967949e-02
 -5.40372264e-03  2.61169989e-02  2.39889696e-02 -4.07316139e-33
 -5.67306317e-02  8.04768223e-03 -6.30542636e-02  1.49772376e-01
  1.96981467e-02  2.92054079e-02 -6.27814755e-02 -1.41235394e-02
 -1.82338431e-02 -3.36627960e-02  7.74336327e-03 -4.24981490e-02
  1.09588131e-01  8.09199512e-02  3.38097871e-03 -4.06671688e-02
 -7.93628320e-02 -6.10322319e-02  3.67290084e-03  4.22326513e-02
 -6.32613990e-03  7.41740465e-02  3.26944143e-02 -1.11633018e-01
  8.00848603e-02 -4.85844985e-02 -2.52180621e-02  8.63151923e-02
  8.91089905e-03 -2.79898643e-02 -2.55869981e-02  2.23040320e-02
 -3.00421566e-03 -1.41869852e-04  2.21237279e-02  8.62103421e-03
  8.34058747e-02  2.83588618e-02 -1.88258234e-02 -2.31840648e-02
  1.04788162e-01 -7.32579082e-02 -1.51904449e-02 -5.56721389e-02
 -1.93823762e-02  3.72155965e-03 -6.45883083e-02  2.54532751e-02
  3.97760868e-02 -6.30369186e-02 -7.67802969e-02 -8.19184724e-03
 -1.89275555e-02 -1.44237345e-02  3.25295539e-03 -4.34099101e-02
  2.29113568e-02  2.89822407e-02  3.67502458e-02 -2.43241191e-02
 -4.51740101e-02  3.08800358e-02 -1.69934109e-02  6.73772767e-02
  4.63439561e-02  5.30610010e-02 -5.05504273e-02  9.67998523e-03
 -9.47905481e-02 -2.47733202e-04 -1.49346646e-02 -6.20690268e-03
 -1.40856849e-02  1.85209571e-03 -2.22547688e-02  4.02041115e-02
 -3.86358574e-02  4.80642132e-02  5.17700193e-03  5.44265918e-02
  6.93074912e-02 -2.76192892e-02  6.05414137e-02  6.24373890e-02
  8.91596153e-02  2.07769182e-02  3.85857709e-02  1.75146526e-03
  2.41554864e-02  1.00347707e-02 -5.24904355e-02  6.10251203e-02
  6.50931522e-02  1.25282943e-01 -3.89463976e-02 -3.05466727e-08
 -4.54132445e-02  3.59344743e-02  1.20130889e-01  1.74939409e-02
  7.08457977e-02  6.59100115e-02  1.94251817e-02  5.25094084e-02
 -2.58982629e-02  8.05012658e-02  1.87521037e-02 -4.94557098e-02
 -1.96245257e-02 -7.00158030e-02  3.90480421e-02  8.73787031e-02
 -3.31142098e-02  3.22487019e-02 -3.28687504e-02 -3.12736891e-02
  4.94383089e-03  4.16494980e-02  1.94805930e-03  2.37063207e-02
  3.41915190e-02 -7.34152049e-02  1.91295743e-02 -4.05039871e-03
 -9.79681266e-04 -4.91357073e-02 -5.39759919e-03  5.37980199e-02
  1.13331057e-01 -8.32528323e-02  9.36518088e-02  7.15341121e-02
 -5.45282997e-02  2.43466143e-02  1.96621157e-02  1.93192717e-02
 -6.09068535e-02  1.67198032e-02 -8.84460062e-02 -3.24943550e-02
  7.48374686e-02  2.53964718e-02  3.33973393e-02 -5.26966006e-02
 -5.62021323e-02  4.54100408e-02  4.90639126e-04  5.14965446e-04
 -1.90809388e-02  2.92723030e-02  2.66134348e-02  1.09735899e-01
  7.74520785e-02 -1.00391671e-01 -2.42712852e-02  6.87122643e-02
 -4.01039645e-02  9.69679374e-03 -1.47508159e-02  1.37321036e-02]",2,0
ptflops,1,this script is designed to compute the theoretical amount of multiply add operation in convolutional neural network it can also compute the number of parameter and print per layer computational cost of a given network supported layer experimental support requirement pytorch torchvision thanks to warmspringwinds for the initial version of script from pypi from this repository,"[('experimental support requirement pytorch torchvision thanks', 0.445), ('convolutional neural network', 0.4121), ('computational cost', 0.3902), ('layer', 0.3193), ('multiply', 0.2836), ('theoretical amount', 0.2823), ('network', 0.2489), ('operation', 0.2026), ('script', 0.2015), ('parameter', 0.1837)]","[-1.02151267e-01 -9.09815356e-02 -4.26314212e-03 -5.30650020e-02
 -1.10218227e-02 -3.13629173e-02 -5.01353852e-02  5.95823787e-02
 -6.21154979e-02 -4.40713838e-02 -5.15528917e-02 -6.99065179e-02
 -1.00484341e-01  4.65025045e-02 -5.87709844e-02  4.76352982e-02
  2.63016038e-02  1.17926635e-02 -8.19724053e-02 -1.18552439e-01
  7.12676421e-02 -1.28322393e-01  5.35673834e-02 -7.90471509e-02
 -1.96201331e-03 -3.37876077e-03  1.33853266e-02 -7.42988521e-03
  4.49261479e-02  4.50395234e-03 -1.26252556e-02  5.43148082e-04
 -7.82965496e-03  1.50105054e-03  8.72209854e-03 -3.89957912e-02
 -2.82923896e-02 -1.42713077e-02 -6.61917105e-02  3.73604484e-02
 -3.78440171e-02 -5.85537078e-03 -1.48128103e-02 -2.26680581e-02
  4.52459678e-02  2.14905366e-02  4.57118973e-02 -2.43076999e-02
 -3.64030413e-02 -3.11673936e-02 -4.26802896e-02 -4.04950008e-02
 -2.04005502e-02  3.65701430e-02  1.26810819e-02 -2.41932999e-02
 -5.42883994e-03 -3.63388248e-02  1.07230097e-02 -6.97701275e-02
  4.68277372e-02 -1.86840985e-02 -6.91151842e-02 -3.68238203e-02
  1.99591648e-03  2.62975711e-02 -2.82363687e-02 -3.17205563e-02
  1.14725545e-01 -8.75473246e-02 -4.97428291e-02  3.41075733e-02
 -3.82285900e-02  6.24685958e-02 -1.74008440e-02  1.02145039e-02
  1.42858431e-01  8.64676666e-03  1.94314346e-02 -1.00771040e-01
  4.73052450e-02 -1.91741055e-04 -1.43622588e-02  4.94289547e-02
  7.06609711e-02  2.42654216e-02 -5.48830405e-02  1.32748961e-01
 -1.85565688e-02 -1.95729714e-02 -6.23275898e-03 -1.68550871e-02
 -4.55457345e-02 -3.78054231e-02 -1.59659721e-02 -2.07643281e-03
  6.70502260e-02 -1.16585620e-01 -6.62698448e-02  1.70726795e-02
 -1.56770516e-02 -1.06397837e-01  5.40360361e-02  2.13603359e-02
 -4.03445549e-02  5.31047098e-02  3.07965297e-02  8.42906460e-02
  8.21510106e-02 -1.95419341e-02  2.82132924e-02 -5.14147058e-03
 -3.23510468e-02 -1.02103822e-01  1.04198202e-01 -3.14728469e-02
 -6.40877187e-02  3.05135883e-02  8.88118967e-02  5.41608073e-02
 -3.68192419e-02  4.09052372e-02 -5.78695796e-02  4.11656275e-02
 -1.01409340e-02 -3.26664634e-02 -6.15176223e-02  2.25495320e-33
 -5.58261871e-02  6.74153119e-02 -8.38433430e-02 -6.17403584e-03
  1.15478849e-02 -1.45803243e-02  5.59324063e-02  4.04960252e-02
 -1.10665383e-02 -5.77715412e-02 -1.61412880e-02 -2.89981961e-02
 -5.96590601e-02  8.35890844e-02 -7.20608002e-03 -4.56224047e-02
  6.99104443e-02  6.43550232e-02  3.09484955e-02  3.82087119e-02
 -4.64434959e-02 -5.71283363e-02  2.38704514e-02  4.19471003e-02
 -5.74685633e-02  4.64034714e-02 -2.11856514e-02  2.92262714e-03
  3.26259732e-02 -1.02693727e-02  4.95556109e-02  6.93729743e-02
 -1.64101785e-03 -2.89990138e-02 -5.12355343e-02 -2.93059554e-02
 -4.99058999e-02 -6.75692931e-02  4.30347957e-02 -7.61007331e-03
 -2.34571639e-02  1.22230589e-01  2.23781057e-02 -1.65244304e-02
 -2.96465252e-02  2.51572710e-02 -2.52136867e-02  1.08309910e-01
  8.15578643e-03 -3.94880353e-03 -5.45136370e-02  2.32202224e-02
 -4.85726222e-02  7.55964499e-03  9.39599350e-02  1.05403503e-02
  4.89866808e-02  3.10478657e-02  7.63328820e-02  4.19352688e-02
  9.64201801e-03  5.14405183e-02 -1.00883152e-02  3.78722437e-02
 -8.76305066e-03 -9.49538872e-03 -2.51613045e-03 -2.63335323e-03
 -5.78268394e-02  4.02736999e-02 -1.11927450e-01  3.17547508e-02
  2.98041850e-04 -5.64077124e-02  1.83953289e-02  1.51789747e-02
  2.88243555e-02 -2.80507207e-02 -2.49470118e-02  1.05543360e-01
 -1.30106315e-01  4.77498993e-02  4.43547033e-02 -3.51223014e-02
 -3.35879140e-02  3.89236510e-02 -1.13419145e-02  1.72571037e-02
 -3.48081626e-02  3.31676030e-03 -2.59266105e-02 -6.13438003e-02
 -4.46593948e-03  1.70908496e-03 -1.17130168e-02 -6.73772974e-34
 -4.93392162e-02  1.10002637e-01 -4.66870405e-02  9.75604132e-02
  6.69359863e-02  2.00845469e-02  5.36839059e-03 -9.22069252e-02
  7.11608827e-02  3.29722539e-02  2.92045698e-02 -7.18661491e-03
 -3.79113927e-02 -6.23162277e-03 -2.37781694e-03  1.53693851e-04
 -6.38000146e-02 -3.46198753e-02  4.16795798e-02 -3.85278948e-02
 -4.12878916e-02  1.08695462e-01 -8.83421749e-02 -1.09674567e-02
 -8.53798985e-02 -1.25818755e-02 -8.28357320e-03 -3.59977931e-02
 -4.21370305e-02 -2.84339562e-02  1.36155086e-02 -4.06349339e-02
 -3.61135080e-02  3.36807407e-02 -1.79569256e-02  1.17982486e-02
  1.17294610e-01 -1.99202858e-02 -2.16531157e-02  1.34207094e-02
  1.17342457e-01  6.38678819e-02 -3.43420058e-02  3.10058370e-02
 -8.83070752e-02 -3.35275638e-03 -2.02771816e-02  1.79499201e-02
  1.97300799e-02  4.80708368e-02  1.86824445e-02 -1.72794890e-02
 -5.56503348e-02 -6.73619956e-02  2.35453658e-02 -6.16112491e-03
  2.50371229e-02  6.06993809e-02  3.39859650e-02  1.26122823e-02
 -5.43404073e-02 -8.95223990e-02  3.59596722e-02  1.16056791e-02
 -1.21063702e-02  4.76998165e-02 -1.40431691e-02  1.13111205e-01
  3.03632952e-02 -6.24030326e-06  5.58368675e-02  8.40975866e-02
  1.79472297e-01  8.14184844e-02 -4.39678458e-03 -7.76906963e-03
  1.98936816e-02  3.96610387e-02  6.55442923e-02  2.28111856e-02
 -3.95526513e-02 -2.40758378e-02  1.88847817e-02  4.63120453e-02
  8.08229819e-02  7.87007660e-02  4.48744670e-02 -2.68366677e-03
  3.59405689e-02 -9.97406617e-03  2.67048534e-02  4.44579832e-02
  1.07535765e-01  2.59338133e-02  4.87891138e-02 -2.90876727e-08
  1.98585782e-02  6.42056838e-02 -1.74560938e-02 -2.97582075e-02
 -1.58702265e-02 -4.94453982e-02  8.49651024e-02  3.45049724e-02
 -1.94499269e-02 -4.19042073e-03  2.71966103e-02 -4.25974242e-02
  1.09213963e-02  6.72446638e-02  1.67690460e-02  4.86732237e-02
  3.52545530e-02  4.42669839e-02  1.02527216e-02 -3.30979489e-02
  9.01034661e-03  2.03217827e-02  1.27060069e-02  3.32558118e-02
 -1.37195494e-02 -4.45063487e-02 -2.04686355e-02  3.75227071e-02
 -3.48981731e-02  3.35993944e-03  1.81620643e-02 -1.98473409e-02
  1.00693040e-01 -6.18455000e-02  1.39387369e-01  4.73584980e-02
 -1.01136938e-01  1.87131688e-02 -1.23970117e-02  2.92945858e-02
 -5.49593680e-02  1.23561744e-03  1.98758971e-02 -1.01649873e-02
  7.27898628e-02  1.34689519e-02 -7.52783492e-02 -1.34594604e-01
 -6.90800622e-02  2.24918872e-02  2.89195254e-02  4.92269844e-02
 -1.37031851e-02  1.53297894e-02  5.66623360e-02  3.39752100e-02
  2.78069582e-02 -3.37777697e-02 -2.96842959e-02  1.64480563e-02
  3.57543565e-02  7.23078102e-02 -6.23818152e-02 -3.58173326e-02]",2,2
brc-pytorch,1,pytorch implementation of bistable recurrent cell with baseline comparison this repository contains the pytorch implementation of the paper a bio inspired bistable recurrent cell allows for long lasting memory the original tensorflow implementation by the author nicolas vecoven can be found here another important feature of this repository is the implementation of a base class that return a recurrent neural network for a given recurrent cell based on the hyperparameters provided the network can have multiple layer be bidirectional and the input can either have batch first or not the output from the network mimic that returned by gru lstm network developed by pytorch with an additional option of returning only the hidden state from the last layer and last time step brc pytorch is pypi installable create a venv activate it install dependency and package in editable mode first the implementation of both the brc and nbrc are validated on the copy first input task benchmark from the original paper moreover it is well known that standard rnns have difficulty in discrete counting especially for longer sequence see neurips paper to this end we here identify binary addition a another task for which the nbrc is superior to lstm gru which begs implication for a set of task involving more explicit memorization for both task the performance of brc and nbrc are compared with that of the lstm and gru cell the goal of this task is to correctly predict the number at the start of a sequence of a certain length this task is reproduced from the paper layer model with unit each trained on datasets with increasing sequence length the plot is obtained by taking a moving average of the training loss per gradient iteration with window size for length and and window size for length the result from copy first input task show trend similar to that in the paper thus confirming their finding it should however be noted that the absolute loss are higher than reported in the paper this is mostly due to the training and testing size being much smaller and no hyperparameter tuning being done to reproduce this task do or if training take a very long time run the script cell wise i e specify cell name a an additional argument and run multiple job in parallell one for each cell additional testing on binary addition wa done to test the capability of these cell the goal of this task is to correctly predict the sum of two binary number in integer form both single layer and layer model with constant hidden unit are evaluated based on the accuracy of their prediction the result from this task prove the usefulness of both the nbrc and brc layer which consistently perform better than both the lstm and gru moreover it is interesting to note the potential of nbrc in the binary addition task which is consistent around near perfect accuracy upto sequence length the plot are obtained by averaging the result over run of the experiment and highlighting the standard error of the average while the copy first input task highlight the performance superiority of these cell over the conventional lstm and gru the binary addition task which requires counting is witness to their usefulness beyond just long lasting memory to reproduce this task do for the layer implementation simply add another to the hidden size variable in the training file and repeat the step,"[('recurrent neural network', 0.5277), ('conventional lstm', 0.5145), ('standard rnns', 0.5098), ('pytorch implementation', 0.5081), ('gru lstm network', 0.4939), ('last time step brc pytorch', 0.4869), ('lstm', 0.4796), ('bistable recurrent cell', 0.4674), ('first input task benchmark', 0.4349), ('original tensorflow implementation', 0.4201)]","[-9.10501480e-02 -1.20436132e-01  2.02004258e-02 -5.87435812e-02
  2.65365350e-04  5.33408225e-02 -5.28159328e-02  4.24352884e-02
 -7.39943162e-02 -6.92706257e-02 -5.56253754e-02  4.26666401e-02
 -5.64181507e-02 -4.34156088e-03 -1.02175966e-01 -4.73503256e-03
 -3.64763173e-03  3.51111256e-02  2.79289074e-02 -1.19176485e-01
 -3.92979831e-02  3.46128293e-03 -4.84717218e-03  4.07801531e-02
  5.17051294e-02 -8.30067694e-03  2.97896620e-02  3.48234065e-02
  5.79271950e-02 -2.85737161e-02  6.63326681e-02  1.20231835e-02
  9.71094593e-02  5.92294373e-02 -1.09615996e-01 -1.20516494e-02
 -7.97287226e-02 -8.43401477e-02 -4.39763349e-03  5.05524501e-03
 -2.31340751e-02 -1.14548439e-02 -1.77209675e-02 -5.09822704e-02
  1.33684903e-01  7.01762130e-03  6.19627722e-02 -5.54796755e-02
 -7.52139166e-02  2.38309819e-02 -1.30786467e-02  3.00002936e-02
 -1.09111974e-02  9.80157480e-02 -7.18351081e-03 -3.44561599e-02
  7.39286048e-03  2.73591764e-02 -1.57131290e-03 -7.01054633e-02
 -2.68507879e-02 -3.14712748e-02 -5.43481074e-02  5.03586046e-03
  4.21261037e-04  3.09455376e-02 -1.91077695e-03  3.82555053e-02
  1.13660254e-01 -5.74315898e-02 -2.40927357e-02  2.83141844e-02
 -4.39582020e-02  3.17154266e-02 -4.91820537e-02 -2.16230024e-02
  8.45661536e-02  2.47995649e-02  7.46131269e-03 -1.57540515e-02
 -2.15813518e-02 -8.43877625e-03  7.62342475e-03 -4.95474562e-02
  3.89669091e-02 -1.33998021e-02 -7.34526962e-02  8.05717409e-02
 -6.30289037e-03 -7.71110207e-02  1.08403638e-01 -6.40078261e-02
  3.37901153e-02  1.13954600e-02  1.91689413e-02  6.69889823e-02
 -4.83840331e-03 -5.37737720e-02  8.84001609e-03  1.87000558e-02
  3.28580965e-03  7.74004543e-03  3.69316004e-02  5.49665503e-02
  4.00484167e-02  4.44813110e-02  8.82638991e-02 -5.52321831e-03
  8.19989666e-02 -4.62489128e-02 -1.33620892e-02 -3.29947658e-03
 -3.53319235e-02 -2.84554698e-02  8.49087238e-02 -5.78404628e-02
 -3.63709219e-02 -2.94799078e-02  6.62240461e-02  1.24786347e-01
 -8.06848183e-02  3.40223461e-02 -8.42823982e-02  1.42431892e-02
 -7.32103437e-02  3.16767767e-02 -5.72530665e-02  8.09247119e-33
 -1.69881079e-02 -1.51015110e-02  3.76122072e-03 -3.71858738e-02
  5.59142046e-02 -3.60698029e-02  4.71912324e-02  5.50485775e-02
  3.96107286e-02 -4.86113653e-02 -3.92826274e-02  6.32026717e-02
 -3.31119262e-02  6.42863736e-02 -5.82439527e-02 -1.95267517e-02
 -5.36075160e-02  1.28906751e-02  1.95015874e-02  3.03176697e-02
  2.20938586e-02  3.93219851e-02  5.25937043e-02  2.26864498e-02
  3.52087058e-02  6.01318143e-02  3.96937206e-02  1.56455208e-02
  1.07698166e-03 -3.02999909e-03 -3.74075919e-02 -1.68381520e-02
 -2.53705513e-02 -2.61752829e-02  1.08241653e-02 -3.62336487e-02
 -9.96614108e-04 -3.63940038e-02  7.76730012e-03 -7.66300038e-02
 -4.32208329e-02  3.10034845e-02 -5.58299869e-02  1.57429464e-02
 -3.28047909e-02 -6.57538921e-02 -3.12049389e-02  7.87698627e-02
  6.43612593e-02 -2.32572462e-02 -4.17044014e-02  1.49659747e-02
 -1.09723344e-01 -3.73660512e-02  4.42400239e-02 -2.09952109e-02
  2.41446812e-02  2.89923344e-02  3.52578536e-02  9.66028422e-02
  2.55346950e-02  6.95945397e-02  6.43628165e-02  2.03838646e-02
 -7.71523546e-03  4.99852374e-02 -8.40976164e-02  2.29516607e-02
  6.76865354e-02 -9.02990904e-03 -6.93246424e-02  2.78819725e-02
  2.17233207e-02  1.36061246e-02  4.97390665e-02 -2.78040650e-03
  3.85068245e-02 -1.12749919e-01 -2.21689288e-02  4.86893915e-02
 -5.81703298e-02  3.47009003e-02 -1.23051731e-02  1.94090810e-02
 -7.20409080e-02 -5.68150841e-02  2.22336203e-02 -3.00792325e-02
  2.36135442e-02 -1.05244959e-04 -7.57418573e-02 -2.09482014e-02
  8.63409713e-02  7.06151277e-02 -3.87554243e-02 -6.04847900e-33
  1.06702996e-02  8.74643549e-02 -8.75202417e-02  7.42016360e-02
  5.85552007e-02 -1.05223190e-02 -8.19990560e-02 -3.62565257e-02
  5.04684411e-02  1.26596212e-01  4.82761189e-02 -3.75124849e-02
  7.07834139e-02  5.19539230e-03  4.23453152e-02 -3.36830653e-02
  2.88183093e-02 -9.42495465e-02 -2.63561234e-02  3.98674980e-03
 -1.29548982e-02  1.16635352e-01 -1.13674819e-01  3.26403813e-03
 -4.67023700e-02 -1.38328690e-02 -4.46216390e-02  4.75553144e-03
  2.64066812e-02 -8.27866867e-02 -4.25742269e-02 -3.63950953e-02
  2.32875422e-02  6.21428192e-02  8.40427168e-03  2.60342248e-02
  6.84591904e-02  2.95751193e-03  2.26348322e-02 -8.30189064e-02
  1.63333312e-01  4.28971201e-02  5.71535192e-02  3.13321017e-02
  8.56467336e-03 -4.06267941e-02 -1.24773830e-01  7.11577237e-02
 -2.27709468e-02 -3.94843519e-04 -5.65467104e-02 -2.47398242e-02
 -2.91171633e-02 -3.97054516e-02  2.42776214e-03 -2.82893106e-02
 -6.29566982e-03 -2.28758920e-02  2.65645534e-02  7.05457479e-03
 -1.66379884e-02 -9.81660485e-02 -1.03983621e-04 -5.51157352e-03
 -9.54361726e-03  5.90681564e-03 -5.15820235e-02  6.96304962e-02
 -2.51099318e-02 -5.53400116e-03  2.95409709e-02  6.06588498e-02
  5.11152968e-02  1.08838655e-01 -5.46148121e-02 -8.07790272e-03
 -5.48865311e-02 -4.25130017e-02 -4.87284660e-02 -3.35746855e-02
  2.37884987e-02 -1.87778678e-02  1.37927551e-02  6.19372316e-02
  6.03485554e-02  1.20258644e-01 -2.21358538e-02 -2.23630592e-02
  9.33113247e-02 -2.81190351e-02 -1.03005609e-02 -5.44966757e-02
 -4.59979400e-02  7.90283829e-02  1.96289010e-02 -3.34032784e-08
 -7.32547268e-02  2.00976357e-02  4.33239527e-02  3.83378379e-02
  3.37329470e-02 -5.35768867e-02  7.28548393e-02  4.32257354e-02
 -3.24465451e-03 -3.92118320e-02  1.64945144e-02 -9.69082024e-03
 -9.57526937e-02 -3.57362665e-02  5.00617027e-02  5.65749817e-02
  4.75201290e-03  1.61479618e-02  3.29880603e-02 -1.89683996e-02
  6.01239614e-02  3.35822888e-02 -8.05160031e-03 -1.59397186e-03
  2.57605538e-02 -2.23974977e-02 -5.33769429e-02  3.00073121e-02
  2.92293989e-04 -1.82976574e-02  1.78916212e-02  2.70285141e-02
  9.65004340e-02 -6.86084479e-02  6.30374402e-02  4.52347696e-02
  7.38729089e-02  2.67716441e-02 -3.62409428e-02  4.79646251e-02
 -3.74384373e-02  4.21941169e-02 -4.84681614e-02 -3.90242971e-02
 -7.15741739e-02 -1.08108297e-02 -8.97729024e-03 -1.43704712e-01
  1.13946712e-02 -4.70180549e-02  4.14996631e-02  8.07490572e-03
 -2.80285757e-02  4.52438444e-02  4.18750644e-02  8.54908973e-02
 -2.98411511e-02 -9.28946435e-02 -1.93621442e-02  2.93516990e-04
  1.41355628e-02  9.66096669e-02  1.47592053e-02  6.78407922e-02]",2,2
torchio,1,tool like torchio are a symptom of the maturation of medical ai research using deep learning technique jack clark policy director at openai link queue for patch based training torchio is a python package containing a set of tool to efficiently read preprocess sample augment and write d medical image in deep learning application written in pytorch including intensity and spatial transforms for data augmentation and preprocessing transforms include typical computer vision operation such a random affine transformation and also domain specific one such a simulation of intensity artifact due to mri magnetic field inhomogeneity or k space motion artifact this package ha been greatly inspired by niftynet which is not actively maintained anymore if you like this repository please click on star if you use this package for your research please cite our paper f p rez garc a r spark and s ourselin torchio a python library for efficient loading preprocessing augmentation and patch based sampling of medical image in deep learning computer method and program in biomedicine june p issn doi j cmpb bibtex entry this project is supported by the following institution see getting started for installation instruction and a hello world example longer usage example can be found in the tutorial all the documentation is hosted on read the doc please open a new issue if you think something is missing thanks go to all these people emoji key this project follows the all contributor specification contribution of any kind welcome,"[('training torchio', 0.5081), ('medical image', 0.4581), ('torchio', 0.4453), ('deep learning application', 0.4437), ('medical ai research', 0.422), ('data augmentation', 0.419), ('deep learning computer method', 0.4186), ('deep learning technique jack clark policy director', 0.417), ('pytorch', 0.3544), ('augmentation', 0.3445)]","[-5.60845770e-02 -1.07662506e-01  1.59614030e-02  3.32367094e-03
 -3.73219065e-02 -1.35846259e-02 -5.14404429e-03  7.35323653e-02
 -7.38723949e-02 -5.98246381e-02 -7.71121457e-02  2.33096629e-02
 -8.66387933e-02  2.54365336e-02 -1.92010384e-02  1.79002266e-02
 -3.19073796e-02  4.67418581e-02 -7.37570673e-02 -1.01846471e-01
  5.66964112e-02 -2.18590312e-02  7.20660612e-02 -1.93327405e-02
 -2.18777116e-02  4.17482667e-02  6.23593703e-02 -5.92792556e-02
  2.04181820e-02 -9.41063929e-03 -4.14964892e-02  2.40426958e-02
 -1.09671270e-02 -1.17994184e-02 -3.77127416e-02  4.95623127e-02
 -4.90786023e-02  2.70423759e-02  2.15607900e-02  5.18058240e-02
  2.20894185e-03 -3.76202054e-02  2.21440978e-02 -5.18429885e-03
  6.43659309e-02 -1.77502036e-02  2.65483372e-02 -9.13178548e-02
  2.99295653e-02 -2.29122094e-03 -4.62529436e-02 -3.71565893e-02
 -5.81498928e-02  8.82628411e-02 -3.85586768e-02 -1.08921707e-01
  1.93710476e-02 -2.19624732e-02 -1.01392763e-02 -7.29048550e-02
  7.17094168e-02  1.31948199e-02  7.58680934e-03  5.55409677e-03
 -5.15052602e-02  1.91108957e-02 -3.74080315e-02  1.97849572e-02
  4.50008512e-02 -1.00811139e-01  6.31094202e-02  3.11784521e-02
 -3.37675661e-02  3.43790390e-02 -1.08681321e-02 -8.93255975e-03
  8.19018632e-02  1.75881721e-02  8.74401629e-02 -1.52176052e-01
  9.78780612e-02 -3.39455828e-02  2.76096687e-02  4.55174334e-02
 -1.58162937e-02  2.51051206e-02 -6.13007024e-02  1.58028060e-03
 -4.94290814e-02 -4.18669507e-02  3.63444947e-02 -1.25919625e-01
 -4.30886187e-02  3.02374223e-03 -9.28407535e-03 -4.72646281e-02
 -5.41195236e-02 -4.26108986e-02 -4.95034233e-02  1.52623765e-02
 -5.74975349e-02 -9.31968093e-02  5.77004813e-03  3.21810618e-02
 -3.66028249e-02  6.25391379e-02  2.11763009e-02 -1.42497076e-02
  6.78062141e-02  4.30803485e-02 -9.02231969e-03  6.07401542e-02
 -2.29100212e-02 -6.55779094e-02  5.14301956e-02  2.74877585e-02
 -6.64535835e-02  5.83094507e-02 -8.06681532e-03  1.44300401e-01
 -7.63413012e-02  2.52706315e-02 -1.84153635e-02 -2.40316801e-02
 -1.30315628e-02 -4.01252024e-02 -1.00354210e-01  4.69658629e-33
 -1.61996402e-03  1.74503122e-02  3.43184508e-02 -4.22884412e-02
  2.78882571e-02 -5.62473349e-02  1.76477637e-02 -1.79129504e-02
  2.65670987e-03 -6.62060082e-02 -3.67118120e-02  6.36725724e-02
 -4.00711596e-02  1.00681096e-01 -3.22147906e-02  4.96887118e-02
  5.99809997e-02  5.92612438e-02  3.21795121e-02 -1.40053323e-02
  2.58269832e-02 -1.98735483e-02 -2.12444514e-02  6.09171763e-02
 -5.26035614e-02  4.93694246e-02 -5.07441759e-02 -2.04018652e-02
  4.64085899e-02  1.71822533e-02 -4.42274436e-02  9.08261389e-02
  2.44526416e-02 -1.62993670e-02  3.98813784e-02  2.64955629e-02
  2.06438750e-02 -4.37174849e-02  6.69683330e-03  7.08493516e-02
 -2.47976580e-03  5.09812310e-02 -4.57302779e-02 -7.63346581e-03
 -1.92156173e-02 -5.71100414e-02  4.31263894e-02  2.04953216e-02
  5.51157594e-02 -2.56682374e-02 -3.05397157e-02  7.36041972e-03
 -1.32532204e-02 -3.99255753e-02  4.37876433e-02 -1.42077291e-02
 -3.17040570e-02  2.64194179e-02  8.11602082e-03  8.49556457e-03
  8.86333063e-02 -1.67816033e-04  2.18402743e-02  2.38706432e-02
 -8.33732858e-02 -3.91415954e-02  3.37604918e-02 -2.40319241e-02
  6.29705237e-03  2.77847499e-02 -1.16315618e-01  4.71986234e-02
  2.08276585e-02 -2.54128855e-02 -6.85605258e-02 -2.86775026e-02
 -1.41227094e-03 -1.00092724e-01 -9.78483073e-03  7.21760169e-02
 -7.24440664e-02  3.93870175e-02 -1.25873396e-02  1.71404500e-02
  3.15211788e-02  3.23772691e-02 -3.48655879e-02 -1.38968714e-02
  2.01055594e-02  2.31642071e-02 -9.54528674e-02 -4.44051519e-04
 -4.37147042e-04  5.62060475e-02  5.23434915e-02 -3.44443171e-33
 -3.04544996e-02  1.01403050e-01 -1.06420137e-01  4.38943580e-02
  6.82423115e-02  6.64067641e-02 -2.14380901e-02 -5.58705702e-02
  5.53020760e-02  4.67840378e-04  1.21138245e-02  2.01856028e-02
 -2.41779201e-02 -2.88065057e-02 -2.34141275e-02 -5.69322035e-02
 -8.80577043e-02 -3.71501744e-02 -4.58870381e-02  5.54583631e-02
  1.15820924e-02  3.94975878e-02 -5.99547364e-02 -9.39704198e-03
  3.30800842e-03  4.13201237e-03 -2.58028656e-02  1.94245260e-02
  9.16377679e-02  6.38612360e-03  4.60254587e-02 -2.06385907e-02
 -1.35071818e-02 -2.06764080e-02 -8.62551760e-03  7.64140487e-02
  1.06161289e-01 -7.14025348e-02  2.16254015e-02 -3.14334892e-02
  1.31777689e-01 -7.12358439e-03  6.59203082e-02  5.46069182e-02
 -4.88515124e-02 -5.39088808e-02 -9.83930901e-02  2.67408602e-02
  1.85260326e-02 -5.58236167e-02  2.37922417e-03 -8.84096026e-02
 -6.94798604e-02  2.43185218e-02 -2.41971835e-02  1.74284982e-03
 -1.96977588e-03 -2.17391294e-03 -9.11918841e-03  2.36378890e-02
 -6.70666546e-02 -5.11024706e-02  2.15163473e-02 -1.54034691e-02
 -3.74580212e-02  6.42958581e-02 -2.20385808e-02  6.24787509e-02
 -5.32346405e-02 -5.36864949e-03  5.67250736e-02  7.59696215e-02
  3.06995995e-02  1.24760590e-01 -5.08614630e-02  9.74784233e-03
 -4.59288945e-03 -3.13138007e-03  1.67457927e-02 -2.29444522e-02
  2.66663898e-02 -7.65454024e-02 -6.65863156e-02  1.07253321e-01
  1.85568184e-02  1.40864193e-01  4.80066575e-02 -1.11370087e-02
 -2.05445942e-02 -1.15110226e-01 -7.99307898e-02 -1.59525443e-02
  4.71776873e-02  2.69020461e-02 -4.36994322e-02 -2.52603645e-08
 -2.51767300e-02 -3.87600474e-02  7.25452006e-02  1.52047435e-02
  8.29095487e-03  1.20906334e-03 -6.57837540e-02  1.60792843e-01
 -3.59995663e-02  1.45423962e-02 -2.41798796e-02  3.30692381e-02
  3.27356346e-03 -6.46312675e-03  9.21676755e-02  6.45759627e-02
  8.30995068e-02  4.32848707e-02  5.49154133e-02 -7.57668838e-02
  1.70817934e-02  3.03321332e-02  5.28847054e-02 -1.95942353e-02
  3.79848033e-02 -2.71313023e-02  4.32533808e-02 -1.87325161e-02
 -5.57000116e-02  6.50549680e-02  7.91192707e-03 -5.81240021e-02
  1.17163911e-01 -1.19250324e-02  9.55996215e-02  9.23619270e-02
  3.84253860e-02 -3.05536110e-02 -6.23971783e-02  1.01317104e-03
  4.30830196e-02  6.56581745e-02  4.01248261e-02 -6.61630603e-03
  3.52358557e-02 -4.22773547e-02 -1.15884468e-02 -1.40864402e-02
  2.02305187e-02 -5.94743900e-02 -7.76098818e-02  5.97639345e-02
 -1.42420484e-02  2.70813573e-02  2.60366146e-02  8.93083140e-02
  6.12039715e-02 -5.02070151e-02 -5.81491627e-02  9.47384909e-02
 -5.35558769e-03 -1.34631188e-03 -4.37901653e-02  2.15911381e-02]",2,2
torch-runstats,1,running online statistic for pytorch torch runstats implement memory efficient online reduction on tensor notable feature note the implementation currently heavily us in place operation for peformance and memory efficiency this probably doesn t play nice with the autograd engine this is currently likely the wrong library for accumulating running statistic you want to backward through see torchmetrics for a possible alternative for more information please see the doc torch runstats requires pytorch the library can be installed from pypi the latest development version of the code can also be installed from git and install it by runningyou can run the test withpytorch runstats is distributed under an mit license,"[('pytorch torch runstats', 0.7091), ('doc torch runstats', 0.5378), ('test withpytorch runstats', 0.5134), ('pytorch', 0.4676), ('efficient online reduction', 0.4033), ('online statistic', 0.3731), ('torchmetrics', 0.3419), ('memory efficiency', 0.3394), ('autograd engine', 0.268), ('memory', 0.2533)]","[-3.75012159e-02  1.30869532e-02 -1.23265944e-01  6.65459260e-02
  1.77013613e-02 -2.87755281e-02 -3.90138179e-02  1.06188372e-01
 -6.89578876e-02 -3.32212105e-04 -4.92469640e-03  4.66676652e-02
 -9.11255702e-02 -8.87822080e-03  5.41228242e-03  1.19399689e-02
  6.00886706e-04  4.06142510e-02 -9.82880667e-02 -1.50649354e-01
  3.29699218e-02 -8.53446126e-02  7.27939084e-02  3.71199027e-02
 -1.90346744e-02  9.45110712e-03 -6.73276484e-02 -4.94044237e-02
  4.50672917e-02  4.12247796e-03 -1.17523288e-02 -2.15024985e-02
  3.64174247e-02  2.03306265e-02  5.64089837e-03 -6.44650683e-02
 -1.26834083e-02 -4.06865999e-02  1.33864619e-02  8.83052200e-02
  3.02772433e-03 -2.49732751e-02 -2.75425296e-02  3.96224372e-02
 -2.76602469e-02  5.71220815e-02 -1.13242900e-03 -3.48419771e-02
 -1.06857583e-01 -7.15661272e-02 -5.12818880e-02 -5.39522851e-03
 -7.70189166e-02  6.70703053e-02  4.57835682e-02 -1.80864222e-02
  3.21435891e-02 -4.34554927e-02 -9.75338649e-03 -1.92365367e-02
  4.57380228e-02 -6.46966100e-02 -3.05855479e-02 -2.12808847e-02
 -4.02827822e-02  2.02505365e-02  6.33736104e-02 -1.10118743e-02
  1.35689139e-01 -1.79101713e-02 -6.86170459e-02 -1.19904540e-02
 -5.08721396e-02  6.99967071e-02 -4.72231917e-02 -4.86022830e-02
  3.80598269e-02 -2.66171098e-02 -2.59255897e-03 -1.48735404e-01
 -9.54331551e-03 -6.96033239e-02 -1.13486582e-02  3.13603953e-02
  5.92249557e-02  4.70278971e-02 -4.49986616e-03  2.71626655e-02
  1.26113454e-02 -3.89368534e-02  2.16871351e-02 -5.79721155e-03
 -9.66234878e-03  3.63265164e-02 -1.65120345e-02 -7.83815701e-03
  8.13469589e-02 -4.95698452e-02 -3.16187590e-02  1.97594836e-02
  5.69616184e-02  1.48651237e-02 -2.81536020e-02  2.52912249e-02
 -5.56054935e-02  2.72295494e-02 -6.26427308e-03  1.01595707e-01
  2.49682348e-02 -1.13886017e-02  5.74731939e-02  3.19815874e-02
 -4.41733450e-02 -5.52616678e-02  8.39847028e-02 -6.27507493e-02
 -5.56124225e-02  1.71695501e-02  4.21562791e-02  1.04263566e-01
  1.99049581e-02  4.52481471e-02  1.96778644e-02  9.53455493e-02
 -2.33601332e-02 -4.00970876e-02 -7.66741559e-02  4.92785893e-33
 -4.04033996e-02  2.99956407e-02 -4.21731062e-02 -1.16317393e-02
 -2.06340235e-02  1.61011815e-02  2.09050607e-02  3.33931707e-02
  2.80567650e-02 -1.30828386e-02 -5.79324958e-04  7.15634003e-02
 -3.91871333e-02  6.53273687e-02  2.63537858e-02 -3.04779746e-02
  1.59846079e-02  8.01332518e-02  6.64335862e-02 -1.01983706e-02
  1.01340771e-01  5.78683913e-02 -2.43087914e-02  4.61556464e-02
  4.07871567e-02  7.82911479e-03 -3.38212326e-02 -4.01202589e-02
 -4.37933803e-02  2.32023355e-02  1.96050536e-02 -2.00901534e-02
 -7.47547075e-02  2.15663556e-02  7.81879388e-03 -3.46257463e-02
 -7.26225823e-02 -5.57158813e-02 -6.05967781e-03 -5.05749136e-02
 -1.02441251e-01  6.11477047e-02 -5.13069816e-02 -6.37315661e-02
 -5.20496741e-02 -3.61453071e-02 -8.43380317e-02  4.76008393e-02
  6.85619339e-02 -5.13467081e-02 -5.53652532e-02  8.84739030e-03
  4.70613223e-03  7.52385035e-02  3.79516780e-02  6.67060865e-03
  2.20013242e-02 -3.38414544e-03  7.37496391e-02  1.00695483e-01
 -5.69783570e-03  6.84893653e-02 -4.26934212e-02 -2.78266650e-02
 -3.83531563e-02 -3.96456523e-03 -1.89264137e-02 -1.66150031e-03
 -4.64313924e-02  4.99826521e-02 -7.74807781e-02  3.43126319e-02
 -3.31142768e-02  2.77870409e-02  4.45067845e-02  1.82435531e-02
  5.39847091e-02 -4.83774059e-02 -1.50271192e-01 -3.88340279e-02
 -6.91541359e-02  3.05559244e-02  4.37353067e-02 -7.27174878e-02
 -4.32887375e-02 -1.16702365e-02  1.42574692e-02 -3.06334794e-02
  9.14846023e-04  2.53391615e-03 -1.71570797e-02 -4.42721993e-02
  2.08011046e-02  2.63281609e-03 -3.28991264e-02 -4.41393647e-33
 -6.47768080e-02  5.92664927e-02 -5.62593304e-02  1.82955295e-01
 -2.08798256e-02  7.03066122e-03 -5.96863590e-03 -8.56933817e-02
  3.48646729e-03 -3.69961001e-02 -1.05885956e-02 -3.16467695e-02
 -5.82042150e-02 -1.85942799e-02  2.73861140e-02  2.51987134e-03
 -3.09100673e-02 -4.11470532e-02  1.49289500e-02  6.55805543e-02
 -5.47408946e-02  1.19979471e-01 -2.40826923e-02 -7.09269419e-02
 -6.41081706e-02  1.38705531e-02 -3.87388952e-02 -7.51146674e-02
  2.61637196e-02 -2.19571851e-02  6.11916855e-02  8.29978555e-04
 -4.71705012e-02  5.19049875e-02 -6.00648373e-02 -3.12068686e-02
  8.42172578e-02  2.36100778e-02 -1.18464958e-02  5.84771931e-02
  1.90864980e-01  3.89279164e-02  9.26825777e-03  2.46607531e-02
 -9.52978954e-02  1.01804599e-01 -1.58432379e-01  7.10916938e-03
  1.30829941e-02  2.76322979e-02  9.02595967e-02 -2.54463032e-02
 -1.72352344e-02  1.08956955e-02  1.18680613e-03 -7.65773728e-02
  9.16443579e-03  1.04702543e-02 -6.95576817e-02  4.79319058e-02
  3.06490972e-03 -5.54687642e-02  7.34879682e-03  5.25909998e-02
 -4.21620235e-02 -3.50307114e-02 -4.42939103e-02  4.98214439e-02
 -2.81380769e-03 -7.27060856e-03  1.94638446e-02  5.86389005e-02
  3.54356281e-02  4.60998863e-02 -4.34205383e-02  1.99908838e-02
  2.54019455e-04 -1.50915105e-02 -3.12043857e-02  1.01610329e-02
 -4.64158505e-02  3.03170830e-02  1.71174537e-02 -5.74675724e-02
 -2.50439327e-02  3.80878225e-02  1.83035433e-02  4.56679799e-02
 -1.02694901e-02 -6.71991184e-02  1.03324791e-02  1.93953589e-02
  7.18516707e-02  3.13124694e-02  3.01492903e-02 -3.05685077e-08
 -1.58745702e-02  6.91813789e-03  4.93755192e-02  5.47529981e-02
  4.49997857e-02 -2.55600782e-03  6.22295993e-05  1.74564227e-01
 -8.51174758e-04 -3.60850501e-03  5.45535125e-02 -6.08005784e-02
 -6.25917036e-03 -2.53207255e-02  5.78142740e-02  4.53513302e-02
 -3.85680539e-03  2.54654028e-02  7.17411116e-02 -5.71196377e-02
 -1.93727072e-02 -2.34520156e-02  9.01832804e-03  2.93023791e-02
 -9.83052794e-03  1.42754626e-03  7.73063228e-02  2.80210213e-03
  2.28056125e-02  3.15056033e-02  4.04532291e-02 -3.82107086e-02
  1.13689706e-01 -4.16440107e-02  1.40328035e-01  7.17507228e-02
 -4.06971462e-02  2.21325941e-02 -2.20715944e-02  5.50419837e-02
 -5.74856736e-02 -1.04904417e-02 -4.80796285e-02  3.59492265e-02
  9.17051509e-02 -3.20170932e-02 -7.92683139e-02 -1.82651039e-02
 -3.33670788e-02 -3.10859107e-03  3.11370450e-03  3.96708362e-02
  9.62067395e-03  1.53220072e-02 -6.87221717e-03  9.76772904e-02
 -1.32632593e-03 -1.00769680e-02 -4.36489545e-02 -8.19343701e-03
  2.82219723e-02  1.30377803e-02 -2.61370353e-02  9.00358707e-03]",2,2
pfrl,1,pfrl is a deep reinforcement learning library that implement various state of the art deep reinforcement algorithm in python using pytorch pfrl is tested with python for other requirement see requirement txt pfrl can be installed via pypi it can also be installed from the source code refer to installation for more information on installation you can try pfrl quickstart guide first or check the example ready for atari and open ai gym for more information you can refer to pfrl s documentation note on pretrained model pfrl provides pretrained model sometimes called a model zoo for our reproducibility script on atari environment dqn iqn rainbow and a c and mujoco environment ddpg trpo ppo td sac for each benchmarked environment following algorithm have been implemented in pfrl following useful technique have been also implemented in pfrl environment that support the subset of openai gym s interface reset and step method can be used any kind of contribution to pfrl would be highly appreciated if you are interested in contributing to pfrl please read contributing md mit license to cite pfrl in publication please cite our paper on chainerrl the library on which pfrl is based,"[('model pfrl', 0.6028), ('pfrl environment', 0.5926), ('pytorch pfrl', 0.5729), ('pfrl', 0.5666), ('deep reinforcement learning library', 0.5463), ('deep reinforcement algorithm', 0.4312), ('openai gym', 0.4083), ('requirement txt pfrl', 0.408), ('atari environment dqn', 0.3883), ('ddpg trpo', 0.3257)]","[-7.53851980e-02 -1.39530346e-01  2.55794600e-02 -3.03682629e-02
 -1.77774448e-02 -2.09340104e-03 -1.58694200e-02  7.58634694e-03
 -9.13987868e-03 -1.99810900e-02 -2.71541011e-02 -1.50418878e-02
 -6.42212331e-02  1.25964070e-02  2.68599465e-02 -1.99415609e-02
 -1.31174736e-02  5.94292656e-02 -3.72827388e-02 -1.39823109e-01
 -1.90107133e-02 -1.30435545e-02  5.95743880e-02  1.77267175e-02
 -8.23748782e-02  6.11212961e-02  2.26960648e-02  4.16918099e-02
  2.30379365e-02 -5.72124161e-02 -1.58460205e-03  1.20103713e-02
  1.08488560e-01 -3.83496545e-02 -1.73552465e-02  8.79143402e-02
 -8.24605152e-02 -5.62087111e-02 -6.00226074e-02 -4.09642980e-02
 -2.48574838e-02 -3.41502056e-02  3.91857028e-02  4.09247167e-03
  7.80087486e-02 -3.61598432e-02 -3.14302705e-02 -9.43357721e-02
  4.10905667e-02 -3.91164757e-02 -2.47685444e-02  1.35165881e-02
  4.80828620e-03  2.43642442e-02  4.66092452e-02  4.33110520e-02
  5.40195554e-02 -3.38143818e-02  2.89884279e-03 -6.78648278e-02
 -3.00912727e-02  1.67769333e-03 -4.31261435e-02  3.00096571e-02
 -4.74713966e-02  8.31133649e-02 -4.37571630e-02  9.34277847e-02
  1.09550603e-01 -8.23536441e-02 -1.14230148e-01  3.42632085e-02
  1.32490089e-02 -4.20874134e-02 -4.12118388e-03  4.07430120e-02
 -3.73090780e-03 -3.96059113e-05  2.68258192e-02 -7.22140446e-02
  2.17078235e-02 -6.91620111e-02 -3.78718525e-02 -2.86573451e-03
  3.03525534e-02  3.19492514e-03 -6.44101799e-02 -3.17810886e-02
  2.33831462e-02 -3.60399112e-03  8.89453664e-02 -1.36820879e-02
  3.42614055e-02  2.99564730e-02 -2.51508076e-02  4.76877131e-02
 -4.51031625e-02 -1.20545104e-01 -6.68560788e-02  4.09046113e-02
  2.23154277e-02 -3.20002786e-03  2.96979826e-02  5.77935316e-02
 -6.63134381e-02 -1.70069132e-02  3.83004509e-02 -3.18880263e-03
  8.42419267e-02  6.81018457e-03 -2.92456821e-02 -2.59276349e-02
  4.15716805e-02 -1.47663653e-02  7.58169442e-02 -5.74005069e-04
 -1.40070748e-02  9.31390096e-03 -2.59925257e-02  5.07659055e-02
 -9.88962278e-02 -2.50051413e-02 -1.06236665e-02 -1.32392300e-03
 -2.40745693e-02 -4.62818816e-02 -6.67294785e-02  7.75469436e-33
  5.43494895e-02  7.98997656e-03  3.75713222e-02 -1.36300642e-02
  9.18454155e-02 -7.05026835e-02  8.54930654e-02 -6.75084069e-03
  1.30903749e-02 -6.65323064e-02  3.67065631e-02  6.95140511e-02
 -1.19774505e-01  9.68294218e-02  8.55384916e-02 -5.72365522e-02
 -1.18064806e-01  7.68916532e-02  5.31468093e-02  1.08172260e-02
  6.38888925e-02  7.74689391e-03 -2.40609664e-02  2.49323621e-02
  2.49633398e-02  1.16227634e-01 -4.20992821e-02  1.09245656e-02
 -1.75058153e-02  2.11855527e-02 -2.08558682e-02 -1.91901214e-02
 -4.65803035e-02  2.93032862e-02  2.44911257e-02  2.18782071e-02
 -5.13848029e-02 -3.36287804e-02 -2.09858976e-02 -1.36520108e-02
 -2.76499399e-04 -2.91622113e-02  1.01877330e-02  2.07095109e-02
 -2.99684070e-02 -6.53518885e-02  5.70011996e-02  4.54077683e-02
 -4.85219769e-02 -1.21344747e-02 -5.04915640e-02 -1.92306004e-02
 -5.47365844e-02 -9.84002277e-02  1.33275399e-02 -3.68951969e-02
  5.04084900e-02  3.23331840e-02 -2.56042052e-02  1.13621652e-01
  5.80001660e-02  7.04796687e-02  2.19448041e-02 -7.22932220e-02
 -4.55752900e-03  7.45936334e-02 -5.91186956e-02 -6.26408905e-02
  5.41474745e-02  2.85232309e-02 -1.52946031e-02  3.73873860e-02
  4.29598540e-02  1.75098404e-02  6.55097365e-02  9.54637770e-03
 -6.24514483e-02 -5.30695468e-02 -3.46125849e-02  6.01427536e-03
 -1.14646718e-01  5.24078682e-02 -3.74353789e-02  3.92235853e-02
 -1.51169552e-02 -3.47865522e-02  2.34995876e-02 -3.54832858e-02
 -7.01147038e-03 -1.08024105e-02 -1.16573110e-01 -8.00790340e-02
  2.60885004e-02  8.13641772e-02  4.60616834e-02 -6.26866746e-33
 -3.70456316e-02  6.02536043e-03 -9.78754275e-03  4.18066308e-02
  4.01593074e-02  1.16407294e-02 -4.60847430e-02 -1.18588107e-02
  1.00763261e-01  1.43740354e-02  5.54214465e-03  3.86524983e-02
  1.08661965e-01 -8.08976311e-03 -3.80104384e-03 -8.16078484e-02
  4.09262208e-03 -5.64749464e-02 -1.69319883e-02  6.43281639e-02
  6.85898121e-03  2.92767826e-02 -8.81981701e-02  5.44267111e-02
  8.16774368e-02  3.53857540e-02 -1.54520082e-03 -1.71564706e-02
  1.47080608e-02  6.86200187e-02  1.77906349e-03  3.10999155e-02
  4.58247727e-03  3.38260867e-02 -4.01623733e-02 -3.39671634e-02
  1.05406553e-01 -2.58926190e-02 -6.68805391e-02 -2.81053688e-02
  9.24509317e-02 -5.11151552e-02 -1.10370461e-02  1.05116656e-03
 -3.75646316e-02  3.42245623e-02 -1.01573460e-01 -2.43632644e-02
  4.63667139e-02 -3.02646458e-02  1.08882226e-02  3.04734465e-02
 -1.06860790e-03 -1.86729152e-02 -4.23110239e-02 -1.20931873e-02
  2.21277159e-02  4.94431257e-02  3.14631276e-02  1.16137005e-02
 -1.08448677e-01 -6.45710714e-03 -4.10894863e-02  7.01644719e-02
 -1.20149944e-02 -1.22349914e-02  8.71030707e-03  9.58945975e-02
 -2.60519851e-02 -9.88547504e-02 -1.65010542e-02  7.40008503e-02
  8.90783072e-02  8.53952542e-02 -8.56392900e-04 -3.15632410e-02
 -2.44548209e-02  6.05803020e-02  4.30439301e-02  2.46079676e-02
  4.07770984e-02  5.73739596e-03 -5.44571951e-02  2.09099054e-02
  2.47242115e-03  3.11588217e-02  9.59023759e-02  2.06359848e-02
  9.65660345e-03 -7.74078369e-02  9.23531875e-03  3.31904180e-02
 -1.51430145e-02  7.08493069e-02 -1.09069161e-02 -3.36586226e-08
 -3.99810448e-02  1.36069600e-02  8.40326250e-02 -4.97130584e-03
  6.41057715e-02  2.07025837e-02 -6.38977950e-03  4.48396839e-02
 -3.66436318e-02  5.02448864e-02 -9.06032138e-03  3.27278301e-02
 -2.60243453e-02  1.05276555e-02  9.79741104e-03  8.89119059e-02
 -2.97477026e-03 -1.27700008e-02  2.37255413e-02 -7.76301175e-02
  8.37680623e-02  2.42623389e-02 -2.99331080e-02 -5.57409935e-02
  4.19910774e-02 -7.78306201e-02 -2.14327425e-02 -8.87665972e-02
  2.19995342e-02  6.11010753e-02  2.72486676e-02 -4.90183495e-02
  8.69229510e-02 -8.00723955e-02  6.95017353e-02  1.32635564e-01
  6.68966025e-02 -7.28496462e-02 -5.12280948e-02  7.46539012e-02
 -7.92975724e-02  3.76280360e-02 -1.02095911e-02 -6.26616180e-02
  3.27554978e-02  4.57871854e-02 -8.47023204e-02 -6.56790212e-02
  5.16833328e-02  1.64416730e-02 -1.57716591e-02  4.07018699e-02
 -4.08807173e-02 -4.15182561e-02  1.73934270e-02  6.61101714e-02
  6.21521771e-02 -8.98175910e-02 -5.64629100e-02  9.94467270e-03
  2.86841989e-02  2.45921221e-02  2.49516703e-02  7.12666214e-02]",2,0
cogdl,1,homepage paper gnn paper leaderboards documentation datasets join our slack cogdl is a graph deep learning toolkit that allows researcher and developer to easily train and compare baseline or customized model for node classification graph classification and other important task in the graph domain we summarize the contribution of cogdl a follows the new v release support mixed precision training by setting textit fp true and provides a basic example written by jittor it also update the tutorial in the document fix downloading link of some datasets and fix potential bug of operator the new v release add a gnn example for ogbn product and update geom datasets it also fix some potential bug including setting device using cpu for inference etc the new v release add fast operator including spmm cpu version and scatter max cuda version it also add lot of datasets for node classification which can be found in this link the new v release design and implement a unified training loop for gnn it introduces datawrapper to help prepare the training validation test data and modelwrapper to define the training validation test step the new v release add the implementation of deep gnns and the recommendation task it also support new pipeline for generating embeddings and recommendation welcome to join our tutorial on kdd at am am aug th singapore time more detail can be found in http kdd graph github io the new v release refactors the data storage from data to graph and provides more fast operator to speed up gnn training it also includes many self supervised learning method on graph btw we are glad to announce that we will give a tutorial on kdd in august please see this link for more detail cogdl support gnn model with mixture of expert moe you can install fastmoe and try moe gcn in cogdl now the new v release provides a fast spmm operator to speed up gnn training we also release the first version of cogdl paper in arxiv you can join our slack for discussion the new v release includes easy to use experiment and pipeline apis for all experiment and application the experiment api support automl feature of searching hyper parameter this release also provides oagbert api for model inference oagbert is trained on large scale academic corpus by our lab some feature and model are added by the open source community thanks to all the contributor the new v release includes a pre training task many example ogb datasets some knowledge graph embedding method and some graph neural network model the coverage of cogdl is increased to some new apis such a trainer and sampler are developed and being tested the new v release includes the knowledge link prediction task many state of the art model and optuna support we also have a chinese wechat post about the cogdl release please follow the instruction here to install pytorch http github com pytorch pytorch installation when pytorch ha been installed cogdl can be installed using pip a follows install from source via or clone the repository and install with the following command you can run all kind of experiment through cogdl apis especially experiment you can also use your own datasets and model for experiment a quickstart example can be found in the quick start py more example are provided in the example some interesting application can be used through pipeline api an example can be found in the pipeline py more detail of the oagbert usage can be found here you can also use python script train py dataset example dataset model example model to run example model on example data for example if you want to run gcn and gat on the cora dataset with different seed expected output if you have any difficulty to get thing working in the above step feel free to open an issue you can expect a reply within hour if you have a well performed algorithm and are willing to implement it in our toolkit to help more people you can first open an issue and then create a pull request detailed information can be found here before committing your modification please first run pre commit install to setup the git hook for checking code format and style using black and flake then the pre commit will run automatically on git commit detailed information of pre commit can be found here if you want to run parallel experiment on your server with multiple gpus on multiple model gcn and gat on the cora dataset expected output you might be confused why your pull request wa rejected because of coverage decreased issue even though your model is working fine locally this is because you have not included a unit test which essentially run through the extra line of code you added the travis ci service used by github conduct all unit test on the code you committed and check how many line of the code have been checked by the unit test and if a significant portion of your code ha not been checked insufficient coverage the pull request is rejected so how do you do a unit test cogdl is developed and maintained by tsinghua zju baai damo academy and zhipu ai the core development team can be reached at cogdlteam gmail com please cite our paper if you find our code or result useful for your research,"[('homepage paper gnn paper leaderboards documentation datasets', 0.6418), ('gnn training', 0.5727), ('deep learning toolkit', 0.5226), ('gnn example', 0.5195), ('deep gnns', 0.4204), ('gnn', 0.418), ('unified training loop', 0.4155), ('graph neural network model', 0.4103), ('gcn', 0.4032), ('ogbn product', 0.4011)]","[-1.07724316e-01 -6.66122735e-02  1.33669237e-02 -2.89052576e-02
  2.74535753e-02 -1.70020834e-02 -4.20283526e-02  2.52664741e-02
 -9.92974937e-02 -1.71781294e-02 -1.70134567e-02  9.12953243e-02
 -2.59876717e-02  9.46229789e-03 -6.62401021e-02 -9.63551179e-02
  4.45170375e-03  3.88356447e-02 -1.05743101e-02 -4.94455844e-02
 -3.02584171e-02  2.47215200e-02  4.70006652e-02  1.64450854e-02
  6.72248006e-02 -2.48488951e-02 -1.63039062e-02 -1.13964766e-01
  6.90972954e-02 -1.99979562e-02 -1.68936851e-04 -5.23824058e-02
  6.09268993e-02  4.39767167e-02 -2.11048741e-02  2.64933100e-03
 -6.45507826e-03 -2.33224034e-02  3.47937904e-02 -1.48215825e-02
 -6.23107795e-03 -1.97702609e-02 -1.81481130e-02  5.27945496e-02
  1.36901289e-01  3.93727794e-02 -7.57723898e-02 -5.93980066e-02
 -4.93188649e-02  8.99389479e-03 -4.44405898e-02 -1.31335050e-01
 -6.90528154e-02  4.29032147e-02  6.60347044e-02 -5.51312380e-02
 -1.16166867e-01  8.01474380e-04  3.58102359e-02 -4.64945436e-02
  1.81693956e-02 -2.99088657e-02 -8.42501968e-02  1.32233230e-02
  1.84518229e-02  4.93263416e-02 -2.13400577e-03  1.03057846e-01
  4.16319668e-02 -3.36233787e-02  4.23847549e-02  3.23183276e-02
  2.80898884e-02 -4.62126359e-02 -5.40277455e-04  1.60418954e-02
 -1.61394440e-02  2.80144084e-02  5.45366406e-02 -1.06236279e-01
  3.19916122e-02  1.56650506e-02  7.99526423e-02  6.13740422e-02
 -7.30832433e-03 -7.86849186e-02 -6.09467812e-02  5.32961525e-02
 -3.18423398e-02 -1.05325673e-02  2.08218247e-02  7.93565647e-04
  1.14242164e-02 -5.44757955e-03 -5.27278781e-02  1.82835348e-02
 -2.46518962e-02 -3.51911411e-02 -3.44854081e-03  2.65558772e-02
 -3.16907354e-02  7.34526804e-03  1.05004899e-01  9.07741720e-04
 -2.29068026e-02  4.56032455e-02  4.13875356e-02  2.74829064e-02
  1.12169184e-01  1.45977391e-02 -3.67970020e-02  8.82177502e-02
 -1.18926764e-01 -1.10744983e-01  9.95947607e-03 -4.95924465e-02
 -2.04344019e-02 -8.81634187e-03  2.32954249e-02  2.99099796e-02
 -1.01738356e-01  4.65190746e-02  2.54726950e-02 -3.66873406e-02
 -5.26851341e-02  6.77433312e-02 -6.86240196e-02  4.62278619e-33
  6.32985309e-02 -1.37112115e-03 -2.84777321e-02 -5.10878265e-02
  8.21315199e-02 -4.12597694e-03  1.95212718e-02 -2.46498436e-02
 -1.28247207e-02 -1.59425437e-02 -5.67284226e-02  7.85444230e-02
 -5.36746569e-02  1.15336485e-01 -3.04893497e-02  4.83421758e-02
 -5.82731934e-03 -3.20981145e-02  4.42404523e-02 -4.92827268e-03
  8.93235728e-02 -2.47680340e-02  4.87886742e-02  7.06875771e-02
  6.06091172e-02  3.70074273e-03 -6.83474317e-02 -2.10058726e-02
  1.75589987e-03  2.26425510e-02  7.52431166e-04 -4.06667739e-02
 -1.51540954e-02 -3.80029641e-02 -2.87636351e-02  2.45842636e-02
 -4.61137667e-02 -9.16603580e-02  7.50139430e-02 -4.18631844e-02
 -7.86113739e-02 -1.66221634e-02 -2.32564732e-02 -4.16303873e-02
  5.07696979e-02  8.13958049e-03 -2.05655061e-02  2.86061857e-02
  1.14819206e-01  1.62670389e-03 -7.22302645e-02 -2.53214035e-02
 -8.05469900e-02  3.33569315e-03  1.79178268e-02 -1.38906064e-02
  8.79846513e-02  6.44940138e-02  2.34833658e-02  2.63483934e-02
  3.06759849e-02  9.25567672e-02 -2.08966117e-02 -2.82821469e-02
 -6.34619743e-02 -2.64265966e-02 -4.36340719e-02  2.31676809e-02
  1.75109096e-02 -5.77220581e-02 -9.47416108e-03 -4.36896347e-02
 -4.12729941e-02 -3.04033142e-02 -5.51014394e-02 -3.85627878e-04
 -4.31913324e-02 -1.02783538e-01 -6.90988973e-02  2.97823455e-02
 -2.79534589e-02 -3.24480832e-02  7.42582371e-03 -1.67476051e-02
 -6.11331724e-02  8.72923899e-03  3.87928635e-02 -2.99648810e-02
  3.20958719e-02  3.45026366e-02 -1.35808554e-03 -3.26400762e-03
  5.43186516e-02  3.52229960e-02 -7.87423849e-02 -3.14632157e-33
 -2.18039211e-02  8.90535787e-02 -4.31014039e-02  1.03197373e-01
  9.02381763e-02  3.11011598e-02 -4.52176388e-03 -2.59105247e-02
  1.22946557e-02  5.11456355e-02  8.28077719e-02 -1.73596255e-02
  1.94028337e-02  7.53267258e-02  1.23151466e-02 -5.05883507e-02
 -1.11357562e-01 -2.39330754e-02  1.14872847e-02  3.63466074e-03
  8.66877194e-03  3.71236205e-02 -8.28581452e-02  5.13432436e-02
  6.12577423e-02 -4.23966907e-02 -8.44147336e-03  3.25295106e-02
  5.23437522e-02  4.55691032e-02 -2.07920512e-03 -4.98335995e-02
 -2.86773294e-02  2.75660213e-02  6.44543916e-02  1.86736602e-02
  1.10464603e-01 -3.37810144e-02 -8.61299317e-03 -6.34965971e-02
  1.00284584e-01 -1.51676433e-02  2.95754522e-02 -2.53764968e-02
 -4.58223112e-02  4.39237542e-02 -6.55297488e-02  4.98290583e-02
 -7.68416375e-02 -2.68755667e-02  4.13242579e-02  3.01302150e-02
  1.89155675e-02 -6.24299496e-02 -7.42252246e-02  2.09289771e-02
  7.77346492e-02  3.83547470e-02  1.17308088e-03  3.37919109e-02
 -2.94998307e-02 -1.80585235e-02 -2.73336470e-02  7.12224171e-02
 -9.21389181e-03 -5.47188222e-02 -5.12421429e-02  5.42915277e-02
 -5.62829003e-02  1.56827364e-02 -7.66493678e-02  1.17425770e-02
  3.13602760e-02  1.08001018e-02 -5.31602986e-02  8.90110061e-02
 -1.64950006e-02 -4.03926000e-02  2.67658457e-02 -9.22859758e-02
  7.43080899e-02  3.34404707e-02  1.12309847e-02  9.94533226e-02
  4.79124002e-02  9.15408880e-02  4.86737862e-02  4.93692011e-02
 -6.84858300e-03  3.67574245e-02 -2.98433118e-02 -3.85171957e-02
  2.40140539e-02  1.27139241e-01 -2.54036207e-02 -2.59600714e-08
 -1.32647932e-01  5.04381284e-02  5.47044650e-02 -9.30189562e-04
  3.65476869e-02 -7.03784637e-03  8.20328891e-02  4.63165976e-02
 -3.44434530e-02  1.18579760e-01  1.26193576e-02 -3.70883495e-02
 -8.23761970e-02 -2.34063827e-02  1.03220651e-02  3.26364525e-02
 -4.29747021e-03  3.52259213e-03  4.06813622e-02 -4.45480831e-02
  5.10026738e-02  3.43055725e-02  2.20131911e-02  2.61081252e-02
  3.23161073e-02 -6.15430959e-02 -7.59178540e-03  1.42903492e-01
 -4.90780808e-02 -2.96568684e-02 -3.55416536e-03  3.20289247e-02
  1.09538235e-01 -6.62191957e-02  1.06492750e-01  6.69295043e-02
  2.79598543e-03  1.41155887e-02 -2.18934659e-02 -3.03393486e-03
 -6.23204261e-02  5.45254014e-02  7.85937160e-03 -5.82288578e-02
 -1.44419074e-02  1.74182151e-02  3.09830271e-02 -5.71763851e-02
 -2.34451867e-03  3.94422524e-02 -1.96410716e-02 -4.28767353e-02
  2.37910952e-02  6.64525433e-03  2.23794896e-02  2.55996156e-02
 -8.79103970e-03 -3.81616205e-02  4.08941805e-02 -3.50619182e-02
 -5.64891472e-02  4.45191711e-02 -3.47767174e-02  6.14739172e-02]",2,2
google-cloud-aiplatform,1,vertex ai google vertex ai is an integrated suite of machine learning tool and service for building and using ml model with automl or custom code it offer both novice and expert the best workbench for the entire machine learning development lifecycle client library documentationproduct documentationin order to use this library you first need to go through the following step select or create a cloud platform project enable billing for your project enable the vertex ai api setup authentication install this library in a virtualenv using pip virtualenv is a tool to create isolated python environment the basic problem it address is one of dependency and version and indirectly permission with virtualenv it s possible to install this library without needing system install permission and without clashing with the installed system dependency python python the last version of this library compatible with python is google cloud aiplatform this section provides a brief overview of the vertex ai sdk for python you can also reference the notebook in vertex ai sample for example all publicly available sdk feature can be found in the google cloud aiplatform directory under the hood vertex sdk build on top of gapic which stand for google api codegen the gapic library code sits in google cloud aiplatform v and google cloud aiplatform v beta and it is auto generated from google s service proto file for most developer programmatic need they can follow these step to figure out which library to import look through google cloud aiplatform first vertex sdk s apis will almost always be easier to use and more concise comparing with gapicif the feature that you are looking for cannot be found there look through aiplatform v to see if it s available in gapicif it is still in beta phase it will be available in aiplatform v beta if none of the above scenario could help you find the right tool for your task please feel free to open a github issue and send u a feature request sdk functionality can be used from the root of the package initialize the sdk to store common configuration that you use with the sdk vertex ai provides managed tabular text image and video datasets in the sdk datasets can be used downstream to train model to create a tabular dataset you can also create and import a dataset in separate step to get a previously created dataset vertex ai support a variety of dataset schema reference to these schema are available under the aiplatform schema dataset namespace for more information on the supported dataset schema please refer to the preparing data doc the vertex ai sdk for python allows you train custom and automl model you can train custom model using a custom python script custom python package or container preparing your custom codevertex ai custom training enables you to train on vertex ai datasets and produce vertex ai model to do so your script must adhere to the following contract it must read datasets from the environment variable populated by the training service please visit using a managed dataset in a custom training application for a detailed overview it must write the model artifact to the environment variable populated by the training service running trainingin the code block above my dataset is managed dataset created in the dataset section above the model variable is a managed vertex ai model that can be deployed or exported the vertex ai sdk for python support automl tabular image text video and forecasting to train an automl tabular model to get a model to upload a model to deploy a model please visit importing model to vertex ai for a detailed overview the vertex ai sdk for python currently support getting model evaluation metric for all automl model to list all model evaluation for a model to get the model evaluation resource for a given model you can also create a reference to your model evaluation directly by passing in the resource name of the model evaluation alternatively you can create a reference to your evaluation by passing in the model and evaluation id to create a batch prediction job you can also create a batch prediction job asynchronously by including the sync false argument to create an endpoint to deploy a model to a created endpoint to get prediction from endpoint to undeploy model from an endpoint to delete an endpoint to create a vertex ai pipeline run and monitor until completion to create a vertex ai pipeline without monitoring until completion use submit instead of run to get metadata in dictionary format from tensorflow model to get metadata in dictionary format from tensorflow model to use explanation metadata in endpoint deployment and model upload cloud profiler allows you to profile your remote vertex ai training job on demand and visualize the result in vertex ai tensorboard to start using the profiler with tensorflow update your training script to include the following next run the job with with a vertex ai tensorboard instance for full detail on how to do this visit http cloud google com vertex ai doc experiment tensorboard overviewfinally visit your tensorboard in your google cloud console navigate to the profile tab and click the capture profile button this will allow user to capture profiling statistic for the running job read the client library documentation for vertex ai api to see other available method on the client read the vertex ai api product documentation to learn more about the product and see how to guide view this readme to see the full list of cloud apis that we cover,"[('visit http cloud google com vertex', 0.583), ('vertex ai pipeline', 0.5683), ('google cloud aiplatform first vertex sdk', 0.5641), ('google vertex', 0.5047), ('hood vertex sdk', 0.4887), ('google cloud aiplatform v beta', 0.4807), ('cloud apis', 0.4751), ('sdk vertex', 0.4614), ('google cloud aiplatform v', 0.4599), ('cloud platform project', 0.4446)]","[-1.29553095e-01 -7.89072271e-03  4.82383324e-03 -5.36890365e-02
  3.41927144e-03 -2.46072616e-02 -6.55757636e-02  4.51553948e-02
 -3.56198922e-02  3.31845023e-02 -3.07458732e-02 -6.55595511e-02
 -2.21185051e-02 -1.46056842e-02  3.83757316e-02  5.01759984e-02
  5.80742955e-03  4.73975576e-02 -2.82015596e-02 -8.54422525e-02
  2.46369112e-02 -9.50783677e-03 -7.84387961e-02 -1.01935363e-03
  3.14355679e-02 -1.33735966e-02 -3.12578008e-02 -7.91057870e-02
  6.85249567e-02 -4.96059172e-02 -8.65352154e-02 -9.56879333e-02
  1.64481793e-02  9.27574709e-02 -9.61100624e-04  1.45874871e-02
 -2.90559791e-02 -5.92304133e-02 -1.39833475e-02 -4.17766199e-02
  2.76121031e-02 -7.77472556e-02 -5.37414029e-02 -7.37252757e-02
  7.71971196e-02  4.42738310e-02 -5.80192730e-02 -7.35086063e-03
  5.72089590e-02  3.16064426e-04 -4.32298332e-02 -1.05708756e-01
 -8.74167010e-02 -3.15079652e-02 -6.08487986e-02 -2.03181468e-02
 -2.10392233e-02 -1.41308345e-02  7.16754571e-02  4.16998379e-02
  4.81525138e-02 -2.57422905e-02 -4.61306684e-02  8.10179412e-02
 -4.14175317e-02  7.05856383e-02  5.94466040e-03 -2.44357791e-02
  4.27826568e-02 -2.28685346e-02  2.97781569e-03 -7.65709020e-03
 -1.24433450e-01 -2.02604644e-02 -3.69260982e-02 -2.73715425e-02
  5.42855859e-02  3.79683413e-02  4.46983799e-02 -3.87872607e-02
 -3.59863900e-02  4.34408598e-02  5.57703115e-02  1.05455175e-01
 -5.19368574e-02  4.89274859e-02 -5.68366498e-02  3.25201973e-02
  8.20607040e-03 -4.44898494e-02  1.95482448e-02 -4.14874777e-02
 -1.16843255e-02  3.00735012e-02  2.64774840e-02  3.64085734e-02
 -5.26247267e-03 -3.66961136e-02  3.09388824e-02  5.58247045e-02
 -7.35815018e-02 -4.23274003e-02  4.09661680e-02 -3.03408466e-02
 -6.50324151e-02  6.05035387e-02 -8.56338535e-03  4.37176302e-02
  3.00565036e-03 -2.05337629e-02 -4.88480330e-02 -1.96188455e-04
 -6.33727154e-03 -7.59348124e-02  4.59910370e-02  5.47691621e-02
 -8.76874700e-02 -2.31350306e-02  5.07986769e-02  4.39110734e-02
 -3.58773619e-02 -2.45761126e-03 -2.80002747e-02  1.44976983e-02
 -6.35952875e-02 -3.15140113e-02 -6.89810440e-02  6.79284243e-33
  4.67421301e-02 -6.80164527e-03  7.91910365e-02 -5.83604053e-02
  2.07949970e-02 -2.24906020e-02  2.93271635e-02 -2.49223262e-02
 -6.06184751e-02 -2.94276141e-02 -6.97964728e-02  8.73497594e-03
 -3.44710760e-02  9.02613476e-02  3.87897454e-02 -5.37894443e-02
  3.01137138e-02  4.57902104e-02 -2.73174457e-02 -1.51059628e-02
  5.83779663e-02 -5.76529093e-02  7.30165094e-02  1.94154084e-02
  1.02824740e-01  6.83615059e-02  4.52923626e-02 -2.83850171e-02
 -8.39476008e-03  5.73565774e-02 -7.52978586e-03 -1.85152385e-02
 -3.41425315e-02  3.43941674e-02 -8.70654806e-02  3.23818363e-02
 -6.40578195e-02 -3.14001404e-02 -2.73585804e-02  3.41090485e-02
  4.80322987e-02  6.42787293e-02 -7.87882060e-02 -4.15681526e-02
  1.17083276e-02  2.54349429e-02 -6.04916783e-03 -1.48454132e-02
  9.05602947e-02 -8.51459429e-02 -2.26427633e-02  8.68964866e-02
 -4.06843908e-02 -1.71515010e-02  5.72994389e-02  5.49345184e-03
 -3.82505707e-03 -2.38435827e-02 -5.76574961e-03  2.37098821e-02
 -3.62523012e-02 -6.08515402e-04 -4.21372950e-02  3.70106027e-02
 -8.13144669e-02 -4.26508263e-02 -6.04903810e-02 -1.42522820e-03
  1.21316789e-02  1.26106583e-03 -1.85211878e-02  5.46348467e-03
  3.73816080e-02 -3.26612443e-02  4.38502384e-03 -7.09557310e-02
 -4.74991985e-02 -1.01194538e-01 -7.18990415e-02 -3.26816784e-03
 -1.35095775e-01 -1.44376289e-02 -2.24806387e-02 -4.41249833e-02
 -1.44780017e-02 -8.14921334e-02  6.26752973e-02 -2.15968341e-02
  2.00863113e-03 -3.46142543e-03 -1.27350897e-01 -3.04291639e-02
 -7.39297597e-03  2.47841235e-02 -2.58581713e-02 -7.56884503e-33
 -5.73188290e-02 -8.75784736e-03 -4.89714257e-02  7.74890035e-02
  3.79009731e-02 -3.78458388e-03  5.94211593e-02 -5.12655154e-02
  2.98420209e-02  1.74071878e-01 -3.35101001e-02  4.42159139e-02
  4.40630242e-02  8.89758952e-03 -1.16680199e-02  1.18976263e-02
 -4.94051427e-02 -1.01814643e-01  3.69268330e-03  8.69024661e-04
 -2.30356902e-02  1.56042567e-02  1.75548904e-02 -4.96608280e-02
  5.34085035e-02  2.81276787e-03 -6.42230287e-02  2.76931487e-02
  8.19499344e-02 -1.97724290e-02 -4.48319651e-02  3.11799813e-02
 -4.81250435e-02  1.96677614e-02 -1.83875524e-02 -1.34298764e-02
  2.58291345e-02  2.38776226e-02  5.94158843e-02 -4.38555740e-02
  6.41943067e-02 -3.71149704e-02  2.41927523e-02 -1.95410158e-02
  3.67599763e-02  9.91144851e-02  3.15296836e-02  4.68530841e-02
 -8.77092555e-02 -1.21112153e-01 -2.04871166e-02  4.11642939e-02
  4.39526048e-03  5.56835979e-02 -2.65603177e-02  2.44190288e-03
  1.23364441e-02  9.44385231e-02 -1.04184292e-01  1.79441273e-02
 -1.42052090e-02 -3.33069265e-02  1.09011829e-02 -3.73132224e-03
  8.98803957e-03 -1.70759927e-03  1.62904002e-02 -2.14753188e-02
 -4.93435226e-02  3.19536822e-03 -3.96924615e-02 -2.84138620e-02
 -5.11427224e-02  1.80016086e-03 -3.15224344e-04 -3.37637193e-03
 -2.37978045e-02  8.10196996e-03 -6.95749279e-03 -5.02016246e-02
  1.94261260e-02  6.17842749e-02  8.91492516e-03  5.33104278e-02
  7.64052123e-02  1.34579437e-02  5.80673516e-02  6.08959943e-02
  3.02804243e-02  1.22978248e-01 -8.05176049e-02  8.23074058e-02
 -2.97800321e-02  1.16717920e-01 -1.94556043e-02 -3.24988072e-08
  3.46190147e-02  8.08927566e-02  7.40789995e-02  7.21065095e-03
  2.99613681e-02 -1.01701785e-02  4.98721488e-02  1.45410448e-01
 -3.43967751e-02  3.24342196e-04 -5.60943708e-02 -5.08430153e-02
  3.25126504e-03  3.47318836e-02  9.45583507e-02  3.35548520e-02
  6.08491153e-03  4.61780932e-03  2.53716502e-02 -9.02858004e-02
 -1.86654227e-03  5.66319190e-02  1.57483406e-02 -4.95703109e-02
 -2.20322609e-02 -3.29921395e-02  4.55346443e-02  4.56549646e-03
 -5.42674959e-02  3.88191976e-02 -5.57796471e-02  8.74403119e-02
  6.44301698e-02 -1.04441755e-01  1.08860940e-01  3.85172553e-02
  1.26747359e-02  1.03242807e-02  7.00120404e-02  9.35767125e-03
 -4.26924648e-03  8.80754367e-02  7.18513951e-02 -1.51727572e-02
  5.09615429e-02  1.92126762e-02  3.92876975e-02 -2.34604534e-02
  9.22086590e-04  5.21387793e-02 -8.47636238e-02 -4.16665487e-02
 -6.86188936e-02  3.76404859e-02  1.81132909e-02  1.22485101e-01
 -4.23436835e-02 -1.43915251e-01  6.78893551e-02  4.20248136e-02
  4.94368374e-02  4.08181623e-02  2.89307851e-02 -2.60052420e-02]",2,2
higher,1,higher is a library providing support for higher order optimization e g through unrolled first order optimization loop of meta aspect of these loop it provides tool for turning existing torch nn module instance stateless meaning that change to the parameter thereof can be tracked and gradient with regard to intermediate parameter can be taken it also provides a suite of differentiable optimizers to facilitate the implementation of various meta learning approach full documentation is available at http higher readthedocs io en latest to install higher from pypi to install higher from source alternatively python setup py install will do the same thing if you use higher in your research and found it helpful please consider citing the following paper you have a model with parameter p where p t denotes the parameter at update timestep t you want to update the model through k step of optimization and compute gradient through the optimization process i e compute torch autograd grad p k p or obtain gradient that depend on this gradient pathway existing you are using some existing code for your model so the parameter are stateful preventing you from forming a graph with p t a node even if you roll your own solution you want to use optimization technique beyond normal sgd and torch optim optimizers don t let you optimize through them good news higher ha got you covered using our growing set of tool and utility function you can backpropagate through an unbounded number of model update step for all your meta learning need this library includes say your training code look like this to turn this into a differentiable version the following change should be introduced beware that when unrolling your optimisation like this for k all gradient and all activation of your model at each step is kept in memory meaning the memory footprint of your model is k time greater it is possible to use optimizers other that those found in torch optim a differentiable version must be implemented first this can be done by subclassing higher optim differentiableoptimizer and overriding the update method following the argument of the original assuming the logic of the optimizer being added follows the logic of those found in torch optim the step to follow are more or le you can find example of how to test for gradient correctness using finite difference method in test test optim py please note that some stability trick may be needed to avoid nan in the gradient see the higher optim differentiableadam implementation for example of mitigation strategy e g identify operation that yield exploding gradient e g typically those taking the square root of moving average which are intially zero and register a backward hook using x register hook on the input x to those function using the helper function get mask closure from higher optim see the changelog for release note higher is released under apache license version thanks to adam paszke whose gist wa the source of inspiration and starting point for our method for monkey patching arbitrary torch nn module thanks for the many intern researcher and engineer who helped road test early version of this library,"[('higher order optimization e g', 0.5921), ('e compute torch autograd grad p', 0.5024), ('optimizers', 0.5011), ('optimizer', 0.4818), ('optimization process', 0.4777), ('optimization technique', 0.4409), ('optimization', 0.4407), ('optimisation', 0.4334), ('differentiable optimizers', 0.4242), ('unrolled first order optimization loop', 0.4191)]","[-5.79917803e-02 -1.18247718e-02 -1.46846548e-02 -8.69242176e-02
 -1.54619301e-02 -7.44864568e-02 -1.98237393e-02  3.27403210e-02
 -7.38046393e-02  2.08493844e-02 -1.42771518e-02  1.00936167e-01
 -7.89327249e-02 -4.09302907e-03  2.06051338e-02  1.32834502e-02
 -4.03661951e-02  7.59899020e-02 -7.80876949e-02 -1.68668136e-01
  5.80563173e-02 -7.76226819e-02 -6.38188049e-03 -2.05671191e-02
 -2.85872240e-02  2.97956243e-02 -1.07833724e-02  3.67499478e-02
  8.61476362e-02 -4.40296307e-02 -7.51015963e-03 -3.65509018e-02
  6.37446567e-02  1.55553101e-02 -5.03118485e-02  5.38794696e-02
 -7.14039952e-02  3.27309631e-02  3.95312570e-02  2.84685064e-02
 -6.23232797e-02 -1.61250718e-02  2.21871305e-02 -5.35364598e-02
  8.61257017e-02 -3.71930264e-02 -2.04437170e-02 -5.16488440e-02
 -6.79216757e-02 -8.01216587e-02 -5.10364734e-02 -4.12427448e-02
 -9.38589498e-02 -3.42765786e-02  6.33744374e-02 -1.45050399e-02
  4.14554887e-02 -1.77444965e-02  3.21370326e-02 -4.78842258e-02
 -2.35612523e-02 -1.05847329e-01 -4.32230271e-02 -1.15138516e-02
 -8.76930170e-03 -3.84707190e-02  6.40205592e-02 -4.99948896e-02
  8.02951232e-02  8.58163983e-02 -3.83208580e-02 -8.55469052e-03
 -3.71325947e-02 -2.04847902e-02  1.71029996e-02  4.79195863e-02
  9.91347358e-02  9.86548979e-03 -3.68646570e-02 -8.72715265e-02
  1.54095618e-02  1.52056338e-02 -7.21919769e-03  2.59082438e-03
 -5.08702314e-03  3.72671410e-02 -4.81358357e-02 -4.77372110e-02
  8.13589916e-02  1.40658692e-02 -1.46980826e-02 -2.85631660e-02
 -9.53690633e-02  4.77441214e-03 -9.77904070e-03 -6.95535680e-03
  4.51329648e-02 -6.59071505e-02 -3.61764282e-02  3.42329666e-02
  3.19665857e-02  3.14594172e-02  5.16225547e-02 -1.08298119e-02
 -4.76807207e-02 -4.53300625e-02  2.54697278e-02  9.04411152e-02
  3.71311717e-02 -3.11859120e-02 -5.51146269e-02  6.35364875e-02
  2.44071167e-02 -1.04886726e-01 -3.49539034e-02 -1.38369128e-02
 -2.38730144e-02 -8.53478536e-03  6.46719635e-02  7.12464303e-02
 -3.50122117e-02 -8.59954488e-03 -3.79844680e-02  4.53085303e-02
 -6.52116537e-02  2.73617101e-03 -9.50631052e-02  5.04386626e-33
 -1.01816669e-01  1.21696386e-03 -3.64481732e-02 -7.85497203e-02
 -1.06058049e-03  1.14899380e-02  2.99880635e-02 -7.12241139e-03
 -3.68783511e-02  4.58243862e-02  1.25718052e-02  6.04811758e-02
 -6.13087974e-02  8.70599300e-02  6.81057647e-02 -9.46508050e-02
  2.58085653e-02  7.10006878e-02  1.60275307e-02 -7.75047066e-03
  7.12816119e-02 -3.15364189e-02  4.44236398e-02  2.51301490e-02
  3.67782973e-02  6.03105873e-02  8.66807476e-02  1.96421146e-02
 -9.13400277e-02 -4.28720703e-03  2.16362023e-04 -1.48749826e-02
 -2.92971935e-02  1.55138178e-02  2.67767254e-02  4.58839238e-02
 -2.88732480e-02 -4.22541834e-02  3.71774733e-02  4.33493815e-02
 -5.60952444e-03  2.97938827e-02 -1.12702921e-02  9.69868246e-03
 -2.28839535e-02 -4.64609452e-03 -3.78476307e-02  1.03403606e-01
  5.42928278e-02 -5.58763072e-02 -7.06671625e-02  4.13567871e-02
  2.12124772e-02 -1.18059926e-02  5.11320308e-02  3.12862433e-02
  5.48281744e-02  1.65532771e-02  4.53792661e-02  9.13816094e-02
 -3.77165638e-02  2.15327460e-02 -1.83226950e-02 -6.60344362e-02
  1.76108006e-04 -3.93362455e-02 -1.73504092e-02 -5.33452556e-02
  3.44696455e-02  2.30530538e-02 -7.15274960e-02  4.76895012e-02
  4.71153520e-02 -1.83719918e-02  3.40010002e-02  8.72011855e-03
 -1.63871944e-02 -5.03902994e-02 -9.38881561e-03 -4.24170680e-02
 -1.00486338e-01  1.40752405e-01  4.39741462e-02 -2.02117618e-02
 -1.36860376e-02 -3.26647535e-02 -1.94102079e-02  4.05177660e-02
 -4.29447740e-02 -7.56074637e-02 -1.68284699e-01  3.48122492e-02
  9.30103213e-02  3.47885638e-02  7.30374176e-03 -5.68847173e-33
 -3.17621082e-02 -1.20196082e-02 -4.20425087e-02  8.18664134e-02
  5.29902801e-02  3.45436558e-02 -2.31885668e-02 -1.25971273e-01
 -2.09394619e-02 -2.65921596e-02  1.68610681e-02  4.01877686e-02
  2.36960407e-02  1.01536438e-02  5.45305572e-02 -5.53861866e-03
 -5.10517322e-02 -6.63762912e-02  1.99708417e-02  2.37410534e-02
  7.37083470e-03  9.66920406e-02 -6.49380237e-02  1.78497692e-04
 -5.17137833e-02 -3.18412296e-02 -2.26504859e-02  4.39892150e-02
  1.93856545e-02  3.05641852e-02  3.87389325e-02  5.73568493e-02
 -3.65042463e-02  1.49083152e-01 -8.43924582e-02  4.16274108e-02
  6.87655658e-02 -1.93611160e-02  1.91876125e-02  5.37872538e-02
  6.67516291e-02  9.06004198e-03  8.75329003e-02 -5.27145676e-02
 -2.85160020e-02  1.95353664e-02 -8.19249228e-02 -1.48592778e-02
 -6.43651932e-03  1.07815964e-02  4.55533247e-03 -4.82470617e-02
 -5.69777340e-02 -1.61237530e-02 -3.04717179e-02 -1.00679500e-02
 -2.52623111e-02 -4.46163043e-02  8.43431950e-02 -1.80477116e-04
  4.01959429e-03 -1.53192151e-02  6.71236143e-02  9.72579978e-03
 -6.26074895e-02  2.23580450e-02 -4.88802791e-03 -2.48578172e-02
 -5.37567846e-02  7.66582042e-03 -2.24922635e-02  6.56867325e-02
  2.52914894e-02  7.00578764e-02 -2.64650974e-02 -4.08828072e-02
  2.86405310e-02  7.39427134e-02  2.37150826e-02 -6.28225654e-02
  5.70919923e-02  6.36258572e-02 -1.96714308e-02  1.26395732e-01
 -8.57677013e-02  5.75873367e-02 -1.50842257e-02  3.33254002e-02
  2.71517504e-02 -9.15767848e-02  6.28876407e-03 -3.25183161e-02
  7.09782615e-02  2.76641883e-02 -4.24893796e-02 -2.93211766e-08
 -3.46042998e-02 -6.76499829e-02  8.31458196e-02  1.55819813e-02
  3.20529789e-02 -8.04395229e-03 -4.43353876e-03  9.30734277e-02
  4.25844640e-02 -9.07168686e-02 -8.63484573e-03 -1.61238741e-02
  4.34812829e-02  2.59419642e-02  3.67709994e-02  1.80230103e-02
  9.60104819e-03  2.01689284e-02  1.24419127e-02 -1.34427398e-01
 -9.86631513e-02  4.59626317e-02 -3.38532613e-03  7.32581364e-03
  9.37109515e-02 -4.75630909e-02  2.21407339e-02 -5.41947894e-02
  3.41435224e-02  7.30922222e-02 -3.24035791e-04  2.15672329e-02
  9.74655598e-02  1.06328530e-02  9.60944891e-02  1.19929098e-01
  2.35025622e-02  7.80139212e-03 -7.58348554e-02  2.41654962e-02
 -7.14943977e-04  3.65430452e-02 -1.46229444e-02 -4.99462290e-03
  1.97258759e-02 -1.05305584e-02 -5.89060374e-02 -8.29198882e-02
 -4.08444442e-02  5.77935902e-03  2.69536469e-02  6.71100318e-02
 -6.53780103e-02 -4.01630113e-03  8.67549563e-04  3.19708697e-02
 -1.14181861e-02 -9.16580707e-02  8.02777894e-03  1.31752724e-02
  2.48332322e-02  2.47466061e-02 -1.36991199e-02 -2.20275833e-03]",2,0
snorkel-metal,1,v announcement we are excited to have achieved a new state of the art score on the glue benchmark and four of it component task using snorkel metal check out the corresponding blog post for an overview of how we did it the code we used to accomplish this wa part of a significant restructuring of multi task end model in snorkel metal to make it a easy a possible to perform massive multi task learning mmtl with supervision at varying level of granularity and over an arbitrarily large number of task that mmtl package ha been released a a part of snorkel metal v along with a basic tutorial additional tutorial showing more advanced usage e g using a pre trained bert network a a shared input module using multiple label set supervising at the token and sentence level simultaneously etc will be released in future minor version update though such functionality is already supported stay tuned on other development in the snorkel ecosystem at our project landing page snorkel stanford edu this project build on snorkel in an attempt to understand how massively multi task supervision and learning change the way people program multitask learning mtl is an established technique that effectively pool sample by sharing representation across related task leading to better performance with le training data for a great primer of recent advance see this survey however most existing multi task system rely on two or three fixed hand labeled training set instead weak supervision open the floodgate allowing user to add arbitrarily many weakly supervised task we call this setting massively multitask learning and envision model with ten or hundred of task with supervision of widely varying quality our goal with the snorkel metal project is to understand this new regime and the programming model it entail more concretely snorkel metal is a framework for using multi task weak supervision mt provided by user in the form of labeling function applied over unlabeled data to train multi task model snorkel metal can use the output of labeling function developed and executed in snorkel or take in arbitrary label matrix representing weak supervision from multiple source of unknown quality and then use this to train auto compiled mtl network snorkel metal us a new matrix approximation approach to learn the accuracy of diverse source with unknown accuracy arbitrary dependency structure and structured multi task output this make it significantly more scalable than our previous approach if you are looking for help regarding how to use a particular class or method the best reference are in order this sample is for a single task problem for a multi task example see tutorial multitask ipynb note for snorkel user snorkel metal even in the single task case learns a slightly different label model than snorkel doe e g here we learn class conditional accuracy for each lf etc so expect slightly different hopefully better result install anaconda instruction here http www anaconda com download clone the repository create virtual environment run unit test if the test run successfully you should see dot followed by ok check out the tutorial to get familiar with the snorkel metal codebase or to use snorkel metal in another project install it with pip first read the metal commandment which describe the major design principle terminology and style guideline for snorkel metal if you are interested in contributing to snorkel metal and we welcome whole heartedly contribution via pull request follow the setup guideline above then run the following additional command this will install a few additional tool that help to ensure that any commits or pull request you submit conform with our established standard we use the following package after running make dev to install the necessary tool you can run make check to see if any change you ve made violate the repo standard and make fix to fix any related to isort black fix for flake violation will need to be made manually metal support gpu usage but doe not include this in automatically run test to run these test first install the requirement in test gpu requirement txt then run,"[('multitask learning mtl', 0.6289), ('multi task model snorkel metal', 0.606), ('multitask learning', 0.5657), ('structured multi task output', 0.5394), ('task supervision', 0.5321), ('multi task', 0.5164), ('massive multi task', 0.5115), ('multi task system', 0.4896), ('many weakly supervised task', 0.4895), ('multi task end model', 0.4895)]","[-2.70428825e-02 -4.72752191e-02  3.94259617e-02 -9.92831867e-03
  4.48786989e-02 -2.97601111e-02 -5.65863252e-02  1.90728344e-03
  4.92131989e-03 -1.67941861e-02 -3.85757983e-02 -7.56404549e-02
  8.13900009e-02  5.51297367e-02 -2.85805743e-02 -4.28816155e-02
  2.08444912e-02  3.35914120e-02 -6.08820766e-02 -3.74486037e-02
  9.56983045e-02 -1.75436772e-02  1.19146239e-02  3.55614200e-02
 -3.11835539e-02  4.34749052e-02 -5.96546605e-02 -8.57255533e-02
 -1.84200630e-02 -4.05101888e-02 -2.04658937e-02  1.88681278e-02
 -4.13292833e-03  2.98459753e-02 -3.63314338e-02  9.32708904e-02
 -4.64922078e-02  1.52922273e-02 -1.57036018e-02 -9.28434450e-03
 -1.81816593e-02  1.16160962e-04  2.98730396e-02 -5.86553998e-02
  8.57970193e-02 -4.72970679e-02 -6.22571148e-02 -1.44152761e-01
 -2.56376825e-02  2.39499416e-02 -4.98778410e-02 -9.63328034e-02
 -1.56131638e-02  1.15066186e-01  2.97993068e-02 -7.92238489e-02
 -2.74048802e-02 -1.07820462e-02  9.70733538e-03 -3.50567512e-02
 -2.17256346e-03 -6.76594526e-02 -7.23393783e-02 -1.66098326e-02
  9.50331092e-02  4.04842757e-02 -6.80534020e-02 -2.87872776e-02
  2.15169284e-02 -4.11894955e-02 -7.82687888e-02  4.65244753e-03
 -4.74677645e-02  3.49456305e-03  1.52238850e-02  4.13047057e-03
  6.61389008e-02 -7.80072287e-02  1.73442680e-02 -8.39071646e-02
 -5.07469364e-02  3.94987762e-02 -4.26958688e-02 -9.08411071e-02
  1.20541729e-01 -1.89423072e-03 -8.70216414e-02  7.57916942e-02
 -7.41637172e-03 -9.50942189e-03  3.89178842e-02 -4.54433821e-02
 -3.90132219e-02  5.79492003e-02 -7.51435310e-02 -3.53381075e-02
 -4.72556539e-02 -2.51828432e-02 -2.70105042e-02  1.69627108e-02
  5.17368875e-02 -5.21523738e-03  9.14788395e-02  1.61432605e-02
  1.12387659e-02 -3.59235652e-04 -1.30172623e-02  3.32810543e-02
  1.42268157e-02 -8.08320865e-02 -3.25884745e-02  1.59789454e-02
 -3.11183780e-02 -2.44881995e-02  2.57152808e-03  3.62901203e-02
 -6.09459765e-02  2.27006078e-02  6.48948029e-02  1.21552005e-01
 -9.46373716e-02 -3.61515358e-02  4.06798758e-02 -3.60767469e-02
  2.52863113e-02 -6.43244907e-02 -4.85566184e-02  6.17496880e-33
  1.32395457e-02 -1.13391075e-02  3.22969407e-02 -2.96679977e-02
  6.51008189e-02 -6.18024580e-02  3.84399407e-02  4.65178378e-02
 -9.01396479e-03 -3.63985449e-02 -1.56090287e-02  1.32008463e-01
 -4.95229401e-02  1.12907276e-01  1.74016953e-02 -2.77181510e-02
  2.20301785e-02 -2.50855065e-03  1.61621552e-02  3.91695127e-02
  5.45812584e-02  7.83546269e-02  2.32172851e-02  5.47668338e-03
  5.25094718e-02  4.49361764e-02  5.09823523e-02 -2.71675624e-02
 -5.88759854e-02  6.59422800e-02 -5.59305474e-02  6.80643925e-03
 -2.62495242e-02  7.74698257e-02  4.44578789e-02  4.93248887e-02
 -3.11995670e-02 -2.04298608e-02  6.34751841e-02 -8.63594711e-02
 -8.72025117e-02 -6.44076392e-02 -2.77600847e-02 -1.00263366e-02
 -6.65040803e-04  7.02680787e-03 -2.55974289e-02 -1.07261054e-02
 -4.23485562e-02 -6.76836148e-02  3.66664715e-02 -5.25202788e-03
 -8.08181521e-03 -8.81672055e-02 -4.43862267e-02  3.56610119e-02
 -3.97407450e-02  4.80310507e-02  5.35509475e-02  4.79478575e-02
  1.57631817e-03  1.77242141e-02 -6.37260824e-03  8.48434307e-03
  1.07698463e-01  1.51496967e-02  1.16499290e-02 -5.66592030e-02
  7.25554973e-02 -1.18628666e-02 -3.52292694e-02  4.65985611e-02
  3.99350151e-02  5.30795082e-02  4.56599109e-02 -3.05843651e-02
  3.29249464e-02 -1.04188800e-01 -4.92599281e-03  1.19280532e-01
 -1.96216106e-02  1.73020754e-02  6.17761863e-03 -6.64692223e-02
 -4.22553867e-02  1.36800809e-02 -8.30848049e-03 -1.11057609e-01
 -2.03847270e-02  1.07344734e-02 -9.52148288e-02  6.32079840e-02
  2.41204025e-03  4.36776616e-02 -5.96732236e-02 -5.09333841e-33
  8.48191045e-03  5.30084744e-02 -9.13611054e-02  6.02959469e-02
  1.47203887e-02  5.05001992e-02 -2.16502529e-02 -4.88018282e-02
 -8.72133225e-02  1.88114326e-02  4.73346300e-02 -5.10751903e-02
 -2.78397184e-02 -8.31144489e-03  2.03344431e-02 -6.27856888e-03
 -1.99587792e-02  2.48006321e-02  5.38329035e-03  3.57769802e-02
 -6.04913745e-04  1.03595696e-01 -3.73552293e-02  9.17719975e-02
  2.89790276e-02 -2.25988738e-02 -7.60371238e-02 -2.44400408e-02
  4.29624580e-02  2.47569829e-02  1.11307241e-02 -5.40276207e-02
  1.15536496e-01  2.55666347e-03 -3.82714681e-02 -1.39101152e-03
 -6.76992489e-03 -8.68608151e-03  1.69903226e-02  3.58920060e-02
  7.11517632e-02  4.20397706e-03  4.95734252e-02  3.71594280e-02
  1.58156846e-02 -1.04820663e-02 -1.28474489e-01  2.33181752e-02
 -3.29803005e-02 -3.53119709e-02  4.72388975e-03 -1.93553958e-02
 -8.59078094e-02 -6.08941466e-02  1.76911913e-02 -2.34436598e-02
  6.10803366e-02 -1.06075801e-01 -2.97768507e-02 -4.97084595e-02
 -4.03718278e-02 -2.17210897e-03  6.87715262e-02  6.34165034e-02
  9.36427191e-02  4.67546992e-02  2.01857481e-02 -6.68803304e-02
 -8.84761438e-02 -6.43482106e-03  2.08589919e-02  4.74104099e-02
  4.14035562e-03 -2.31682528e-02  4.26531956e-02 -4.29652743e-02
 -9.11232978e-02  1.49005912e-02 -5.87457567e-02 -6.20961301e-02
 -1.89682224e-03 -7.34491944e-02  3.35892811e-02  1.09101504e-01
 -6.18864608e-04  4.86037992e-02  7.93242306e-02  8.28721076e-02
  3.73373181e-02 -6.86806962e-02  5.90353273e-03  3.60810943e-02
  1.40343988e-02  4.91041914e-02 -2.67828461e-02 -2.98299128e-08
 -7.82592595e-03  2.63169240e-02  8.76935199e-03 -2.09468473e-02
  3.54746506e-02  4.86744568e-02 -1.41573362e-02  1.22554109e-01
  4.81453724e-02  6.20280840e-02  8.55191611e-03 -4.64091673e-02
 -2.80847829e-02 -5.37078343e-02  5.11178039e-02  3.19302431e-03
  8.71719196e-02  7.13156760e-02  4.09082659e-02 -4.31016237e-02
  1.24665894e-01 -3.51488171e-03  3.80112790e-02  7.02474965e-03
  4.48813811e-02 -4.73753214e-02 -7.32223466e-02  9.23669785e-02
  7.38622062e-03  2.06385367e-02  7.82070402e-03 -2.37461794e-02
 -4.55117133e-03 -2.76812799e-02  3.53758931e-02  8.30087885e-02
  6.67032003e-02 -1.63296405e-02  3.74183059e-03  5.72900623e-02
 -3.72546278e-02  7.17763677e-02  1.79948490e-02  4.81428728e-02
  3.01046055e-02 -1.29622854e-02 -5.54931052e-02 -5.74835576e-02
  3.10169887e-02 -4.65674326e-02 -1.10676792e-02  1.72509048e-02
 -3.91936414e-02  4.48082611e-02  4.56876270e-02  3.48996967e-02
  8.41421038e-02 -1.02072813e-01 -3.50984335e-02 -2.45274827e-02
  1.07701518e-01 -3.73741146e-03 -7.83788115e-02  6.80127889e-02]",2,0
lightning,1,build and train pytorch model and connect them to the ml lifecycle using lightning app template without handling diy infrastructure cost management scaling and other headache lightning gallery key feature how to use doc example community license lightning disentangles pytorch code to decouple the science from the engineering once you re done building model publish a paper demo or build a full production end to end ml system with lightning apps lightning apps remove the cloud infrastructure boilerplate so you can focus on solving the research or business problem lightning apps can run on the lightning cloud your own cluster or a private cloud browse available lightning apps herelightning structure pytorch code with these principle lightning force the following structure to your code which make it reusable and shareable once you do this you can train on multiple gpus tpus cpu and even in bit precision without changing your code get started in just minuteslightning is rigorously tested across multiple cpu gpus tpus ipus and hpus and against major python and pytorch version simple installation from pypia lightningmodule defines a full system ie a gan autoencoder bert or a simple image classifier note training step defines the training loop forward defines how the lightningmodule behaves during inference prediction lightning ha over advanced feature designed for professional ai research at scale here are some example for complex professional level work you have optional full control of the training loop and optimizers in the lightning v release lightninglite now enables you to leverage all the capability of pytorch lightning accelerator without any refactoring to your training loop check out the blogpost and doc for more info the lightning community is maintained bywant to help u build lightning and reduce boilerplate for thousand of researcher learn how to make your first contribution herelightning is also part of the pytorch ecosystem which requires project to have solid testing documentation and support if you have any question please,"[('pytorch lightning accelerator', 0.6227), ('pytorch version', 0.474), ('pytorch model', 0.4711), ('lightning cloud', 0.4567), ('available lightning apps', 0.4243), ('pytorch code', 0.4127), ('lightning community', 0.3918), ('lightning apps lightning apps', 0.3845), ('inference prediction lightning', 0.3786), ('structure pytorch code', 0.3777)]","[-1.23593420e-01 -9.76132751e-02 -2.11933348e-02  3.35373916e-02
  5.73000275e-02  1.29570458e-02 -3.42995487e-02  2.52227448e-02
 -4.22300473e-02 -2.58577503e-02 -7.34757306e-03 -2.59516761e-03
 -9.75244418e-02  7.18817522e-04  5.05124964e-02  3.35822701e-02
 -1.77006485e-04  9.05367360e-02 -3.82171646e-02 -1.18197232e-01
 -1.19767739e-02 -2.82348692e-02 -5.20862360e-03  5.72335161e-02
  5.88713214e-02  1.91985816e-02 -1.15394900e-02 -5.24097346e-02
  4.21228558e-02  9.28435102e-03 -4.13567238e-02 -4.56420146e-02
  6.52512386e-02  8.22637305e-02 -1.71568487e-02 -3.33333872e-02
 -6.16126060e-02 -1.05621153e-02 -4.18642582e-03 -2.80176084e-02
 -1.43976435e-02 -3.17578465e-02 -6.34009093e-02  2.03903932e-02
  3.70170921e-02  1.89316906e-02  1.63369831e-02 -7.55867586e-02
 -8.13968405e-02 -3.74849364e-02 -1.61898155e-02 -7.46531188e-02
 -4.76934426e-02  5.09072505e-02  8.82889796e-03 -3.19445059e-02
  3.32925059e-02 -9.83507652e-03  8.91309008e-02  1.30875651e-02
 -1.94312762e-02 -1.24165285e-02 -4.01125737e-02  5.06269149e-02
 -5.31435013e-02  4.27281782e-02 -1.14898318e-02  4.26603034e-02
  1.11880921e-01 -1.39699996e-01  5.10718208e-03 -2.87666153e-02
 -3.30658853e-02  4.22955267e-02 -2.21373737e-02 -4.33699414e-02
  6.43528774e-02  1.88737605e-02 -4.49744202e-02 -1.09628499e-01
 -6.90940907e-03 -1.86496526e-02 -8.22401885e-03  9.25374627e-02
  1.53470319e-02  5.81677221e-02 -6.78637549e-02  8.84199739e-02
 -3.88420909e-03 -3.05666234e-02  5.17037548e-02 -4.22477685e-02
  2.41791923e-02  7.18613714e-02 -1.94018614e-02  5.00163659e-02
  5.07375747e-02 -1.23547256e-01 -7.35814571e-02  2.52858344e-02
 -4.50373814e-02 -1.68451797e-02  3.09107322e-02 -3.72371301e-02
 -8.77454411e-03  1.94822326e-02  2.49666348e-02  5.19600604e-03
  7.02371895e-02  4.38808873e-02  3.88242044e-02 -3.07154022e-02
 -3.66834849e-02 -6.69057891e-02  1.18442677e-01 -3.00018992e-02
 -5.45891784e-02  8.36650431e-02  5.89937530e-02  4.14426029e-02
 -3.42541859e-02  3.68923508e-02  5.61315892e-03  5.19195059e-03
 -5.01290224e-02 -1.31350821e-02 -1.05616756e-01  6.17806770e-33
  1.30570063e-03 -5.92523953e-03 -5.58452588e-03 -3.07475720e-02
  4.49171104e-02 -1.08851105e-01  9.38296020e-02  5.15225763e-03
  1.42346732e-02 -5.03153503e-02  2.17370111e-02  1.87807512e-02
 -3.66645828e-02  5.59098870e-02  8.07476975e-03 -2.23212689e-02
 -6.15852699e-02  4.27014753e-02  3.51487808e-02  6.13766648e-02
  4.36536036e-02 -2.19412427e-02 -2.65332926e-02  1.43266007e-01
 -6.03748187e-02  2.88949441e-02  3.18603441e-02 -9.05119721e-03
 -3.52935530e-02  1.24926856e-02 -1.13682030e-03  4.54499424e-02
  1.16601018e-02 -1.52736083e-02 -4.74224351e-02 -4.76314453e-03
 -2.01722458e-02 -4.96322215e-02 -2.81309672e-02 -4.52795923e-02
 -1.12215549e-01 -1.66615658e-03 -9.34688523e-02 -2.61155032e-02
  5.43794408e-02 -6.26792312e-02 -5.96266910e-02  4.46650013e-02
  1.94046758e-02 -1.40691502e-03 -2.13426519e-02  3.64586078e-02
 -1.78274382e-02  6.28362820e-02  9.44871269e-03  4.35034791e-03
  6.23494899e-03  4.29777242e-02  1.32514670e-01  3.77746187e-02
  5.65417390e-03 -4.32943646e-03  4.75778133e-02 -9.57868174e-02
 -3.70238796e-02  3.33686545e-02 -6.88309819e-02 -3.09077837e-02
 -7.64269605e-02  1.05152532e-01 -6.44758344e-02  2.10176948e-02
 -4.14429419e-02  1.19161624e-02  4.85904850e-02 -4.98985946e-02
 -2.52777841e-02  4.81913425e-03 -2.15167049e-02 -3.50996703e-02
 -1.15259901e-01  1.39051452e-02  2.77061835e-02  5.73890693e-02
 -4.19454016e-02 -4.80713658e-02  2.92665465e-03  1.74038392e-02
 -3.94593813e-02  5.75355254e-03 -5.57927005e-02 -9.04100668e-03
  3.86758633e-02  4.32209298e-03 -1.05509795e-02 -4.98323096e-33
  2.46657152e-02  4.20845672e-02 -1.17546558e-01  9.42771360e-02
  1.11853443e-02 -2.71267425e-02 -5.50389253e-02 -4.74450439e-02
 -7.20321434e-03  2.07397807e-02  3.26214507e-02  4.48615775e-02
  2.58091390e-02 -2.56708520e-03  4.22068462e-02  8.97823181e-03
 -4.27806266e-02 -2.32171994e-02  8.21198151e-03  5.49262278e-02
 -4.38119099e-02  5.12744952e-03 -4.33775224e-02 -4.26063500e-02
 -4.45820056e-02 -8.50838795e-03  2.12077005e-03 -3.13391201e-02
  7.15297982e-02 -4.93417457e-02  2.12404542e-02  5.32950237e-02
 -8.89755692e-03 -1.57450736e-02 -4.97748666e-02 -1.94782652e-02
  5.73852845e-02 -4.34536077e-02 -3.32036354e-02 -3.34482603e-02
  1.41144589e-01  2.79271174e-02  5.28335869e-02 -2.87187584e-02
  2.20480072e-03  7.68648908e-02 -4.27855700e-02  3.94940451e-02
  2.14367118e-02 -2.91938498e-03  3.77396084e-02  3.05092931e-02
 -3.99272889e-02  2.27327514e-02  6.02816511e-03 -1.94092398e-03
  8.86783898e-02  3.28943804e-02  1.71298329e-02 -5.88996941e-03
 -8.88287723e-02 -6.79199472e-02  4.26368676e-02  3.52848954e-02
 -2.29661632e-02 -5.21351472e-02 -4.38356809e-02  5.93260042e-02
 -6.14530267e-03 -2.88375579e-02  6.00900948e-02  1.53374439e-02
  4.72823121e-02  4.52783480e-02 -8.84022862e-02  3.98274586e-02
  7.92196393e-02  2.44695158e-03 -2.50120228e-03 -7.48798996e-02
  8.44761953e-02  7.71508738e-02  7.59555623e-02 -1.76588502e-02
  3.18948850e-02  3.50860804e-02  3.43795605e-02  5.38212061e-02
  1.81695577e-02 -3.94543968e-02 -3.59113589e-02  2.53589321e-02
  6.53803349e-02  7.37953782e-02  2.42336597e-02 -3.23942864e-08
 -3.49296890e-02  7.64287934e-02  2.59997863e-02  4.04347554e-02
  4.95518520e-02  1.91482306e-02  2.52793841e-02  1.42021328e-01
  5.18084737e-03  1.48233995e-02 -3.76140722e-03 -5.12844399e-02
 -5.71032055e-02  2.07928400e-02  6.92351013e-02  1.01332463e-01
  5.87027799e-03 -1.30421331e-03  3.57022509e-02 -9.03540757e-03
 -3.45893279e-02  1.00199804e-02 -4.65581417e-02  1.15407696e-02
 -2.38904115e-02  3.07006342e-03  2.13618241e-02  2.10268535e-02
 -4.38703876e-03 -2.45345943e-02 -1.08670807e-02  1.33136911e-02
  1.25824079e-01 -1.02823853e-01  1.50013536e-01  7.80209079e-02
 -5.16055003e-02 -4.42703962e-02  3.75230797e-02  5.36336154e-02
 -6.84262589e-02 -9.79136527e-02  1.23579567e-02 -4.30179015e-02
  1.53338686e-02 -1.04391912e-03 -1.00825774e-03 -1.21285953e-01
 -5.63069880e-02  8.22026879e-02 -1.22013623e-02  3.08382120e-02
 -2.65291519e-02 -4.52899151e-02  7.43847806e-04  1.58428356e-01
 -4.57690842e-02 -8.58908296e-02 -2.30223704e-02 -6.30750805e-02
  4.83459271e-02  3.76067832e-02 -1.30930953e-02  3.33423056e-02]",2,2
seqio-nightly,1,seqio is a library for processing sequential data to be fed into downstream sequence model it us tf data dataset to create scalable data pipeline but requires minimal use of tensorflow in particular with one line of code the returned dataset can be transformed to a numpy iterator and hence it is fully compatible with other framework such a jax or pytorch currently seqio assumes that the dataset is a sequence i e each feature is one dimensional array modality such a text or audio are naturally supported image are supported a long a they are represented a sequence e g image gpt we will release this constraint in the future in order to support higher dimensional data seqio is a refactor of the t data library used in conjunction with the mesh tensorflow transformer implementation to train the t model introduced in exploring the limit of transfer learning with a unified text to text transformer if you have used t data in the past and want to know how seqio differs please read this section at a high level we use seqio with the following step define a task and optionally a mixture define or use an existing a featureconverter based on the model architecture use the top level function seqio get dataset to obtain the tf data dataset instance we will look at each of these step in detail the most important class in seqio is the task it is an abstraction that combine oftentimes a task line up with a common benchmark in this tutorial we use wmt english german machine translation task in the end our task will look like this we typically add the task to the global registry when we define it a shown above to make it easier to use with model configs and flag thus it must have a unique string name wmt ende in this case note however that you may also instantiate a seqio task directly without adding it to the registry if desired we ll now break down each part of the task definition data source are the first step in your pipeline providing a way to load raw data in many format a a tf data dataset all data source are subclass of the datasource base class and are defined in dataset provider existing implementation include in our example we are using the tfdsdatasource we specify the name of the wmt dataset in tfds wmt translate the specific config for the language pair that excludes the context for the open domain setting de en and the version number the output feature field expects a dictionary that map string feature name to seqio feature object this defines what the task is expected to produce in it output example the output example may contain additional field but they must contain these field in the specified format or exception will be raised each feature includes note specifying these option on feature doe not by itself ensure the proper transformation are applied you must also include the necessary preprocessors the task used in t all produce input and target feature to be consumed by the text to text model for a decoder only language model only a single feature e g target would be necessary nevertheless seqio is flexible enough to generate arbitrary output feature what will be converted into model feature by the featureconverter later in the pipeline preprocessors are function that transform one tf data dataset into a new tf data dataset typically this involves executing a map over the given dataset the preprocessors provided to the task will be executed sequentially a an example let s look at the previously undefined translate from the wmt ende example above the tfds dataset provides the dataset where each example ha the form de da ist gut en that is good we convert this to input and target with the appropriate prompt to inform the model of the task a few important note when instantiating a task the preprocessor function can have the following argument dataset output feature and sequence length the first positional dataset argument is always required if an argument named output feature is provided the output feature mapping will be passed to the preprocessor if sequence length is provided a mapping from feature name to it maximum final sequence length provided by the caller will be passed any sequence that are too long after preprocessing will be automatically truncated if a preprocessor function doe have other argument they must have default value or be bound e g with functools partial a used in translate before instantiating the task mapping function operate on and return tf tensor using tensorflow operation this is more flexible than it may sound see tf data dataset documentation for more detail note that translate take a input an individual example then seqio map over dataset decorates it to a function that take in a tf data dataset instance if num seed the arg will instead be called seed and will contain a sequence of seed in our wmt ende task we also use the predefined preprocessors seqio preprocessors tokenize and seqio preprocessors append eos the former us each feature vocabulary to tokenize it and the the latter appends feature vocabulary eos id to the feature if the feaure add eos is true see preprocessors py for their implementation and other useful preprocessors during evaluation the model output are first detokenized using the output feature vocabulary before passing these prediction to the metric function they can be run through a python postprocessing function alongside the full input example similarly the raw target are run through this function before being passed to the metric since the postprocess function is used on both the model output and the target it is passed an is target boolean in case the behavior should be different it is also passed the fully preprocessed example including field that were excluded from output feature for the wmt ende we don t need any postprocessors see trivia qa open task in the advanced postprocessing task for an example postprocessor metric are function that are passed by the evaluator the fully materialized list of postprocessed model output or score and target and return a mapping from string name to metricvalue object containing their value these are most commonly floating point scalar but may also be text image audio histogram etc see metric py for the full list the first argument of a metric function must always be called target if the second argument of a metric function is called prediction it will be passed the decoded and detokenized model prediction if it is called score it will be passed a list of log likelihood score for each example if multiple metric function are provided they will all be used and their returned mapping merged prediction metric are computed using the postprocessed target and model output prediction the args must be named target and prediction let s look at the metric function used for wmt ende task a standard metric for the translation task is bleu and we use sacrebleu implementation score metric are computed using the postprocessed target and their log likelihood score according to the model the args must be named target and score once you have multiple task added to the taskregistry you can define mixture that will combine the example from them according to some specified rate example will then be sampled from each task in proportion to it rate a an example multilingual t us a mixture of per language task with tail language up weighted in the mixture there are way to specify the task and their rate you can also include mixture in your mixture for example the following task would contain from mix task from mix of task and task now that your task and or mixture is defined it primary functionality is to use it to generate a dataset you may first need to use seqio get mixture or task mixture or task name to access your dataset provider from the registry after that you can call get dataset to build the tf data dataset for example some note on a few the argument for improved performance at load time and avoid redundant computation for commonly used task you can pre cache your task with all or part of the preprocessing done in advance of training the first step to doing so is to add a seqio cachedatasetplaceholder required false a one of the step in your preprocessing pipeline all step before the placeholder will be cached offline and all step after will be executed on the fly at load time you may set required true if you want get dataset to fail unless use cached true caveat once your task is registered you can run cache task main to execute the offline preprocessing providing it with the module containing your task definition via the module import flag for very large datasets it s recommended you run this apache beam script on a distributed framework like google cloud dataflow finally you are ready to load the cached version of your task or mixture containing it you will need to add the path to the directory you passed to output cache dir via seqio add global cache dirs my cache dir now when you call task or mixture get dataset use cached true the data will be loaded from the cache directory instead of the raw data source the role of task is to provide the dataset object with a little model specific feature e g generic input and target while the feature converter transform the model agnostic feature to model specific feature e g encoder input token we refer to the former a task feature and the latter a model feature let s use machine translation english to german a a running example the raw data consists of sentence pair such asa task registered to task e g wmt t t ende v read these sentence pair from the data source and applies a series of preprocessors one of the internal representation look likethe final output from the task is a tokenized version of the parallel sentence in the following toy example the token id do not correspond to the above string example the dataset consists of example the format is in the tf data dataset i e each example is a dictionary with input and target field the featureconverter then take this a an input and convert to the model specific feature in addition the feature converter performs padding and optionally packing for model implementation that support it for efficiency for example let s assume that we are using the standard transformer architecture with an encoder and a decoder the output of the feature converter isin this case two task example are packed into one segment id and position are the field used to denote the membership and position of packed token in the original sequence the eos id i e are appended in addition each field is padded to the specified length we will look at the detail of this example in encoder decoder architecture seqio encdecfeatureconverter section we provide feature converter for three common architecture encoder decoder decoder only and encoder only here we describe how user can use the feature converter for each of these architecture out of the box a a part of the seqio library in the seqio library each architecture ha a class defining how the task feature are converted to model feature since these feature converter are already implemented it is straightforward to use them by providing the class a a feature converter argument of the seqio get dataset function the following section will show the example usage of seqio get dataset this is the architecture of the original transformer paper for the english to german translation task the following function call retrieves the tf data dataset object with the model feature the resulting dataset object ha the following fieldsthis architecture consists of a single autoregressive stack which we denote a a decoder a decoder autoregressively produce an output sequence therefore it can be used a a standard language model if the task dataset ha only target feature i e self supervised if the task dataset also ha an input field e g supervised machine translation the decoder can still be used by concatenating the input and target field see raffel et al section for more detailed take on this topic we support both us case and refer to the former a standard language model and the latter a prefix language model each of these model is described separately below note that we do not provide special feature to denote how the dataset should be consumed for example a transformer based fully autoregressive decoder ha a fully causal self attention layer since there are many way of implementing the masking pattern for such attention layer and more importantly seqio is not limited to attention based model we leave it up to the model implementation to apply the masking pattern there is one exception and we cover this in the prefix lm section below a common use pattern is to pretrain a decoder model with the left to right language modeling objective unsupervised using seqio lmfeatureconverter and then fine tune supervised using seqio prefixlmfeatureconverter for the standard language model the task dataset only ha target field therefore the sequence length specification only need to specify target note that standard lm is not a registered task in the codebase it is the left to right language modeling task i e predict the next token given the previous token on some language corpus e g c the output dataset ha the following model feature the decoder target token is a shifted version of decoder input token for the standard teacher forced autoregressive training if the input dataset ha a notion of input and target we can concatenate them so that we can still use a single stack decoder therefore the output only contains target just like standard lm case we use the same toy example for english to german translation task a a running example to be consumed by the decoder only stack seqio prefixlmfeatureconverter concatenates them form the new target consider layer decoder architecture whose activation are shown belowlet s u denote the first layer s activation in the ith position a vi similarly let ui denote the activation of the second layer in the ith position for attention based sequence model such a transformer decoder the self attention layer is used to encode contextualized representation of the sequence at a given layer each position s representation is computed a a function of the representation of the token before it position in the previous layer referring to the toy example when computing u with fully causing masking we do not use v this result in a representation u of the word is that doe not take into account the word good which is unnecessarily limiting for prefix lm this issue is resolved by having the fully visible masking pattern for the input portion only for example when computing u v v v v and v are all visible and taken into account for the token in the target of the task dataset we use the causal masking for example when computing u all vi for i are taken into account but not v seqio prefixlmfeatureconverter provides a feature decoder causal attention to encode this information for the above example we haveindicating that the non causal attention can be applied to the first five position note that this feature seems trivial but for a packed dataset the input and target boundary are more nuanced a final consideration for the prefix lm is that because we concatenate input and target which token are used for the loss computation is a modeling decision for example we can penalize the model only for the target token or we may choose to penalize building the representation for input token this is controlled by loss on target only argument default to true to seqio prefixlmfeatureconverter constructor in the above example we would getthis indicates that the last position are used for the loss computation to get the dataset with prefix lm feature we can usethe resulting feature have length because it concatenates input and target each with length the output dataset ha the following model feature note that the only additional feature is decoder causal attention like decoder only architecture this one is a single stack but not autoregressive one notable assumption is that the input and target are aligned i e they have the same sequence length and ith position in the target correspond to the output representation of the ith token in the input a common model using encoder only architecture is bert we provide encoder feature converter class to support the masked language modeling mlm objective from bert we assume that a unique sentinel such a mask token is used to mask some fraction of the input text and the task is to recover the original text therefore the target is naturally defined a the original text whereas input are the masked text encoder only model are often used for classification task in bert a special token cl is prepended to the input sequence the last layer s activation corresponding to this sentinel token is the contextualized representation of the sequence we assume that such classification sentinel is prepended consider the following example for the mlm task the input dataset ha two example which is packed to one example we assume that mask id and the cl token ha id of note that the packed sequence ha cl token at the beginning of each sequence also note that the loss is taken only on the masked position to use the pre defined encoderfeatureconverter provide mask id a an argument the resulting dataset object ha the following fieldsfor a model architecture you would need to create a subclass of featureconverter and override two method convert feature and get model feature length to define how task feature are mapped to the model feature including the length relationship the existing feature converter e g seqio encdecfeatureconverter follows the same pattern so this can be useful starting point todo hwchung the original t library introduced and implemented the t data task abstraction for specifying preprocessing and evaluation metric for text to text task when creating a task user specify a source dataset of raw text some preprocessing step a vocabulary for tokenization and evaluation metric the fully specified task can then be used to pre train or fine tune a encoder decoder transformer model however the design included many baked in assumption about the type of task user could specify seqio remove some of the constraint of this abstraction furthermore seqio ha been made more modular with respect to the mesh tensorflow transformer this allows it to be used with other model implementation with more consistency and much le code duplication this version of triviaqa wa introduced in robert et al in this example we are using the tfdsdatasource we specify the name of the triviaqa dataset in tfds trivia qa the specific config that excludes the context for the open domain setting unfiltered nocontext and the version number we also override the default split to match what is commonly used for the open domain setting specifically we set our test split to be the tfds validation split and create a small pseudo validation set by taking example out of the tfds train split the preprocessor tqa open preprocessor is defined a follows or with the seqio map overdataset decorator we havehere we made a thin wrapper to emphasize that the function decorated by seqio map over dataset take in an instance of tf data dataset in practice this wrapper is not necessary the postprocessor for this example is tqa open postprocessor which is defined a follows when processing the target we ignore output or target equivalent to example target since it is just selecting a single answer in trivia qa open instead we extract the full list of answer from the example and convert them from byte to text when handling the model output we simply convert it to text from detokenized byte the metric function tqa metric is defined a please use the following bibtex entry to cite seqio,"[('t data task abstraction', 0.4573), ('tensorflow', 0.4557), ('feature converter e g seqio', 0.4508), ('seqio task', 0.4505), ('seqio feature object', 0.4443), ('higher dimensional data seqio', 0.441), ('tensorflow operation', 0.4384), ('mesh tensorflow transformer implementation', 0.4353), ('tf data dataset', 0.4331), ('tfds dataset', 0.4317)]","[-4.58925590e-02 -2.58503091e-02  2.54120845e-02 -8.21590796e-02
 -1.17215058e-02 -4.42916378e-02 -1.74574722e-02  3.12940665e-02
 -1.16507979e-02  1.40904589e-02 -2.35930011e-02 -3.62182520e-02
 -4.25372310e-02  3.63983214e-02  2.43390277e-02 -1.40743377e-02
  4.17156108e-02  4.43196334e-02 -4.59556654e-02 -1.55162424e-01
 -4.12922241e-02  4.88157794e-02 -3.02410796e-02  8.26364607e-02
 -2.86819991e-02  1.45518212e-02  1.52723705e-02 -6.22238219e-02
  7.46162981e-03  4.77108657e-02 -7.50938058e-02  5.17823435e-02
  4.91517633e-02  1.83673874e-02 -6.31175041e-02  1.52376713e-02
 -1.87536571e-02 -1.92697197e-02 -9.72412825e-02 -3.66320238e-02
 -4.86496314e-02 -6.04748018e-02 -1.85197834e-02 -5.88777922e-02
  6.14152737e-02 -2.51044687e-02  3.19723897e-02 -1.10210396e-01
 -2.87170336e-02  2.82250773e-02 -5.54851890e-02  1.10594993e-02
 -7.43756592e-02  4.66226004e-02 -1.85623392e-02 -6.74879132e-03
  8.50265995e-02  2.00453885e-02 -6.55034278e-03 -3.43220457e-02
 -6.54705018e-02 -2.55301204e-02  3.72309349e-02  4.14135717e-02
  4.70210016e-02  2.15606671e-02 -5.61791211e-02 -8.08861665e-03
  4.68463451e-02 -1.28988385e-01 -5.83038852e-02 -1.79047533e-03
 -5.48089221e-02  1.13485046e-02 -4.95045483e-02 -2.90430579e-02
  8.16493705e-02 -6.51805922e-02  5.59142269e-02 -9.51496735e-02
 -1.54874884e-02  1.94457080e-02  1.98925771e-02 -9.83733684e-03
  6.38543023e-03 -3.80224665e-03 -3.70375812e-02  8.56981501e-02
 -2.76479572e-02  1.73734763e-04  6.97612241e-02 -3.18968110e-02
 -7.89488759e-03 -7.48702930e-03  6.51731193e-02  1.26702711e-02
 -6.13784194e-02 -5.87202469e-03  1.36198755e-02 -7.06834421e-02
 -8.09264705e-02 -1.93370748e-02  3.35690230e-02  4.73909415e-02
 -2.63696108e-02  5.42698056e-02  5.37293889e-02 -4.16383334e-02
  5.64117767e-02 -2.53911912e-02 -2.45734341e-02  3.88595909e-02
 -1.52725587e-02 -8.85632783e-02  3.96464206e-02 -2.63641383e-02
 -8.39130953e-02  8.38599876e-02  3.28465435e-03  9.31783542e-02
 -4.59855832e-02  1.56729985e-02 -2.52931230e-02  4.82915416e-02
  1.30796693e-02  1.34915207e-02 -4.47994210e-02  5.74632075e-33
  7.79512944e-03 -2.49203201e-02 -1.08441329e-02  3.47068161e-02
  2.74634026e-02 -4.41963896e-02  4.11702469e-02  4.30859961e-02
  3.83367948e-02  2.19560619e-02 -1.07111521e-01  7.70908669e-02
 -2.20241249e-02  9.29635912e-02  4.24405262e-02 -9.65790451e-02
 -2.20360234e-02  2.06294414e-02 -1.51842711e-02  4.68178093e-02
  1.05512947e-01  6.69269785e-02  3.87945808e-02  8.39142203e-02
  4.30240817e-02  3.86270620e-02 -4.44730744e-02 -1.18293343e-02
 -7.12510347e-02  1.57462172e-02 -9.44290310e-02  1.18557513e-02
 -5.51454909e-02  1.76507491e-03 -1.05215432e-02 -6.87305117e-03
 -5.41730644e-03 -1.77775323e-02  2.09543426e-02 -4.61531244e-02
  4.42080423e-02  5.43575883e-02 -2.11877432e-02 -1.62424576e-02
 -3.07922140e-02 -3.73962894e-02  4.76724021e-02  7.04675913e-03
  1.05686598e-01 -5.82368160e-03  2.88293697e-03 -3.82412001e-02
 -3.37151587e-02 -7.84076825e-02  7.14709014e-02 -3.09725460e-02
 -1.02995466e-02 -3.17750610e-02  7.31918663e-02  1.23907804e-01
 -8.71142000e-02  2.64883297e-03 -2.27311570e-02 -3.43713649e-02
 -1.03931250e-02 -5.01349531e-02 -2.61656996e-02  1.25007867e-03
  2.54077893e-02 -2.41080076e-02 -4.45840359e-02  9.50079933e-02
 -2.48237718e-02 -6.16699178e-03  1.37057006e-02 -3.90895642e-03
  3.25007997e-02 -7.75059164e-02 -4.17727008e-02 -1.66581571e-02
 -8.54580849e-02  3.76471058e-02  6.62689134e-02 -3.04504819e-02
  4.80789412e-03  7.71284709e-03 -3.25314775e-02  4.48686332e-02
  1.43419364e-02 -6.11379296e-02 -8.03384781e-02  3.42938006e-02
  3.62291783e-02  3.54106240e-02 -3.51284333e-02 -5.51269933e-33
 -3.50350514e-02  4.82323095e-02 -1.04677930e-01  2.32393779e-02
  3.55505086e-02  3.48650739e-02  4.24164422e-02 -3.64153869e-02
  6.53931722e-02  2.80462541e-02  5.91301639e-03 -6.36459813e-02
  4.35354188e-02 -1.17829934e-01  8.06135871e-03 -7.22528324e-02
 -4.86674458e-02 -6.87635243e-02 -7.25221559e-02 -1.02140242e-02
 -6.10163547e-02  3.09184883e-02 -7.16712698e-02 -3.61821726e-02
  2.98990356e-03  1.64204352e-02 -8.79554749e-02 -1.67254116e-02
  5.96797913e-02  6.37840182e-02  9.50472895e-03 -9.54361260e-02
  5.44865010e-03  9.57931951e-02  3.62255685e-02  9.59156454e-03
  6.27867207e-02 -4.89764243e-05  1.97809655e-02 -7.50330801e-04
  5.47116324e-02  3.83780599e-02  6.30980283e-02  7.48898238e-02
 -6.99067563e-02  5.09491488e-02 -1.07655257e-01  3.47942710e-02
 -5.61056808e-02 -4.26807888e-02  2.24927813e-02  5.54904528e-03
  2.97460705e-02 -6.47093207e-02 -1.54294940e-02 -1.10767456e-02
  1.50553167e-01 -6.13350235e-02 -5.31457216e-02  1.39743937e-02
  6.05276003e-02  2.44843261e-03  1.07148923e-01  1.90043300e-02
 -5.08164568e-03  6.25712425e-02 -5.66451102e-02 -1.76569186e-02
 -5.46062663e-02 -2.19253451e-02 -7.03862542e-03 -3.79202478e-02
 -2.42537148e-02 -1.97316427e-02 -7.45520135e-03 -7.81765431e-02
 -1.26625150e-02  2.28491109e-02  7.90356100e-02  1.45046422e-02
  5.35486825e-02 -2.93625495e-03  2.93976553e-02  4.90483269e-02
  3.48434336e-02  1.08606592e-01  8.69291555e-03 -3.75425778e-02
  4.80550937e-02 -1.37846749e-02 -4.03150171e-02 -1.71296149e-02
 -9.89723764e-03  1.64290294e-01  9.92970392e-02 -2.97642284e-08
 -8.07504579e-02  2.63249576e-02  7.82678947e-02 -1.76978521e-02
 -4.85400291e-04  1.39283184e-02  1.77215915e-02  1.48177981e-01
  3.90135869e-02  7.22250529e-03 -2.30735131e-02 -1.10023953e-02
 -6.36377707e-02 -1.09242098e-02  1.08370513e-01 -9.13468096e-03
  3.09780426e-03 -3.54649639e-03  4.43488173e-02 -3.50119807e-02
  6.22159466e-02  3.05104628e-02 -8.88161659e-02 -5.45452833e-02
  3.24330702e-02 -4.53869179e-02 -4.17664908e-02  4.15658019e-02
  2.35644430e-02  2.24469462e-03 -5.27275018e-02 -5.83028570e-02
  3.41512263e-02 -1.19257923e-02  9.25486386e-02  1.43592544e-02
  8.52080062e-02 -4.14591283e-02 -9.03147161e-02  2.42783595e-02
 -5.65658230e-03  7.71361738e-02  2.78800000e-02 -3.03680655e-02
  7.23137707e-02 -1.92493782e-03  4.27195290e-03 -5.41200396e-03
  4.32438403e-03  4.10739779e-02 -3.94903682e-02  6.56928718e-02
 -8.18526968e-02  1.06903546e-01  4.17315587e-02  4.84127812e-02
  1.13312919e-02 -9.89507288e-02  1.15206474e-02 -3.15265730e-02
  2.32888982e-02  6.58186749e-02 -4.28835303e-03 -2.75595654e-02]",2,2
flowvision,1,the flowvision package consists of popular datasets sota computer vision model layer utility scheduler advanced data augmentation and common image transformation based on oneflow first install oneflow please refer to install oneflow for more detail then install the latest stable release of flowvisionplease refer to doc for full api documentation and tutorialsplease refer to changelog for detail and release historywe have conducted all the test under the same setting please refer to the model page here for more detail in flowvision we support two way to create a model modelcreator model table return a tabular result of available model in flowvision to check all of pretrained model pas in pretrained true in modelcreator model table you can get the result like it is easy to search for model architecture by using wildcard a below you can get the result like modelcreator model list ha similar function a modelcreator model table but return a list object which give the user a more flexible way to check the supported model in flowvision you can get the result like you can get the result like this is a utility library that downloads and prepares public datasets we do not host or distribute these datasets vouch for their quality or fairness or claim that you have license to use the dataset it is your responsibility to determine whether you have permission to use the dataset under the dataset s license if you re a dataset owner and wish to update any part of it description citation etc or do not want your dataset to be included in this library please get in touch through a github issue thanks for your contribution to the ml community,"[('flowvision package', 0.6438), ('flowvision', 0.5959), ('popular datasets sota computer vision model layer utility', 0.5862), ('flowvisionplease', 0.5407), ('modelcreator model list', 0.4685), ('modelcreator model table', 0.4281), ('model modelcreator model table', 0.4277), ('available model', 0.3981), ('model architecture', 0.3867), ('model pas', 0.315)]","[-9.81581677e-03 -7.31990263e-02  1.08026508e-02 -6.70946017e-02
  9.46190804e-02  3.40545326e-02  5.60967950e-03  6.88278452e-02
 -1.89096760e-02  1.85356960e-02  1.79899018e-02 -8.05619434e-02
 -3.57686207e-02  1.45020299e-02  1.00257341e-02 -3.60297784e-02
 -3.07638794e-02  6.63904175e-02 -3.28423665e-03 -2.33142953e-02
  9.56526678e-03 -1.10112941e-02 -2.23681740e-02  4.40143347e-02
 -3.78830507e-02  7.12640509e-02  3.09289806e-02  9.13686585e-03
 -1.28337378e-02 -7.58595169e-02 -8.56031328e-02  1.59660224e-02
  6.44893348e-02  3.34312432e-02  5.52097568e-04  9.03394073e-03
  6.55079931e-02 -7.90101755e-03 -8.18706602e-02 -1.27119590e-02
 -6.59688264e-02  9.49887559e-03 -7.79900094e-03 -4.92780805e-02
  8.14949572e-02  1.36161763e-02 -3.14502046e-02 -9.40513685e-02
 -2.49780826e-02 -3.59890126e-02 -9.80195478e-02 -1.74319819e-02
 -2.61580329e-02  1.01593221e-02  4.45315288e-03 -1.81216244e-02
 -1.13721136e-02 -6.19426779e-02 -2.50830166e-02  9.32618976e-04
 -5.71047887e-02 -3.97709236e-02 -5.52551709e-02  4.73404676e-02
 -7.75336325e-02  4.95032184e-02 -3.03921402e-02  5.76942191e-02
  1.11026593e-01 -1.14873707e-01 -1.07488193e-01  7.89687131e-03
  7.43267499e-03  1.73333716e-02 -5.62225133e-02 -3.53408307e-02
  1.30931243e-01 -1.56404767e-02  2.37753019e-02 -1.34645596e-01
  2.60387734e-02  5.56084998e-02 -1.20816557e-02  1.32083558e-02
 -3.08762919e-02  3.05899736e-02 -4.71153781e-02  5.82016185e-02
  2.42300127e-02  2.06193700e-03 -3.12745422e-02 -1.81566980e-02
  2.38291174e-02 -6.20165877e-02  6.30620718e-02  3.14213447e-02
  1.53210694e-02 -1.06591590e-01 -1.79530196e-02 -1.18541969e-02
 -3.29476446e-02 -5.41114248e-02  1.01957284e-01  2.04709787e-02
 -2.76039075e-02  1.89160171e-04  3.19181569e-02  1.32602369e-02
  1.05295889e-01 -1.05423825e-02 -1.37310810e-02 -1.92031879e-02
 -4.19909991e-02 -1.54300526e-01  2.43508872e-02 -1.96861718e-02
 -1.64232329e-01 -3.86893703e-03 -3.85495485e-03  4.52179946e-02
 -8.01611692e-02 -5.88166341e-03  2.57710535e-02 -5.23124970e-02
 -3.42376530e-03  6.11682003e-03 -7.74000064e-02  4.13569035e-33
  2.84057242e-05  2.70474497e-02 -8.97458475e-03 -6.73034322e-03
  5.10729253e-02 -1.97061393e-02  8.62402245e-02  2.67558228e-02
 -5.45867579e-03 -7.44585553e-03 -7.63473734e-02  5.71357086e-02
 -1.39952153e-01  1.58794194e-01  1.15993414e-02 -1.61182843e-02
 -1.41559122e-02  7.47960433e-02 -3.04894093e-02  4.74207215e-02
  6.55733943e-02 -2.64178589e-02  5.75779863e-02  6.42466247e-02
  8.37007612e-02  6.41683415e-02 -1.66474620e-03  5.35991862e-02
 -8.30787234e-03  1.71259288e-02  6.55945614e-02  2.07106788e-02
  2.40128115e-02 -3.63548240e-03  4.40714806e-02  1.51912244e-02
 -5.62374033e-02 -2.63312794e-02  8.71869847e-02 -1.39952693e-02
  1.56631116e-02  7.06999674e-02 -1.08721696e-01  1.23838596e-02
 -3.50100957e-02 -5.08610345e-02  1.56719459e-03  1.05783463e-01
  1.96145605e-02  1.09216990e-02  3.65398489e-02  7.27473662e-06
 -5.41478805e-02 -2.09250748e-02 -5.21556474e-02  7.17317732e-03
 -2.44446713e-02  3.10532525e-02  2.60997862e-02  4.89368886e-02
 -5.99681512e-02  5.80045953e-02 -3.34873493e-03 -8.71058181e-02
  4.95133325e-02 -1.71590894e-02 -3.19057703e-03 -3.98133434e-02
 -2.17420813e-02  5.56820212e-03 -1.12576768e-01 -2.41537073e-05
 -3.46613079e-02 -9.50496364e-03  6.99272230e-02 -7.71632697e-03
 -6.83215186e-02 -3.20351198e-02 -5.84690534e-02  6.17309213e-02
 -1.54771701e-01  2.39013787e-02 -1.32767130e-02  2.50446852e-02
  1.56194447e-02  2.93602366e-02  6.54321611e-02 -1.10997586e-02
 -4.63087112e-02 -2.47175507e-02 -1.10060386e-02  1.39758242e-02
  1.24235023e-02  1.58372112e-02 -5.84135856e-03 -4.85826562e-33
  2.64002904e-02  4.22371738e-02 -2.88849846e-02  2.93124560e-02
  4.28291038e-02 -1.77839063e-02  4.31978330e-02 -2.10486073e-02
  4.44119647e-02 -2.97528319e-02  5.90902008e-02 -4.07926599e-03
 -1.76895212e-03 -5.05768545e-02  7.14331269e-02 -3.00681405e-02
 -2.10342668e-02 -1.10271662e-01 -2.96394955e-02 -3.41025786e-03
 -6.75690128e-03  8.50461796e-02 -7.48174414e-02 -3.82587872e-02
 -3.84730436e-02  1.06020728e-02 -2.02593897e-02 -1.81253683e-02
  1.04873709e-01 -1.14365621e-03  1.58847272e-02 -3.86335626e-02
 -2.34275055e-03  3.52170058e-02 -6.30634725e-02 -4.74003749e-03
  4.37579304e-02 -1.54111376e-02 -2.25499216e-02  2.81155817e-02
  2.55426783e-02  2.20319759e-02  1.92471538e-02  4.86225113e-02
 -4.90331985e-02 -2.98410319e-02 -3.22568938e-02  1.20929899e-02
 -1.11330980e-02 -4.59775329e-02 -8.59379917e-02  7.50153046e-03
 -6.02477640e-02 -1.80478096e-02  2.02042307e-03  1.32070137e-02
  3.95220779e-02  4.88828532e-02  2.17762217e-02 -3.34986299e-02
 -9.40838363e-03 -5.68633759e-03 -7.87627138e-03  4.13689762e-02
  2.68568248e-02  3.12958360e-02 -6.46418780e-02 -1.08748585e-01
 -4.94968928e-02 -4.51302202e-03 -2.26708669e-02  2.03012656e-02
 -2.18191464e-02  2.93774903e-02  9.31722112e-04 -3.21086198e-02
 -1.12202711e-01  1.00395128e-01  9.34106857e-02  1.63749326e-02
  1.79776791e-02 -9.77364089e-03  5.64126410e-02  7.54338950e-02
  9.72918868e-02  4.97336425e-02  3.44122527e-04 -5.22663146e-02
  3.67785543e-02  3.82653438e-02 -5.35225607e-02 -3.67827490e-02
 -9.52889957e-03  1.18109941e-01 -1.68850319e-03 -2.90811961e-08
 -5.48635870e-02  2.30906177e-02  5.30591793e-02 -4.28000055e-02
  1.52106294e-02 -1.67066567e-02  1.20152961e-02  1.46263450e-01
  1.37776239e-02  3.46312113e-03  2.52156667e-02  3.03325392e-02
 -2.32420284e-02 -1.86128449e-02  8.68802369e-02  2.00855453e-02
  6.15737513e-02  6.69824108e-02 -1.61717515e-02 -1.62117034e-02
 -6.71822065e-03 -5.83805004e-03 -2.77468376e-02  3.07151559e-03
  3.22122537e-02 -2.54890192e-02 -2.30952445e-02  8.21721833e-03
 -1.93449594e-02  2.81269923e-02  6.41950816e-02 -6.58305455e-03
  1.25396252e-01 -1.03695944e-01  1.52264923e-01  4.69508991e-02
 -2.34985240e-02  6.71627233e-03 -2.46853940e-03  1.93801206e-02
 -1.52859511e-02 -2.93990858e-02  2.13951454e-03 -2.32711527e-02
  3.62694822e-02  7.13170022e-02  1.77014172e-02 -8.61652493e-02
 -4.40200977e-02  1.31156622e-02 -6.23786859e-02  3.06719858e-02
 -8.35947022e-02  7.29556456e-02 -4.63629737e-02  5.41914925e-02
  7.71173984e-02 -1.22768320e-01  3.48518826e-02 -1.30835734e-02
 -5.37567995e-02  8.56006965e-02  3.12641226e-02  1.07190423e-02]",2,2
auraloss,1,a collection of audio focused loss function in pytorch pdf we categorize the loss function a either time domain or frequency domain approach additionally we include perceptual transforms wang et al also propose a multi resolution spectral loss that engel et al follow but they do not include both the log magnitude l distance and spectral convergence term introduced in arik et al and then extended for the multi resolution case in yamamoto et al currently we include an example using a set of the loss function to train a tcn for modeling an analog dynamic range compressor for detail please refer to the detail in example compressor we provide pre trained model evaluation script to compute the metric in the paper a well a script to retrain model there are some more advanced thing you can do based upon the stftloss class for example you can compute both linear and log scaled stft error a in engel et al in this case we do not include the spectral convergence term there is also a mel scaled stft loss which ha some special requirement this loss requires you set the sample rate a well a specify the correct device you can also build a multi resolution mel scaled stft loss with bin easily make sure you pas the correct device where the tensor you are comparing will be we currently have no test but those will also be coming soon so use caution at the moment future loss function to be included will target neural network based perceptual loss which tend to be a bit more sophisticated than those we have included so far if you are interested in adding a loss function please make a pull request if you use this code in your work please consider citing u,"[('spectral loss', 0.4165), ('stft loss', 0.3607), ('analog dynamic range compressor', 0.3528), ('future loss function', 0.337), ('perceptual loss', 0.3255), ('loss function', 0.3202), ('multi resolution mel', 0.3034), ('stftloss class', 0.2765), ('pytorch pdf', 0.2753), ('example compressor', 0.2618)]","[-3.10131963e-02 -1.03864241e-02 -8.43273625e-02 -1.56168276e-02
 -1.20107755e-02  7.02632815e-02 -2.85723284e-02  4.44958732e-02
 -2.41786037e-02 -2.77773030e-02 -2.59608161e-02 -7.26605672e-03
 -9.78406053e-03 -6.67889938e-02  3.60469194e-03 -4.88769915e-03
 -7.74715081e-05  8.65038410e-02 -7.76668563e-02 -1.47202648e-02
  1.00744106e-01  1.03666941e-02 -1.77362154e-03  3.23565304e-02
 -1.58451602e-03 -6.40593544e-02 -5.53365275e-02  2.22888333e-03
  4.11452018e-02 -4.32235561e-02  5.13190925e-02 -1.08675640e-02
  6.62147403e-02 -7.69399689e-04 -2.54464317e-02 -4.14633565e-02
  3.28101479e-02 -4.54490632e-02 -4.45743976e-03  1.83443278e-02
 -4.14694063e-02  1.59216672e-02  8.99682101e-03  3.16538177e-02
 -3.78101915e-02 -4.72572297e-02  6.29995912e-02 -1.02621853e-01
 -6.97165653e-02 -6.22649193e-02 -3.48180458e-02  2.82029491e-02
 -9.77364853e-02  4.13002595e-02  3.35087664e-02  3.38100046e-02
 -1.24336366e-04 -2.49678846e-02 -3.05482224e-02 -3.63513455e-02
  2.35047936e-02 -4.37358096e-02 -3.10803223e-02  1.01469262e-02
  2.86430623e-02  2.04597842e-02  5.95607460e-02 -1.25535503e-02
  1.11473873e-01 -3.68061699e-02 -1.49041548e-01 -1.91560332e-02
 -1.72147602e-02  9.20420140e-02  5.28137125e-02  4.85278219e-02
  6.81326911e-02  1.13698775e-02 -3.26833688e-02 -3.19671258e-02
  7.14856312e-02  1.17604844e-02 -4.07176241e-02 -4.93739024e-02
  2.00253911e-02  4.38240692e-02 -6.66174144e-02  2.15991065e-02
 -2.60155164e-02 -4.32044789e-02 -4.49510738e-02 -6.48152903e-02
 -5.90476692e-02 -1.33386478e-02 -2.48502474e-02 -3.82150076e-02
  1.32921217e-02 -6.29471317e-02  9.69613343e-02  1.95373222e-02
  2.39294767e-02 -5.70544042e-02  3.18437777e-02  4.48795501e-03
 -1.27512306e-01 -6.74830973e-02 -2.41700076e-02  3.60330530e-02
  2.59742737e-02 -3.01955780e-03  3.09306066e-02  9.72042046e-03
 -6.34256601e-02 -9.45973024e-02  1.21553563e-01  4.78537455e-02
 -3.59697156e-02  1.90940301e-03  5.27249761e-02  7.41282059e-03
 -1.32509634e-01 -3.64417173e-02 -6.41596541e-02  1.99416112e-02
 -6.40893169e-03 -6.56134710e-02 -6.94696605e-02  3.28598977e-33
 -4.83674742e-03  3.31784715e-03 -4.13496755e-02 -1.37200663e-02
  4.59286869e-02 -1.59408730e-02 -5.51383756e-02 -6.28153328e-03
  8.73950347e-02  6.52271509e-02  4.40739561e-03  9.67894122e-02
 -1.06380340e-02  1.02353685e-01 -3.20960797e-04 -2.98015065e-02
 -4.73812409e-03  5.89257814e-02  2.12610997e-02 -2.31324360e-02
  4.72332686e-02  9.24009681e-02 -1.58002239e-03  2.63296776e-02
 -2.58236714e-02  1.08387396e-01 -3.02962549e-02 -1.05848936e-02
 -9.15844925e-03  2.58439817e-02 -2.00275742e-02  7.01570660e-02
 -6.60639331e-02 -8.80086869e-02  4.43047881e-02 -1.48348389e-02
 -8.10256973e-02  1.41150029e-02 -4.68020253e-02 -8.70311558e-02
 -7.70801753e-02  2.29685847e-02 -7.91460574e-02  3.37359458e-02
 -1.72712784e-02 -1.10718325e-01  7.29650110e-02  4.74838391e-02
 -1.52069505e-03 -3.49840447e-02 -5.06452061e-02 -2.78191939e-02
 -5.38356043e-02  7.01981410e-02  8.72090384e-02  5.51242828e-02
  3.74442339e-02  6.15316676e-03  5.15845008e-02  1.02573492e-01
 -7.70349205e-02  2.66246125e-02 -4.15092614e-03 -8.23072940e-02
 -3.86396749e-03 -7.96306878e-03 -3.39647345e-02 -4.71890494e-02
  1.73674952e-02  4.78214510e-02  5.40482923e-02  3.85953858e-02
  5.31614479e-03 -7.97326490e-02  5.69510870e-02  4.37046327e-02
 -1.83034837e-02  1.35330716e-03 -3.63614224e-02  1.86234154e-02
 -1.19319759e-01  2.65287161e-02 -1.91349164e-02 -2.56488975e-02
 -8.39881599e-02  4.91862297e-02 -1.22976741e-02 -4.04202975e-02
  1.44352932e-02 -5.84014058e-02 -1.05169535e-01 -1.87429376e-02
 -3.13373320e-02  5.60441241e-03  3.18576815e-03 -4.35942072e-33
 -1.30817788e-02  1.00177735e-01 -1.15480967e-01  9.10521522e-02
 -1.03666084e-02  5.56860380e-02  8.53567570e-03  4.44988757e-02
  7.01760000e-05 -5.68406619e-02 -2.42096018e-02 -2.62670889e-02
 -7.12300763e-02  3.18923704e-02 -5.31885698e-02 -4.14301679e-02
 -1.78098306e-02 -1.12372883e-01 -1.03118634e-02  3.81655842e-02
 -1.13009606e-02  9.50543955e-02 -7.20197409e-02  1.50861833e-02
  4.49420139e-02 -6.21467945e-04 -5.32145761e-02  1.75068527e-02
  3.54144536e-02 -1.61804408e-02  5.09578735e-02 -4.62058224e-02
  6.47596344e-02  8.19798931e-02 -2.83225849e-02 -2.80795731e-02
  6.20578155e-02  4.95579131e-02 -3.05227283e-02  1.78336147e-02
  1.06294356e-01  6.77273721e-02  1.08705409e-01 -1.66036319e-02
  3.13846245e-02  2.43792515e-02 -8.63735154e-02 -5.25104627e-02
  1.01931266e-01  6.30141869e-02  3.42860632e-02 -4.15338716e-03
 -6.54826015e-02 -2.17295997e-02 -1.26519985e-02 -3.39164445e-03
  6.68946654e-04 -9.74405482e-02  1.87198576e-02  1.18483648e-01
 -6.35565966e-02 -3.97445112e-02  1.39404004e-02  4.90962900e-02
 -3.73021164e-03 -2.72217374e-02 -5.84164672e-02 -1.57692302e-02
  7.34163299e-02  3.78594585e-02  2.23845076e-02  2.73597240e-02
  5.68170771e-02  2.56303139e-02 -1.28248043e-03 -1.08143110e-02
 -4.27089669e-02  6.96697682e-02 -2.24312600e-02  6.01992235e-02
  3.76383625e-02 -1.45757515e-02  6.70479089e-02  6.13533519e-02
  5.11887390e-03  8.30672830e-02  1.99545268e-02 -3.58505771e-02
  4.97754514e-02 -7.92028382e-02  2.14097220e-02  7.76366889e-02
  1.74702483e-03  1.39155006e-02  5.41329123e-02 -2.85972526e-08
 -6.61464259e-02  6.28136024e-02  8.05831179e-02  9.38964356e-03
  6.41737059e-02  5.40242065e-03  3.60851325e-02  5.47298677e-02
 -8.05859547e-03 -1.99763216e-02 -7.97009002e-03 -7.90596083e-02
 -2.61324085e-02  9.22775269e-03  4.48062755e-02 -8.71770456e-03
  3.88235003e-02  2.84257252e-02  2.52442453e-02 -3.09605543e-02
 -5.72679676e-02  1.78902056e-02  1.72558483e-02 -1.17355632e-02
 -3.94346565e-02 -3.30125727e-02  2.45571528e-02  2.38646958e-02
  7.65612163e-03  6.26662821e-02 -3.65406573e-02  7.33341649e-02
  3.49868312e-02 -5.83756529e-02 -2.07699146e-02  6.95240274e-02
  5.83068840e-02 -1.69766024e-02 -5.12640700e-02  8.61727223e-02
 -5.89222535e-02  4.08462137e-02 -6.58831373e-02 -3.77316400e-03
  8.12098533e-02  1.52295334e-02  5.12835309e-02 -6.16792254e-02
 -9.29751620e-02  6.50139749e-02  6.05671958e-04  1.02689445e-01
 -1.10680304e-01  1.82010755e-02  3.00842244e-02 -1.14292065e-02
 -3.41472961e-02 -4.03745808e-02 -4.24011648e-02 -3.49868927e-03
 -3.25178029e-03  3.27152386e-02  3.49469017e-03  2.32992545e-02]",2,0
basicsr,1,howto installation training command datasetprepare model zoo plot script introduction todo list faq we add basicsr example which provides guidance and template of using basicsr a a python package qq qq basicsr basic super restoration is an open source image and video restoration toolbox based on pytorch such a super resolution denoise deblurring jpeg artifact removal etc basicsr basic super restoration pytorch jpeg new feature updatesacmmm edge oriented convolution block for real time super resolution on mobile devicescvpr basicvsr the search for essential component in video super resolution and beyondif basicsr help your research or work please help to this repo or recommend it to your friend thanks other recommended project real esrgan a practical algorithm for general image restoration gfpgan a practical algorithm for real world face restoration facexlib a collection that provides useful face relation function handyview a pyqt based image viewer that is handy for view and comparison handyfigure open source of paper figure esrgan edvr dni sftgan handycrawler handywriting we provide simple pipeline to train test inference model for a quick start these pipeline command cannot cover all the case and more detail are in the following section if you use basicsr in your open source project welcome to contact me by email or opening an issue pull request i will add your project to the above list this project is released under the apache license more detail about license and acknowledgement are in license if basicsr help your research or work please consider citing basicsr the following is a bibtex reference the bibtex entry requires the url latex package xintao wang ke yu kelvin c k chan chao dong and chen change loy basicsr open source image and video restoration toolbox http github com xinntao basicsr if you have any question please email xintao wang outlook com,"[('basic super restoration pytorch jpeg', 0.5884), ('video restoration toolbox http github com xinntao basicsr', 0.5299), ('video restoration toolbox', 0.4778), ('general image restoration', 0.4376), ('basic super restoration', 0.4307), ('super resolution denoise', 0.4011), ('super resolution', 0.3788), ('restoration facexlib', 0.3744), ('python package qq qq basicsr', 0.3593), ('simple pipeline', 0.3421)]","[-9.89327282e-02 -7.38859624e-02  2.21148431e-02 -4.37831767e-02
 -1.06952165e-03 -4.31378111e-02 -5.33099696e-02  5.09320460e-02
 -1.14202060e-01 -2.75058136e-03  2.93782670e-02  6.87773079e-02
 -3.33443992e-02  7.59017770e-04 -1.40147600e-02  7.55716721e-03
 -3.94478403e-02  8.92452151e-02 -4.68790643e-02 -1.23608187e-01
 -1.06129153e-02 -4.03527878e-02 -3.98311950e-02 -7.04936013e-02
  5.21245226e-02  2.81929523e-02 -2.22269576e-02 -4.33518609e-04
  6.17855228e-02 -1.30442986e-02 -3.66012007e-02  3.39259133e-02
  1.06844120e-01 -1.63622648e-02  1.40619194e-02  4.71749492e-02
  5.31064197e-02 -2.24279184e-02 -2.98619047e-02 -3.38579006e-02
 -3.84982973e-02  6.27061399e-03 -6.23189434e-02 -1.09107703e-01
  4.04648036e-02  2.42279656e-02 -2.24646535e-02 -9.22169238e-02
  3.00169587e-02 -1.11889169e-01 -8.80662818e-03 -7.02359900e-02
 -8.20832029e-02  6.37159199e-02  5.12623787e-02  2.80052461e-02
 -6.28013571e-04 -6.47692680e-02  1.05240494e-02  1.47607895e-02
 -4.86252597e-03  6.57160906e-03 -5.05178384e-02  6.72668517e-02
  4.78543937e-02  2.89807096e-02 -2.00480260e-02  8.94865766e-03
  1.20063148e-01 -4.09907550e-02 -1.63005784e-01 -2.61749327e-02
 -1.23870140e-02  1.07719235e-01 -3.73636447e-02 -1.00445785e-01
 -1.10803200e-02 -1.59935802e-02 -2.97530796e-02 -1.10781798e-02
  2.23077331e-02 -4.31234315e-02  5.44154868e-02  6.26493916e-02
  1.44535238e-02  3.99456955e-02  7.20445300e-03  5.66710792e-02
  1.65516399e-02  8.71667732e-03  5.50127588e-02 -6.03351295e-02
  2.11055391e-02  2.44809724e-02 -3.90509367e-02  1.56620960e-03
  6.84771389e-02 -3.41968127e-02 -1.63076892e-02  1.68283600e-02
  1.17961392e-02 -1.57401472e-01  6.64156303e-02 -7.25348666e-02
  2.90493853e-02  9.78784915e-03  7.09536076e-02  4.22922596e-02
  8.70553628e-02  3.34334448e-02 -8.86888802e-02 -4.24230918e-02
 -8.90708864e-02 -1.09849311e-01  9.94183719e-02 -5.68812666e-03
  3.30288708e-02  6.64658323e-02 -4.11400534e-02 -9.83963236e-02
  2.15406716e-02  3.18171596e-03  1.41488090e-02 -7.38872960e-02
  4.97473404e-02  4.45955992e-02 -5.63965850e-02  5.38810244e-33
  6.62964284e-02  2.32929662e-02 -1.12282652e-02  1.32110086e-04
  5.77944070e-02 -2.47397404e-02  1.00730412e-01  1.64401829e-02
 -2.39960756e-02  2.04477254e-02  5.59936985e-02  4.41800356e-02
 -1.04936145e-01  4.22152467e-02 -6.62341341e-02 -3.03676408e-02
 -1.05770417e-01  1.44055421e-02  1.69438925e-02  5.11626527e-03
  5.35542630e-02  5.14668226e-02  2.23525409e-02  1.21490598e-01
 -3.31307203e-02 -3.22875343e-02  5.26851863e-02 -1.43336011e-02
  2.08298787e-02 -1.80107336e-02 -8.94086286e-02 -1.33904405e-02
  1.33679220e-02 -6.43068925e-03 -5.64507730e-02  2.17566895e-03
 -1.35484841e-02 -5.12902662e-02 -3.08724158e-02 -4.70426446e-03
 -6.32420480e-02  5.69503717e-02 -6.60211220e-02 -7.11225793e-02
  6.15729652e-02 -7.10202614e-04 -2.76819654e-02  1.55260578e-01
  3.67091596e-02 -2.39199307e-03 -2.83645578e-02 -1.21706761e-02
 -3.10304184e-02  7.81395473e-03 -4.63384241e-02  7.83205479e-02
  5.81067652e-02  7.06220716e-02  3.80739104e-03  8.30665790e-03
  5.88951893e-02 -1.22496774e-02 -1.50624011e-02  4.59915698e-02
 -5.99153992e-03  2.12656166e-02 -7.28486804e-03  3.21500227e-02
 -6.46548271e-02  5.93832284e-02 -1.00771032e-01  6.33792952e-02
  8.25970527e-03 -5.44743314e-02  7.96433389e-02 -1.80282642e-03
  1.62462948e-03 -1.65727176e-02 -3.54973674e-02 -3.69494106e-03
 -1.14003986e-01  6.64320067e-02  3.95778827e-02 -1.56361517e-02
 -4.61059101e-02  1.74819753e-02  7.18771070e-02  8.60575028e-03
 -5.25812898e-03 -6.95099160e-02 -3.95518653e-02 -1.64958239e-02
 -8.37815367e-03  5.78774214e-02 -2.36295070e-02 -4.45604452e-33
  6.74768463e-02  7.33972341e-02 -1.47156864e-02  1.25378996e-01
  1.33741703e-02  4.40429747e-02 -2.60014739e-02 -2.13849563e-02
  9.87504199e-02 -6.47060201e-02  1.66727360e-02  4.80898395e-02
  3.04470155e-02 -5.58865480e-02 -2.37847753e-02 -3.36995497e-02
 -2.56594177e-03 -8.08631033e-02 -6.86786771e-02  4.22814582e-03
 -1.36066731e-02  9.57407206e-02 -3.40806395e-02 -3.73271219e-02
 -7.39282444e-02 -4.32768557e-03  5.33454027e-03  3.42338160e-02
  8.22253823e-02 -1.38965286e-02  5.65569699e-02 -2.65339594e-02
  3.58441868e-03 -1.11858202e-02 -4.78153527e-02 -1.51042705e-02
  8.32960531e-02  1.19876880e-02 -6.64848238e-02  4.87213880e-02
  7.31967241e-02  4.23168652e-02 -3.02277356e-02  2.77890600e-02
  1.34330885e-02 -6.43575750e-03 -3.36628668e-02  1.03203133e-02
  1.63927011e-03  1.42561113e-02  1.07132895e-02 -9.65415537e-02
 -6.02421500e-02  5.12175299e-02 -1.05642211e-02 -6.32270472e-03
  6.16482198e-02  7.34707117e-02  1.56670995e-02  4.72196192e-02
 -4.51226048e-02 -6.29562363e-02 -3.54075879e-02  2.76820231e-02
 -8.58459175e-02  2.41944399e-02 -4.50263582e-02  5.79219200e-02
 -1.01539709e-01 -3.51166464e-02 -3.09095182e-03  1.24559291e-02
  5.81780039e-02  3.60635258e-02  8.89470652e-02 -2.40673199e-02
 -5.34555390e-02  3.82912084e-02  4.32002358e-03  4.36488390e-02
  3.17171998e-02  7.34032271e-03  2.17069574e-02  4.22105454e-02
  4.67641912e-02  6.47580624e-02 -7.08296942e-03 -6.84329169e-03
  9.21215937e-02 -7.39705563e-02 -8.30850303e-02  9.44739655e-02
  5.00909984e-02  4.54533696e-02 -7.37795141e-03 -2.59386486e-08
 -2.60241684e-02  3.85374315e-02 -4.17590849e-02  1.20754112e-02
 -1.03086662e-02 -3.61079015e-02 -2.35228078e-03  8.45177621e-02
  1.00556286e-02 -5.73361628e-02  1.78380869e-02 -3.41224372e-02
 -8.08070675e-02  1.94724668e-02 -1.06158445e-03  1.10550923e-02
 -1.57964695e-02  6.26369342e-02  1.55164758e-02 -3.49854827e-02
 -5.99352233e-02 -5.18510118e-02  5.76010086e-02 -3.07670571e-02
 -1.69860851e-02  2.50532273e-02  3.06815002e-02  1.23734446e-02
  7.28982128e-03 -7.47656077e-03 -3.61665036e-03  2.66019683e-02
  1.23739175e-01 -6.76499605e-02  9.00296718e-02  5.07631078e-02
 -3.29109766e-02 -3.02570257e-02  1.33909220e-02  4.63443808e-02
 -3.64002027e-02 -5.74636534e-02 -1.74494945e-02 -3.20472643e-02
  2.22567469e-02  1.03266248e-02 -1.65889133e-02 -7.03800619e-02
 -2.50094123e-02 -5.85511327e-03  2.41123829e-02  1.53462961e-02
 -2.44420934e-02 -6.78435247e-03  7.47795179e-02  5.53106330e-02
  4.72603031e-02 -1.96746383e-02  4.87674624e-02  2.76621357e-02
  2.16802079e-02  4.88220267e-02  7.81947561e-03  7.38023780e-03]",2,2
bitsandbytes,1,the bitsandbytes is a lightweight wrapper around cuda custom function in particular bit optimizers matrix multiplication llm int and quantization function resource bit optimizer paper video docsllm int paper llm int software blog post llm int emergent feature blog postrequirements linux distribution ubuntu macos etc cuda llm int requires turing or ampere gpus installation pip install bitsandbytesusing bit optimizer using bit inference requirement anaconda cudatoolkit pytorchhardware requirement supported cuda version the bitsandbytes library is currently only supported on linux distribution window is not supported at the moment the requirement can best be fulfilled by installing pytorch via anaconda you can install pytorch by following the get started instruction on the official website for straight int matrix multiplication with mixed precision decomposition you can use bnb matmul to enable mixed precision decomposition use the threshold parameter for instruction how to use llm int inference layer in your own code see the tl dr above or for extended instruction see this blog post with bitsandbytes bit optimizers can be used by changing a single line of code in your codebase for nlp model we recommend also to use the stableembedding layer see below which improves result and help with stable bit optimization to get started with bit optimizers it is sufficient to replace your old optimizer with the bit optimizer in the following way note that by default all parameter tensor with le than element are kept at bit even if you initialize those parameter with bit optimizers this is done since such small tensor do not save much memory and often contain highly variable parameter bias or parameter that require high precision batch norm layer norm you can change this behavior like so if you want to optimize some unstable parameter with bit adam and others with bit adam you can use the globaloptimmanager with this we can also configure specific hyperparameters for particular layer such a embedding layer to do that we need two thing register the parameter while they are still on the cpu override the config with the new desired hyperparameters anytime anywhere see our guide for more detailsto use the stable embedding layer override the respective build embedding function of your model make sure to also use the no scale embedding flag to disable scaling of the word embedding layer nor replaced with layer norm you can use the optimizers by replacing the optimizer in the respective file adam py etc for upcoming feature and change and full history see patch note to compile from source please follow the compile from source md instruction the majority of bitsandbytes is licensed under mit however portion of the project are available under separate license term pytorch is licensed under the bsd license we thank fabio cannizzo for his work on fastbinarysearch which we use for cpu quantization if you found this library and found llm int useful please consider citing our work for bit optimizers or quantization routine please consider citing the following work,"[('bit inference requirement anaconda cudatoolkit pytorchhardware requirement', 0.7142), ('bitsandbytes library', 0.6042), ('bitsandbytes bit optimizers', 0.5988), ('bitsandbytesusing bit optimizer', 0.5543), ('bit optimizers', 0.5534), ('bit optimizer', 0.5333), ('quantization function resource bit optimizer paper video', 0.5142), ('particular bit optimizers matrix multiplication', 0.4735), ('cuda custom function', 0.4469), ('bitsandbytes', 0.4371)]","[-1.72440838e-02 -2.44563222e-02 -1.18002214e-01 -9.18368697e-02
 -1.71252042e-02 -1.67398043e-02  8.83866474e-03 -2.78078206e-02
 -8.60007778e-02 -1.17325848e-02 -5.60495071e-02 -9.70611125e-02
 -5.04217744e-02 -3.96205932e-02  4.10877950e-02  3.29901502e-02
  6.02174103e-02  2.02250760e-02 -2.42861230e-02 -9.17723104e-02
  2.30501089e-02 -7.74490554e-03 -1.87815800e-02 -6.78239614e-02
 -1.63198030e-03 -6.50824681e-02  6.80854321e-02 -3.22360843e-02
  9.20215845e-02 -6.82462156e-02  6.07552566e-02  4.22362052e-02
  1.27814323e-01 -1.37485040e-03  4.23656218e-02  1.46167795e-03
  4.39855382e-02 -6.92615584e-02 -2.40974221e-02  5.14733279e-03
 -1.87627356e-02  1.91476941e-02 -2.68785283e-02 -1.88792609e-02
  5.78885723e-04  3.05444933e-02  8.12796652e-02  2.23091077e-02
  1.00479973e-02 -3.02570295e-02  1.37751792e-02  1.08449524e-02
 -4.95725125e-02 -2.25710645e-02 -5.16993627e-02 -6.74753860e-02
 -7.75378838e-04 -7.42741860e-03  4.24097218e-02  4.56236675e-02
 -1.80718284e-02 -1.56629886e-02 -3.80540714e-02  4.92413566e-02
 -2.67324168e-02 -2.00749449e-02  6.31398484e-02 -1.25436606e-02
  8.69526863e-02 -2.81351041e-02 -4.90320474e-02 -5.33866398e-02
 -5.32394387e-02  4.66840714e-02 -5.83162308e-02  1.89100945e-04
  1.01463489e-01 -2.43518017e-02  2.20928844e-02 -1.04559697e-01
 -8.30432475e-02  2.89432444e-02  6.05688654e-02 -7.19093019e-03
  8.06357190e-02 -1.27439732e-02 -8.60282630e-02  6.02676049e-02
 -2.14286726e-02 -4.33748960e-02  4.72707003e-02 -8.26813839e-03
  1.65599529e-02  3.35496143e-02  9.01077464e-02 -4.85661402e-02
 -1.05158063e-02 -3.44787501e-02 -4.79515269e-02  3.43666151e-02
 -2.83695031e-02  2.36432836e-03 -1.95493028e-02 -8.17124266e-03
 -4.11795713e-02  9.63145122e-03  3.37742716e-02  7.65810013e-02
  4.09671664e-02  7.31789842e-02  2.40086727e-02  5.32880276e-02
 -1.94215439e-02 -1.22822784e-01  9.21105742e-02 -5.61033264e-02
 -1.01610616e-01  4.08811532e-02  2.17513274e-02 -7.49577656e-02
 -8.38728026e-02 -4.15958464e-02 -7.43495598e-02 -2.22343858e-02
 -2.12834310e-02 -3.77031080e-02 -9.65412930e-02  1.19435222e-32
 -1.59045570e-02  3.83837484e-02 -4.67567258e-02 -2.48647737e-03
  2.37149298e-02  1.78183876e-02 -1.01687899e-02 -2.13411963e-03
  2.55220309e-02 -2.03778893e-02 -3.63745540e-02  2.67986115e-02
 -7.22151622e-02  6.53151572e-02  3.96051221e-02 -1.09782047e-03
 -6.66521564e-02 -1.41055947e-02  4.64342423e-02 -1.66465365e-03
  1.70951951e-02  4.71918248e-02 -6.76944107e-02  9.09352601e-02
  1.91473812e-02  1.16322180e-02  4.83282357e-02 -2.07450632e-02
  2.52067726e-02  1.40105756e-02 -8.26013237e-02 -2.26282813e-02
 -3.54572125e-02 -2.52492260e-02  2.20115446e-02 -2.21805274e-02
 -1.54891470e-02 -4.38414291e-02 -1.84460673e-02 -6.06543981e-02
 -3.62039469e-02  6.48449734e-02  2.23555807e-02 -1.12133719e-01
 -8.89283866e-02 -5.38011678e-02  1.54799987e-02  1.11860327e-01
  2.99292356e-02  2.74159294e-02 -1.93820223e-02 -1.75144873e-03
  1.79152563e-02  2.78022606e-02  7.73311853e-02 -3.50279547e-02
  4.69248109e-02  1.87473390e-02  1.41519904e-01  1.67475492e-01
 -4.33522426e-02 -1.88150797e-02  8.88455212e-02 -6.57339767e-02
 -2.95086973e-03  6.10756595e-03 -2.43175440e-02 -2.19150651e-02
 -2.55721733e-02  8.05568472e-02 -1.15584396e-01  4.70154695e-02
  4.67757955e-02 -2.51135267e-02  6.09145872e-02 -1.01094265e-02
 -2.84919683e-02 -2.81694271e-02 -5.03269136e-02  1.52073381e-02
 -2.58077737e-02  6.63542077e-02  9.10410360e-02 -4.16237004e-02
 -5.10120913e-02 -5.51271774e-02  1.49304490e-03 -4.22781222e-02
 -2.95455921e-02 -3.73324525e-04 -3.37745063e-02  7.69092981e-03
  2.50536464e-02  8.25913846e-02 -9.04348213e-03 -1.05407137e-32
 -4.61623520e-02  6.76352382e-02 -3.88814248e-02  1.26208961e-01
  8.48654378e-03 -3.64062786e-02  5.65400124e-02 -6.77577034e-02
  3.97329852e-02 -2.55124178e-02  2.40703747e-02 -3.21805812e-02
  5.80313429e-02  3.20382193e-02 -4.46686782e-02 -7.16867449e-04
 -2.82604154e-02 -2.44618580e-02 -1.64952725e-02  9.73485410e-03
 -8.04872960e-02  9.69233960e-02  3.30202654e-02 -8.69065374e-02
  3.43509465e-02  3.30718048e-02 -4.41655368e-02 -3.50220278e-02
  3.59749459e-02  3.79899964e-02  3.50020919e-03 -1.48112876e-02
 -1.01199500e-01  7.51565164e-03 -1.38211837e-02 -9.78616029e-02
  9.89443585e-02  6.26808358e-03 -4.25094068e-02  4.90321331e-02
  1.64267659e-01  8.38823915e-02  6.26995042e-03  4.88501303e-02
 -3.02974544e-02  1.95112601e-02 -2.31943522e-02  4.93474007e-02
  2.02825293e-02 -9.22550634e-02  4.15465422e-02  5.34071848e-02
  7.50106052e-02  3.18812616e-02  5.97459860e-02  2.18627322e-02
  7.29247276e-03  1.03326153e-03  1.17164865e-01  2.51448471e-02
 -1.08506121e-01 -3.18528414e-02 -6.73069339e-03 -1.14159612e-02
 -7.33161019e-03 -3.07869613e-02 -7.12746531e-02  6.27188161e-02
 -2.73708794e-02  2.44371146e-02 -2.29815803e-02  2.92885229e-02
  7.87804723e-02  1.34029854e-02 -1.14977762e-01  4.36964110e-02
 -2.79598776e-03 -9.73482337e-03  5.09958751e-02  9.79497582e-02
  2.45105214e-02  4.65967283e-02  8.45036283e-03  2.29265410e-02
 -3.42881531e-02  7.87261352e-02  2.69398894e-02 -6.82704374e-02
  2.49392260e-02 -4.07940410e-02 -5.33948950e-02  4.62745763e-02
  5.58251701e-02  1.10842817e-01 -1.56109314e-02 -4.09344985e-08
 -4.86718602e-02 -5.82263768e-02  1.25688355e-04 -6.81190789e-02
  5.78682534e-02  2.77222097e-02 -2.07770057e-02  3.88479270e-02
 -3.14337295e-03 -1.57547370e-02  6.38986826e-02  1.32078938e-02
 -8.43912140e-02 -7.19134184e-03 -2.86689289e-02  6.55647144e-02
  5.16811386e-02 -8.42563063e-02  5.97865433e-02 -3.03355828e-02
  1.68402661e-02 -1.21319667e-02 -2.56445222e-02  2.11798679e-02
 -2.11676545e-02  6.80955593e-03  6.38598278e-02  5.66734094e-03
  5.36044948e-02  5.55484518e-02 -9.25100520e-02 -2.19876575e-03
  4.56757918e-02 -3.15598994e-02  5.99965081e-02  4.90811355e-02
 -1.05420928e-02  3.49444672e-02 -6.05806988e-03 -2.26178113e-02
 -6.80553615e-02 -2.53849477e-03 -3.15278545e-02 -7.40123494e-03
  2.48899236e-02 -5.34462184e-03  5.56117743e-02  2.57194191e-02
 -8.62204432e-02 -1.15015656e-02 -2.39938274e-02  2.35277489e-02
  1.18134599e-02  3.20260110e-03 -3.87102575e-03  4.02151793e-02
 -7.49035776e-02 -8.76559690e-02 -1.17981313e-02  5.35673574e-02
 -7.72056030e-03  3.53157483e-02  1.25722084e-02 -1.76192094e-02]",2,2
gfpgan,1,rocket thanks for your interest in our work you may also want to check our new update on the tiny model for anime image and video in real esrgan blush gfpgan aim at developing a practical algorithm for real world face restoration it leverage rich and diverse prior encapsulated in a pretrained face gan e g stylegan for blind face restoration question frequently asked question can be found in faq md triangular flag on post updatesif gfpgan is helpful in your photo project please help to star this repo or recommend it to your friend thanks blush other recommended project arrow forward real esrgan a practical algorithm for general image restoration arrow forward basicsr an open source image and video restoration toolbox arrow forward facexlib a collection that provides useful face relation function arrow forward handyview a pyqt based image viewer that is handy for view and comparison paper project page demo xintao wang yu li honglun zhang ying shan applied research center arc tencent pcg we now provide a clean version of gfpgan which doe not require customized cuda extension if you want to use the original model in our paper please see papermodel md for installation clone repoinstall dependent packageswe take the v version for an example more model can be found here download pre trained model gfpganv pthinference if you want to use the original model in our paper please see papermodel md for installation and inference the comparison are in comparison md note that v is not always better than v you may need to select different model based on your purpose and input you can find more model such a the discriminator here google drive or tencent cloud we provide the training code for gfpgan used in our paper you could improve it according to your own need tipsprocedures you can try a simple version option train gfpgan v simple yml that doe not require face component landmark dataset preparation ffhqdownload pre trained model and other data put them in the experiment pretrained model folder modify the configuration file option train gfpgan v yml accordingly trainingpython m torch distributed launch nproc per node master port gfpgan train py opt option train gfpgan v yml launcher pytorchgfpgan is released under apache license version if you have any question please email xintao wang outlook com or xintaowang tencent com,"[('face gan e g stylegan', 0.5157), ('model gfpganv pthinference', 0.5038), ('general image restoration arrow', 0.4915), ('gfpgan', 0.4815), ('open source image', 0.4358), ('blind face restoration question', 0.4232), ('video restoration toolbox arrow', 0.4101), ('facexlib', 0.3958), ('face component landmark', 0.3806), ('useful face relation function arrow', 0.3627)]","[-6.61424696e-02 -9.95045528e-02  5.43040559e-02  3.21524520e-03
  1.09678628e-02 -1.47808660e-02 -4.83507290e-02  2.59944685e-02
 -7.08530769e-02 -1.06562171e-02  3.27063426e-02  1.46613196e-02
 -2.37781145e-02 -5.55886663e-02  6.43357337e-02 -6.76298663e-02
  1.59686978e-03  7.54587278e-02  1.70105305e-02 -2.22847853e-02
 -4.19014767e-02  2.68556084e-02 -1.99800800e-03 -4.25211936e-02
 -1.52134784e-02  2.61279512e-02  3.91227901e-02 -3.48243155e-02
  5.85902929e-02 -6.75147623e-02 -5.09903245e-02  5.45542203e-02
  1.43518476e-02  1.51464192e-03 -1.22161239e-01  1.06913857e-01
  9.91184264e-03  2.62477193e-02 -5.46934195e-02 -5.82957938e-02
 -1.00652479e-01 -3.72664183e-02 -1.34248994e-02 -8.15572441e-02
  1.18242681e-01 -5.12368511e-03  1.99300610e-02 -6.93280771e-02
 -2.35248869e-03 -8.21279511e-02 -5.05267316e-03 -1.17178164e-01
 -1.17946886e-01  5.87327108e-02  1.19327992e-01 -4.85012401e-03
 -2.60404008e-03 -9.57584977e-02 -1.29084736e-02  7.57318735e-02
 -2.70454716e-02 -1.69741064e-02 -1.10471033e-01  4.35215980e-02
 -2.80137360e-03  7.76844332e-03  2.58381125e-02 -3.09175849e-02
  5.92975765e-02 -3.27546485e-02 -8.63516480e-02  2.13005953e-02
 -2.41455939e-02  2.28889361e-02 -1.34286508e-02  1.69751595e-03
  1.73508655e-02 -2.64991615e-02  3.37092429e-02 -7.12160841e-02
  8.90818238e-02  3.36889736e-02  6.93738684e-02  2.96729635e-02
 -1.31545402e-02  5.80606610e-02 -9.09397230e-02 -5.35769621e-03
 -3.08018103e-02  2.80089234e-03  2.68550701e-02  1.18510704e-02
  1.89727973e-02  1.07401218e-02  3.48436646e-02 -3.26510072e-02
  3.10859252e-02 -4.44624722e-02 -3.82952839e-02  4.51897383e-02
 -3.40189114e-02 -5.90714067e-02  3.63087207e-02 -4.70061861e-02
  2.76530925e-02  5.08073419e-02  5.46558574e-02 -3.84463626e-03
  5.28024100e-02  2.28654761e-02 -4.12658378e-02 -3.84289250e-02
 -7.37420693e-02 -1.26812905e-01  1.88127328e-02 -6.77601621e-02
 -8.63059163e-02 -3.44770104e-02  3.03196013e-02 -4.52806093e-02
 -3.84344310e-02  3.64150330e-02  5.60043342e-02 -7.16485754e-02
 -5.82936779e-02 -1.71754472e-02 -2.03352869e-02  5.06887270e-33
  4.35502604e-02  4.43900712e-02 -3.38823050e-02 -1.30703300e-02
  6.23752438e-02  1.25427288e-03  3.36982240e-03 -2.32413900e-03
 -1.68774817e-02 -6.36257678e-02  1.37867816e-02  5.79466596e-02
 -1.84086055e-01  1.31652221e-01 -1.76355278e-03  6.17372151e-03
 -2.61219451e-03  6.93010688e-02 -4.51104492e-02  3.47217838e-06
  2.54043192e-02  4.29328121e-02 -1.67381007e-03  1.50915965e-01
  3.06231398e-02  4.29039523e-02  1.24341901e-02 -3.18059698e-02
 -5.12904450e-02  1.91823026e-04 -6.69180080e-02  5.28971255e-02
  6.34764060e-02 -9.80571061e-02  2.09358260e-02 -2.17145104e-02
 -3.62911560e-02 -1.25267789e-01 -6.66160509e-02 -7.52799958e-03
 -6.18258081e-02  7.03471005e-02 -7.22364932e-02 -1.22634387e-02
  1.17030216e-03  5.78122251e-02 -1.17576374e-02  5.74053414e-02
 -2.15797126e-02  6.71349689e-02  2.74883006e-02  6.98174387e-02
 -4.30705771e-02 -7.60692405e-03 -7.46276006e-02  1.58207007e-02
 -2.56141387e-02  2.30460130e-02  2.67284047e-02  3.77608612e-02
 -2.38938481e-02  4.67191674e-02  5.34509718e-02 -2.11005621e-02
 -4.41430919e-02  4.97850329e-02  2.09193472e-02 -6.37652054e-02
 -4.82995361e-02  5.72593659e-02  6.94791204e-04  2.42649652e-02
 -5.55489771e-02 -6.34442344e-02  3.86857055e-02 -7.47486297e-03
 -1.41947409e-02 -5.22008492e-03 -2.55046263e-02  1.72874965e-02
 -1.28326610e-01  3.30052860e-02 -3.80794182e-02  1.58595876e-03
 -9.65211913e-03 -3.85658331e-02  5.76686263e-02 -5.89096583e-02
  2.17349418e-02 -3.06036100e-02 -2.26822142e-02  1.03962123e-02
 -1.98536981e-02  2.88639683e-02 -1.22741405e-02 -4.32884905e-33
 -7.86764640e-03  6.69742078e-02 -6.68632463e-02  8.92361924e-02
 -6.01001084e-03  4.49490249e-02 -1.71378590e-02 -1.76879335e-02
  1.03653289e-01  3.84405931e-03  1.42131187e-02  3.40758450e-02
  8.30454472e-03 -5.45103252e-02 -9.16114915e-03 -8.51044059e-02
 -3.49098584e-03 -7.28189796e-02 -1.24596715e-01  3.22986282e-02
  6.98930323e-02  1.03185385e-01 -5.98111153e-02 -4.81630266e-02
  6.01687049e-03  3.46285999e-02 -2.11030175e-03  1.93551574e-02
  4.21543419e-02 -1.27202040e-02  8.95526633e-02  1.12797385e-02
  5.55786863e-02  7.67002031e-02 -2.80933045e-02  2.61079501e-02
  1.04700714e-01 -1.32844457e-02 -5.59384152e-02  2.46723983e-02
  1.04934290e-01  3.53227518e-02  4.17601615e-02  2.65615038e-03
 -1.91258080e-02  3.12960148e-02 -8.78516585e-02  3.22559215e-02
  9.96763632e-03  2.06333194e-02 -4.85424697e-03 -3.00432276e-02
 -5.93509413e-02 -8.81171133e-03 -4.40127589e-02 -1.03191780e-02
  1.19702376e-01  4.49978262e-02  8.91490877e-02  5.51491268e-02
 -4.15078178e-02 -6.48033470e-02  1.53111396e-02  6.09479658e-02
 -5.71448989e-02 -1.87316735e-03  3.04199103e-02  9.51166358e-03
 -9.68295243e-03 -2.49838866e-02 -2.84931883e-02  4.61421758e-02
  3.58034335e-02  2.76433416e-02  3.40162925e-02  3.88354957e-02
  2.92180069e-02  4.92883176e-02 -6.69995975e-03  1.51749821e-02
  8.00984874e-02 -6.47692531e-02  7.76852965e-02  9.37677398e-02
  3.32302637e-02  4.50808257e-02  4.82183099e-02 -1.24825370e-02
  5.36098816e-02 -1.07589113e-02 -7.45144263e-02  4.46905680e-02
 -2.32856255e-02  6.62726313e-02  4.14370652e-03 -2.76765295e-08
 -8.62199217e-02  4.02677655e-02  2.18574423e-03 -1.67885721e-02
 -8.63000005e-03 -5.11432439e-02 -5.49308490e-03  1.09041058e-01
 -2.88849678e-02  4.69052792e-02  1.39184790e-02  2.57935077e-02
 -9.38787982e-02 -2.73556225e-02  3.82091664e-02  1.47029152e-02
 -8.90320726e-03  1.00806514e-02 -3.89925614e-02 -6.77553117e-02
 -6.48326725e-02 -1.58108625e-04 -1.26937088e-02  4.08290103e-02
 -1.04428614e-02 -3.61705124e-02  4.25688922e-02  7.69016743e-02
 -9.02075786e-03  2.86704395e-02  2.54592821e-02  4.35362011e-02
  9.90687981e-02 -6.38590008e-02  3.73276435e-02  3.40400636e-02
 -7.15696067e-03 -1.66873299e-02  1.01105124e-02  6.27363101e-02
  4.36789822e-03 -3.20176035e-02  4.60957065e-02 -2.89785396e-02
 -3.77724618e-02  5.43083623e-02  9.99508426e-02 -5.56858741e-02
 -1.74390897e-02  4.65327203e-02 -5.82282580e-02  3.53877880e-02
 -3.82351317e-02  3.26161161e-02 -4.74654371e-03  7.80585930e-02
  1.67645123e-02 -4.59439717e-02  5.71605749e-02 -3.85096595e-02
  4.48480994e-02  3.67841274e-02  2.47692075e-02  5.42535149e-02]",2,2
lpips,1,the unreasonable effectiveness of deep feature a a perceptual metric richard zhang phillip isola alexei a efros eli shechtman oliver wang in cvpr run pip install lpips the following python code is all you need more thorough information about variant is below this repository contains our perceptual metric lpips and dataset bapps it can also be used a a perceptual loss this us pytorch a tensorflow alternative is here table of contentsevaluate the distance between image patch higher mean further more different lower mean more similar example script to take the distance between specific image all corresponding pair of image in directory or all pair of image within a directory file test network py show example usage this snippet is all you really need variable im im is a pytorch tensor variable with shape nx xhxw n patch of size hxw rgb image scaled in this return d a length n tensor variable run python test network py to take the distance between example reference image ex ref png to distorted image ex p png and ex p png before running it which do you think should be closer some option by default in model initialize file lpips loss py show how to iteratively optimize using the metric run python lpips loss py for a demo the code can also be used to implement vanilla vgg loss without our learned weight higher mean further more different lower mean more similar we found that deep network activation work surprisingly well a a perceptual similarity metric this wa true across network architecture squeezenet mb alexnet mb and vgg mb provided similar score and supervisory signal unsupervised self supervised and supervised all perform strongly we slightly improved score by linearly calibrating network adding a linear layer on top of off the shelf classification network we provide variant using linear layer on top of the squeezenet alexnet default and vgg network if you use lpips in your publication please specify which version you are using the current version is you can set version for the initial release run bash script download dataset sh to download and unzip the dataset into directory dataset it take gb total alternatively run bash script download dataset valonly sh to only download the validation set gb script test dataset model py evaluates a perceptual model on a subset of the dataset dataset flagsperceptual similarity model flagsmisc flagsan example usage is a follows python test dataset model py dataset mode afc datasets val traditional val cnn model lpips net alex use gpu batch size this would evaluate our model on the traditional and cnn validation datasets the dataset contains two type of perceptual judgement two alternative forced choice afc and just noticeable difference jnd afc evaluator were given a patch triplet reference distorted they were asked to select which of the distorted wa closer to the reference training set contain judgment triplet validation set contain judgment triplet each afc subdirectory contains the following folder jnd evaluator were presented with two patch a reference and a distorted for a limited time they were asked if the patch were the same identically or different each set contains human evaluation example each jnd subdirectory contains the following folder see script train test metric sh for an example of training and testing the metric the script will train a model on the full training set for epoch and then test the learned metric on all of the validation set the number should roughly match the alex lin row in table in the paper the code support training a linear layer on top of an existing representation training will add a subdirectory in the checkpoint directory you can also train scratch and tune version by running train test metric scratch sh and train test metric tune sh respectively if you find this repository useful for your research please use the following this repository borrows partially from the pytorch cyclegan and pix pix repository the average precision ap code is borrowed from the py faster rcnn repository angjoo kanazawa connelly barnes gaurav mittal wilhelmhb filippo mameli supershinyeyes minyoung huh helped to improve the codebase,"[('tensorflow alternative', 0.4224), ('pytorch tensor variable', 0.4175), ('metric run python lpips loss py', 0.4128), ('deep feature', 0.4117), ('perceptual similarity metric', 0.4097), ('perceptual metric lpips', 0.4082), ('image patch', 0.3869), ('perceptual loss', 0.3812), ('file lpips loss py', 0.3666), ('directory file test network py', 0.3664)]","[-4.59570847e-02 -9.20524001e-02  5.98335499e-03 -1.58119909e-02
  1.88201312e-02  2.93781497e-02 -2.75070649e-02  5.79685047e-02
  2.54852977e-03 -6.37202635e-02 -5.74157275e-02 -1.24109080e-02
 -5.92798181e-02  7.12489635e-02 -1.49224726e-02 -5.93999913e-03
 -1.43508743e-02  1.42917991e-01 -4.28035995e-03 -5.21039404e-02
 -1.40708899e-02  2.60294750e-02  3.48473107e-03  1.10684996e-02
  2.00889055e-02 -2.03269571e-02  2.85741072e-02 -3.82277705e-02
 -2.32632700e-02 -1.97233679e-03  1.80439372e-02  6.50238022e-02
  2.00636848e-03 -4.59841406e-03  3.55199203e-02  4.16534394e-02
 -6.42975494e-02 -5.89175299e-02 -1.47097362e-02 -7.62655735e-02
  1.48656089e-02 -1.29835028e-02 -3.92779894e-02 -9.09599010e-03
 -1.07309660e-02  4.42469027e-03 -1.20201707e-02 -3.96081023e-02
  2.71695796e-02 -2.85322163e-02 -2.48066243e-02 -5.74273728e-02
 -3.69256884e-02  9.16656479e-03  2.93389317e-02 -4.35535535e-02
  5.07039577e-02 -8.76857806e-03 -3.72855850e-02 -4.34136875e-02
  2.12804284e-02  2.08009481e-02 -5.43622822e-02  8.72585736e-03
  4.54873405e-03  4.66840044e-02 -2.05441192e-03 -1.95705495e-03
  9.03320685e-02 -7.61062205e-02 -1.16452284e-01  3.92196178e-02
 -6.83604404e-02  1.75491292e-02 -2.80266218e-02  9.18657798e-03
  1.39600053e-01 -2.01106798e-02 -3.55760008e-02 -1.13487639e-01
  9.78243630e-03  1.50278714e-02 -3.69702955e-03  1.02290111e-02
  6.08021542e-02 -6.43627569e-02  3.02845072e-02  7.48857781e-02
  8.41378048e-03 -7.87849128e-02  2.52565509e-03  6.32027537e-03
 -7.28156269e-02 -1.95058156e-02  2.15229001e-02  4.92526330e-02
 -1.46889186e-03 -9.41519141e-02 -2.17676833e-02  5.37737235e-02
 -8.73386767e-03 -3.30489501e-02 -3.87963327e-03  9.88217629e-03
  6.65686056e-02  7.45983124e-02  8.66265222e-02 -5.82672767e-02
  5.49103878e-02 -1.40255149e-02  2.61680633e-02 -8.94844998e-03
  3.79466871e-03 -4.36555967e-02  9.45748687e-02 -2.13178620e-02
 -3.50536630e-02  7.01215118e-02  1.02458149e-02  9.75550264e-02
 -1.43248454e-01 -3.46848108e-02 -4.03038859e-02  9.21539683e-03
 -2.31976062e-02  1.93365924e-02 -1.59626193e-02  9.18234033e-33
 -6.62210435e-02 -1.78649221e-02  4.18860167e-02 -6.42779469e-02
  1.04242556e-01 -2.95522027e-02  5.11632338e-02 -2.94643212e-02
 -2.46943068e-03 -4.30400185e-02 -1.15551792e-01 -1.33738585e-03
 -3.44031490e-02  1.05613217e-01 -1.77258719e-02 -6.71053231e-02
 -5.75466603e-02  1.01100676e-01  9.09267366e-02  8.42750818e-02
  2.90329829e-02  1.56741068e-02  1.35500813e-02  3.65203582e-02
  2.88924128e-02 -4.70637642e-02  1.99769698e-02 -2.27957573e-02
  1.42987538e-02 -3.95182520e-03 -4.66179252e-02  2.10048966e-04
  1.64893158e-02 -1.23485103e-02 -1.01711690e-01 -6.46241084e-02
 -7.11546615e-02 -9.83641203e-03 -2.11197603e-02 -1.02717534e-01
 -5.37568629e-02  3.03892307e-02 -6.30100444e-02 -4.72928509e-02
 -5.85203394e-02 -3.45136113e-02  3.06422953e-02  7.43364394e-02
 -1.41605139e-02 -3.36525701e-02  4.14587632e-02 -4.97669801e-02
 -7.68350363e-02 -1.55365188e-02 -3.05920895e-02 -6.32802546e-02
  1.51898041e-02  4.76760827e-02  1.04181498e-01  2.61060596e-02
  6.27198964e-02  5.95588014e-02  2.82076430e-02 -3.10102459e-02
  5.10263927e-02 -6.08872771e-02  2.44873054e-02  2.37872661e-03
 -5.39798811e-02  8.33571702e-02 -9.49794948e-02  7.71534219e-02
 -4.41878885e-02 -5.39297424e-02  5.75072132e-02 -4.38812450e-02
  1.64048634e-02 -1.83196329e-02  9.40998737e-03  2.92174555e-02
 -1.14400685e-01  5.80193587e-02  3.06898691e-02 -7.91582242e-02
 -4.42783237e-02 -3.78681235e-02  3.50863822e-02 -3.42348292e-02
 -3.63478474e-02 -4.06686291e-02 -4.09131646e-02  1.38100795e-02
  4.31821048e-02  6.74687549e-02 -1.71156432e-02 -7.39101991e-33
 -3.82020213e-02  5.26033379e-02 -1.87321119e-02  8.45688581e-02
 -6.03084499e-03 -2.67518461e-02  7.00687096e-02  1.33880451e-02
  7.86072388e-03  2.28154343e-02  1.59216560e-02 -5.19068837e-02
 -3.10921371e-02 -2.40033157e-02  3.20686288e-02  3.63871828e-03
 -4.57830802e-02 -9.83818807e-03  2.76420172e-03  6.09178422e-03
 -2.70424057e-02  1.26390383e-01 -4.48180027e-02  3.40860337e-02
 -1.05442993e-01  3.08564026e-03 -2.15124097e-02  3.82092607e-04
 -5.73250204e-02 -6.01358004e-02  2.94773187e-03  4.69211414e-02
 -1.02209762e-01  2.91153919e-02  2.26343367e-02  6.19618185e-02
  4.69889678e-02 -5.01345319e-04 -2.31233705e-02  8.89410749e-02
  9.31177288e-02  1.13823004e-01 -3.58389467e-02  4.35016453e-02
 -1.88444443e-02 -2.61140931e-02 -4.42729704e-02 -2.72243451e-02
 -2.92138895e-03 -3.64442170e-02 -1.38277840e-02 -1.26457857e-02
 -5.96428663e-03  3.26264687e-02 -1.34806186e-02  4.37235087e-02
  4.48588505e-02 -3.29580456e-02  3.37910792e-03  5.67889884e-02
 -5.35362363e-02 -8.35248828e-02 -7.82237109e-03  5.02087586e-02
 -3.30269663e-03  4.88599874e-02 -7.00736642e-02  4.56772633e-02
 -4.65946831e-02  3.85073386e-02  5.57468273e-02  4.87167798e-02
  1.21379942e-02  7.72294775e-02 -1.43627431e-02  4.95334491e-02
  2.08261274e-02  3.09587885e-02  3.47968470e-03  4.68799360e-02
  9.99260843e-02 -1.33686503e-02 -1.16915908e-02  5.24539202e-02
  2.91739162e-02  1.07631996e-01  4.94780904e-03  1.60852391e-02
  1.07274413e-01  2.94733308e-02 -3.33741792e-02  3.34844142e-02
  3.84061821e-02  2.67385133e-03  1.11112922e-01 -3.65047477e-08
 -7.51650408e-02  7.55113885e-02  8.40705112e-02  2.53155045e-02
 -6.87455684e-02 -1.02189202e-02  4.82612615e-03  1.05654113e-01
  2.82618310e-03  3.53892408e-02 -3.65299024e-02 -7.87191689e-02
 -7.05396682e-02  4.80251946e-02 -1.55877313e-02  8.25442895e-02
  2.96272151e-02  3.40343155e-02  3.87581252e-02  4.19160537e-02
  4.53189872e-02  2.31740028e-02 -1.94596499e-03 -3.84317571e-03
 -3.94056216e-02 -8.12733397e-02  8.24550763e-02  3.03052785e-03
 -5.50570972e-02 -6.53408319e-02  4.68189642e-02 -2.79382579e-02
  5.81464469e-02 -5.94058968e-02  9.30033177e-02  1.17814116e-01
 -2.70111579e-02 -4.38622609e-02 -1.64255630e-02  1.38748065e-01
 -1.20678850e-01  1.82265732e-02  2.50282814e-03 -7.85264447e-02
  5.80479763e-02 -4.79957275e-03  1.88145600e-02 -3.57502475e-02
 -7.12700039e-02 -3.15621011e-02  6.88444972e-02  4.18061297e-03
 -4.33475226e-02  3.54521647e-02  1.02629110e-01 -1.79756675e-02
 -6.02146536e-02  7.61191966e-03  2.94058267e-02  1.62675045e-02
 -1.70677882e-02  1.97679922e-02 -2.34415033e-03 -4.32868823e-02]",2,2
prdc,1,paper reliable fidelity and diversity metric for generative modelsmuhammad ferjad naeem seong joon oh yunjey choi youngjung uh jaejun yoo work done at clova ai research equal contribution clova ai research naver corp clova ai research line plus corp technische universit t m nchen epfldevising indicative evaluation metric for the image generation task remains an open problem the most widely used metric for measuring the similarity between real and generated image ha been the fr chet inception distance fid score because it doe not differentiate the fidelity and diversity aspect of the generated image recent paper have introduced variant of precision and recall metric to diagnose those property separately in this paper we show that even the latest version of the precision and recall kynk nniemi et al metric are not reliable yet for example they fail to detect the match between two identical distribution they are not robust against outlier and the evaluation hyperparameters are selected arbitrarily we propose density and coverage metric that solve the above issue we analytically and experimentally show that density and coverage provide more interpretable and reliable signal for practitioner than the existing metric precision and recall are defined below where the manifold is the defined a is the ball around the point x with radius r is the distance to the kth nearest neighbour density and coverage are defined below precision versus density because of the real outlier sample the manifold is overestimated generating many fake sample around the real outlier is enough to increase the precision measure the problem of overestimating precision is resolved using the density estimate recall versus coverage the real and fake sample are identical across left and right since model often generate many unrealistic yet diverse sample the fake manifold is often an overestimation of the true fake distribution in the figure above while the fake sample are generally far from the mode in real sample the recall measure is rewarded by the fact that real sample are contained in the overestimated fake manifold test real and fake sample form the standard normal distribution n i in dimensional euclidean space set the nearest neighbour k we compute precision recall density and coverage estimate below above test code will result in the following estimate may fluctuate due to randomness kynk nniemi et al improved precision and recall metric for assessing generative model neurips,"[('diversity metric', 0.5331), ('coverage metric', 0.4905), ('density estimate recall', 0.4856), ('precision recall density', 0.4731), ('indicative evaluation metric', 0.4162), ('recall measure', 0.4071), ('image generation task', 0.3941), ('metric precision', 0.3856), ('generative modelsmuhammad ferjad naeem seong joon', 0.3816), ('fr chet inception distance', 0.3723)]","[ 3.43389548e-02 -8.69537294e-02 -5.27603924e-02 -3.41099650e-02
 -2.88252756e-02  7.86885694e-02 -1.89678036e-02  6.01535216e-02
 -9.91160702e-03 -3.62481810e-02  4.09709942e-03 -1.10717155e-01
  8.57014433e-02  3.90000530e-02 -5.03244363e-02 -4.51934477e-03
  3.52649577e-02  5.34121618e-02 -5.46528809e-02 -8.79702941e-02
  3.10557373e-02  3.34251896e-02  1.16739653e-01  2.16302965e-02
 -2.36334857e-02 -5.19383028e-02  1.29815210e-02 -4.26870473e-02
  2.69735279e-03 -3.65638360e-02  1.10723544e-03  6.14602417e-02
  3.38904820e-02  4.24872190e-02  5.62897418e-03  7.34538063e-02
 -5.96776120e-02  6.74216915e-03  2.53363214e-02 -6.44277930e-02
 -6.31778985e-02  1.41754057e-02  3.56693417e-02  5.91289368e-04
  5.39803430e-02  3.20467316e-02 -7.86930844e-02 -6.09749705e-02
 -5.16983587e-03  5.48898382e-03 -9.65140760e-02  2.03826968e-02
 -1.10938549e-01 -5.98137314e-03  4.86943685e-02  4.73228581e-02
  4.37623337e-02 -4.03827354e-02 -1.12079186e-02 -4.22419794e-02
 -4.31378633e-02 -1.28441960e-01 -1.45727649e-01 -2.77161356e-02
 -8.11534841e-03 -3.92035693e-02 -2.91866679e-02 -1.07135018e-02
  1.05970643e-01 -7.16330856e-02 -5.63242212e-02  6.17564321e-02
 -2.24048644e-02  3.72287594e-02 -4.88304067e-03  5.18961474e-02
  5.89761175e-02  4.24224585e-02 -5.40410401e-05 -1.10785238e-01
 -3.46818641e-02  1.50127159e-02  4.43132184e-02 -1.17151951e-02
  1.07682034e-01 -1.38968145e-02 -1.23714311e-02  4.48589846e-02
  7.14938343e-03 -4.55332212e-02  1.19072003e-02 -5.72001152e-02
 -4.78170104e-02 -6.50601387e-02 -8.89765471e-03 -6.03838824e-03
  1.40422378e-02 -9.36453193e-02  5.96260503e-02 -9.08126216e-03
  3.20362598e-02  2.71809641e-02 -1.68602413e-03 -4.10547964e-02
 -3.15116681e-02 -2.45244354e-02  4.41022106e-02  1.65061373e-02
  1.01144657e-01  1.08890682e-02  1.72026120e-02 -3.39171477e-02
 -5.13025299e-02 -3.13077378e-03  2.13099215e-02 -1.91076484e-03
 -6.42020628e-02  2.70852400e-03  2.40688138e-02  5.19484580e-02
 -7.78238550e-02 -2.23489944e-02  3.13432589e-02 -5.77390231e-02
 -4.94332239e-02 -5.67414314e-02 -2.86400001e-02  5.35149791e-33
  4.26281504e-02  3.55812314e-04 -4.65911347e-03  9.55376327e-02
  3.85774896e-02  2.50097387e-03 -8.68121907e-02  3.50717567e-02
 -3.09051923e-03 -5.59929498e-02 -5.00271842e-02  1.04611777e-01
 -7.29236975e-02  1.23458862e-01  8.64188746e-02  3.24325301e-02
 -7.04274625e-02  9.99408439e-02 -6.70499131e-02  2.67440509e-02
  2.60909554e-02 -4.25649174e-02  8.56801867e-02  4.81450511e-03
  6.83448240e-02 -2.35454598e-03 -1.11305350e-02  7.26219499e-04
 -7.85806999e-02  4.24283072e-02 -6.65203631e-02  2.70881178e-03
 -1.33483643e-02  4.09114175e-03  7.74124190e-02  4.56808768e-02
 -3.73847298e-02 -5.68485353e-04  1.66431963e-02 -4.99182083e-02
  7.59562012e-03  3.65067571e-02 -1.94201432e-02 -6.90788999e-02
 -5.74504845e-02 -3.57364565e-02  3.62029597e-02 -3.46435769e-03
  4.10339385e-02  1.61157362e-02  2.41225716e-02 -2.94623412e-02
 -5.06516248e-02 -5.27866147e-02  7.60142691e-03  4.18828018e-02
 -1.36347990e-02  2.76387017e-02  4.85838018e-02  7.17063397e-02
 -3.53960041e-03  6.16524145e-02 -2.04468402e-03  3.47351283e-02
  4.30103652e-02 -5.20532206e-02 -5.20041697e-02  1.44503405e-02
  4.94180098e-02  6.83926716e-02  2.65869871e-02 -3.02060563e-02
 -3.37493904e-02 -7.94185232e-03  2.44687069e-02  9.03922692e-03
  8.57420415e-02 -3.22015174e-02 -1.68831963e-02  1.03198148e-01
 -6.35859370e-02  7.82319251e-03 -6.97164088e-02 -1.04473673e-01
 -1.05338573e-01  1.10083148e-02  8.71929377e-02  1.83309813e-03
 -8.37335810e-02 -1.48557278e-03 -2.43467744e-02  5.76894060e-02
 -4.78791483e-02  8.78006453e-04 -1.17669560e-01 -6.55642921e-33
 -4.23251316e-02  8.95894319e-02 -2.23964490e-02  6.57358915e-02
  1.92764867e-02 -2.54305601e-02  4.03328016e-02  7.14262798e-02
  2.17491365e-03 -6.50521368e-02 -3.95211987e-02  3.91220301e-02
  1.82429031e-02 -4.19859849e-02 -8.79119039e-02 -1.17739886e-02
 -6.19818456e-02 -6.30014986e-02 -5.71286716e-02  6.77577406e-02
  3.53133641e-02  1.99476350e-02 -7.75094107e-02  1.89814512e-02
 -2.13776641e-02  3.59213315e-02 -6.64234918e-04 -7.37576094e-03
  1.19651128e-02 -4.29014452e-02  6.60330802e-02 -8.44964981e-02
 -6.02342514e-03  9.98250842e-02 -7.24546378e-03 -6.83767125e-02
  1.48369953e-01 -8.21916573e-03 -7.04483986e-02  1.36288211e-01
  3.59627940e-02  6.51672781e-02 -6.47756457e-02  1.73687153e-02
 -6.71379222e-03 -4.78213280e-02 -5.23535572e-02 -6.42555580e-03
  9.61116608e-03 -4.57081310e-02  3.79986363e-03  1.00060622e-03
 -6.13344461e-02  1.19591160e-02 -1.30305141e-02 -7.64041170e-02
 -6.73688902e-03 -6.64977953e-02  2.14284640e-02  4.84415218e-02
 -2.57988796e-02 -4.19498421e-02  3.93742602e-03  2.45576464e-02
  6.84931991e-04 -3.69056910e-02 -4.81230952e-02 -8.73991940e-03
 -1.14002660e-01  8.90104175e-02 -6.08727662e-03  9.56296548e-03
  2.21991558e-02 -2.38897409e-02 -7.20279813e-02 -2.61530150e-02
 -2.41852030e-02  3.60436030e-02  1.07018361e-02 -6.96357759e-03
  3.53977121e-02 -4.17848444e-03 -9.82357934e-03  9.66157168e-02
  5.02358079e-02  8.74318406e-02  4.89644147e-02  3.08148516e-03
  2.92979013e-02 -3.22228521e-02 -4.87550534e-02  4.09033485e-02
  1.25858141e-03  1.14255976e-02 -8.32522288e-03 -3.28257705e-08
 -1.76801737e-02  1.10877948e-02  7.13152215e-02  1.55721232e-02
  1.24180568e-02 -2.91100517e-03 -7.49361515e-02  9.45675373e-02
  7.63025973e-03 -9.39479272e-04  1.74953379e-02 -1.57852713e-02
 -1.66086674e-01  4.83886674e-02  7.88640529e-02 -3.38906259e-03
  5.88402264e-02  1.37357920e-01  1.01829423e-02  2.38375273e-02
  1.19808558e-02  3.35419737e-02  3.01384404e-02 -2.10248213e-02
 -1.08572077e-02 -3.60529609e-02 -1.60209816e-02  2.29200479e-02
 -3.21684070e-02  2.45316755e-02  7.29184970e-02  2.75301486e-02
  6.31791800e-02 -7.24497736e-02 -4.15693857e-02  4.80145253e-02
  1.76478282e-03  2.61162617e-03 -3.61853722e-03  2.76041385e-02
 -1.34973461e-02  2.95795165e-02 -1.89790465e-02  3.94403264e-02
  2.37381291e-02  3.03785745e-02  4.24589738e-02 -1.02036828e-02
 -9.55658257e-02  3.79890539e-02 -3.66369262e-02  4.32654563e-03
 -2.15690229e-02  4.37448919e-02  3.11194267e-02  3.53492424e-02
  3.78352441e-02 -1.25473216e-01  5.23153208e-02 -4.36052494e-03
  4.57046181e-02  4.49380092e-02 -6.40475675e-02 -4.84530674e-03]",2,0
pytorch_optimizer,1,build quality package status http pytorch optimizers readthedocs io en latest also you can load the optimizer via torch huband you can check the supported optimizers lr scheduler optimizerdescriptionofficial codepaperadabeliefadapting step size by the belief in observed gradientsgithubhttps arxiv org ab adaboundadaptive gradient method with dynamic bound of learning rategithubhttps openreview net forum id bkg g r fxadahessianan adaptive second order optimizer for machine learninggithubhttps arxiv org ab adamdimproved bias correction in adamhttps arxiv org ab adampslowing down the slowdown for momentum optimizers on scale invariant weightsgithubhttps arxiv org ab diffgradan optimization method for convolutional neural networksgithubhttps arxiv org ab v madgrada momentumized adaptive dual averaged gradient method for stochasticgithubhttps arxiv org ab radamon the variance of the adaptive learning rate and beyondgithubhttps arxiv org ab rangera synergistic optimizer combining radam and lookahead and now gc in one optimizergithubhttps bit ly zyspc ranger a synergistic deep learning optimizergithubhttps arxiv org ab lamblarge batch optimization for deep learninggithubhttps arxiv org ab shampoopreconditioned stochastic tensor optimizationgithubhttps arxiv org ab nerolearning by turning neural architecture aware optimisationgithubhttps arxiv org ab adanadaptive nesterov momentum algorithm for faster optimizing deep modelsgithubhttps arxiv org ab several optimization idea to regularize stabilize the training most of the idea are applied in ranger optimizer also most of the capture are taken from ranger paper adaptive gradient clippinggradient centralizationsoftplus transformationgradient normalizationnorm losspositive negative momentumlinear learning rate warmupstable weight decayexplore exploit learning rate schedulelookaheadchebyshev learning rate schedule adaptive sharpness aware minimizationon the convergence of adam and beyondgradient surgery for multi task learningcode githubpaper arxivgradient centralization gc operates directly on gradient by centralizing the gradient to have zero mean code githubpaper arxivby running the final variance denom through the softplus function it lift extremely tiny value to keep them viable paper arxivpaper arxivcode githubpaper arxivpaper arxivcode githubpaper arxivcode githubpaper arxivcode githubpaper arxivacceleration via fractal learning rate schedulespaper arxivsam paper paperasam paper papera sam code githubpaper paperpaper paperadampadaptive gradient clippingchebyshev lr schedulesgradient centralizationlookaheadradamnorm losspositive negative momentumexplore exploit learning rate scheduleon the adequacy of untuned warmup for adaptive optimizationstable weight decay regularizationsoftplus transformationmadgradadahessianadaboundadabeliefsharpness aware minimizationadaptive sharpness aware minimizationdiffgradon the convergence of adam and beyondgradient surgery for multi task learningadamdshampooneroadanhyeongchan kim kozistr,"[('quality package status http pytorch optimizers', 0.6126), ('optimizergithubhttps arxiv org ab lamblarge batch optimization', 0.562), ('machine learninggithubhttps arxiv org ab', 0.5222), ('convolutional neural networksgithubhttps arxiv org ab v madgrada', 0.5196), ('stochastic tensor optimizationgithubhttps arxiv org ab nerolearning', 0.5113), ('synergistic optimizer', 0.5083), ('momentum optimizers', 0.5066), ('deep learninggithubhttps arxiv org ab', 0.5039), ('optimizers lr', 0.4996), ('optimizer', 0.4774)]","[-9.64165702e-02 -2.75427643e-02  4.82239528e-03 -4.09425050e-02
 -1.74296778e-02 -8.34469944e-02 -7.99141824e-03 -3.86491790e-02
 -1.25857383e-01 -2.44436823e-02 -3.00220773e-02  3.24093662e-02
 -7.54880011e-02 -5.38305901e-02  1.28838597e-02  4.95443158e-02
  3.05501539e-02  6.63935244e-02 -6.76470101e-02 -1.53178200e-01
 -6.51544109e-02 -9.86241922e-03  3.06043960e-03 -5.32751204e-03
  1.89098734e-02  1.91944521e-02 -6.05440848e-02 -2.33040340e-02
  7.05358908e-02 -1.11557409e-01  2.68056970e-02 -2.31393017e-02
  7.36881420e-02 -3.53992023e-02  1.05624180e-02  8.43150467e-02
 -7.94602558e-02 -7.79568702e-02  5.31854406e-02  8.44429154e-03
 -3.79370749e-02  4.27714037e-03 -5.68042360e-02 -1.36622775e-03
  4.07234766e-02  9.12009925e-03  7.86650646e-03 -5.80174997e-02
  1.44832069e-02 -4.24093604e-02 -9.53902677e-02 -5.21348268e-02
 -2.81800739e-02 -4.06615138e-02 -8.29191878e-03 -2.29161829e-02
  1.66218132e-02  3.09838336e-02 -2.00538766e-02 -2.65711686e-03
  6.28153514e-03 -5.59353456e-02 -4.54482026e-02  2.27933601e-02
 -1.41294794e-02 -4.39393520e-03  5.50101250e-02  1.08354073e-03
  9.07838047e-02 -1.51549559e-02 -4.67387736e-02  4.00432274e-02
 -5.53466231e-02  6.80873245e-02 -1.48986075e-02  4.00481001e-02
  1.11343294e-01 -1.29089039e-02  1.61397848e-02 -7.14349821e-02
  1.50630130e-02 -5.14595658e-02 -1.73286349e-02  2.14237552e-02
  5.67085817e-02 -2.54704040e-02 -5.18525131e-02  4.18561846e-02
 -6.30059931e-03 -6.63311929e-02  2.51482688e-02 -2.86728814e-02
 -3.18938022e-04  9.83528979e-03 -3.18302810e-02  2.95956209e-02
  2.27076411e-02 -9.91714001e-02 -9.56141576e-02  1.00355990e-01
 -6.31929282e-03 -4.89677675e-02  5.44575341e-02 -5.13867997e-02
 -5.73841073e-02  2.78582820e-03  5.25826328e-02  8.01549330e-02
  9.77755487e-02 -2.28590937e-03 -2.57979776e-03  3.36769149e-02
 -8.77383631e-04 -1.37261599e-01  1.49008501e-02  1.19848438e-02
 -3.03699896e-02  9.65438318e-03  7.34289438e-02 -1.28706647e-02
 -5.47177643e-02  1.71310399e-02  5.93236974e-03 -4.22031246e-03
 -4.60360721e-02  1.21230376e-03 -6.61328956e-02  1.78575166e-32
 -8.81741568e-02  9.67069282e-05 -3.08597144e-02 -8.15536901e-02
  2.06160489e-02 -3.09228506e-02  7.21562728e-02  2.70279800e-03
 -3.79426293e-02 -9.06902403e-02 -7.23896623e-02 -5.25945388e-02
 -1.02856517e-01  1.22174658e-01  6.41428754e-02 -1.19667232e-01
 -7.24935085e-02  8.77473205e-02 -6.00925973e-03 -9.06614028e-03
  1.72017515e-02 -6.15019687e-02 -9.77011747e-04  8.13350827e-02
  2.23398432e-02  5.15454151e-02  8.87969509e-02  3.27056907e-02
 -7.02100322e-02  1.53284203e-02 -2.90419292e-02  2.84480229e-02
 -1.94259472e-02 -2.26092851e-03 -2.41621834e-04  1.51459286e-02
 -1.04290687e-01 -2.39650384e-02  1.28986640e-03 -6.92954659e-02
 -3.12808380e-02  8.14475343e-02 -4.97324876e-02  2.90770233e-02
 -6.45220419e-03 -2.60243416e-02  8.26975610e-03  6.44727349e-02
  5.28061911e-02 -6.03928901e-02 -8.48456174e-02 -8.17468204e-03
  5.65147139e-02  1.90912038e-02  1.58395041e-02  1.37225604e-02
  1.14167430e-01  5.95790111e-02  4.78223637e-02  6.64588511e-02
  1.64009202e-02  7.49508943e-03 -1.05712349e-02 -8.47322568e-02
  9.20862425e-03 -9.12314653e-03  3.66269872e-02  6.78508962e-03
  1.19723594e-02  3.37150842e-02 -5.03702164e-02  4.14774679e-02
  4.83476482e-02 -5.76666594e-02  5.76299876e-02 -2.38062106e-02
  1.72807314e-02  1.95414703e-02  2.07428914e-03 -1.64584145e-02
 -7.67761692e-02  7.88878724e-02  4.95747849e-02 -7.43659511e-02
 -2.47877724e-02 -2.53418274e-02  5.46380831e-03 -1.05174240e-02
 -9.49051511e-03  5.57342805e-02 -9.81910601e-02  3.99251767e-02
  5.42957969e-02  6.59619868e-02 -2.07554945e-03 -1.61356848e-32
 -4.39990796e-02  8.24412853e-02 -1.28688235e-02  1.81160688e-01
 -2.54322551e-02  2.16797106e-02 -5.09832725e-02 -6.92886561e-02
  2.35238597e-02  3.71101685e-02  2.94769667e-02  2.93586543e-03
  2.77847871e-02 -3.63795497e-02  1.50899049e-02  2.06897687e-03
 -4.67335200e-03 -6.63088486e-02  2.26549469e-02  9.06925881e-04
  8.02398100e-03  9.50224921e-02 -6.38762489e-02 -9.18176316e-04
 -6.96141273e-03 -4.60388586e-02 -6.77097589e-03 -3.03813890e-02
  4.78665121e-02 -4.26881686e-02  7.82939233e-03  8.61274824e-03
 -1.04928665e-01  1.00577936e-01 -6.82719098e-03  6.85039908e-03
  1.00490086e-01  3.66044790e-02 -2.37445626e-02  5.80626428e-02
  1.36708856e-01  6.21291213e-02 -3.61454636e-02  3.39013413e-02
  1.59600389e-03 -3.44640538e-02 -5.57879992e-02 -2.90332716e-02
 -2.60918643e-02 -1.78952385e-02 -3.79159711e-02 -1.02688847e-02
 -2.27468424e-02  1.74845997e-02 -1.73407625e-02  1.15677398e-02
 -2.70228460e-02  2.18350943e-02  6.55858144e-02  1.56820379e-02
 -1.03900269e-01 -4.33363542e-02  8.18273809e-04 -2.81393267e-02
 -8.26507062e-03  2.38829684e-02 -3.13927382e-02  1.02299914e-01
 -3.07369325e-02  1.25967031e-02  8.02687649e-03  2.30469611e-02
  2.13661063e-02  6.55110255e-02 -5.49275838e-02 -9.49858222e-03
  3.61792706e-02  7.29065612e-02 -1.77653115e-02 -2.78628953e-02
  7.90225789e-02  3.54644991e-02 -1.65547784e-02  8.33903402e-02
  1.01079056e-02  6.74262941e-02  1.54189765e-02  1.97350932e-03
  6.74042329e-02  1.90126467e-02 -7.95060210e-03  4.70820516e-02
  6.47160932e-02  2.16648243e-02 -1.12411492e-02 -5.32266853e-08
 -4.00026217e-02  8.84716678e-03 -2.06636619e-02 -2.16290914e-02
  8.53421018e-02  3.44678340e-03 -2.29153559e-02  1.06413223e-01
 -1.44097703e-02 -1.34180300e-02  1.37997400e-02 -5.07978257e-04
 -1.29305333e-01 -1.95392538e-02  4.99910954e-03  3.27376947e-02
 -6.44246265e-02  9.77975875e-02  3.18737067e-02 -5.01818657e-02
  3.77970003e-02  1.88646782e-02  3.20546627e-02  2.97169629e-02
  7.65319634e-03 -6.90578967e-02  9.43046063e-03 -1.37390438e-02
  6.14501648e-02 -3.78670990e-02 -1.42213330e-02  4.64645959e-02
  7.24859536e-02 -1.45528749e-01  1.06417082e-01  7.74376765e-02
 -3.27168219e-02 -1.77163128e-02  2.63477745e-03  5.48781157e-02
 -5.21765314e-02  6.11552447e-02 -1.48353046e-02 -3.76469865e-02
  2.11428199e-02  2.96719815e-03 -4.67097796e-02 -8.93516541e-02
 -1.94149986e-02 -5.18042259e-02  7.83862025e-02 -3.30188647e-02
 -3.15777138e-02  5.06981695e-03  7.53683271e-03  2.33514775e-02
  1.03385467e-02 -9.11162421e-02  3.44371982e-02  5.52372914e-03
  5.58027327e-02 -1.87602045e-03  2.22633383e-03  1.51813114e-02]",2,2
swissarmytransformer,1,swissarmytransformer is a flexible and powerful library to develop your own transformer variant swissarmytransformer is named after swiss army knife meaning that all the model e g bert gpt t glm cogview vit share the same backone code and cater for versatile usage with some extra light weight mixins swissarmytransformer is powered by deepspeed zero and model parallelism aiming to provide the best practice for pretraining and finetuning large model m b parameter add model agnostic component e g prefix tuning in just one line build your transformer based model with minimal code we mentioned glm which only differs from standard transformer called basemodel on position embedding and training loss we only need to focus on the related part when coding class blockpositionembeddingmixin basemixin here define parameter for the mixin def init self max sequence length hidden size init method std super blockpositionembeddingmixin self init self max sequence length max sequence length self hidden size hidden size self block position embeddings torch nn embedding max sequence length hidden size torch nn init normal self block position embeddings weight mean std init method std here define the method for the mixin def position embedding forward self position id kwargs position id block position id position id position id position embeddings self transformer position embeddings position id block position embeddings self block position embeddings block position id return position embeddings block position embeddings class glmmodel basemodel def init self args transformer none parallel output true super init args transformer transformer parallel output parallel output self add mixin block position embedding blockpositionembeddingmixin args max sequence length args hidden size add the mixin for glm comprehensive support for training swissarmytransformer aim to provide the best practice for pretraining and finetuning where you only need to finish forward step and create dataset function but with hyperparameters to alter useful training configuration the most typical python file to use bert in swissarmytransformer for inference is a follows then we can run the code viaall officially supported model name are in url py to finetune or pretrain a transformer is also extremely easy then we can run the code viahere we use data parallel on gpus we can also launch the training on many inter connected machine via hostfile path to hostfile see the tutorial for more detail to write your own model you only need to consider the difference between the standard transformer for example if you have a idea to improve the attention operation here attention fn is a hook function replacing the default action by the new function all available hook are in transformer default py now we can use add mixin to apply our change to all the transformer such a bert vit and cogview see the tutorial for more detail to be released soon currently we don t have a paper so you don t need to formally cite u if this project help your research or engineering use footnote http github com thudm swissarmytransformer to mention u and recommend swissarmytransformer to others the tutorial for contributing swissarmytransformer is on the way the project is based on a user of deepspeed megatron lm and huggingface transformer thanks for their awesome work,"[('training swissarmytransformer', 0.5785), ('variant swissarmytransformer', 0.5505), ('swissarmytransformer', 0.5458), ('extra light weight mixins swissarmytransformer', 0.4437), ('transformer', 0.4111), ('thudm swissarmytransformer', 0.4049), ('own transformer', 0.3932), ('glm', 0.3816), ('huggingface transformer thanks', 0.3575), ('basemodel', 0.3475)]","[-9.88652259e-02 -4.72004302e-02  1.36086706e-03 -4.47455458e-02
  3.32303792e-02 -1.16288522e-02 -9.21926089e-03  4.67400700e-02
 -9.20415819e-02 -7.36008435e-02  3.56038362e-02 -5.24201505e-02
 -1.57215446e-02  2.93364804e-02 -2.86537800e-02 -1.56629328e-02
 -4.73347679e-02  8.11594352e-02 -1.00072771e-01 -2.04518363e-02
 -4.87131141e-02  1.31480731e-02  3.27982940e-02  6.47977144e-02
 -2.84708999e-02 -5.40957181e-03  3.34168859e-02 -2.34960932e-02
  3.43036093e-02 -1.44836217e-01 -3.53538245e-02  4.47720475e-02
 -2.93168090e-02  1.49002615e-02 -5.21602705e-02 -1.22623405e-05
  6.96064997e-03  3.19042057e-02 -3.64354476e-02  2.66886558e-02
  2.67562028e-02 -8.91221166e-02  5.16520143e-02 -4.45915721e-02
  5.85811287e-02 -1.06826508e-02  3.12682390e-02 -8.57196525e-02
  1.99132860e-02  4.06796038e-02 -2.79421657e-02 -1.34105757e-01
 -3.40253673e-02  1.00838251e-01 -6.01437408e-03 -5.65898269e-02
 -2.66676606e-03 -3.99705544e-02 -1.84723027e-02 -4.83065993e-02
 -8.75203032e-03  1.93438902e-02 -1.13855891e-01  4.80190665e-03
 -7.66974464e-02  5.45143988e-03  3.32558565e-02 -4.68001841e-03
  3.38937044e-02 -6.10946342e-02 -8.06270763e-02 -2.75845081e-02
 -7.13222288e-03  6.92719296e-02  4.93112914e-02 -2.50095744e-02
  5.37458919e-02  1.97955333e-02  2.60700826e-02 -5.84710911e-02
  1.94081869e-02  2.76040402e-03 -5.83309941e-02  5.94855323e-02
  7.80310407e-02  3.22283879e-02 -5.06136604e-02  5.01973033e-02
 -1.97166465e-02 -1.73146389e-02  1.37677146e-02  2.32998491e-03
  4.90855500e-02 -2.69373879e-02 -9.04252529e-02  1.90604216e-04
 -2.27477532e-02 -2.35539787e-02 -3.06496415e-02  9.54837278e-02
 -1.51307946e-02 -7.30101317e-02  5.06292619e-02  7.77982622e-02
 -1.15084775e-01 -3.50624509e-02 -2.42739655e-02  6.04190528e-02
  2.04193797e-02 -1.56933777e-02 -2.98638875e-03  5.54867722e-02
 -5.68946227e-02 -1.01937369e-01  4.14903574e-02 -7.81955104e-03
 -5.86059913e-02 -4.56500426e-02  2.29525138e-02  4.19358276e-02
 -3.55276391e-02 -6.59519574e-03  8.73649307e-03 -3.78572941e-02
 -1.54622039e-02 -2.49534417e-02 -1.27021790e-01  1.07710408e-32
  2.14302875e-02  9.21107009e-02  3.06274388e-02  3.04393228e-02
  1.40896030e-02 -2.64437366e-02 -1.23853481e-03  4.02554423e-02
  1.34510966e-02 -4.62889001e-02 -8.29997584e-02  1.16760746e-01
 -5.24360985e-02  1.27701893e-01  5.45735508e-02 -7.23948851e-02
 -1.37075782e-02  6.67717904e-02  2.09222292e-03  4.25671153e-02
  2.71180011e-02  4.48596478e-02 -8.40388611e-03  8.44702497e-02
 -2.52993908e-02  4.63512540e-02  3.37127708e-02  3.68845351e-02
 -4.96259667e-02  2.02696417e-02 -1.37738613e-02  7.70466179e-02
 -9.59725212e-03 -3.74853946e-02  3.34661752e-02 -2.09069233e-02
 -2.11918522e-02 -1.00795753e-01  1.81629006e-02 -4.51566949e-02
  5.03320247e-03  1.45800589e-02 -9.09477398e-02 -3.38160992e-02
  1.13643959e-01  6.15854524e-02  5.27306944e-02  6.16789721e-02
  1.74888615e-02  1.38225146e-02  5.37594722e-04  2.87221577e-02
 -3.99790183e-02  1.87587808e-03 -2.50500254e-03  9.20261070e-02
 -2.07323022e-02 -7.12699257e-03  8.05662572e-03  4.57430705e-02
 -4.87111919e-02  9.16911587e-02  2.94579770e-02 -2.15444807e-03
 -9.82909277e-03 -3.59997861e-02  7.59601081e-03 -1.09835230e-01
 -1.74561124e-02 -1.04383957e-02 -5.13343103e-02 -8.38412065e-03
  1.12571931e-02  9.41234827e-02  1.00971647e-01 -8.11481569e-03
 -6.86892075e-03  8.25260878e-02  2.10394096e-02 -6.79154247e-02
 -6.47640526e-02  4.95275967e-02  2.83083264e-02  6.59545045e-03
 -9.41553265e-02  1.93504349e-03  6.31877109e-02 -7.57709667e-02
 -4.49463911e-02  6.09175349e-03 -8.05278048e-02 -1.20797092e-02
 -3.30121769e-03 -7.28064915e-04  1.00437459e-03 -1.02531665e-32
  7.93778524e-02  2.27925386e-02 -6.30653426e-02 -1.30096106e-02
  5.91619983e-02  5.39669208e-02  6.68066368e-02 -4.21862584e-03
 -1.89451920e-03  2.78003234e-02  1.00316122e-01  3.91699038e-02
  9.57985688e-03 -1.63069163e-02  3.44285108e-02 -5.26920706e-02
 -1.27199134e-02 -4.16008048e-02 -1.26843927e-02 -4.25758548e-02
  7.83498734e-02  1.09545730e-01 -3.71408276e-02 -2.26454791e-02
 -4.28361334e-02  9.08305962e-03 -9.49987844e-02  1.16558962e-01
  5.35904057e-02  5.12644425e-02 -6.05881661e-02 -5.76835684e-02
  5.20690270e-02 -2.14468203e-02 -1.22050084e-01  4.36790846e-02
  1.47251235e-02  3.45336422e-02 -7.73815089e-04  3.44872251e-02
 -1.85558889e-02 -5.01226634e-02  1.68339759e-02  5.01901768e-02
 -2.77210362e-02 -7.53903687e-02 -5.20227775e-02 -6.53932765e-02
  1.93744972e-02 -1.96502842e-02  1.54693052e-02  3.19483504e-02
 -5.00966161e-02 -4.58911508e-02  5.26407361e-03 -2.50579626e-03
  6.65946975e-02 -8.96809623e-02  1.89771224e-02 -2.15197429e-02
 -2.63216421e-02  4.97473404e-03 -3.10649019e-04 -2.90348753e-02
  2.63048010e-03 -9.41193774e-02  2.95092072e-03 -8.71009603e-02
  2.34135725e-02  6.21607639e-02  5.71258888e-02  1.97880492e-02
  1.12408958e-01  4.89752926e-02  6.90435059e-03 -3.69972885e-02
  6.55567087e-03 -1.43954968e-02 -1.22000678e-02 -4.33305576e-02
 -2.85862163e-02 -1.04083389e-01  6.18917197e-02  1.38285413e-01
  6.90685632e-03  3.01997811e-02  6.34826645e-02 -5.14841676e-02
  4.56894934e-02 -3.85854510e-03 -4.71786931e-02  9.36316997e-02
  3.50902788e-02  6.81981742e-02 -1.38188759e-03 -4.17405239e-08
 -2.65260902e-03  3.45296264e-02  2.09785412e-05 -1.22140450e-02
 -3.25363316e-02  2.36506928e-02 -4.39711884e-02  4.60023284e-02
 -9.17370152e-03 -4.21630917e-03 -1.27070360e-02  3.62794325e-02
 -3.13857533e-02  3.71468551e-02  5.28099202e-02 -1.09235477e-02
  3.04159895e-02  1.02809682e-01 -6.82164431e-02 -2.38016024e-02
 -3.79036926e-02  6.83734640e-02  1.01893488e-02 -7.92962089e-02
  3.39493267e-02 -7.20437691e-02 -3.90694011e-03  1.22448206e-02
  3.25771514e-03  6.98198974e-02 -3.79289500e-02  8.04752577e-03
  6.12513646e-02 -6.28040209e-02  2.90096328e-02  4.49888185e-02
 -7.49398097e-02 -4.27377112e-02 -9.85503756e-03 -4.76603769e-03
  8.99249129e-03  2.05325074e-02 -6.59836233e-02  4.92492132e-02
  6.63696155e-02 -3.68729569e-02 -6.37735873e-02 -1.39082730e-01
 -6.70617148e-02  5.23214005e-02  6.71756193e-02  3.33583690e-02
 -4.68313619e-02  1.30146323e-02  1.67415477e-02  8.15552250e-02
  1.32685713e-02 -8.13853890e-02 -2.86755967e-03  1.99608561e-02
 -2.42417422e-03 -3.49649489e-02  1.70271397e-02  7.22018704e-02]",2,0
nnv,1,simple and easy to use tool to generate neural network visualization it is possible to customize the node size color title font size spacing between node and layer and maximum number of node to show nnv documentation is still being created for now if you have any question please look directly the library source code or open an issue some useful feature that may be added in the future help is welcome if you use this library and would like to cite it you can use or,"[('nnv documentation', 0.6323), ('neural network visualization', 0.62), ('node size color title font size', 0.4288), ('node', 0.3721), ('layer', 0.2914), ('library source code', 0.2705), ('library', 0.1922), ('useful feature', 0.1642), ('tool', 0.1295), ('future help', 0.0963)]","[-4.64159921e-02 -7.87046645e-03  2.48116162e-02 -3.27304378e-02
  4.04789895e-02  4.82685976e-02 -5.80281578e-02  3.78910042e-02
 -8.62194672e-02 -1.81787815e-02 -4.01148424e-02  1.30718928e-02
 -5.17385751e-02 -1.49273705e-02 -6.34573326e-02  2.64619756e-03
  1.33959698e-02  4.73389700e-02  1.74035598e-02 -5.92181459e-02
  3.47205326e-02  7.16538075e-03  4.83175032e-02 -7.73625523e-02
 -1.27317756e-03  3.22681293e-02 -2.83820145e-02  1.40149174e-02
  6.81507364e-02 -9.67250988e-02  6.78355340e-03  3.67463119e-02
  3.79594080e-02  4.62629497e-02 -3.53407972e-02 -2.18516681e-03
  3.14697810e-02  7.68985460e-03 -1.07215405e-01  3.29421647e-02
 -2.01913230e-02  2.04428448e-03 -2.22015250e-02 -5.56264119e-03
  1.24280713e-01  5.49883842e-02 -5.90804666e-02 -7.07140490e-02
 -1.34408502e-02 -3.85940783e-02 -6.31135479e-02 -1.25432536e-01
 -2.87123621e-02 -3.58550181e-03  1.01216465e-01  2.83141937e-02
 -4.54733409e-02 -2.92384569e-02 -2.92689092e-02 -1.88070927e-02
  1.04517207e-01  2.77859401e-02 -4.79939021e-02  2.91949231e-02
  7.32464716e-02  8.81364420e-02  6.31105676e-02  6.53783306e-02
  5.27480654e-02 -1.07995830e-01 -7.44684925e-03  1.51890041e-02
 -1.65202692e-02  3.62801626e-02 -3.32261957e-02  3.76139581e-02
  3.26532200e-02 -3.26592196e-03 -1.19384110e-03 -1.23832077e-01
 -6.68734685e-02  1.11918561e-01 -4.87009471e-04  1.11815795e-01
 -2.56789681e-02  7.52197877e-02  1.40208714e-02  7.73887523e-03
  1.31624443e-02  4.34496813e-02  2.96175908e-02 -5.89641482e-02
  1.01061296e-02 -3.78315672e-02  2.43463200e-02  4.83762138e-02
 -2.01109573e-02 -9.56504270e-02  1.44930966e-02  8.38613417e-03
  1.66389737e-02 -7.13416114e-02  6.13551922e-02 -4.63212095e-02
 -4.03563194e-02  4.80069518e-02  8.29012766e-02  4.48895022e-02
  7.49309957e-02  4.01008055e-02 -2.05273516e-02  4.92246039e-02
 -1.32182613e-01 -1.13490276e-01  7.70682022e-02  3.00249886e-02
 -1.95974763e-02 -1.60375983e-02  1.08577125e-02  2.19830144e-02
 -9.08079073e-02 -2.15807091e-02 -5.20173945e-02  2.03748397e-03
 -2.86109224e-02  6.77745137e-03 -2.87428405e-02  1.89483794e-33
 -5.58735803e-03  2.38801558e-02 -6.75852150e-02  5.50649129e-02
  1.11728922e-01 -1.62456892e-02  7.95340091e-02 -2.29505301e-02
 -6.63033873e-02 -4.93659116e-02 -5.58239687e-03  9.77697298e-02
 -6.03788532e-02  8.83713290e-02 -2.01548878e-02 -6.34643659e-02
  2.29195529e-03  3.81483771e-02 -6.03944659e-02 -4.53927554e-03
  5.35190701e-02 -4.92633991e-02  7.39330426e-02  7.58921951e-02
  1.04288170e-02  1.10375285e-02 -4.81901169e-02 -2.01948755e-03
  1.05486764e-02 -4.14887108e-02 -3.16516273e-02 -9.61529184e-03
  6.05256855e-02 -2.22346429e-02  1.86272543e-02  1.53938551e-02
 -6.43906370e-02 -4.61091921e-02  1.66754331e-02  2.61691362e-02
 -3.09177767e-02  4.10966799e-02 -6.62181079e-02 -1.96492523e-02
 -3.69712040e-02  5.46960952e-03 -1.51634542e-02 -2.84687877e-02
  5.15457056e-02 -5.75124361e-02 -7.57881440e-03  4.39662300e-02
  1.03408741e-02 -5.81469834e-02  5.95413335e-02  2.00550538e-02
  6.67738616e-02  6.07050285e-02  6.25029160e-03  1.75666139e-02
  1.38517795e-02 -4.18080817e-05  1.96121298e-02 -4.12342586e-02
 -1.61794538e-03  3.55630405e-02 -5.26307225e-02 -3.88088375e-02
  6.84681237e-02 -7.61394054e-02 -5.44622988e-02  7.48073757e-02
  7.28613324e-03 -2.37826202e-02  1.87956523e-02 -1.07022366e-02
 -8.26601163e-02 -9.27705616e-02 -4.74976702e-03  5.01604378e-02
 -7.44167268e-02  1.27352020e-02  1.90513898e-02 -3.39594372e-02
  2.38446984e-02 -4.56045642e-02  6.72068074e-02  1.56860352e-02
 -2.35219020e-04 -4.83967587e-02  4.86830622e-02 -1.07264491e-02
 -2.03743987e-02 -1.07860148e-01 -1.85825787e-02 -1.37723297e-33
 -4.57947329e-02  5.11129163e-02 -8.79582241e-02  7.03702122e-02
 -1.67328808e-02  4.28455770e-02 -8.10633898e-02  6.12339899e-02
  2.61754189e-02  3.43963597e-03  3.54885831e-02 -4.34472896e-02
  3.64403613e-02 -1.36782238e-02 -1.77206472e-02 -2.87159793e-02
 -5.81680089e-02 -2.53918655e-02  4.80363667e-02 -2.50565950e-02
 -5.93904108e-02  8.64972398e-02 -5.47558144e-02 -1.35989673e-02
  1.97946243e-02 -8.98570847e-03 -3.70065821e-03 -1.57948006e-02
 -9.50486865e-03 -3.31673175e-02  1.66393351e-02 -5.43197431e-03
 -2.77385935e-02  3.28606404e-02  3.87755781e-02 -1.51810171e-02
  1.27049044e-01 -1.92277450e-02 -1.18827652e-02  1.50943790e-02
  1.11781359e-01 -2.18924023e-02 -1.27389729e-02 -8.91211703e-02
 -9.75370035e-02  3.35234893e-03 -5.61276488e-02  6.28419667e-02
 -7.52063375e-03  1.79730114e-02  9.73159745e-02 -2.71281786e-02
  3.28279324e-02 -9.42461193e-03  5.69368480e-03  4.85640988e-02
  4.81424853e-02  1.05433032e-01  1.48694932e-01  1.36034684e-02
 -4.63381335e-02 -6.23555966e-02 -4.50906269e-02  5.70599269e-03
 -8.35766420e-02 -7.73889720e-02 -8.00292715e-02  1.61164105e-02
 -4.33901697e-02 -9.34362262e-02 -6.74405545e-02  5.02771921e-02
  6.71976879e-02 -2.87701329e-03 -6.54078349e-02 -7.14406520e-02
  4.53619175e-02  1.55454911e-02  2.90493052e-02 -2.95031108e-02
  7.67852413e-03  2.70906165e-02  1.66655332e-02  6.64082989e-02
  9.12538767e-02  1.62743539e-01  6.31879643e-02  7.57532474e-03
  4.30948325e-02  1.63333155e-02 -5.12030087e-02 -3.41976847e-04
 -5.56140803e-02  1.06185295e-01 -3.47986259e-02 -2.46854803e-08
 -2.53208503e-02  1.16533292e-02  1.33825513e-02  2.53596506e-03
  5.01378477e-02  2.82048546e-02  6.28616363e-02  8.62378776e-02
 -4.34087440e-02  1.08094104e-01  8.04727226e-02 -2.30728537e-02
 -7.32814744e-02 -7.05220643e-03  5.04192784e-02  4.52754423e-02
  2.19555050e-02  3.61893401e-02 -2.53686830e-02 -4.62296233e-02
 -1.12084048e-02  3.05819754e-02 -7.43195275e-03  4.27729972e-02
  1.51767917e-02 -2.60045733e-02  1.54254586e-03  6.04226328e-02
  2.26738397e-02 -8.67860094e-02  1.00599797e-02  6.25332519e-02
  9.62969810e-02 -8.43819752e-02  5.24601601e-02  5.11932969e-02
 -2.95445044e-02  6.34032935e-02  2.69831792e-02  5.44924848e-02
 -7.37703592e-02  2.00427435e-02  1.72079112e-02 -1.67688573e-04
 -1.73404235e-02  3.72243789e-03 -4.42187078e-02 -5.89781664e-02
 -6.20859377e-02 -3.59328426e-02 -3.87863559e-03 -3.82112749e-02
 -3.45937200e-02  4.33880202e-02 -2.46063303e-02  4.30734456e-02
  1.57019980e-02 -1.07228318e-02  1.08279660e-02 -7.71283079e-03
 -5.72890155e-02  6.62531480e-02  1.16309812e-02  1.37949465e-02]",2,2
forte,1,download quick start contribution guide license documentation publication bring good software engineering to your ml solution starting from data forte is a data centric framework designed to engineer complex ml workflow forte allows practitioner to build ml component in a composable and modular way behind the scene it introduces datapack a standardized data structure for unstructured data distilling good software engineering practice such a reusability extensibility and flexibility into ml solution datapacks are standard data package in an ml workflow that can represent the source data e g text audio image and additional markup e g entity mention bounding box it is powered by a customizable data schema named ontology allowing domain expert to inject their knowledge into ml engineering process easily to install the released version from pypi to install from source to install some forte adapter for some existing library install from pypi install from source some component or module in forte may require some extra requirement writing nlp pipeline with forte is easy the following example creates a simple pipeline that analyzes the sentence token and named entity from a piece of text before we start make sure the spacy wrapper is installed let s start by writing a simple processor that analyze po tag to token using the good old nltk library if we break it down we will notice there are two main function in the initialize function we download and prepare the model and then in the process function we actually process the datapack object take the some token from it and use the nltk tagger to create po tag the result are stored a the po attribute of the token before we go into the detail of the implementation let s try it in a full pipeline here we have successfully created a pipeline with a few component let s see it run in action we have successfully created a simple pipeline in the nutshell the datapacks are the standard package flowing on the pipeline they are created by the reader and then pas along the pipeline each processor such a our nltkpostagger interface directly with datapacks and do not need to worry about the other part of the pipeline making the engineering process more modular in this example pipeline spacyprocessor creates the sentence and token and then we implemented the nltkpostagger to add part of speech tag to the token to learn more about the detail check out of documentation the class used in this guide can also be found in this repository or the forte wrapper repositorythe data centric abstraction of forte open the gate to many other opportunity not only doe forte allow engineer to develop reusable component easily it further provides a simple way to develop composable ml module for example forte allows u to to learn more about these you can visit forte wa originally developed in cmu and is actively contributed by petuum in collaboration with other institute this project is part of the casl open source family if you are interested in making enhancement to forte please first go over our code of conduct and contribution guideline apache license,"[('nlp pipeline', 0.5605), ('complex ml workflow forte', 0.5371), ('ml solution datapacks', 0.5105), ('ml workflow', 0.4936), ('forte wrapper repositorythe data', 0.4499), ('data forte', 0.436), ('documentation', 0.4175), ('ml engineering process', 0.4173), ('example forte', 0.405), ('good software engineering practice', 0.3864)]","[-8.56999028e-03 -6.64162487e-02  3.38939242e-02 -3.07577793e-02
  1.08654732e-02 -7.75577500e-02 -4.02466394e-03  5.74328117e-02
 -4.75764461e-02 -3.57399099e-02 -9.18401927e-02  2.33560125e-03
 -4.91797775e-02 -3.18332873e-02  2.39706915e-02  8.68176669e-02
 -6.08546622e-02  2.15511471e-02 -1.97150949e-02 -8.62578899e-02
 -1.15165412e-02  1.86872925e-03 -4.94887829e-02  2.33957432e-02
 -9.76450965e-02  7.56191313e-02  2.66509149e-02 -2.54424103e-02
  7.63108358e-02 -3.33766714e-02 -2.53988840e-02  3.69206518e-02
  5.49294278e-02  6.64486736e-02 -4.13477980e-02  6.55828789e-02
  4.87959087e-02 -2.16591116e-02  3.05884089e-02  2.87624765e-02
 -8.26594085e-02 -5.39664999e-02 -2.07759589e-02 -1.00070953e-01
  8.03717524e-02 -8.24159980e-02 -2.54311934e-02 -1.65242165e-01
 -4.78784516e-02  3.47044580e-02 -1.75759956e-01 -3.80521491e-02
 -6.58839867e-02  8.73827860e-02  3.11328899e-02 -1.72332246e-02
  9.16110724e-02 -1.24755327e-03 -3.63517227e-03 -4.63978015e-02
 -4.88010198e-02 -1.67076662e-02 -6.92359135e-02 -3.22014792e-03
 -1.47448881e-02  6.10660994e-03 -3.50551903e-02  7.00540692e-02
  1.20848745e-01  1.85380261e-02 -5.31067029e-02  3.12678963e-02
 -2.11136583e-02 -5.44803310e-03 -1.39888590e-02  6.57011122e-02
  6.09707832e-02 -5.24860993e-02  3.73997167e-02 -1.03825264e-01
 -7.42084300e-03  6.80085793e-02 -5.38233714e-03  3.97918634e-02
 -2.93961708e-02 -6.84568426e-03 -6.07235208e-02  2.15571597e-02
  3.01537272e-02  2.04569250e-02 -9.79563966e-03 -9.23611000e-02
 -3.01170303e-03  1.87106766e-02  1.45524051e-02  4.13181707e-02
 -3.44137065e-02 -6.30926713e-02  2.97842566e-02  2.05463031e-03
 -3.58187109e-02  7.37905502e-02 -9.14770365e-03  4.87832178e-04
 -8.94563049e-02  1.42755564e-02  2.32272390e-02  1.39214806e-02
  5.63259982e-02  1.30360927e-02 -2.54188646e-02  3.53205891e-04
 -2.14890651e-02 -4.44522165e-02  1.84643529e-02 -7.39804506e-02
 -3.39494087e-02  9.15770885e-03 -5.16646095e-02  1.68518275e-01
 -5.33184931e-02  1.22743569e-01 -8.97597708e-03 -1.49352783e-02
 -1.93079244e-02  7.76293641e-03 -3.54512148e-02  1.17737716e-33
 -1.05285821e-02  5.06311730e-02 -4.54006977e-02  3.32170986e-02
  4.67568636e-02 -9.75480750e-02  3.07568982e-02  8.81372578e-03
  8.18763208e-03  1.61358882e-02  1.59642957e-02  1.02275789e-01
 -9.74372104e-02  2.01984737e-02 -2.78037395e-02 -3.91104706e-02
 -5.46404254e-03  4.78105657e-02 -1.70132425e-02  1.23404600e-02
  7.14855269e-03  1.87870171e-02  5.86831160e-02  6.36224495e-03
  3.17488424e-02  4.27202508e-02 -2.56463210e-03 -3.27600702e-03
 -7.66192796e-03  3.94001007e-02 -2.52560731e-02 -3.37998681e-02
  8.83591268e-03  7.81906620e-02  4.61757742e-02  4.21526320e-02
 -4.36055847e-02 -5.12131713e-02  3.42037044e-02  3.86308096e-02
 -4.78133895e-02  1.90104283e-02  1.41812554e-02  1.03738848e-02
  4.63127606e-02 -3.84454355e-02 -5.75111546e-02 -1.03541240e-02
  9.43555981e-02 -6.95143789e-02 -7.86498375e-03 -6.57644719e-02
  5.31239063e-02 -2.79758610e-02  3.33126001e-02 -3.36452276e-02
 -2.08784901e-02 -9.28895455e-03  6.15886878e-03  1.24089830e-01
 -4.73142155e-02  1.28540583e-02  2.30583269e-02 -3.16292048e-02
  2.52431687e-02 -3.19631249e-02 -2.62904889e-03  2.45148130e-02
  3.42317708e-02 -9.53632966e-03 -2.32054349e-02  1.11060597e-01
  4.09183986e-02  6.11340906e-03 -1.19663430e-02 -1.12509616e-02
  2.46810392e-02 -8.50089639e-02  1.09441532e-02  4.65842672e-02
 -4.85250652e-02  7.47546973e-03  2.35525556e-02  2.18449743e-03
  1.65914912e-02 -1.15864612e-02 -2.01918669e-02  4.21733670e-02
 -3.85839790e-02 -3.83373089e-02 -5.31808883e-02  1.75742358e-02
  1.55184930e-02  7.36494735e-03  7.24700186e-03 -2.50659109e-33
 -6.32888973e-02  6.44950494e-02 -8.05107430e-02  1.19221494e-01
 -3.32853786e-04  9.69165787e-02 -5.77490265e-03 -3.98775227e-02
  2.48759184e-02  1.02221914e-01  2.90888106e-03 -6.57287836e-02
 -1.42318867e-02 -1.34862568e-02 -4.86092940e-02 -7.33158886e-02
 -2.69029085e-02 -5.59880100e-02 -1.06529547e-02  9.70277339e-02
 -3.70327383e-02  5.18911853e-02 -8.07548985e-02  6.88259155e-02
  6.87092096e-02  6.61221985e-03 -1.00213505e-01 -4.27250117e-02
  1.13174617e-02  5.74217662e-02  8.99756774e-02 -3.58824362e-03
  1.12070180e-02 -1.52756767e-02  5.06588183e-02 -1.89618710e-02
  4.18721512e-02 -1.43318139e-02  9.40054730e-02  4.01630215e-02
  6.46775812e-02  2.65288725e-03  8.60500708e-02 -7.36519024e-02
 -7.00790733e-02  5.34198387e-03 -4.84570526e-02  2.00595660e-03
  7.19372258e-02 -9.90307704e-02  5.52263996e-03 -1.19441673e-02
 -9.32225585e-02 -3.20958160e-02 -2.34481283e-02 -1.33676035e-02
  7.03952461e-02 -7.19841644e-02 -6.28366619e-02  4.59373817e-02
 -6.91921916e-03  2.96817254e-02  1.07515767e-01  7.02455547e-03
 -1.68523137e-02 -7.48865958e-03  1.65971816e-02 -8.57269950e-03
 -1.36171654e-01 -4.40906398e-02  9.38749965e-03  2.84725633e-02
  3.92806940e-02  5.71379289e-02 -2.03594025e-02 -2.98237279e-02
 -4.00605500e-02 -4.25156541e-02 -4.16245088e-02  7.66661158e-03
  1.05638593e-01 -6.51413053e-02 -1.17326463e-02  6.37236759e-02
  1.98697113e-02  3.85057889e-02  6.21545091e-02 -2.41937302e-02
 -2.19052602e-02 -7.77774751e-02 -5.24109043e-02  1.00221783e-02
 -7.02862367e-02  4.31969948e-02 -1.34093855e-02 -2.46313459e-08
 -2.87103131e-02 -2.55824476e-02  3.53933200e-02  3.84782515e-02
 -2.08290294e-02  1.61840934e-02 -1.19004659e-01  7.54055232e-02
 -4.30378579e-02  2.63326354e-02 -1.77113377e-02  1.81012992e-02
 -8.81989226e-02 -4.55474714e-03  7.10538551e-02 -2.28195358e-02
  4.71382290e-02 -7.81049719e-03 -6.56517781e-03 -7.21493810e-02
  4.12728749e-02  5.79731949e-02 -2.29706708e-02 -4.97108214e-02
  6.84085786e-02 -1.04482636e-01  1.07879303e-02 -1.32655147e-02
  2.60185134e-02 -4.69259582e-02 -2.36636251e-02  1.47155253e-02
  1.11109978e-02  1.34978443e-02  6.05414845e-02  1.62102059e-02
  1.39714643e-01 -1.13857472e-02 -4.49040681e-02  4.99097258e-02
  2.71604909e-03  9.66444537e-02 -4.40081619e-02 -5.86674921e-02
  2.16599554e-02  3.96290980e-02 -3.49530727e-02  1.34350657e-02
 -2.09782179e-02  2.96722017e-02 -1.03823598e-02  1.43929804e-02
  3.78360525e-02  7.20148534e-02  4.84165698e-02  5.33168316e-02
 -4.87625133e-03 -6.65176138e-02 -1.63243152e-02 -5.71213141e-02
 -1.97322778e-02  7.57519677e-02  8.58839229e-02  1.08850177e-03]",2,0
torchkeras,1,the torchkeras library is a simple tool for training neural network in pytorch jusk in a kera style with torchkeras you need not to write your training loop with many line of code all you need to do is justlike these two step a below i create your network and wrap it and the loss fn together with torchkeras kerasmodel like this model torchkeras kerasmodel net loss fn nn bcewithlogitsloss a metric dict parameter is optional ii fit your model with the training data and validate data the main code of use torchkeras is like below this project seems somehow powerful but the source code is very simple actually le than line of python code if you want to understand or modify some detail of this project feel free to read and change the source code besides the basic torchkeras kerasmodel another much more powerful class torchkeras lightmodel is created to support many other feature the kerasmodel is much simpler and is recommended for beginner user the lightmodel borrows many feature from the library pytorch lightning and show a best practice although different the usage of torchkeras kerasmodel and torchkeras lightmodel is very similar you can follow these full example to get started with torchkeras have fun torchkeras kerasmodel example torchkeras lightmodel example torchkeras lightmodel with tensorboard exampleif you want to understand or modify some detail of this project feel free to read and change the source code any other question you can contact the author form the wechat official account below,"[('torchkeras kerasmodel', 0.7077), ('model torchkeras kerasmodel net loss fn nn bcewithlogitsloss', 0.6802), ('torchkeras lightmodel', 0.6372), ('use torchkeras', 0.6334), ('basic torchkeras', 0.6292), ('powerful class torchkeras lightmodel', 0.6284), ('torchkeras library', 0.6189), ('torchkeras', 0.5523), ('library pytorch lightning', 0.5164), ('kerasmodel', 0.4684)]","[-4.84989695e-02 -9.05681476e-02  7.24739349e-03  3.48319933e-02
 -5.16851841e-08  1.33458516e-02 -1.37433913e-02  6.20566569e-02
 -2.97737177e-02 -2.20842808e-02  1.81555375e-02 -4.82241921e-02
 -7.10121244e-02  2.12282618e-03 -3.84968370e-02  1.22059137e-02
 -5.27350679e-02  1.03838138e-01 -1.16604224e-01 -5.75397387e-02
  1.11450359e-01 -6.54038787e-02  2.12191101e-02  2.75857188e-02
  7.13971211e-03 -2.28769202e-02  4.42014374e-02  1.65555999e-02
  4.70874161e-02 -5.20943031e-02 -5.31736538e-02 -8.56053829e-03
 -9.53726396e-02  8.22891891e-02 -2.62469258e-02 -2.54438422e-03
 -3.51428986e-02 -3.54956612e-02  2.78107896e-02  1.53578194e-02
 -3.99606954e-03 -3.65314707e-02 -3.07920445e-02 -3.41090858e-02
 -6.31093653e-03 -4.08123508e-02  4.10369113e-02 -5.46137951e-02
  1.81616284e-03 -7.83310831e-02  3.66812162e-02 -8.87500867e-02
 -1.23405047e-02  8.12741816e-02  7.95572102e-02 -2.71229949e-02
 -2.62183300e-03 -7.13638887e-02  3.00689638e-02 -1.55160269e-02
  6.78452998e-02  3.16170864e-02 -9.83127654e-02 -2.24233288e-02
 -5.33402478e-03  1.22431796e-02  4.64728521e-03  5.34872971e-02
  8.57418850e-02 -1.64445229e-02 -5.24293743e-02 -8.69228970e-03
 -3.08671258e-02  9.60548744e-02 -1.10138291e-02 -1.50503614e-03
  1.44333154e-01  3.00611574e-02  3.88936177e-02 -7.67349452e-02
 -3.64075787e-02  6.74948990e-02  8.38178955e-03  1.41491042e-02
  5.40243238e-02  1.77786592e-02 -1.59038212e-02  6.74583018e-02
  4.51786853e-02 -4.86187637e-02 -4.86544371e-02 -9.07355268e-03
  1.86810307e-02  3.92378159e-02 -5.35452105e-02 -4.12819907e-03
  8.24939460e-02 -2.42556073e-02 -7.66007006e-02  3.50002721e-02
 -2.33364943e-02 -1.86202489e-02  3.21868174e-02  1.59598682e-02
 -4.59223725e-02 -1.75921097e-02 -5.81063144e-03  6.20672405e-02
  2.41446514e-02 -3.17661949e-02  4.75749895e-02  1.63730029e-02
 -7.84191042e-02 -7.79386610e-02  1.13974258e-01 -4.77597751e-02
 -6.60139024e-02  5.51177189e-03 -2.73268297e-03  8.05265531e-02
 -8.45682621e-02  6.87642843e-02 -1.29126972e-02  2.87502445e-02
 -5.51927425e-02  1.19767394e-02 -1.14670627e-01  9.87894709e-33
  2.47975104e-02  3.93697917e-02 -7.65907541e-02  4.03337702e-02
  8.56288970e-02 -3.29354256e-02  2.70554498e-02 -2.82406583e-02
 -3.38017009e-02 -1.58069525e-02 -7.38610551e-02  1.13738989e-02
 -6.76403269e-02  3.94846499e-02  2.27163360e-02 -2.08528922e-03
  4.45779879e-03  2.42771041e-02  2.65673697e-02  3.35924700e-02
  3.28553654e-02  4.33157105e-03  2.71010343e-02  4.84108180e-02
 -3.13934386e-02  6.79924339e-02  2.48063095e-02 -3.71165983e-02
 -3.73470262e-02  3.97495180e-02  1.01279095e-02 -2.02321075e-02
  4.92399298e-02 -3.33873071e-02 -5.48058748e-03  1.34428812e-03
  3.30601633e-02 -2.36391146e-02  2.12497599e-02 -1.50154158e-01
 -5.45407236e-02  5.21745533e-02 -5.04630059e-02 -2.19812915e-02
 -1.19077796e-02  2.63194437e-04 -2.17626356e-02  2.31945701e-02
  6.58755004e-02 -2.62621827e-02 -2.84056133e-03 -2.91383006e-02
 -1.09621119e-02  3.75006385e-02  1.40747186e-02  4.72638793e-02
  2.12421063e-02  4.44171242e-02  1.36817425e-01  2.46771872e-02
 -1.63494311e-02  4.26597260e-02 -1.52858924e-02  1.81548782e-02
  3.94445751e-03 -7.63477408e-04 -8.64557028e-02 -4.40544076e-02
 -4.87643778e-02 -4.92603797e-03 -1.35399729e-01  1.06993858e-02
 -5.90401255e-02  1.41418194e-02  6.02618791e-02 -6.94030672e-02
 -3.68375182e-02 -6.85043037e-02 -8.01758096e-02  2.69932151e-02
 -9.38061252e-02  3.21628712e-02 -2.25047451e-02 -8.15441236e-02
 -8.33315402e-02 -2.55499762e-02  2.87655108e-02  3.90791195e-03
 -1.44967511e-02 -1.93822826e-03 -6.20604008e-02 -1.78969186e-02
  8.27935860e-02 -1.04445880e-02 -2.61340961e-02 -9.41711226e-33
 -2.14616749e-02  9.69383717e-02 -8.50861818e-02  6.17794208e-02
  5.26914448e-02  4.06721719e-02 -1.98408458e-02 -6.10524528e-02
 -3.67475860e-02 -1.88007224e-02  4.14416976e-02  1.24432128e-02
 -3.71075422e-02 -5.52877830e-03  8.66520852e-02 -6.11156709e-02
 -1.60040818e-02 -8.93550366e-02  1.20095490e-02 -1.67835429e-02
 -3.81861404e-02  1.08255252e-01 -1.41720206e-01 -3.95936593e-02
 -5.02494648e-02 -9.73237492e-03 -6.14132248e-02  1.19467363e-01
  8.54009856e-03 -3.14510390e-02  6.53759465e-02  3.22699361e-02
 -1.62974987e-02  1.82956997e-02 -9.50822160e-02  2.18766481e-02
  9.66949016e-02 -4.48834524e-02  8.69253278e-03  3.27486843e-02
  1.15141131e-01  4.56901528e-02 -3.16952504e-02  4.72024083e-02
 -9.78054851e-02 -6.22146344e-03 -6.82778955e-02 -5.04567511e-02
  1.53628122e-02 -1.58821896e-03  1.49997761e-02 -8.67694542e-02
 -8.43323842e-02  1.81163866e-02 -1.24099839e-03  3.77438180e-02
  8.07570443e-02  3.94960493e-03  3.18674035e-02  7.60655329e-02
 -1.07673094e-01 -9.82371643e-02  6.03452995e-02  4.99054156e-02
 -9.59291868e-03 -4.42375615e-02 -7.48393312e-02  4.10140678e-02
  3.44685167e-02  4.77152877e-03  3.65165100e-02  1.60128608e-01
 -6.41030259e-03  6.42034635e-02  5.83275687e-03 -3.36692445e-02
 -1.65024232e-02  6.46606609e-02  2.59123966e-02 -2.56297886e-02
  1.78496055e-02  7.65352161e-04  1.47804124e-02  1.01310760e-02
  7.81327039e-02  2.63017528e-02  8.40883926e-02  5.25357537e-02
 -4.89852019e-02 -6.41411394e-02  1.96126103e-02  1.02924637e-03
  8.27278942e-02  2.88727954e-02  3.63720693e-02 -3.81851990e-08
 -7.42773861e-02  6.42056987e-02  3.36396247e-02  2.47886823e-03
  6.14854321e-02 -1.69931911e-02  2.81307008e-02  1.07880905e-01
  2.53379494e-02 -1.64672788e-02  1.35993892e-02 -4.30080779e-02
  5.90854790e-03  4.98314761e-02  2.29564998e-02  1.77640487e-02
  4.17186357e-02  2.69028172e-02  4.09737200e-04 -2.11079177e-02
 -1.37345837e-02 -2.56326981e-02  1.16536086e-02  4.47323583e-02
  3.17493416e-02 -2.74750721e-02  7.12313801e-02  5.31219766e-02
 -5.31687923e-02  5.74133471e-02 -2.67177802e-02  1.85448751e-02
  6.03495501e-02 -3.72579545e-02  5.39607145e-02  1.09018862e-01
 -5.59386648e-02  1.60770807e-02 -4.89931554e-02  6.04584701e-02
 -5.34861125e-02 -4.78734486e-02 -3.21824662e-02 -9.80031583e-03
  2.35683862e-02  5.22852968e-03  9.73782316e-03 -1.17246345e-01
  3.14872572e-03  5.04872613e-02 -9.76828509e-04  2.48172451e-02
 -1.10515868e-02 -4.28765872e-03 -7.78149068e-02  2.60412935e-02
 -4.08296008e-03 -3.50929759e-02 -1.29804844e-02 -9.24895555e-02
  2.58051325e-02 -7.87605625e-03 -4.32332344e-02  2.78805569e-02]",2,2
torch_sparse,1,this package consists of a small extension library of optimized sparse matrix operation with autograd support this package currently consists of the following method all included operation work on varying data type and are implemented both for cpu and gpu to avoid the hazzle of creating torch sparse coo tensor this package defines operation on sparse tensor by simply passing index and value tensor a argument with same shape a defined in pytorch note that only value come with autograd support a index is discrete and therefore not differentiable update you can now install pytorch sparse via anaconda for all major o pytorch cuda combination given that you have pytorch installed simply runwe alternatively provide pip wheel for all major o pytorch cuda combination see here to install the binary for pytorch simply runwhere cuda should be replaced by either cpu cu cu or cu depending on your pytorch installation to install the binary for pytorch simply runwhere cuda should be replaced by either cpu cu cu or cu depending on your pytorch installation note binary of older version are also provided for pytorch pytorch pytorch pytorch pytorch pytorch and pytorch following the same procedure for older version you might need to explicitly specify the latest supported version number in order to prevent a manual installation from source you can look up the latest supported version number here ensure that at least pytorch is installed and verify that cuda bin and cuda include are in your path and cpath respectively e g if you want to additionally build torch sparse with metis support e g for partioning please download and install the metis library by following the instruction in the install txt file note that metis need to be installed with bit idxtypewidth by changing include metis h afterwards set the environment variable with metis then run when running in a docker container without nvidia driver pytorch need to evaluate the compute capability and may fail in this case ensure that the compute capability are set via torch cuda arch list e g row wise sort index and remove duplicate entry duplicate entry are removed by scattering them together for scattering any operation of torch scatter can be used transpose dimension and of a sparse matrix matrix product of a sparse matrix with a dense matrix matrix product of two sparse tensor both input sparse matrix need to be coalesced use the coalesced attribute to force torch sparse also offer a c api that contains c equivalent of python model for this we need to add torchlib to the dcmake prefix path e g it may exists in conda lib python x x site package torch if installed via conda,"[('pytorch sparse', 0.6934), ('pytorch installation', 0.6223), ('major o pytorch cuda combination', 0.5922), ('torch sparse coo tensor', 0.5868), ('sparse tensor', 0.5372), ('nvidia driver pytorch', 0.5198), ('pytorch pytorch pytorch pytorch pytorch pytorch', 0.5038), ('pytorch', 0.4872), ('least pytorch', 0.4852), ('torch sparse', 0.4706)]","[-7.80796185e-02 -3.83932739e-02 -2.59154588e-02  8.53528362e-03
  2.76532266e-02  1.01607665e-02 -3.41356248e-02  2.40911283e-02
 -9.92982164e-02 -7.82004893e-02 -1.11251436e-02 -1.09381159e-03
 -1.25817895e-01  5.01683243e-02  9.00447462e-03  5.18352874e-02
  1.82639901e-02  5.54157123e-02 -2.10937746e-02 -1.05802417e-01
 -6.18311353e-02 -1.06780022e-01  2.56579891e-02  7.15189986e-03
  1.25794057e-02 -1.17081152e-04  3.91387977e-02 -5.55290170e-02
  5.60030341e-02 -5.35039743e-03 -1.32659096e-02  2.20343620e-02
  3.02276574e-02  2.38650236e-02  2.45719459e-02 -4.68862802e-02
  2.40801740e-03 -2.44449731e-02 -4.95682545e-02  2.62598377e-02
 -2.35636864e-04 -2.86940811e-03 -4.97977212e-02  5.77853024e-02
  9.13673267e-03 -4.91097830e-02  8.18197876e-02  1.45880301e-02
  6.02966128e-03 -1.34483084e-01 -4.55107633e-03 -4.53714579e-02
 -8.33197162e-02  1.43290404e-02 -7.71038560e-03  8.77679791e-03
 -5.10639837e-03 -5.78860268e-02  2.31002383e-02 -7.00801685e-02
  5.00613786e-02 -5.44610508e-02 -3.55535150e-02  6.70525655e-02
  1.55189764e-02  6.97431788e-02  4.65257019e-02 -1.59882568e-02
  1.05365992e-01 -7.33227357e-02 -1.01808803e-02 -8.71488731e-03
 -6.49700165e-02  8.01893026e-02 -1.82506610e-02 -2.39713639e-02
  6.80382028e-02  1.05557060e-02  2.38479860e-02 -5.42363636e-02
 -1.92563999e-02  1.18734157e-02 -2.20104610e-03 -7.56277656e-03
  6.73678964e-02 -1.58613175e-02 -3.65267992e-02  7.33419061e-02
  2.29538040e-04 -4.85042147e-02  7.41439909e-02 -7.57988989e-02
 -3.35795581e-02  7.95059428e-02 -5.58282211e-02 -1.08619023e-03
  9.66139212e-02 -6.44082129e-02 -8.78851414e-02  4.74882089e-02
 -3.10094003e-02 -1.15657218e-01  9.65099968e-03  9.79059637e-02
 -6.22752011e-02  4.33353856e-02  2.53895042e-03  9.35257077e-02
  2.82432586e-02  5.89143969e-02  3.84937599e-02  3.70379500e-02
 -6.85210852e-03 -6.27575740e-02  9.50543862e-03 -2.35432331e-02
 -5.79796098e-02  5.29181287e-02  8.81807506e-02  7.38077983e-02
 -6.04130290e-02  6.62778318e-03 -2.83745956e-02  6.52197283e-03
 -3.14931758e-02  4.38654609e-02 -6.22955747e-02  1.26476653e-32
  3.15882266e-02  8.32259953e-02 -2.32454371e-02 -4.68544513e-02
  1.27918441e-02 -2.91606113e-02  6.77378029e-02  2.00231336e-02
  3.42561565e-02 -1.60624497e-02 -4.01996821e-02  2.99022533e-02
 -1.27323894e-02  7.65623152e-02 -3.52591425e-02 -5.50691374e-02
  9.83303133e-03  7.34158978e-02  2.73429155e-02  8.76321085e-03
  4.17096950e-02  1.39279410e-01 -3.89467254e-02  8.39760602e-02
 -5.51587604e-02  9.54484940e-03 -2.41139140e-02 -7.90308714e-02
 -2.05097301e-03  2.75456831e-02 -7.42513388e-02 -1.89904552e-02
  6.00739755e-02 -2.34375219e-03  1.69307124e-02  2.14999504e-02
 -4.28483337e-02 -6.99685514e-02  9.70328785e-03 -9.44498777e-02
 -6.50948659e-02  5.78131825e-02 -2.03221962e-02 -1.12848490e-01
 -2.15171427e-02  2.66036149e-02 -1.79869344e-03  6.21839464e-02
 -6.18538773e-03 -4.55989987e-02 -1.03369385e-01  6.80325627e-02
 -5.58731370e-02  1.16327725e-01  4.33383472e-02 -6.90641105e-02
  3.30277300e-03 -4.73392494e-02  1.30070865e-01 -9.56061587e-04
  4.75546019e-03  4.91517819e-02  2.20166538e-02 -1.57743897e-02
 -4.10764590e-02 -4.74754199e-02 -3.70678715e-02 -3.45517695e-03
 -3.94187905e-02  6.16994761e-02 -1.11371584e-01  7.05510452e-02
 -4.48998176e-02 -3.09092682e-02  7.42900074e-02 -2.11202707e-02
  5.67740090e-02 -5.27048707e-02 -4.50583212e-02 -1.18743824e-02
 -1.33306131e-01  2.16112640e-02  4.26081941e-02 -1.71965007e-02
 -9.87943336e-02  6.13199286e-02  1.11573853e-03  2.09465344e-02
  7.71571463e-03 -2.73480825e-02 -1.10150702e-01  5.02255885e-03
  8.45498592e-02 -9.27188620e-03  1.46545898e-02 -9.97404753e-33
  1.68226715e-02 -2.45121885e-02  1.74466856e-02  3.57871987e-02
  2.93395147e-02  4.86156940e-02 -5.41041829e-02 -8.16714317e-02
  4.73878831e-02 -2.65726205e-02  3.88243166e-03  3.37908566e-02
  2.90133022e-02 -3.74764726e-02  7.38807907e-03  3.39647941e-02
 -3.62971164e-02  2.29165014e-02 -5.48968418e-03  2.03293301e-02
 -5.48682958e-02  7.08009303e-02 -9.58266109e-02 -5.81430644e-02
 -1.57364123e-02 -1.46158207e-02  4.57395660e-03 -8.85059778e-03
  9.51859402e-04  1.77304167e-02  4.49876003e-02  2.04198584e-02
 -4.65669036e-02  2.39837989e-02 -6.98713288e-02  5.97635135e-02
  6.73007593e-02  1.88273024e-02 -7.45919570e-02  6.20352663e-02
  1.22831389e-01  6.80668429e-02 -3.05880196e-02  1.20358188e-02
 -3.92185152e-02  1.83715597e-02 -1.01173304e-01 -2.45694667e-02
 -3.47357653e-02  5.90715632e-02  2.50729825e-02 -1.44832479e-02
 -3.40563394e-02  6.28738431e-04 -2.06241719e-02  6.17880281e-03
  8.63494202e-02  1.30283302e-02  7.24454690e-03  2.11609956e-02
 -4.47742939e-02 -5.65574467e-02 -1.66224912e-02 -3.67612764e-02
  1.19272824e-02 -3.04716099e-02 -2.30417307e-02  1.24170601e-01
 -1.30442274e-03 -2.44857986e-02 -4.71750014e-02  7.39068985e-02
  5.23379333e-02  6.87237754e-02 -1.12587912e-02  8.12971070e-02
  3.63914110e-02  3.89165580e-02  3.75934392e-02  1.28020123e-02
  4.70044911e-02 -3.47997360e-02  3.65667269e-02  4.25565019e-02
  2.23231222e-02  1.23505041e-01  5.53030223e-02 -3.42985019e-02
  4.90912348e-02 -7.55631132e-03 -3.31048481e-02 -2.27395748e-03
  1.03701003e-01  6.95949793e-02  5.62924258e-02 -3.88196248e-08
 -3.53322960e-02  2.16453802e-02 -1.01978807e-02  2.42574862e-03
  4.37669978e-02 -1.95360295e-02  2.50535626e-02  1.13186568e-01
 -2.72648353e-02 -9.10577364e-03  4.98821437e-02 -5.94912469e-02
 -4.04629521e-02  1.15390532e-02  2.77873408e-02  8.08401555e-02
 -5.17597701e-03 -2.78555858e-03  6.93312613e-03 -3.42085883e-02
  1.33206602e-02  1.45744942e-02  3.71655519e-03  1.85481478e-02
 -8.46172497e-03 -3.48440930e-02  6.41361549e-02  2.93249283e-02
  3.74153219e-02 -4.93523804e-03 -8.12872201e-02 -3.53189334e-02
  8.58314708e-02 -7.41327256e-02  1.23449758e-01  8.45104679e-02
  2.18131170e-02 -3.31037864e-02 -5.39524369e-02  2.08666306e-02
 -6.12845868e-02 -4.87821773e-02  2.87441798e-02 -4.65674512e-02
  2.71882787e-02 -3.99168842e-02 -3.04630809e-02 -8.19202065e-02
 -4.68608774e-02 -6.53168699e-03 -2.12413128e-02  5.22513725e-02
 -2.85675451e-02  7.06746280e-02  3.57314982e-02  6.26387000e-02
 -1.67770702e-02 -3.62345763e-02 -3.82912681e-02 -7.81926066e-02
  5.89753538e-02  1.84906889e-02 -3.75392996e-02 -1.68862101e-02]",2,2
torch_geometric,1,documentation paper colab notebook and video tutorial external resource ogb examplespyg pytorch geometric is a library built upon pytorch to easily write and train graph neural network gnns for a wide range of application related to structured data it consists of various method for deep learning on graph and other irregular structure also known a geometric deep learning from a variety of published paper in addition it consists of easy to use mini batch loader for operating on many small and single giant graph multi gpu support datapipe support distributed graph learning via quiver a large number of common benchmark datasets based on simple interface to create your own the graphgym experiment manager and helpful transforms both for learning on arbitrary graph a well a on d mesh or point cloud click here to join our slack community whether you are a machine learning researcher or first time user of machine learning toolkits here are some reason to try out pyg for machine learning on graph structured data in this quick tour we highlight the ease of creating and training a gnn model with only a few line of code in the first glimpse of pyg we implement the training of a gnn for classifying paper in a citation graph for this we load the cora dataset and create a simple layer gcn model using the pre defined gcnconv more information about evaluating final model performance can be found in the corresponding example in addition to the easy application of existing gnns pyg make it simple to implement custom graph neural network see here for the accompanying tutorial for example this is all it take to implement the edge convolutional layer from wang et al x i prime max j in mathcal n i textrm mlp theta left x i x j x i right graphgym allows you to manage and launch gnn experiment using a highly modularized pipeline see here for the accompanying tutorial user are highly encouraged to check out the documentation which contains additional tutorial on the essential functionality of pyg including data handling creation of datasets and a full list of implemented method transforms and datasets for a quick start check out our example in example pyg provides a multi layer framework that enables user to build graph neural network solution on both low and high level it comprises of the following component we list currently supported pyg model layer and operator according to category gnn layer all graph neural network layer are implemented via the nn messagepassing interface a gnn layer specifies how to perform message passing i e by designing different message aggregation and update function a defined here these gnn layer can be stacked together to create graph neural network model pooling layer graph pooling layer combine the vectorial representation of a set of node in a graph or a subgraph into a single vector representation that summarizes it property of node it is commonly applied to graph level task which require combining node feature into a single graph representation gnn model our supported gnn model incorporate multiple message passing layer and user can directly use these pre defined model to make prediction on graph unlike simple stacking of gnn layer these model could involve pre processing additional learnable parameter skip connection graph coarsening etc gnn operator and utility pyg come with a rich set of neural network operator that are commonly used in many gnn model they follow an extensible design it is easy to apply these operator and graph utility to existing gnn layer and model to further enhance model performance scalable gnns pyg support the implementation of graph neural network that can scale to large scale graph such application is challenging since the entire graph it associated feature and the gnn parameter cannot fit into gpu memory many state of the art scalability approach tackle this challenge by sampling neighborhood for mini batch training graph clustering and partitioning or by using simplified gnn model these approach have been implemented in pyg and can benefit from the above gnn layer operator and model pyg is available for python to python update you can now install pyg via anaconda for all major o pytorch cuda combination given that you have pytorch installed simply runnote conda package are not published for pytorch yet we alternatively provide pip wheel for all major o pytorch cuda combination see here to install the binary for pytorch simply runwhere cuda should be replaced by either cpu cu cu or cu depending on your pytorch installation for additional but optional functionality runto install the binary for pytorch simply runwhere cuda should be replaced by either cpu cu cu or cu depending on your pytorch installation torch version cuda for additional but optional functionality runnote binary of older version are also provided for pytorch pytorch pytorch pytorch pytorch pytorch and pytorch following the same procedure for older version you might need to explicitly specify the latest supported version number in order to prevent a manual installation from source you can look up the latest supported version number here in case you want to experiment with the latest pyg feature which are not fully released yet ensure that torch scatter and torch sparse are installed by following the step mentioned above and install either the nightly version of pyg viaor install pyg from master viaplease cite our paper and the respective paper of the method used if you use this code in your own work feel free to email u if you wish your work to be listed in the external resource if you notice anything unexpected please open an issue and let u know if you have any question or are missing a specific feature feel free to discus them with u we are motivated to constantly make pyg even better,"[('train graph neural network gnns', 0.6163), ('single graph representation gnn model', 0.601), ('tutorial external resource ogb examplespyg pytorch', 0.5958), ('custom graph neural network', 0.5479), ('gnn model', 0.5404), ('geometric deep learning', 0.5347), ('graphgym experiment manager', 0.5284), ('many gnn model', 0.5277), ('graph neural network model', 0.5244), ('graph neural network layer', 0.4995)]","[-1.26604766e-01 -4.70866039e-02  3.90014574e-02 -3.92921327e-04
  2.71205699e-05 -3.11806016e-02 -5.19718230e-02  4.45435755e-02
 -1.09546088e-01 -2.16815509e-02  9.14476160e-03 -3.76754478e-02
  1.44257965e-02  1.04885790e-02 -2.09322833e-02 -4.56225760e-02
  9.06291306e-02  5.16009964e-02 -3.71705107e-02 -1.02417223e-01
 -1.29796546e-02 -3.07763787e-03  2.58555654e-02 -7.96227343e-03
  2.13779379e-02 -4.80144285e-02  3.26110376e-03 -3.90827190e-03
  8.56911242e-02 -3.22966576e-02  2.88871471e-02 -2.63709389e-02
  1.91432536e-02  5.21198697e-02 -5.39952293e-02 -1.61870793e-02
 -3.89384478e-02 -3.78085226e-02  1.17138866e-02 -2.22393852e-02
 -3.42139453e-02 -6.85902359e-03 -3.21210325e-02  1.75304618e-02
  9.35228094e-02  1.62906814e-02 -1.77912861e-02 -3.03506367e-02
 -1.86688770e-02 -8.45861621e-03 -3.66957821e-02 -1.30996287e-01
 -6.32687807e-02  3.85018997e-03  4.67561185e-02 -7.82262087e-02
 -6.58858288e-03  5.15831169e-03  4.72238800e-03 -1.44728748e-02
  1.80765465e-02 -2.48407722e-02 -6.32897019e-02 -2.01801099e-02
 -2.12693233e-02  3.78859453e-02  1.40832523e-02  9.21586230e-02
  5.39191552e-02 -5.75036034e-02  2.53017563e-02 -2.35772654e-02
 -7.41740391e-02  4.98480303e-03 -3.43937539e-02  2.96023227e-02
 -3.80086363e-03  4.88689207e-02  4.70761806e-02 -7.98506886e-02
 -6.61524385e-03 -3.84089723e-02  6.13651946e-02  9.02544055e-03
 -3.47982012e-02 -1.21673793e-02 -8.85682404e-02  4.45331670e-02
 -2.18263157e-02 -2.64418535e-02  3.62804420e-02 -3.52954888e-03
 -4.80327904e-02  1.21009937e-02  1.53513392e-02  2.42665764e-02
  2.81278929e-03 -1.69576332e-03 -3.73333655e-02  2.26556025e-02
 -2.59910375e-02 -2.23647207e-02  8.09929594e-02  1.02657890e-02
 -3.69020216e-02  8.19183215e-02  3.54306027e-02  7.05674812e-02
  6.35258481e-02 -1.81566142e-02 -4.79972810e-02  7.11227879e-02
 -9.06790271e-02 -8.02196041e-02  3.28186862e-02 -6.84387842e-03
 -6.65990561e-02 -8.08707718e-03  1.89165827e-02  8.03962275e-02
 -1.31439120e-01  7.06465691e-02  2.13386165e-03 -2.02069920e-03
 -7.50981122e-02 -5.04317868e-04 -9.97146666e-02  6.28969117e-33
  6.39572293e-02  4.79780883e-03 -2.14313548e-02 -8.80353600e-02
  1.11568347e-01 -1.88774932e-02 -4.50518448e-04  3.92986573e-02
  3.37609425e-02 -2.79816873e-02 -6.55945912e-02  4.77556325e-02
 -2.36712173e-02  1.06668517e-01 -2.40393784e-02  1.18425600e-02
 -1.84928086e-02 -3.01109552e-02  6.26533404e-02 -1.06019685e-02
  7.00850040e-02 -1.39996866e-02  3.65717188e-02  1.01730853e-01
  5.42340986e-02  9.31815431e-02 -6.60362374e-03 -1.19587118e-02
  1.16900569e-02  9.60354600e-03 -8.34455527e-03  5.67778610e-02
 -1.42643582e-02 -1.43180881e-03 -3.32360938e-02 -5.70317172e-03
 -2.01145392e-02 -5.32015450e-02  3.84786278e-02 -7.53748044e-02
 -2.33043246e-02  1.62063166e-02 -1.46597214e-02 -4.91811857e-02
 -2.56676599e-02 -4.04731818e-02  1.04797082e-02  2.16886890e-03
  7.10911974e-02 -2.19152514e-02 -2.17113458e-02 -6.98506832e-03
 -5.86010218e-02  4.24137451e-02  6.68534487e-02  2.07944755e-02
  9.03384089e-02  6.40809238e-02 -1.77111896e-03  4.57925647e-02
  3.11673824e-02  8.65261257e-02  5.94939329e-02 -2.34327577e-02
 -1.45712839e-02 -3.74445952e-02 -6.73098564e-02 -2.25967495e-03
  8.41420330e-03 -2.61080824e-02 -4.19447720e-02  1.67682357e-02
  2.24929256e-03 -3.84011194e-02 -2.13743020e-02 -3.14125270e-02
 -2.23122574e-02 -1.07814386e-01 -9.98064876e-02  7.29449168e-02
 -6.51889071e-02 -5.19816689e-02  1.56801734e-02 -6.68452773e-03
 -4.35386039e-02 -1.53521569e-02  2.89274361e-02 -1.88671257e-02
  8.48061293e-02  1.78953763e-02  9.49309580e-03 -1.98116787e-02
  6.07961789e-03  4.42341678e-02 -3.45701240e-02 -5.13767402e-33
 -8.84543732e-03  2.01177374e-01 -3.51315439e-02  6.36016056e-02
  1.17977239e-01  2.85082357e-03  8.67324229e-03 -5.95614351e-02
 -3.44559997e-02  7.90199414e-02 -1.25295483e-02 -4.09794673e-02
  5.96134178e-02  6.04191050e-02 -1.81683432e-02 -2.64778566e-02
 -9.11859795e-02 -5.07372878e-02 -2.99061183e-02  8.10777536e-04
 -5.77429086e-02  7.63762891e-02 -1.18189193e-01  1.96512882e-03
  7.40822256e-02 -2.96519976e-02 -5.98376393e-02 -1.95449069e-02
  5.85487187e-02  6.05222583e-02 -2.30058115e-02  8.80522840e-03
  3.19294888e-03  3.55486554e-04  8.47316310e-02  3.71054746e-02
  9.73996222e-02 -5.21877743e-02  2.85696741e-02 -6.86247721e-02
  6.99097514e-02 -9.20652132e-03  3.28821577e-02 -1.10921171e-02
 -2.23945733e-02  3.72867882e-02 -1.02153473e-01  4.77487519e-02
 -1.24419160e-01 -3.48741598e-02 -5.91623830e-03  2.27647237e-02
  2.18379833e-02 -3.36292796e-02 -2.34669279e-02 -4.25668694e-02
  1.10565186e-01  4.65484969e-02 -8.22960213e-03 -3.27236690e-02
 -3.72015014e-02 -1.03430435e-01 -6.94123749e-03  2.18816753e-02
 -5.49337380e-02 -1.79635771e-02 -2.77329683e-02  8.34034290e-03
  1.23068579e-02  3.44679994e-03 -6.53143181e-03  6.55082464e-02
  1.09520974e-03  5.02508283e-02 -1.14276046e-02  5.13309687e-02
 -3.91721688e-02  3.52719501e-02 -2.36849561e-02 -4.05728444e-02
  7.00003728e-02  4.24260534e-02 -4.27213497e-03  8.26210380e-02
  6.46088123e-02  8.14716518e-02  5.83836390e-03  8.33923742e-02
 -8.17716180e-04  1.55959046e-02 -6.27649352e-02  3.14671360e-02
 -2.78677419e-02  1.68121278e-01  9.90810245e-03 -2.79304793e-08
 -1.03623591e-01  1.87370349e-02  8.83182138e-02  4.61338600e-03
  6.09034896e-02  2.84024922e-04  9.32705849e-02  5.68993799e-02
 -2.77881287e-02  1.12349868e-01 -4.95452657e-02  2.28530671e-02
 -4.26356532e-02 -1.11737428e-02  2.97519602e-02 -1.12537527e-03
 -4.46766242e-03 -3.04292534e-02  8.28981847e-02 -2.44509839e-02
  4.08452190e-02  1.51605103e-02 -4.08557132e-02  6.12759404e-02
  3.86759192e-02 -6.68868646e-02 -1.09114479e-02  7.41380826e-02
 -6.79942816e-02  2.55630314e-02  2.43850388e-02 -1.34420488e-03
  1.40209153e-01 -9.60883424e-02  4.39769030e-02  8.11166391e-02
  1.34515455e-02  1.62677828e-03  1.17301662e-02  4.08790968e-02
 -4.88145351e-02  5.46572320e-02  1.00973239e-02 -3.82262357e-02
 -4.33830619e-02  7.06194947e-03  1.67876277e-02 -7.63133913e-02
 -7.14874547e-03  3.10621597e-02 -6.44288957e-02  2.79994812e-02
 -9.60699320e-02 -1.83793297e-03  2.97579188e-02  7.25633353e-02
 -2.74866838e-02 -5.69678359e-02  9.20536369e-03  3.26085202e-02
 -7.32787997e-02  3.47090028e-02 -7.65703246e-02 -9.51861404e-03]",2,2
torch-topological,1,pytorch topological or torch topological is a topological machine learning framework for pytorch it aim to collect loss term and neural network layer in order to simplify building the next generation of topology based machine learning tool topological machine learning refers to a new class of machine learning algorithm that are able to make use of topological feature in data set in contrast to method based on a purely geometrical point of view topological feature are capable of focusing on connectivity aspect of a data set this provides an interesting fresh perspective that can be used to create powerful hybrid algorithm capable of yielding more insight into data this is an emerging research field firmly rooted in computational topology and topological data analysis if you want to learn more about how topology and geometry can work in tandem here are a few resource to get you started am zquita et al the shape of thing to come topological data analysis and biology from molecule to organism developmental dynamic volume issue pp hensel et al a survey of topological machine learning method frontier in artificial intelligence torch topological requires python more recent version might work but necessitate building some dependency by yourself python currently offer the smoothest experience it is recommended to use the excellent poetry framework to install torch topological alternatively use pip to install the package torch topological is still a work in progress you can browse the documentation or if code reading is more your thing dive directly into some example code here is a list of other project that are using torch topological this list is incomplete you can help expanding it by using torch topological in your own project innocent check out the contribution guideline or the road map of the project our software and research doe not exist in a vacuum pytorch topological is standing on the shoulder of proverbial giant in particular we want to thank the following project for constituting the technical backbone of the project furthermore pytorch topological draw inspiration from several project that provide a glimpse into the wonderful world of topological machine learning difftda by mathieu carri reripser by ulrich bauerteaspoon by elizabeth munch and her teamtopologylayer by rickard br el gabrielssontopological autoencoders by michael moor max horn and bastian riecktorchph by christoph hofer and roland kwittfinally pytorch topological make heavy use of pot the python optimal transport library we are indebted to the many contributor of all these project,"[('topological machine learning framework', 0.7211), ('topological machine learning', 0.6792), ('topological machine learning method frontier', 0.6582), ('topological machine learning difftda', 0.6173), ('topological data analysis', 0.5956), ('topological feature', 0.5672), ('computational topology', 0.5664), ('topological draw inspiration', 0.5306), ('topology', 0.4694), ('artificial intelligence torch', 0.4082)]","[ 5.97121892e-03 -1.12803049e-01  5.20385010e-03 -1.00977737e-02
 -8.08027014e-03 -3.11155003e-02 -2.08846014e-02 -9.09692422e-03
 -6.76931739e-02  5.66225266e-03 -6.62527829e-02 -2.05552094e-02
 -1.47000588e-02 -3.80895310e-03  1.60495695e-02  2.68984046e-02
 -2.82883588e-02 -9.62276943e-03  6.82998896e-02 -5.39286584e-02
 -7.99108204e-03  3.13507095e-02 -1.35928462e-03  7.62778753e-03
 -3.42067480e-02  8.85433517e-03  1.90647356e-02  3.79417948e-02
  6.42496794e-02 -1.53795136e-02  3.04977503e-02  6.16997369e-02
 -5.74163869e-02 -7.68759251e-02  2.41244249e-02 -4.74940985e-03
  1.24628367e-02  4.41706516e-02 -1.54890334e-02 -1.83596276e-03
  8.57116282e-03  9.88085382e-03  5.57239279e-02  2.51454655e-02
  6.43277764e-02  9.15505365e-02 -5.12540974e-02 -1.20987497e-01
 -1.77068380e-03 -1.01003177e-01 -5.39464541e-02 -3.91558520e-02
 -9.18469578e-02 -1.39610022e-02 -4.84792935e-03  3.37668583e-02
  7.78669715e-02  9.55549162e-03  4.41268273e-02 -9.06965286e-02
  9.38757733e-02 -2.59568058e-02 -4.01326567e-02  2.93096751e-02
  3.29270251e-02  2.49637887e-02 -4.68905568e-02  1.46938071e-01
  1.06929347e-01 -8.80402178e-02  9.41602420e-03  2.00227257e-02
 -4.25464101e-02  2.73785498e-02  4.79740612e-02  1.47113688e-02
 -1.25144562e-02 -7.36543536e-03  3.65232676e-02 -6.49176538e-02
 -7.87296891e-02  5.93943056e-03  7.98169151e-02  6.75009191e-02
 -1.28891105e-02 -3.36066261e-02 -6.78817481e-02 -3.34737077e-02
 -1.09951414e-01 -4.04103398e-02  7.88842365e-02  1.36629427e-02
 -2.68827807e-02 -6.33006841e-02 -4.42957133e-02  1.62484939e-03
 -7.99246356e-02 -4.13837023e-02  1.02728046e-02  3.96786891e-02
 -8.67200866e-02 -4.25183587e-02  2.36467067e-02 -1.13644777e-02
  2.80740671e-03  5.27756400e-02  6.73077404e-02  3.53637412e-02
  5.14230430e-02 -4.09538262e-02 -1.53949931e-02 -1.06520643e-02
 -4.47481610e-02 -8.70584697e-02 -3.32759917e-02  1.46906022e-02
 -1.91252027e-02 -2.27224175e-02  7.68140182e-02  1.24066792e-01
 -7.43532702e-02  1.84784662e-02  3.98632081e-04 -7.28260726e-02
 -5.48645779e-02 -3.83059308e-02 -5.09888045e-02  4.47896702e-33
  3.50439325e-02 -2.01380569e-02  2.16199481e-03 -6.38234466e-02
  1.01262093e-01 -5.74420244e-02  6.40186574e-03 -5.61928265e-02
  8.09115469e-02  4.73654307e-02 -2.69420221e-02  1.08989872e-01
 -4.81430627e-02  6.72159865e-02  1.93594303e-02 -9.26265493e-03
  8.60014036e-02 -6.69463947e-02 -4.35140841e-02 -1.23083247e-02
  7.29766414e-02  2.29995847e-02  8.70769098e-03  3.31932865e-02
 -3.02647869e-03  4.41350825e-02 -7.26122111e-02 -1.24022691e-02
 -4.02554423e-02  3.76320854e-02 -3.85515317e-02  9.10393968e-02
 -7.77027830e-02  8.18008035e-02  2.34900191e-02  2.00391207e-02
  4.90261391e-02 -5.14976643e-02 -3.92859280e-02 -1.51313292e-02
 -3.29889469e-02  4.22456451e-02  1.82717536e-02  1.58004891e-02
  3.13384878e-03 -1.58912109e-04  7.81250000e-02 -8.04221630e-03
  7.49863163e-02  1.04648098e-02  1.20194154e-02 -4.54381183e-02
 -8.07000175e-02 -2.62933355e-02 -2.27177478e-02  3.89147666e-03
  1.50930248e-02  4.74485685e-04  1.13543302e-01  1.11521147e-01
  3.62507924e-02  1.10217719e-03  3.38817425e-02 -5.13553992e-02
 -5.61923757e-02 -7.53650367e-02  2.55930107e-02 -8.97500515e-02
  5.13650961e-02 -7.36758998e-03 -4.30465080e-02  4.34465967e-02
  2.98881275e-03 -3.59258987e-02  3.97935919e-02 -4.56340685e-02
 -2.71173380e-02 -2.40180362e-02 -5.33896647e-02  2.62478087e-02
 -2.72085164e-02 -1.35523723e-02 -3.58313210e-02 -4.06646095e-02
  3.00485324e-02  1.21885082e-02  5.77277653e-02 -9.76675097e-03
 -6.15429580e-02 -4.14440595e-02 -1.20915681e-01  9.98765754e-05
  1.29376084e-01  2.25153770e-02 -6.99603185e-02 -3.77092527e-33
 -5.53340651e-02  1.98515803e-02 -9.64273512e-03  5.38681187e-02
  6.02699220e-02  5.77642545e-02 -1.67643651e-02 -6.03129454e-02
  4.71064495e-03  3.75292562e-02  4.46851691e-03  1.45464316e-02
  7.67281130e-02  1.31129203e-02 -9.15458426e-03 -1.74509790e-02
 -3.84655558e-02 -1.00042142e-01 -4.60421704e-02  2.98686116e-03
 -4.48644720e-02  2.68364279e-03 -7.21180290e-02 -4.43273336e-02
 -3.13464995e-03  6.14193566e-02 -8.08689091e-03 -7.56160468e-02
  1.04722396e-01  1.59405679e-01 -5.54803684e-02  2.97482982e-02
 -1.97300836e-02  5.81900813e-02 -2.79962625e-02 -1.79117583e-02
  5.76937571e-02 -3.18572037e-02 -8.22441280e-02 -3.74651998e-02
  1.97658595e-02 -1.09780370e-03 -2.92291064e-02 -1.86573341e-02
  6.67817250e-04 -4.30546254e-02 -1.29286975e-01  1.85594782e-02
 -6.52239621e-02 -1.59685537e-02  5.81690110e-02  2.33120043e-02
 -1.05704144e-02 -6.56689778e-02 -4.54318570e-03  5.15276045e-02
  5.14141731e-02  2.27506608e-02  3.58696766e-02  8.44360366e-02
  3.13617699e-02 -9.64149162e-02  5.78572340e-02  1.14631861e-01
 -3.38759646e-02 -1.36844479e-02 -2.00073160e-02  3.30316015e-02
 -1.03227735e-01 -1.98440510e-03 -9.57932323e-02  5.49119711e-02
  9.35654680e-04 -6.99003413e-03 -4.39985506e-02  1.87655780e-02
  4.54627089e-02  3.02378647e-02  6.27481267e-02  2.50107702e-02
  3.44594009e-02 -2.10717693e-02 -7.01121427e-03  6.54364154e-02
  1.04135878e-01  4.08298485e-02  5.50484620e-02  2.66435705e-02
  2.17733793e-02 -7.70883709e-02  8.20959080e-03 -5.84715232e-02
 -3.33772264e-02  9.36019346e-02 -4.83684391e-02 -2.49769432e-08
 -7.19064549e-02 -1.72576625e-02  9.96969491e-02 -2.07414851e-02
 -4.94830729e-03  4.79816757e-02  3.76448669e-02  5.23278490e-02
 -2.81269047e-02  2.86715217e-02 -2.12550303e-03  3.75001319e-02
 -8.97709057e-02  1.21457949e-02  2.49286667e-02  8.66503045e-02
  2.91309990e-02 -1.97970811e-02  2.73236856e-02 -7.28671346e-03
  2.13025715e-02  5.96387684e-02  1.50425481e-02 -3.84610146e-02
  3.06404550e-02 -4.70848568e-02 -1.36604141e-02  1.32975802e-02
 -1.22890668e-02  3.88438217e-02 -4.45396826e-02 -1.92014768e-03
  3.68227996e-02 -2.52280496e-02  2.01612078e-02  9.31283385e-02
  2.76609100e-02 -1.99775901e-02 -1.02476083e-01  2.72146426e-03
 -2.16871891e-02 -2.40762476e-02 -2.05798466e-02 -1.79932490e-02
  1.65408347e-02  4.56699505e-02  1.54184163e-01  4.34803329e-02
  2.41955388e-02  6.03570901e-02 -7.31269196e-02  6.37951195e-02
 -3.70176882e-02 -2.64303386e-02  2.23236363e-02  4.71456125e-02
  1.08560277e-02 -8.26579705e-02 -3.20995376e-02 -2.27480009e-02
 -1.88124292e-02 -2.52351407e-02  2.34967303e-02  5.06106727e-02]",2,0
clip-anytorch,1,blog paper model card colab clip contrastive language image pre training is a neural network trained on a variety of image text pair it can be instructed in natural language to predict the most relevant text snippet given an image without directly optimizing for the task similarly to the zero shot capability of gpt and we found clip match the performance of the original resnet on imagenet zero shot without using any of the original m labeled example overcoming several major challenge in computer vision this repo is a fork maintaining a pypi package for clip change from the main repo you will need to disable jit by doing model preprocess clip load vit b device device jit false if not using torch run pip install clip anytorch to install this package one benefit of not depending on an old torch version is installing clip on colab is super fast try this colab to see it for yourself pip install clip anytorch yes that s it first install pytorch and torchvision a well a small additional dependency and then install this repo a a python package on a cuda gpu machine the following will do the trick replace cudatoolkit above with the appropriate cuda version on your machine or cpuonly when installing on a machine without a gpu the clip module clip provides the following method return the name of the available clip model return the model and the torchvision transform needed by the model specified by the model name returned by clip available model it will download the model a necessary the name argument can also be a path to a local checkpoint the device to run the model can be optionally specified and the default is to use the first cuda device if there is any otherwise the cpu when jit is false a non jit version of the model will be loaded return a longtensor containing tokenized sequence of given text input s this can be used a the input to the modelthe model returned by clip load support the following method given a batch of image return the image feature encoded by the vision portion of the clip model given a batch of text token return the text feature encoded by the language portion of the clip model given a batch of image and a batch of text token return two tensor containing the logit score corresponding to each image and text input the value are cosine similarity between the corresponding image and text feature time the code below performs zero shot prediction using clip a shown in appendix b in the paper this example take an image from the cifar dataset and predicts the most likely label among the textual label from the dataset the output will look like the following the exact number may be slightly different depending on the compute device note that this example us the encode image and encode text method that return the encoded feature of given input the example below us scikit learn to perform logistic regression on image feature note that the c value should be determined via a hyperparameter sweep using a validation split,"[('torch run pip install clip anytorch', 0.5413), ('pip install clip anytorch', 0.4855), ('pytorch', 0.4478), ('clip module clip', 0.4472), ('blog paper model card colab clip', 0.4435), ('imagenet', 0.427), ('torchvision', 0.4143), ('available clip model', 0.4137), ('cuda gpu machine', 0.4014), ('clip model', 0.3949)]","[-3.04786637e-02 -7.11037591e-02 -3.74491848e-02 -3.44474576e-02
  4.44551893e-02  1.42279565e-02 -3.03100292e-02  1.95417907e-02
 -3.10254730e-02 -6.21853881e-02  7.65663907e-02 -1.13746396e-03
 -1.45507529e-01  3.22109349e-02  2.45742332e-02  8.53208359e-04
 -1.73417386e-02  4.11411747e-02 -3.97820212e-03 -6.00799993e-02
  8.11860040e-02 -8.42897296e-02  4.27176692e-02  1.31343771e-02
 -4.21552323e-02  2.67254072e-03  6.28099032e-03 -6.97408617e-02
  3.57330889e-02 -1.86850242e-02  2.51831058e-02 -4.84350324e-02
  1.12422153e-01  4.22313437e-02  7.13493749e-02 -3.65774147e-02
  1.90351605e-02  5.14956238e-03 -2.09261328e-02 -3.73679884e-02
  3.47862095e-02 -2.94807125e-02 -4.06099185e-02 -3.14348824e-02
 -3.42980139e-02 -4.49528284e-02  4.07068282e-02 -2.84261885e-04
  1.95841957e-02 -5.10400832e-02 -4.20354009e-02 -4.51776087e-02
 -4.62787710e-02 -7.53276423e-03  2.06147283e-02 -2.59203762e-02
  2.15422940e-02 -3.87208909e-02  4.31862511e-02 -7.52847046e-02
  3.84925827e-02  2.24260353e-02 -7.87826777e-02  3.97238918e-02
 -3.54919694e-02  5.72824664e-02  1.33439479e-03 -1.19371843e-02
  6.27016500e-02 -8.22696462e-02 -5.62052280e-02 -2.28609908e-02
 -7.37426430e-02  3.95767726e-02 -9.31091681e-02 -5.25804646e-02
  1.05345160e-01  1.29204374e-02 -5.44203445e-02 -3.69708724e-02
 -2.59469617e-02 -3.93598676e-02  1.41187638e-01  9.46266279e-02
 -3.67045123e-03  6.56734481e-02  7.14292889e-03  5.04643433e-02
  3.83085683e-02 -8.13912451e-02 -2.19883844e-02 -5.24664782e-02
  1.70895306e-03  4.45604175e-02  1.74490809e-02 -4.31406088e-02
  5.17939776e-03 -2.61147246e-02 -2.79925577e-02  2.79540978e-02
 -2.78206114e-02 -1.66181680e-02  5.55030182e-02  2.27811858e-02
 -4.20761816e-02  7.17118457e-02 -5.24587743e-02  3.41518619e-03
 -1.78302024e-02  3.74830398e-03  4.32785228e-02  2.91774254e-02
 -9.59210377e-03 -7.70951286e-02  1.17413230e-01 -8.81775543e-02
 -6.70527592e-02  6.81067444e-03 -4.71153669e-02  1.00154459e-01
 -5.22874482e-02  6.98423982e-02 -3.68201360e-02  8.03414509e-02
 -2.68230271e-02  6.23856783e-02  2.67457380e-03  6.15471540e-33
  4.49300632e-02  4.91283424e-02 -7.52211958e-02 -2.22834162e-02
  1.56433675e-02 -8.19942355e-03  1.14468843e-01 -4.35645804e-02
 -6.67198747e-02 -2.09086332e-02  7.17823803e-02 -9.37991310e-03
 -2.34886911e-02  8.15211311e-02 -3.25920954e-02 -1.06267715e-02
  3.52359787e-02  4.31549884e-02  3.01755257e-02  1.45891234e-02
  6.46924041e-03  3.44155617e-02 -6.87320065e-03  8.92108157e-02
  6.41496330e-02  1.42355822e-02 -4.02907953e-02 -1.26581416e-02
  4.69555445e-02  2.10066903e-02 -3.49109387e-03 -3.45640518e-02
  5.21279015e-02  4.67421561e-02 -5.35494015e-02  2.78510619e-02
 -1.78658236e-02 -2.09575072e-02  2.72448678e-02  2.13504266e-02
 -5.55472868e-03  4.70180549e-02 -2.61646472e-02 -1.07727244e-01
 -2.81667449e-02 -7.10364953e-02 -3.07015497e-02  1.74691066e-01
 -2.83412673e-02  4.49965224e-02  8.74640606e-03  1.37381097e-02
 -3.34786661e-02 -2.46471297e-02 -6.79365173e-02 -8.91158432e-02
  8.98600556e-03  1.57091522e-03  1.19808301e-01 -1.38045903e-02
  1.55291408e-02  1.14922874e-01 -1.52527923e-02  2.08301116e-02
  2.64430903e-02 -4.33525220e-02 -2.18773466e-02  3.48149315e-02
 -4.14173827e-02  2.87546106e-02 -1.63245037e-01  1.68129988e-02
 -7.27290511e-02 -4.68269875e-03 -8.50124285e-03 -4.51446287e-02
 -6.96228594e-02 -7.84295052e-02 -8.86088759e-02  5.97685315e-02
 -1.03784956e-01 -8.67126230e-03  3.93715315e-02 -1.10393852e-01
 -3.37025374e-02 -4.46826145e-02  5.12914211e-02  1.90317351e-02
  4.71714325e-02 -8.94212201e-02 -5.92579059e-02 -4.83939983e-02
  9.65316687e-03  1.96023453e-02  4.08454984e-02 -5.13520254e-33
  5.43983318e-02 -3.14499112e-03 -2.74082110e-03  7.08537996e-02
  1.77056286e-02  2.05678102e-02  7.32197706e-03 -7.11791068e-02
 -3.41541804e-02 -5.76214269e-02 -2.63704429e-03  2.78657451e-02
  2.05641557e-02 -2.46204883e-02  2.28930656e-02  2.26110648e-02
  1.34846000e-02  2.13371459e-02 -6.78552827e-03 -2.65263114e-02
 -5.25305979e-02 -5.86167350e-03  9.08443518e-03  5.35034984e-02
 -6.67265058e-02 -1.36360992e-02  5.85801601e-02 -4.02038358e-02
  7.83790648e-02  6.24920533e-04  5.33758067e-02  1.95278637e-02
 -2.11314373e-02  3.63905393e-02 -6.81797788e-02  8.21282938e-02
  6.63294196e-02  6.83914945e-02  4.34920117e-02 -5.71278483e-02
  5.63334711e-02  4.47554663e-02 -2.45324932e-02  3.23942043e-02
 -1.61069945e-01 -2.20891964e-02  6.04346301e-03  2.92315837e-02
 -3.71440947e-02  1.96539015e-02 -2.87415870e-02 -9.11036283e-02
 -1.57916136e-02  4.08562012e-02 -3.28592919e-02  3.66502702e-02
  7.37420842e-02  1.41933495e-02 -2.28804890e-02 -2.25904081e-02
  5.64759318e-03 -1.87242795e-02 -5.42381182e-02 -5.13641834e-02
  8.44440516e-03 -3.25598195e-02 -7.10884035e-02  9.71393511e-02
 -7.19114244e-02 -5.21464497e-02 -2.38306001e-02  1.15478389e-01
  1.25561476e-01  9.36447270e-03  3.93117890e-02  5.00014536e-02
 -4.41581309e-02  5.50337136e-02  4.02507260e-02  2.05158256e-02
  7.09230406e-03 -1.43377725e-02  5.80729656e-02 -1.33929923e-02
  4.35548909e-02  3.31574604e-02  8.28960270e-04  5.51667549e-02
 -7.50151882e-03 -9.02018845e-02 -3.71402353e-02  1.98818576e-02
  1.38725787e-01  8.91787186e-02  7.87352771e-02 -2.64340230e-08
 -7.68454745e-03  1.67836379e-02 -5.18636871e-03  6.04970381e-02
 -2.23163869e-02  3.59300300e-02  7.64210597e-02  1.27505651e-02
  7.21365097e-04 -4.43005115e-02  7.12075531e-02 -3.89210358e-02
 -6.76780241e-03 -4.49633086e-03  3.70650105e-02  5.11268713e-02
  1.48802632e-02  4.01198789e-02  4.29318771e-02 -2.67013628e-02
 -8.93770996e-03 -8.96713808e-02  7.96787292e-02 -3.00041493e-02
 -3.68050598e-02 -2.13421769e-02 -1.24094728e-02  9.48568154e-03
 -1.44186309e-02  2.52128411e-02 -1.39045091e-02 -3.22249457e-02
  2.73687225e-02 -5.96624129e-02  1.78771839e-01  1.95831396e-02
 -1.50651895e-02 -1.66990131e-03  4.20021489e-02  2.07255036e-02
  5.93203939e-02 -4.44020294e-02  1.06908884e-02 -4.41240110e-02
  1.64235141e-02 -4.76058722e-02  1.23375235e-02 -6.82008043e-02
 -6.10637106e-02  3.49029377e-02  1.14897583e-02 -2.15881094e-02
 -2.65034358e-03  8.58071744e-02  3.49481101e-03  7.35474080e-02
  2.08742451e-02 -3.02160643e-02  1.40729360e-02 -6.61147060e-03
 -2.64144130e-03  2.15793885e-02 -1.87623631e-02 -5.45984283e-02]",2,2
open_clip_torch,1,paper colab welcome to an open source implementation of openai s clip contrastive language image pre training the goal of this repository is to enable training model with contrastive image text supervision and to investigate their property such a robustness to distribution shift our starting point is an implementation of clip that match the accuracy of the original clip model when trained on the same dataset specifically a resnet model trained with our codebase on openai s million image subset of yfcc achieves top accuracy on imagenet openai s clip model reach when trained on the same subset of yfcc for ease of experimentation we also provide code for training on the million image in the conceptual caption dataset where a resnet x trained with our codebase reach top imagenet accuracy we further this with a replication study on a dataset of comparable size to openai s laion m and with the larger laion b superset we have trained a we describe in more detail below clip model in a medium accuracy regime already allow u to draw conclusion about the robustness of larger clip model since the model follow reliable scaling law this codebase is work in progress and we invite all to contribute in making it more acessible and useful in the future we plan to add support for tpu training and release larger model we hope this codebase facilitates and promotes further research in contrastive image text learning please submit an issue or send an email if you have any other request or suggestion note that portion of src open clip modelling and tokenizer code are adaptation of openai s official repository to compute billion of embeddings efficiently you can use clip retrieval which ha openclip support this repository is focused on training clip model to fine tune a trained zero shot model on a downstream classification task such a imagenet please see our other repository wise ft the wise ft repository contains code for our paper on robust fine tuning of zero shot model in which we introduce a technique for fine tuning zero shot model while preserving robustness under distribution shift openclip read a csv file with two column a path to an image and a text caption the name of the column are passed a an argument to main py the script src data gather cc py will collect the conceptual caption image first download the conceptual caption url and then run the script from our repository our training set contains m image and our validation set contains k image in addition to specifying the training data via csv file a mentioned above our codebase also support webdataset which is recommended for larger scale datasets the expected format is a series of tar file each of these tar file should contain two file for each training example one for the image and one for the corresponding text both file should have the same name but different extension for instance shard tar could contain file such a abc jpg and abc txt you can learn more about webdataset at http github com webdataset webdataset we use tar file with data point each which we create using tarp you can download the yfcc dataset from multimedia common similar to openai we used a subset of yfcc to reach the aforementioned accuracy number the index of image in this subset are in openai s clip repository install conda pytorch a per http pytorch org get started locally openclip also can be used with virtualenv with these line install pip pytorch a per http pytorch org get started locally test can be run with make install dev then make testinstall open clip pacakge and remaining dependency if you want to train model you will also need to install the package from requirement training txt note imagenet val is the path to the validation set of imagenet for zero shot evaluation not the training set you can remove this argument if you do not want to perform zero shot evaluation on imagenet throughout training note that the val folder should contain subfolders if it doest not please use this script this code ha been battle tested up to a s and offer a variety of solution for distributed training we include native support for slurm cluster a the number of device used to train increase so doe the space complexity of the the logit matrix using a na ve all gather scheme space complexity will be o n instead complexity may become effectively linear if the flag gather with grad and local loss are used this alteration result in one to one numerical result a the na ve method we make use of torchrun to launch distributed job the following launch a a job on a node of gpus the same script above work so long a user include information about the number of node and host node this is likely the easiest solution to utilize the following script wa used to train our largest model when run on a machine with gpus the command should produce the following training curve for conceptual caption more detailed curve for conceptual caption are given at doc clip conceptual caption md when training a rn on yfcc the same hyperparameters a above are used with the exception of lr e and epoch note that to use another model like vit b or rn x or rn x or vit b specify with model rn x we are working on reproducing openai s vit result with the comparably sized and open laion m dataset trained weight may be found in release v the laion m weight have been trained on the juwels supercomputer see acknowledgement section below we replicate openai s result on vit b reaching a top imagenet k zero shot accuracy of zero shot comparison courtesy of andreas f rst vit b wa trained with a gb gpus for hour gpu hour the per gpu batch size wa for a global batch size of is much lower than it could have been due to being sized initially before moving to local contrastive loss the b laion m training reached a top imagenet k zero shot validation score of this wa the first major train session using the updated webdataset x code a bug wa found that prevented shard from being shuffled properly between node worker each epoch this wa fixed part way through training epoch but likely had an impact vit b wa trained with a gb gpus for hour gpu hour batch size per gpu wa for a global batch size of the b x laion m training reached a top imagenet k zero shot validation score of this model is the same depth a the b but increase theunlike the b run above this model wa a clean run with no dataset shuffling issue vit b wa trained with a gb gpus for hour gpu hour batch size per gpu wa for a global batch size of the l laion m training reached a top imagenet k zero shot validation score of vit l wa trained with a gb gpus for hour gpu hour batch size per gpu wa for a global batch size of grad checkpointing wa enabled a b sample subset of laion b with english caption http huggingface co datasets laion laion b en a vit b trained on laion b reaching a top imagenet k zero shot accuracy of vit b wa trained with a gb gpus the per gpu batch size wa for a global batch size of compute generously provided by stability ai a second iteration of b wa trained on stability ai cluster with a larger global batch size and learning rate hitting top see http huggingface co laion clip vit b laion b s b b ka vit l with a top imagenet k zero shot wa trained on juwels booster see model detail here http huggingface co laion clip vit l laion b s b b kthese weight use a different dataset mean and std than others instead of using the openai mean std inception style normalization is used via a mean and std of this is handled automatically if using open clip create model and transforms from pretrained weight a vit h with a top imagenet k zero shot wa trained on juwels booster see model detail here http huggingface co laion clip vit h laion b s b b ka vit h with a top imagenet k zero shot wa trained on juwels booster see model detail here http huggingface co laion clip vit g laion b s b b kthis model wa trained with a shorted schedule than other laion b model with b sample seen instead of b it match laion m training in sample seen many zero shot result are lower a a result but despite this it performs very well in some ood zero shot and retrieval task below are checkpoint of model trained on yfcc m along with their zero shot top accuracy on imagenet and imagenetv these model were trained using gpus and the same hyperparameters described in the sample running code section with the exception of lr e and epoch we offer a simple model interface to instantiate both pre trained and untrained model note many existing checkpoint use the quickgelu activation from the original openai model this activation is actually le efficient that native torch nn gelu in recent version of pytorch the model default are now nn gelu so one should use model definition with quickgelu postfix for the openclip pretrained weight all openai pretrained weight will always default to quickgelu one can also use the non quickgelu model definition with pretrained weight using quickgelu but there will be an accuracy drop for fine tune that will likely vanish for longer run future trained model will use nn gelu the plot below show how zero shot performance of clip model varies a we scale the number of sample used for training zero shot performance increase steadily for both imagenet and imagenetv and is far from saturated at m sample tl dr clip model have high effective robustness even at small scale clip model are particularly intriguing because they are more robust to natural distribution shift see section in the clip paper this phenomenon is illustrated by the figure below with imagenet accuracy on the x axis and imagenetv a reproduction of the imagenet validation set with distribution shift accuracy on the y axis standard training denotes training on the imagenet train set and the clip zero shot model are shown a star a observed by taori et al and miller et al the in distribution and out of distribution accuracy of model trained on imagenet follow a predictable linear trend the red line in the above plot effective robustness quantifies robustness a accuracy beyond this baseline i e how far a model lie above the red line ideally a model would not suffer from distribution shift and fall on the y x line trained human labelers are within a percentage point of the y x line even though the clip model trained with this codebase achieve much lower accuracy than those trained by openai our model still lie on the same trend of improved effective robustness the purple line therefore we can study what make clip robust without requiring industrial scale compute for more information on effective robustness please see to know more about the factor that contribute to clip s robustness refer to fang et al we gratefully acknowledge the gauss centre for supercomputing e v www gauss centre eu for funding this part of work by providing computing time through the john von neumann institute for computing nic on the gc supercomputer juwels booster at j lich supercomputing centre jsc current development of this repository is led by ross wightman cade gordon and vaishaal shankar the original version of this repository is from a group of researcher at uw google stanford amazon columbia and berkeley gabriel ilharco mitchell wortsman nicholas carlini rohan taori achal dave vaishaal shankar john miller hongseok namkoong hannaneh hajishirzi ali farhadi ludwig schmidtspecial thanks to jong wook kim and alec radford for help with reproducing clip if you found this repository useful please consider citing,"[('imagenet openai', 0.5486), ('imagenet train', 0.5476), ('imagenet', 0.5362), ('contrastive language image pre', 0.5081), ('top imagenet accuracy', 0.5044), ('top imagenet k', 0.5027), ('training clip model', 0.4991), ('imagenet accuracy', 0.4871), ('contrastive image text supervision', 0.4845), ('requirement training txt note imagenet val', 0.4762)]","[ 2.21027546e-02 -8.99441689e-02  2.76967026e-02 -8.87034182e-03
  7.35778362e-02  7.61607662e-03  2.21969672e-02  2.68411567e-03
 -1.72478221e-02 -6.18482381e-02  4.38841432e-02  4.45123995e-03
  4.08151187e-02  7.25012422e-02  1.35858981e-02  5.77194523e-03
  6.17299154e-02  3.69633473e-02 -5.08061759e-02 -6.71144947e-02
  3.44738662e-02  5.37043735e-02  1.00814752e-01 -6.29072636e-02
  2.47617275e-03  1.22070285e-02 -2.48365831e-02 -1.86989084e-02
  3.84312570e-02 -4.54303287e-02 -2.45538149e-02 -2.64854450e-03
  1.11655399e-01  5.71250282e-02 -2.47358028e-02  4.11473438e-02
 -4.48164530e-02 -3.18407640e-02 -2.59669349e-02  2.37406697e-02
 -4.04718816e-02 -4.04799171e-02 -2.15148851e-02 -3.82570662e-02
  1.34372965e-01  2.05038209e-03 -1.29158562e-02 -6.95917308e-02
 -4.83839400e-03 -2.23282371e-02 -9.95942727e-02 -7.29628652e-02
 -4.42976430e-02  7.89523050e-02  2.51479913e-03 -9.95290373e-03
 -6.52107522e-02  1.45302983e-02  1.77648831e-02 -1.13324784e-02
  3.87844746e-03 -1.49723208e-02 -1.13237940e-01  1.60267260e-02
  2.24052314e-02  5.18837497e-02  1.57370549e-02 -1.70976040e-03
  5.07922396e-02 -1.35591000e-01  4.06322740e-02  4.71057147e-02
 -2.34326441e-02  3.65369767e-02 -1.41708404e-02  2.59884112e-02
  9.18199196e-02  8.24928377e-03 -1.04300007e-02 -1.28155768e-01
  4.36979048e-02 -2.00564284e-02  7.24289045e-02 -3.26757096e-02
  2.93543916e-02  2.90002935e-02 -5.12577966e-02  5.24467900e-02
 -4.30563539e-02  7.79026246e-04 -3.35317850e-02 -3.45211886e-02
  1.24420393e-02 -1.43556772e-02  7.46359229e-02  1.04665533e-02
 -6.57503381e-02 -7.04207420e-02 -5.18667214e-02  7.00064823e-02
 -2.50056796e-02 -5.46858534e-02  7.31066975e-04  5.44816293e-02
  7.17140688e-03  1.15778553e-03  9.23633128e-02 -3.09187267e-02
  7.65873566e-02 -1.89304072e-03  5.58921620e-02 -2.81403083e-02
 -5.33203520e-02 -1.33305654e-01  7.23809227e-02 -2.19893288e-02
 -1.04400255e-01  2.48350166e-02  8.38955268e-02  7.10059702e-02
 -8.81323144e-02  2.20306162e-02  9.80964978e-04 -3.06140408e-02
  1.53147699e-02 -1.38259968e-02 -8.04790109e-02  7.11646851e-33
  6.43733889e-02  2.62451004e-02  4.85050790e-02 -2.38550063e-02
  4.26570922e-02 -5.04812151e-02 -3.01115308e-02 -1.25178031e-03
 -1.37166707e-02 -1.17571205e-01 -2.75928807e-02  5.50480895e-02
 -5.56563586e-02  6.83541894e-02  9.05102566e-02 -3.24901678e-02
  1.01580173e-02  6.64329305e-02  7.83882365e-02  5.56119345e-02
 -1.13221323e-02 -5.25011634e-03  2.77496092e-02  3.33691128e-02
 -2.30559558e-02 -2.54927184e-02  1.93940382e-02 -3.06279790e-02
  3.53909731e-02  5.70274610e-03 -5.61800264e-02  4.79674432e-03
  7.27978498e-02  2.21108422e-02 -4.90377145e-03 -2.60870513e-02
 -6.46450818e-02 -3.34029496e-02  5.26575260e-02 -4.41347361e-02
 -7.60866627e-02  8.58805403e-02 -1.98533684e-02  4.06899769e-03
 -3.06586083e-02  8.25057272e-03 -2.62450054e-02  3.38209867e-02
 -2.23424397e-02  9.08024311e-02 -9.51521378e-03 -3.63465361e-02
 -1.12914713e-02 -3.76078151e-02 -1.20886564e-02 -1.74452122e-02
  3.69383991e-02  4.03877236e-02  1.99295450e-02  3.00467387e-02
 -2.01101806e-02 -2.91562546e-02  6.31780596e-03  1.34514319e-02
  2.77424138e-02  9.23549291e-03 -1.13592972e-03 -7.09242076e-02
  4.26976606e-02 -5.41510694e-02 -8.50285962e-02  3.63419913e-02
 -2.15276610e-02 -5.02039911e-03  9.98551957e-03  2.58850530e-02
 -1.47284372e-02 -6.98881671e-02 -3.55020761e-02  1.00742996e-01
 -9.09654200e-02  2.93677952e-02 -1.23575013e-02 -1.50957226e-03
 -1.00703269e-01  5.22344410e-02  4.94863801e-02 -9.01367441e-02
  1.14200518e-01  7.07271099e-02 -4.72088829e-02 -6.92043034e-03
 -2.94335186e-02  6.14700504e-02  8.06069653e-03 -2.77312470e-33
 -9.53255221e-03  1.01224259e-01 -9.15590897e-02  1.00334354e-01
  1.52292959e-02  1.49461199e-02  9.56319422e-02  5.75334802e-02
  1.50568504e-02  8.37395620e-03  1.00792825e-01 -5.38285896e-02
 -1.58406328e-02 -7.53589720e-02 -7.51748607e-02 -9.73795801e-02
  5.89572126e-04  6.38153218e-03  7.79261161e-03  7.78440684e-02
 -7.73128960e-03  5.26235998e-02 -6.45049810e-02  4.04478945e-02
  2.05513183e-02  3.62950116e-02 -1.90004818e-02  2.57515144e-02
 -2.44956538e-02 -3.22372094e-02  3.17293666e-02 -6.22175895e-02
  2.62369271e-02  4.16467451e-02  1.58821139e-02  5.38899824e-02
  9.70219746e-02 -1.78219564e-02 -1.56896170e-02  8.43442082e-02
  1.18671723e-01  2.36682664e-03 -8.73176847e-03  2.46497486e-02
 -1.04648940e-01 -3.90404612e-02 -7.13407025e-02  2.81467065e-02
 -6.52953610e-03 -4.70143072e-02 -1.30129382e-02 -3.08608040e-02
 -7.12749735e-02 -2.71509942e-02 -5.30408602e-03 -2.45343987e-02
  5.88403121e-02  4.93053496e-02  1.77805573e-02 -9.74687841e-03
  2.65595084e-03 -6.55659139e-02 -1.38038509e-02  2.86996961e-02
  1.21409837e-02 -1.91510692e-02 -5.96309453e-02  4.84349653e-02
 -4.34429124e-02 -4.96262172e-03 -3.54842544e-02 -5.33205718e-02
  7.62961656e-02  4.28327434e-02 -3.86793762e-02 -3.11467946e-02
  2.97180843e-02  5.21961600e-02  2.49815825e-02 -4.14048880e-02
  4.38474044e-02 -2.62852432e-03 -8.51308480e-02  7.74823576e-02
  1.11284479e-01  1.19695432e-01  7.01176673e-02 -7.49389678e-02
  3.83510739e-02  1.91356260e-02 -2.63197608e-02  5.64629212e-02
  4.00899090e-02  6.00442253e-02 -3.79555598e-02 -2.67916285e-08
 -7.27891475e-02 -9.05925855e-02  1.78067107e-02  4.79779392e-02
  1.26449876e-02 -3.03506460e-02  6.04537921e-03  4.30866554e-02
  2.03954615e-02  7.24236341e-03 -3.13221128e-03  4.18656506e-02
 -1.08951464e-01 -1.31025791e-01 -2.76627932e-02  4.66874242e-02
  3.13781276e-02  5.17827012e-02  5.74968643e-02 -5.32788187e-02
  3.93371657e-02  2.37268079e-02  3.42977257e-03 -1.23915058e-02
 -1.99749600e-02 -3.37561369e-02 -1.94227081e-02  7.66349807e-02
  2.87276879e-02 -8.52685049e-03  6.35481700e-02 -2.21234486e-02
  6.84507713e-02 -1.25647575e-01  4.99604866e-02  7.68873021e-02
 -3.70135270e-02 -2.72644255e-02 -5.85471746e-03  3.61873247e-02
 -7.23556010e-03  2.13567223e-02 -5.83810359e-03 -5.09720296e-02
  4.05154601e-02 -1.85126215e-02 -2.26572412e-03 -6.93743080e-02
 -4.18487154e-02 -8.21506530e-02 -3.48857865e-02  8.38519540e-03
 -1.32851640e-03  4.51133475e-02  4.53396715e-05 -2.59249378e-02
  8.52618814e-02 -4.49526235e-02 -2.51429807e-02  1.32943735e-01
  1.12981815e-02  6.08430617e-02  1.63697619e-02  5.11559658e-03]",2,2
tensorflow-cpu,1,tensorflow is an open source software library for high performance numerical computation it flexible architecture allows easy deployment of computation across a variety of platform cpu gpus tpus and from desktop to cluster of server to mobile and edge device originally developed by researcher and engineer from the google brain team within google s ai organization it come with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domain tensorflow is licensed under apache,"[('tensorflow', 0.7401), ('many other scientific domain tensorflow', 0.6584), ('flexible numerical computation core', 0.4332), ('deep learning', 0.3829), ('google s ai organization', 0.3821), ('flexible architecture', 0.3764), ('google brain team', 0.3684), ('platform cpu gpus tpus', 0.3613), ('open source software library', 0.3378), ('edge device', 0.2987)]","[-4.41984236e-02 -1.09338418e-01  3.75523120e-02 -6.00057282e-02
  8.24890584e-02 -4.75711152e-02 -5.30565754e-02  2.51197536e-02
 -1.34402225e-02 -1.77784972e-02 -8.60479921e-02 -3.01086437e-02
 -6.02132641e-02  1.79548992e-03  1.93502568e-03 -4.11127694e-02
 -2.45012846e-02  3.20869386e-02 -2.12851036e-02 -1.30356878e-01
 -4.25707698e-02  3.98718528e-02 -1.30772097e-02  1.58899475e-03
 -9.78470664e-04  7.42263645e-02 -1.08599998e-02 -1.42022565e-01
  3.27437744e-02 -6.55014766e-03 -1.02071287e-02  9.64850187e-03
  4.71507711e-03  3.81811298e-02 -6.36580959e-02  3.31682228e-02
 -4.90938015e-02 -4.53656018e-02 -1.57287996e-02 -4.45531756e-02
 -3.31613086e-02 -6.49364218e-02  1.22887539e-02 -4.47792821e-02
  8.86408389e-02  1.80261284e-02  2.25173086e-02 -1.15306959e-01
 -2.31295135e-02  1.99361108e-02 -3.49466614e-02 -8.51285681e-02
 -5.48832156e-02  8.48228261e-02 -7.17121512e-02 -6.73733884e-03
  4.10806164e-02 -6.88320026e-03  1.62451004e-03 -8.22282210e-03
 -1.00759100e-02 -8.78835991e-02  4.05119406e-03  1.54486764e-02
 -3.63357663e-02  5.28295226e-02 -9.17309057e-03  1.02966493e-02
  6.14090227e-02 -1.00714304e-01  8.13344568e-02  3.17448117e-02
 -3.28305252e-02 -7.23914290e-03 -3.69609729e-03 -1.43599079e-03
  5.77681474e-02 -2.73799971e-02  8.20818990e-02 -6.62635267e-02
  2.97083259e-02 -1.71580762e-02 -3.10782306e-02  1.28447125e-02
  3.34661715e-02 -2.07291394e-02 -7.15484247e-02  8.80392119e-02
 -1.64926033e-02 -1.48727791e-03  4.25984226e-02 -3.23301293e-02
  2.54538245e-02 -3.26760523e-02  4.63619828e-02  8.25122185e-03
 -7.75806680e-02 -8.91252458e-02 -4.99539562e-02  3.41689922e-02
 -1.18737161e-01  1.49925379e-03  4.47053388e-02  4.81082126e-02
 -2.87359152e-02  8.46514031e-02  4.03921865e-02 -3.37678823e-03
  9.50822160e-02 -2.90156733e-02 -2.99208332e-02  6.71075732e-02
  4.06970084e-02 -7.54432976e-02  1.83600057e-02 -7.02635124e-02
 -3.98105867e-02  7.65025169e-02  3.50047797e-02  1.09249830e-01
 -1.12119086e-01  3.65649313e-02 -3.26654613e-02 -8.72930512e-03
 -2.04302394e-03  1.50718652e-02 -8.76195654e-02  4.60425342e-33
  1.02570942e-02  4.28824779e-03  2.68881787e-02 -6.87931105e-02
  6.11587837e-02 -6.28679469e-02  2.82790624e-02  1.04069605e-03
  1.19376676e-02 -2.09535728e-03 -6.10620193e-02  1.45414501e-01
 -2.77088694e-02  1.31631240e-01  5.33098839e-02 -4.76143509e-02
  1.41801238e-02  5.01978882e-02  7.20799938e-02 -1.55935355e-03
  2.45469920e-02  6.20573387e-02  4.07773815e-02  9.34710652e-02
  2.34671924e-02  6.27924548e-03 -4.57534008e-02 -4.82183322e-02
  6.20497251e-03  1.55118303e-02 -5.82717657e-02  2.86109298e-02
 -4.72726822e-02  8.19104631e-03  1.48596782e-02  1.40242409e-02
 -7.13818287e-03 -6.28968030e-02  7.68921385e-03  1.67898014e-02
 -2.04171371e-02  4.88140881e-02  2.21376605e-02 -4.13218699e-02
 -1.19940946e-02 -2.14574859e-02  2.20675208e-02  2.55821627e-02
  4.94300574e-02 -5.72271608e-02 -2.83469800e-02  2.06819158e-02
 -7.33109657e-03 -7.42880777e-02  4.45740707e-02 -1.86625645e-02
  1.66003243e-03  5.78348897e-02  4.94083539e-02  8.17290470e-02
 -1.52327828e-02  2.14482732e-02 -2.74007898e-02 -1.56885870e-02
  3.61333638e-02 -1.15680881e-02  2.34961193e-02  4.28035334e-02
  9.24402382e-03  2.92042568e-02 -8.23360160e-02  7.03407750e-02
  3.02753523e-02  7.97685049e-03 -4.55205850e-02 -1.75612886e-02
  3.93302739e-02 -1.34543687e-01 -4.39116172e-02 -5.35312574e-03
 -1.35161489e-01  1.69179011e-02  7.37877490e-05  1.96509007e-02
  2.84204120e-03  1.24324122e-02 -2.80277040e-02  1.61576960e-02
 -5.60108647e-02 -5.83440997e-02 -9.79904234e-02 -1.62681602e-02
  8.40131938e-02  6.26077726e-02 -9.34203938e-02 -4.11873715e-33
 -1.21871002e-01 -2.00116094e-02 -6.95795864e-02  9.35161188e-02
  1.06673934e-01  1.64793674e-02  5.29420041e-02 -4.99502346e-02
  7.84921634e-04  5.40840551e-02  3.24851312e-02 -1.49185723e-02
  7.49930143e-02 -3.41328681e-02  1.59471463e-02 -6.72609732e-02
 -9.23045427e-02 -7.28558823e-02 -1.41289271e-02 -1.31858299e-02
 -3.60742360e-02  5.88296913e-02 -1.11268647e-02 -1.11294286e-02
  4.44210209e-02  2.42709126e-02 -9.14493203e-02 -6.56116903e-02
  6.20851517e-02  7.87141398e-02  8.85193329e-03 -2.49410868e-02
  1.07943751e-02  5.60709536e-02  1.08070284e-01  2.54249573e-02
  4.70024757e-02 -3.27337794e-02  4.17715870e-02 -3.21090035e-02
  6.68326393e-02  2.09123977e-02  8.31178799e-02  5.75441495e-03
 -1.57177057e-02  5.13668582e-02 -1.00466989e-01  5.68194799e-02
 -7.79418200e-02 -7.36578256e-02  2.29164143e-03  1.09611116e-02
 -3.92328426e-02 -1.02612153e-01  2.16487814e-02  2.21487638e-02
  4.73303422e-02 -1.24629773e-02  1.68757718e-02 -1.58568248e-02
 -1.49172219e-03 -5.22242896e-02  2.37852894e-02  2.95598339e-02
 -4.57500108e-02  7.54220188e-02 -8.37374944e-03  2.92717386e-02
 -7.08715320e-02 -5.05744107e-02  3.37662622e-02  9.25058499e-03
  6.04491401e-03  5.77810109e-02 -5.38401343e-02  1.88186746e-02
  2.49538794e-02 -2.30869707e-02  4.80298363e-02 -1.32949762e-02
  6.71137720e-02  1.58789922e-02  3.83781157e-02  5.01072630e-02
  5.06595746e-02  7.17647970e-02  4.81168069e-02 -3.42952199e-02
  2.15225033e-02 -2.26974841e-02 -4.45528142e-02  1.36768846e-02
  3.95529121e-02  9.66346115e-02 -7.90077373e-02 -2.76889054e-08
  4.37600647e-05  1.28331250e-02  8.19380730e-02 -1.11790355e-02
  5.15342504e-02 -8.69946275e-03  3.87721956e-02  4.68982942e-02
 -2.15304736e-02  1.09144384e-02 -1.34722795e-02 -5.39821126e-02
 -5.36090359e-02  1.06498577e-04  9.06490088e-02  5.04027084e-02
 -4.07694206e-02 -2.80015147e-03  3.82598266e-02 -8.40364844e-02
  3.06539945e-02  7.10854530e-02 -5.35856653e-03 -4.30149212e-02
 -3.41456267e-03 -7.31946230e-02  4.09532413e-02 -5.65154618e-03
  9.22913849e-03  2.75571328e-02 -5.08137271e-02 -5.10621220e-02
  5.90167232e-02 -3.64067666e-02  9.62798372e-02  4.31413203e-03
  5.34966327e-02 -2.88944710e-02  2.93812994e-03  6.43181428e-02
 -2.65936963e-02  8.72200802e-02  4.80443090e-02 -4.97647300e-02
  7.63691915e-03 -4.89581898e-02  1.61224399e-02 -4.40009758e-02
 -1.88040975e-02  6.26380444e-02 -7.08308667e-02  7.13098049e-02
 -4.59356047e-02  8.05362687e-02  4.94703427e-02  2.27457359e-02
 -2.76032165e-02 -1.40729666e-01 -2.81152502e-02  2.60950923e-02
 -1.65610705e-02  1.23978546e-02  5.14509305e-02  7.77910324e-03]",2,2
